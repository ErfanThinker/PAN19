{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Reading general data of the problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading general data of the problems, done!\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import division\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from MyUtils import clean_folder, read_files\n",
    "from Word2Dim import Word2Dim\n",
    "\n",
    "dataset_path = '.' + os.sep + 'pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23'\n",
    "outpath = '.' + os.sep + 'dev_out'\n",
    "\n",
    "clean_folder(outpath)\n",
    "\n",
    "infocollection = dataset_path + os.sep + 'collection-info.json'\n",
    "problems = []\n",
    "language = []\n",
    "with open(infocollection, 'r') as f:\n",
    "    for attrib in json.load(f):\n",
    "        problems.append(attrib['problem-name'])\n",
    "        language.append(attrib['language'])\n",
    "print('Reading general data of the problems, done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reading problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc count to process:  63\n",
      "Processing doc # 7\n",
      "Processing doc # 1\n",
      "Processing doc # 13\n",
      "Processing doc # 14\n",
      "Processing doc # 2\n",
      "Processing doc # 8\n",
      "Processing doc # 15\n",
      "Processing doc # 9\n",
      "Processing doc # 3\n",
      "Processing doc # 10\n",
      "Processing doc # 4\n",
      "Processing doc # 11\n",
      "Processing doc # 16\n",
      "Processing doc # 12\n",
      "Processing doc # 5\n",
      "Processing doc # 19\n",
      "Processing doc # 6\n",
      "Processing doc # 17\n",
      "Processing doc # 20\n",
      "Processing doc # 25\n",
      "Processing doc # 18\n",
      "Processing doc # 26\n",
      "Processing doc # 31\n",
      "Processing doc # 21\n",
      "Processing doc # 27\n",
      "Processing doc # 32\n",
      "Processing doc # 22\n",
      "Processing doc # 28\n",
      "Processing doc # 23\n",
      "Processing doc # 33\n",
      "Processing doc # 29\n",
      "Processing doc # 24\n",
      "Processing doc # 30\n",
      "Processing doc # 34\n",
      "Processing doc # 37\n",
      "Processing doc # 35\n",
      "Processing doc # 43\n",
      "Processing doc # 38\n",
      "Processing doc # 36\n",
      "Processing doc # 39\n",
      "Processing doc # 44\n",
      "Processing doc # 49\n",
      "Processing doc # 40\n",
      "Processing doc # 45\n",
      "Processing doc # 50\n",
      "Processing doc # 41\n",
      "Processing doc # 46\n",
      "Processing doc # 51\n",
      "Processing doc # 47\n",
      "Processing doc # 42\n",
      "Processing doc # 52\n",
      "Processing doc # 53\n",
      "Processing doc # 55\n",
      "Processing doc # 48\n",
      "Processing doc # 54\n",
      "Processing doc # 56\n",
      "Processing doc # 61\n",
      "Processing doc # 57\n",
      "Processing doc # 58\n",
      "Processing doc # 62\n",
      "Processing doc # 59\n",
      "Processing doc # 63\n",
      "Processing doc # 60\n",
      "process_doc, done!\n",
      "doc count to process:  468\n",
      "Processing doc # 1\n",
      "Processing doc # 40\n",
      "Processing doc # 79\n",
      "Processing doc # 2\n",
      "Processing doc # 80\n",
      "Processing doc # 41\n",
      "Processing doc # 3\n",
      "Processing doc # 81\n",
      "Processing doc # 4\n",
      "Processing doc # 42\n",
      "Processing doc # 82\n",
      "Processing doc # 5\n",
      "Processing doc # 43\n",
      "Processing doc # 83\n",
      "Processing doc # 6\n",
      "Processing doc # 44\n",
      "Processing doc # 84\n",
      "Processing doc # 7\n",
      "Processing doc # 45\n",
      "Processing doc # 85\n",
      "Processing doc # 8\n",
      "Processing doc # 46\n",
      "Processing doc # 86\n",
      "Processing doc # 47\n",
      "Processing doc # 9\n",
      "Processing doc # 48\n",
      "Processing doc # 10\n",
      "Processing doc # 87\n",
      "Processing doc # 11\n",
      "Processing doc # 88\n",
      "Processing doc # 49\n",
      "Processing doc # 12\n",
      "Processing doc # 50\n",
      "Processing doc # 89\n",
      "Processing doc # 13\n",
      "Processing doc # 51\n",
      "Processing doc # 90\n",
      "Processing doc # 14\n",
      "Processing doc # 52\n",
      "Processing doc # 53\n",
      "Processing doc # 15\n",
      "Processing doc # 91\n",
      "Processing doc # 16\n",
      "Processing doc # 54\n",
      "Processing doc # 92\n",
      "Processing doc # 17\n",
      "Processing doc # 93\n",
      "Processing doc # 55\n",
      "Processing doc # 18\n",
      "Processing doc # 19\n",
      "Processing doc # 94\n",
      "Processing doc # 56\n",
      "Processing doc # 95\n",
      "Processing doc # 57\n",
      "Processing doc # 20\n",
      "Processing doc # 21\n",
      "Processing doc # 96\n",
      "Processing doc # 58\n",
      "Processing doc # 97\n",
      "Processing doc # 59\n",
      "Processing doc # 22\n",
      "Processing doc # 60\n",
      "Processing doc # 98\n",
      "Processing doc # 61\n",
      "Processing doc # 99\n",
      "Processing doc # 23\n",
      "Processing doc # 62\n",
      "Processing doc # 100\n",
      "Processing doc # 24\n",
      "Processing doc # 63\n",
      "Processing doc # 25\n",
      "Processing doc # 101\n",
      "Processing doc # 64\n",
      "Processing doc # 26\n",
      "Processing doc # 102\n",
      "Processing doc # 65\n",
      "Processing doc # 27\n",
      "Processing doc # 103\n",
      "Processing doc # 66\n",
      "Processing doc # 104\n",
      "Processing doc # 28\n",
      "Processing doc # 67\n",
      "Processing doc # 105\n",
      "Processing doc # 29\n",
      "Processing doc # 68\n",
      "Processing doc # 30\n",
      "Processing doc # 106\n",
      "Processing doc # 69\n",
      "Processing doc # 70\n",
      "Processing doc # 31\n",
      "Processing doc # 32\n",
      "Processing doc # 71\n",
      "Processing doc # 72\n",
      "Processing doc # 33\n",
      "Processing doc # 107\n",
      "Processing doc # 34\n",
      "Processing doc # 73\n",
      "Processing doc # 108\n",
      "Processing doc # 109\n",
      "Processing doc # 74\n",
      "Processing doc # 35\n",
      "Processing doc # 110\n",
      "Processing doc # 36\n",
      "Processing doc # 75\n",
      "Processing doc # 111\n",
      "Processing doc # 76\n",
      "Processing doc # 112\n",
      "Processing doc # 37\n",
      "Processing doc # 77\n",
      "Processing doc # 38\n",
      "Processing doc # 113\n",
      "Processing doc # 78\n",
      "Processing doc # 39\n",
      "Processing doc # 118\n",
      "Processing doc # 114\n",
      "Processing doc # 157\n",
      "Processing doc # 115\n",
      "Processing doc # 119\n",
      "Processing doc # 158\n",
      "Processing doc # 116\n",
      "Processing doc # 159\n",
      "Processing doc # 120\n",
      "Processing doc # 160\n",
      "Processing doc # 117\n",
      "Processing doc # 196\n",
      "Processing doc # 161\n",
      "Processing doc # 121\n",
      "Processing doc # 197\n",
      "Processing doc # 122\n",
      "Processing doc # 198\n",
      "Processing doc # 123\n",
      "Processing doc # 162\n",
      "Processing doc # 124\n",
      "Processing doc # 163\n",
      "Processing doc # 199\n",
      "Processing doc # 125\n",
      "Processing doc # 164\n",
      "Processing doc # 200\n",
      "Processing doc # 165\n",
      "Processing doc # 126\n",
      "Processing doc # 201\n",
      "Processing doc # 127\n",
      "Processing doc # 166\n",
      "Processing doc # 128\n",
      "Processing doc # 202\n",
      "Processing doc # 167\n",
      "Processing doc # 203\n",
      "Processing doc # 129\n",
      "Processing doc # 168\n",
      "Processing doc # 204\n",
      "Processing doc # 205\n",
      "Processing doc # 169\n",
      "Processing doc # 130\n",
      "Processing doc # 170\n",
      "Processing doc # 206\n",
      "Processing doc # 131\n",
      "Processing doc # 171\n",
      "Processing doc # 207\n",
      "Processing doc # 132\n",
      "Processing doc # 172\n",
      "Processing doc # 208\n",
      "Processing doc # 133\n",
      "Processing doc # 173\n",
      "Processing doc # 209\n",
      "Processing doc # 134\n",
      "Processing doc # 174\n",
      "Processing doc # 210\n",
      "Processing doc # 135\n",
      "Processing doc # 211\n",
      "Processing doc # 175\n",
      "Processing doc # 136\n",
      "Processing doc # 176\n",
      "Processing doc # 212\n",
      "Processing doc # 137\n",
      "Processing doc # 138\n",
      "Processing doc # 177\n",
      "Processing doc # 178\n",
      "Processing doc # 213\n",
      "Processing doc # 179\n",
      "Processing doc # 180\n",
      "Processing doc # 139\n",
      "Processing doc # 214\n",
      "Processing doc # 181\n",
      "Processing doc # 140\n",
      "Processing doc # 215\n",
      "Processing doc # 182\n",
      "Processing doc # 141\n",
      "Processing doc # 183\n",
      "Processing doc # 216\n",
      "Processing doc # 184\n",
      "Processing doc # 217\n",
      "Processing doc # 185\n",
      "Processing doc # 142\n",
      "Processing doc # 186\n",
      "Processing doc # 218\n",
      "Processing doc # 143\n",
      "Processing doc # 187\n",
      "Processing doc # 219\n",
      "Processing doc # 144\n",
      "Processing doc # 188\n",
      "Processing doc # 220\n",
      "Processing doc # 189\n",
      "Processing doc # 221\n",
      "Processing doc # 145\n",
      "Processing doc # 190\n",
      "Processing doc # 146\n",
      "Processing doc # 222\n",
      "Processing doc # 191\n",
      "Processing doc # 147\n",
      "Processing doc # 192\n",
      "Processing doc # 223\n",
      "Processing doc # 148\n",
      "Processing doc # 149\n",
      "Processing doc # 193\n",
      "Processing doc # 224\n",
      "Processing doc # 150\n",
      "Processing doc # 194\n",
      "Processing doc # 151\n",
      "Processing doc # 195\n",
      "Processing doc # 152\n",
      "Processing doc # 225\n",
      "Processing doc # 226\n",
      "Processing doc # 153\n",
      "Processing doc # 235\n",
      "Processing doc # 227\n",
      "Processing doc # 236\n",
      "Processing doc # 228\n",
      "Processing doc # 154\n",
      "Processing doc # 237\n",
      "Processing doc # 229\n",
      "Processing doc # 238\n",
      "Processing doc # 155\n",
      "Processing doc # 230\n",
      "Processing doc # 239\n",
      "Processing doc # 156\n",
      "Processing doc # 274\n",
      "Processing doc # 231\n",
      "Processing doc # 240\n",
      "Processing doc # 275\n",
      "Processing doc # 241\n",
      "Processing doc # 232\n",
      "Processing doc # 242\n",
      "Processing doc # 233\n",
      "Processing doc # 276\n",
      "Processing doc # 243\n",
      "Processing doc # 234\n",
      "Processing doc # 277\n",
      "Processing doc # 313\n",
      "Processing doc # 244\n",
      "Processing doc # 278\n",
      "Processing doc # 314\n",
      "Processing doc # 245\n",
      "Processing doc # 315\n",
      "Processing doc # 316\n",
      "Processing doc # 279\n",
      "Processing doc # 246\n",
      "Processing doc # 317\n",
      "Processing doc # 280\n",
      "Processing doc # 247\n",
      "Processing doc # 318\n",
      "Processing doc # 281\n",
      "Processing doc # 248\n",
      "Processing doc # 319\n",
      "Processing doc # 282\n",
      "Processing doc # 249\n",
      "Processing doc # 320\n",
      "Processing doc # 321\n",
      "Processing doc # 283\n",
      "Processing doc # 250\n",
      "Processing doc # 284\n",
      "Processing doc # 322\n",
      "Processing doc # 251\n",
      "Processing doc # 285\n",
      "Processing doc # 323\n",
      "Processing doc # 252\n",
      "Processing doc # 286\n",
      "Processing doc # 324\n",
      "Processing doc # 253\n",
      "Processing doc # 287\n",
      "Processing doc # 254\n",
      "Processing doc # 288\n",
      "Processing doc # 255\n",
      "Processing doc # 325\n",
      "Processing doc # 289\n",
      "Processing doc # 256\n",
      "Processing doc # 326\n",
      "Processing doc # 290\n",
      "Processing doc # 257\n",
      "Processing doc # 327\n",
      "Processing doc # 291\n",
      "Processing doc # 328\n",
      "Processing doc # 292\n",
      "Processing doc # 258\n",
      "Processing doc # 329\n",
      "Processing doc # 293\n",
      "Processing doc # 294\n",
      "Processing doc # 330\n",
      "Processing doc # 259\n",
      "Processing doc # 331\n",
      "Processing doc # 295\n",
      "Processing doc # 260\n",
      "Processing doc # 296\n",
      "Processing doc # 332\n",
      "Processing doc # 297\n",
      "Processing doc # 298\n",
      "Processing doc # 261\n",
      "Processing doc # 333\n",
      "Processing doc # 299\n",
      "Processing doc # 262\n",
      "Processing doc # 334\n",
      "Processing doc # 263\n",
      "Processing doc # 335\n",
      "Processing doc # 300\n",
      "Processing doc # 264\n",
      "Processing doc # 336\n",
      "Processing doc # 265\n",
      "Processing doc # 301\n",
      "Processing doc # 337\n",
      "Processing doc # 266\n",
      "Processing doc # 302\n",
      "Processing doc # 267\n",
      "Processing doc # 303\n",
      "Processing doc # 338\n",
      "Processing doc # 339\n",
      "Processing doc # 268\n",
      "Processing doc # 340\n",
      "Processing doc # 304\n",
      "Processing doc # 269\n",
      "Processing doc # 341\n",
      "Processing doc # 270\n",
      "Processing doc # 342\n",
      "Processing doc # 305\n",
      "Processing doc # 306\n",
      "Processing doc # 271\n",
      "Processing doc # 343\n",
      "Processing doc # 307\n",
      "Processing doc # 344\n",
      "Processing doc # 272\n",
      "Processing doc # 345\n",
      "Processing doc # 308\n",
      "Processing doc # 346\n",
      "Processing doc # 273\n",
      "Processing doc # 309\n",
      "Processing doc # 352\n",
      "Processing doc # 347\n",
      "Processing doc # 310\n",
      "Processing doc # 353\n",
      "Processing doc # 311\n",
      "Processing doc # 348\n",
      "Processing doc # 354\n",
      "Processing doc # 349\n",
      "Processing doc # 312\n",
      "Processing doc # 355\n",
      "Processing doc # 391\n",
      "Processing doc # 350\n",
      "Processing doc # 351\n",
      "Processing doc # 356\n",
      "Processing doc # 392\n",
      "Processing doc # 430\n",
      "Processing doc # 357\n",
      "Processing doc # 431\n",
      "Processing doc # 393\n",
      "Processing doc # 358\n",
      "Processing doc # 432\n",
      "Processing doc # 394\n",
      "Processing doc # 433\n",
      "Processing doc # 359\n",
      "Processing doc # 434\n",
      "Processing doc # 360\n",
      "Processing doc # 395\n",
      "Processing doc # 435\n",
      "Processing doc # 361\n",
      "Processing doc # 396\n",
      "Processing doc # 436\n",
      "Processing doc # 397\n",
      "Processing doc # 362\n",
      "Processing doc # 437\n",
      "Processing doc # 398\n",
      "Processing doc # 363\n",
      "Processing doc # 399\n",
      "Processing doc # 364\n",
      "Processing doc # 400\n",
      "Processing doc # 438\n",
      "Processing doc # 365\n",
      "Processing doc # 401\n",
      "Processing doc # 439\n",
      "Processing doc # 402\n",
      "Processing doc # 366\n",
      "Processing doc # 440\n",
      "Processing doc # 403\n",
      "Processing doc # 441\n",
      "Processing doc # 367\n",
      "Processing doc # 368\n",
      "Processing doc # 442\n",
      "Processing doc # 404\n",
      "Processing doc # 405\n",
      "Processing doc # 443\n",
      "Processing doc # 369\n",
      "Processing doc # 444\n",
      "Processing doc # 406\n",
      "Processing doc # 370\n",
      "Processing doc # 407\n",
      "Processing doc # 445\n",
      "Processing doc # 408\n",
      "Processing doc # 371\n",
      "Processing doc # 446\n",
      "Processing doc # 409\n",
      "Processing doc # 447\n",
      "Processing doc # 372\n",
      "Processing doc # 448\n",
      "Processing doc # 410\n",
      "Processing doc # 373\n",
      "Processing doc # 449\n",
      "Processing doc # 374\n",
      "Processing doc # 411\n",
      "Processing doc # 450\n",
      "Processing doc # 375\n",
      "Processing doc # 412\n",
      "Processing doc # 451\n",
      "Processing doc # 376\n",
      "Processing doc # 452\n",
      "Processing doc # 413\n",
      "Processing doc # 377\n",
      "Processing doc # 414\n",
      "Processing doc # 453\n",
      "Processing doc # 378\n",
      "Processing doc # 415\n",
      "Processing doc # 454\n",
      "Processing doc # 416\n",
      "Processing doc # 455\n",
      "Processing doc # 379\n",
      "Processing doc # 456\n",
      "Processing doc # 380\n",
      "Processing doc # 417\n",
      "Processing doc # 381\n",
      "Processing doc # 418\n",
      "Processing doc # 457\n",
      "Processing doc # 382\n",
      "Processing doc # 419\n",
      "Processing doc # 458\n",
      "Processing doc # 383\n",
      "Processing doc # 420\n",
      "Processing doc # 459\n",
      "Processing doc # 460\n",
      "Processing doc # 421\n",
      "Processing doc # 461\n",
      "Processing doc # 422\n",
      "Processing doc # 384\n",
      "Processing doc # 385\n",
      "Processing doc # 462\n",
      "Processing doc # 423\n",
      "Processing doc # 386\n",
      "Processing doc # 463\n",
      "Processing doc # 424\n",
      "Processing doc # 387\n",
      "Processing doc # 464\n",
      "Processing doc # 388\n",
      "Processing doc # 465\n",
      "Processing doc # 425\n",
      "Processing doc # 389\n",
      "Processing doc # 466\n",
      "Processing doc # 390\n",
      "Processing doc # 426\n",
      "Processing doc # 467\n",
      "Processing doc # 427\n",
      "Processing doc # 468\n",
      "Processing doc # 428\n",
      "Processing doc # 429\n",
      "Reading problem 1, done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "problem = problems[0]\n",
    "index = 0\n",
    "\n",
    "# used for n_gram extraction and word indexing, a threshold which prevent words appearing lower than this value to be counted in calculations\n",
    "tf = 5\n",
    "\n",
    "\n",
    "infoproblem = dataset_path + os.sep + problem + os.sep + 'problem-info.json'\n",
    "candidates = []\n",
    "with open(infoproblem, 'r') as f:\n",
    "    fj = json.load(f)\n",
    "    unk_folder = fj['unknown-folder']\n",
    "    for attrib in fj['candidate-authors']:\n",
    "        candidates.append(attrib['author-name'])\n",
    "\n",
    "candidates.sort()\n",
    "# Building training set\n",
    "train_docs = []\n",
    "for candidate in candidates:\n",
    "    train_docs.extend(read_files(dataset_path + os.sep + problem, candidate))\n",
    "train_texts = [text for i, (text, label) in enumerate(train_docs)]\n",
    "train_labels = [label for i, (text, label) in enumerate(train_docs)]\n",
    "index_2_label_dict = {i: l for i, l in enumerate(set(train_labels))}\n",
    "label_2_index_dict = {l: i for i, l in enumerate(set(train_labels))}\n",
    "train_labels = [label_2_index_dict[v] for v in train_labels]\n",
    "w2d = Word2Dim()\n",
    "train_tokenized_with_pos, train_tokenized_indexed = w2d.fit_transform_texts(train_texts, train_labels,\n",
    "                                                                            language[index], tf= 5)\n",
    "\n",
    "maxlen = len(max(train_tokenized_indexed, key=len))  # We will cut the texts after # words\n",
    "embedding_dim = w2d.word_embedding.shape[1]\n",
    "\n",
    "# preparing test set\n",
    "ground_truth_file = dataset_path + os.sep + problem + os.sep + 'ground-truth.json'\n",
    "gt = {}\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    for attrib in json.load(f)['ground_truth']:\n",
    "        gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "test_docs = read_files(dataset_path + os.sep + problem, unk_folder, gt)\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs)]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs)]\n",
    "\n",
    "# Filter validation to known authors\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "\n",
    "test_labels = [label_2_index_dict[v] for v in test_labels]\n",
    "\n",
    "test_tokenized_with_pos, test_tokenized_indexed = w2d.transform(test_texts)\n",
    "print(\"Reading problem 1, done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Extraction for Neural Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t17\n",
      "  (0, 1)\t8\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t2\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t6\n",
      "  (0, 8)\t33\n",
      "  (0, 9)\t27\n",
      "  (0, 10)\t5\n",
      "  (0, 11)\t23\n",
      "  (0, 12)\t6\n",
      "  (0, 13)\t2\n",
      "  (0, 14)\t2\n",
      "  (0, 15)\t8\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t10\n",
      "  (0, 18)\t20\n",
      "  (0, 19)\t19\n",
      "  (0, 20)\t1\n",
      "  (0, 21)\t5\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t1\n",
      "  :\t:\n",
      "  (0, 1313)\t1\n",
      "  (0, 1314)\t1\n",
      "  (0, 1315)\t1\n",
      "  (0, 1316)\t1\n",
      "  (0, 1317)\t1\n",
      "  (0, 1318)\t1\n",
      "  (0, 1319)\t1\n",
      "  (0, 1321)\t1\n",
      "  (0, 1322)\t1\n",
      "  (0, 1323)\t1\n",
      "  (0, 1324)\t1\n",
      "  (0, 1325)\t1\n",
      "  (0, 1326)\t1\n",
      "  (0, 1327)\t1\n",
      "  (0, 1328)\t1\n",
      "  (0, 1588)\t1\n",
      "  (0, 1593)\t1\n",
      "  (0, 1594)\t1\n",
      "  (0, 1633)\t1\n",
      "  (0, 1660)\t1\n",
      "  (0, 2468)\t1\n",
      "  (0, 2993)\t1\n",
      "  (0, 3191)\t1\n",
      "  (0, 3200)\t1\n",
      "  (0, 3439)\t1\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from MyUtils import extract_n_grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "n = 3\n",
    "vocabulary = extract_n_grams(train_docs, n, tf)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(n, n), lowercase=False, vocabulary=vocabulary)\n",
    "n_gram_train_data = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "n_gram_train_data = n_gram_train_data.astype(float)\n",
    "\n",
    "for i, v in enumerate(train_texts):\n",
    "    n_gram_train_data[i] = n_gram_train_data[i] / len(train_texts[i])\n",
    "n_gram_test_data = vectorizer.transform(test_texts)\n",
    "n_gram_test_data = n_gram_test_data.astype(float)\n",
    "for i, v in enumerate(test_texts):\n",
    "    n_gram_test_data[i] = n_gram_test_data[i] / len(test_texts[i])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data = max_abs_scaler.fit_transform(n_gram_train_data)\n",
    "scaled_test_data = max_abs_scaler.transform(n_gram_test_data)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.0026738 , 0.00882353, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01388889, 0.02941176, 0.01176471, ..., 0.01809955, 0.04297994,\n",
       "        0.01136364],\n",
       "       ...,\n",
       "       [0.05555556, 0.04812834, 0.07058824, ..., 0.0882353 , 0.04584527,\n",
       "        0.0625    ],\n",
       "       [0.00231481, 0.00534759, 0.        , ..., 0.00226244, 0.00573066,\n",
       "        0.00284091],\n",
       "       [0.        , 0.00534759, 0.00294118, ..., 0.00226244, 0.        ,\n",
       "        0.00852273]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "w2d.word_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: (?, 1019, 32) x: (?, 1019, 32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              (None, 1019)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_56 (Embedding)        (None, 1019, 9)      72225       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, 1019, 32)     320         embedding_56[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, 1019, 32)     5152        conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1019, 32)     0           conv1d_99[0][0]                  \n",
      "                                                                 conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_44 (Global (None, 32)           0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 9)            297         global_max_pooling1d_44[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 77,994\n",
      "Trainable params: 77,994\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 63 samples, validate on 468 samples\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 5s 78ms/step - loss: 2.2037 - acc: 0.0635 - val_loss: 2.2366 - val_acc: 0.1090\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.1876 - acc: 0.2540 - val_loss: 2.2355 - val_acc: 0.1111\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.1737 - acc: 0.3492 - val_loss: 2.2328 - val_acc: 0.1111\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.1618 - acc: 0.4286 - val_loss: 2.2317 - val_acc: 0.1197\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.1501 - acc: 0.4127 - val_loss: 2.2297 - val_acc: 0.1218\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.1368 - acc: 0.4921 - val_loss: 2.2310 - val_acc: 0.1325\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.1233 - acc: 0.5079 - val_loss: 2.2289 - val_acc: 0.1325\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.1102 - acc: 0.5238 - val_loss: 2.2275 - val_acc: 0.1581\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0943 - acc: 0.5238 - val_loss: 2.2294 - val_acc: 0.1389\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.0769 - acc: 0.5397 - val_loss: 2.2242 - val_acc: 0.1645\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0558 - acc: 0.5397 - val_loss: 2.2229 - val_acc: 0.1752\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.0358 - acc: 0.5556 - val_loss: 2.2249 - val_acc: 0.1731\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 2.0106 - acc: 0.6032 - val_loss: 2.2211 - val_acc: 0.1880\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.9829 - acc: 0.6349 - val_loss: 2.2195 - val_acc: 0.1880\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.9549 - acc: 0.6349 - val_loss: 2.2200 - val_acc: 0.1923\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.9195 - acc: 0.6984 - val_loss: 2.2195 - val_acc: 0.1880\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 1.8791 - acc: 0.7778 - val_loss: 2.2060 - val_acc: 0.1880\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.8404 - acc: 0.8095 - val_loss: 2.1988 - val_acc: 0.1902\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.7892 - acc: 0.8730 - val_loss: 2.2024 - val_acc: 0.1774\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 1.7346 - acc: 0.9524 - val_loss: 2.1923 - val_acc: 0.1902\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6768 - acc: 0.9841 - val_loss: 2.1866 - val_acc: 0.1880\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.6136 - acc: 0.9841 - val_loss: 2.1900 - val_acc: 0.1987\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.5496 - acc: 0.9683 - val_loss: 2.1975 - val_acc: 0.1795\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.4702 - acc: 1.0000 - val_loss: 2.1665 - val_acc: 0.1966\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.3901 - acc: 1.0000 - val_loss: 2.1618 - val_acc: 0.2094\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.3082 - acc: 1.0000 - val_loss: 2.1483 - val_acc: 0.1944\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.2194 - acc: 1.0000 - val_loss: 2.1371 - val_acc: 0.2137\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.1279 - acc: 1.0000 - val_loss: 2.1300 - val_acc: 0.2094\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.0422 - acc: 1.0000 - val_loss: 2.0982 - val_acc: 0.2521\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.9509 - acc: 1.0000 - val_loss: 2.1025 - val_acc: 0.2308\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8643 - acc: 1.0000 - val_loss: 2.0991 - val_acc: 0.2201\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7776 - acc: 1.0000 - val_loss: 2.0932 - val_acc: 0.2158\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.6949 - acc: 1.0000 - val_loss: 2.0890 - val_acc: 0.2372\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.6187 - acc: 1.0000 - val_loss: 2.0738 - val_acc: 0.2607\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.5526 - acc: 1.0000 - val_loss: 2.0712 - val_acc: 0.2479\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.4886 - acc: 1.0000 - val_loss: 2.0407 - val_acc: 0.2799\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.4284 - acc: 1.0000 - val_loss: 2.0627 - val_acc: 0.2457\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.3766 - acc: 1.0000 - val_loss: 2.0280 - val_acc: 0.2949\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.3299 - acc: 1.0000 - val_loss: 2.0389 - val_acc: 0.2543\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.2901 - acc: 1.0000 - val_loss: 2.0601 - val_acc: 0.2457\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.2568 - acc: 1.0000 - val_loss: 2.0507 - val_acc: 0.2415\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.2275 - acc: 1.0000 - val_loss: 2.0151 - val_acc: 0.2885\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.2002 - acc: 1.0000 - val_loss: 2.0141 - val_acc: 0.2863\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.1756 - acc: 1.0000 - val_loss: 2.0427 - val_acc: 0.2500\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.1568 - acc: 1.0000 - val_loss: 2.0236 - val_acc: 0.2863\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.1406 - acc: 1.0000 - val_loss: 2.0116 - val_acc: 0.2906\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.1246 - acc: 1.0000 - val_loss: 2.0054 - val_acc: 0.2863\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.1123 - acc: 1.0000 - val_loss: 2.0214 - val_acc: 0.2671\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.1011 - acc: 1.0000 - val_loss: 2.0166 - val_acc: 0.2756\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0907 - acc: 1.0000 - val_loss: 1.9989 - val_acc: 0.2991\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0820 - acc: 1.0000 - val_loss: 1.9980 - val_acc: 0.3034\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0747 - acc: 1.0000 - val_loss: 2.0019 - val_acc: 0.2927\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0678 - acc: 1.0000 - val_loss: 2.0054 - val_acc: 0.2885\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0622 - acc: 1.0000 - val_loss: 2.0057 - val_acc: 0.2863\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0571 - acc: 1.0000 - val_loss: 2.0091 - val_acc: 0.2842\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0522 - acc: 1.0000 - val_loss: 1.9972 - val_acc: 0.2991\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0479 - acc: 1.0000 - val_loss: 2.0031 - val_acc: 0.2863\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0443 - acc: 1.0000 - val_loss: 1.9969 - val_acc: 0.2970\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0406 - acc: 1.0000 - val_loss: 1.9925 - val_acc: 0.3013\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0375 - acc: 1.0000 - val_loss: 2.0041 - val_acc: 0.2863\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0349 - acc: 1.0000 - val_loss: 1.9905 - val_acc: 0.3056\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0323 - acc: 1.0000 - val_loss: 1.9999 - val_acc: 0.2863\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0301 - acc: 1.0000 - val_loss: 1.9938 - val_acc: 0.3056\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0280 - acc: 1.0000 - val_loss: 1.9908 - val_acc: 0.3077\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0260 - acc: 1.0000 - val_loss: 1.9957 - val_acc: 0.3013\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0244 - acc: 1.0000 - val_loss: 1.9927 - val_acc: 0.3056\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0227 - acc: 1.0000 - val_loss: 1.9947 - val_acc: 0.3013\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0213 - acc: 1.0000 - val_loss: 1.9992 - val_acc: 0.2970\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0199 - acc: 1.0000 - val_loss: 1.9926 - val_acc: 0.3077\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0187 - acc: 1.0000 - val_loss: 2.0048 - val_acc: 0.2949\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0175 - acc: 1.0000 - val_loss: 1.9957 - val_acc: 0.3056\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0164 - acc: 1.0000 - val_loss: 2.0003 - val_acc: 0.3013\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 2.0016 - val_acc: 0.2991\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0146 - acc: 1.0000 - val_loss: 1.9999 - val_acc: 0.2970\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0137 - acc: 1.0000 - val_loss: 1.9906 - val_acc: 0.3098\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0129 - acc: 1.0000 - val_loss: 2.0002 - val_acc: 0.3013\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 1.9991 - val_acc: 0.3034\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 2.0020 - val_acc: 0.2949\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 1.9952 - val_acc: 0.3120\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 1.9908 - val_acc: 0.3120\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0098 - acc: 1.0000 - val_loss: 1.9981 - val_acc: 0.3077\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 1.9954 - val_acc: 0.3120\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 2.0059 - val_acc: 0.3056\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0083 - acc: 1.0000 - val_loss: 2.0020 - val_acc: 0.3077\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 1.9951 - val_acc: 0.3120\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 1.9976 - val_acc: 0.3098\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 1.9945 - val_acc: 0.3120\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 1.9924 - val_acc: 0.3120\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0064 - acc: 1.0000 - val_loss: 2.0006 - val_acc: 0.3098\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 1.9988 - val_acc: 0.3120\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 2.0009 - val_acc: 0.3077\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 2.0094 - val_acc: 0.3077\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 1.9967 - val_acc: 0.3141\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 1.9990 - val_acc: 0.3162\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 2.0050 - val_acc: 0.3120\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 1.9967 - val_acc: 0.3184\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 2.0049 - val_acc: 0.3141\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 2.0058 - val_acc: 0.3141\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 1.9953 - val_acc: 0.3184\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 2.0069 - val_acc: 0.3141\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 1.9991 - val_acc: 0.3162\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 2.0041 - val_acc: 0.3141\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 2.0081 - val_acc: 0.3162\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 2.0033 - val_acc: 0.3162\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 2.0109 - val_acc: 0.3141\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 2.0071 - val_acc: 0.3141\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.0103 - val_acc: 0.3141\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 2.0118 - val_acc: 0.3120\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 2.0109 - val_acc: 0.3141\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.0088 - val_acc: 0.3141\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 2.0111 - val_acc: 0.3098\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 2.0155 - val_acc: 0.3141\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 2.0167 - val_acc: 0.3141\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 2.0123 - val_acc: 0.3162\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.0142 - val_acc: 0.3120\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 2.0176 - val_acc: 0.3120\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 2.0157 - val_acc: 0.3141\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 2.0157 - val_acc: 0.3141\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.0195 - val_acc: 0.3120\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.0130 - val_acc: 0.3141\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.0201 - val_acc: 0.3098\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.0270 - val_acc: 0.3120\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.0208 - val_acc: 0.3098\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 2.0217 - val_acc: 0.3141\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.0190 - val_acc: 0.3120\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.0283 - val_acc: 0.3098\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.0233 - val_acc: 0.3077\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.0248 - val_acc: 0.3098\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.0252 - val_acc: 0.3098\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 2.0244 - val_acc: 0.3120\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 9.7956e-04 - acc: 1.0000 - val_loss: 2.0342 - val_acc: 0.3120\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 9.3846e-04 - acc: 1.0000 - val_loss: 2.0295 - val_acc: 0.3120\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 9.0005e-04 - acc: 1.0000 - val_loss: 2.0318 - val_acc: 0.3077\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 8.6447e-04 - acc: 1.0000 - val_loss: 2.0320 - val_acc: 0.3120\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 8.2931e-04 - acc: 1.0000 - val_loss: 2.0338 - val_acc: 0.3141\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 7.9737e-04 - acc: 1.0000 - val_loss: 2.0350 - val_acc: 0.3098\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 7.6482e-04 - acc: 1.0000 - val_loss: 2.0365 - val_acc: 0.3098\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 7.3606e-04 - acc: 1.0000 - val_loss: 2.0335 - val_acc: 0.3162\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 7.0583e-04 - acc: 1.0000 - val_loss: 2.0371 - val_acc: 0.3120\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 6.7809e-04 - acc: 1.0000 - val_loss: 2.0365 - val_acc: 0.3184\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 6.5177e-04 - acc: 1.0000 - val_loss: 2.0406 - val_acc: 0.3098\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 6.2636e-04 - acc: 1.0000 - val_loss: 2.0407 - val_acc: 0.3141\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 6.0157e-04 - acc: 1.0000 - val_loss: 2.0392 - val_acc: 0.3162\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 5.7792e-04 - acc: 1.0000 - val_loss: 2.0407 - val_acc: 0.3184\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 5.5652e-04 - acc: 1.0000 - val_loss: 2.0465 - val_acc: 0.3098\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 5.3413e-04 - acc: 1.0000 - val_loss: 2.0402 - val_acc: 0.3205\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 5.1415e-04 - acc: 1.0000 - val_loss: 2.0419 - val_acc: 0.3184\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 4.9358e-04 - acc: 1.0000 - val_loss: 2.0473 - val_acc: 0.3162\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 4.7443e-04 - acc: 1.0000 - val_loss: 2.0465 - val_acc: 0.3162\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.5579e-04 - acc: 1.0000 - val_loss: 2.0527 - val_acc: 0.3162\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 4.3837e-04 - acc: 1.0000 - val_loss: 2.0511 - val_acc: 0.3162\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.2191e-04 - acc: 1.0000 - val_loss: 2.0500 - val_acc: 0.3205\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.0537e-04 - acc: 1.0000 - val_loss: 2.0509 - val_acc: 0.3184\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.9120e-04 - acc: 1.0000 - val_loss: 2.0532 - val_acc: 0.3184\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.7539e-04 - acc: 1.0000 - val_loss: 2.0588 - val_acc: 0.3162\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.6211e-04 - acc: 1.0000 - val_loss: 2.0593 - val_acc: 0.3141\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 3.4745e-04 - acc: 1.0000 - val_loss: 2.0562 - val_acc: 0.3184\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 3.3429e-04 - acc: 1.0000 - val_loss: 2.0585 - val_acc: 0.3184\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 3.2176e-04 - acc: 1.0000 - val_loss: 2.0576 - val_acc: 0.3248\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 3.0987e-04 - acc: 1.0000 - val_loss: 2.0627 - val_acc: 0.3184\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.9803e-04 - acc: 1.0000 - val_loss: 2.0591 - val_acc: 0.3226\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.8651e-04 - acc: 1.0000 - val_loss: 2.0647 - val_acc: 0.3141\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.7529e-04 - acc: 1.0000 - val_loss: 2.0630 - val_acc: 0.3184\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.6542e-04 - acc: 1.0000 - val_loss: 2.0634 - val_acc: 0.3184\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.5516e-04 - acc: 1.0000 - val_loss: 2.0670 - val_acc: 0.3162\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.4601e-04 - acc: 1.0000 - val_loss: 2.0673 - val_acc: 0.3184\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.3619e-04 - acc: 1.0000 - val_loss: 2.0680 - val_acc: 0.3162\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 2.2762e-04 - acc: 1.0000 - val_loss: 2.0725 - val_acc: 0.3162\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.1923e-04 - acc: 1.0000 - val_loss: 2.0696 - val_acc: 0.3184\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.1095e-04 - acc: 1.0000 - val_loss: 2.0740 - val_acc: 0.3162\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 2.0322e-04 - acc: 1.0000 - val_loss: 2.0752 - val_acc: 0.3141\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.9548e-04 - acc: 1.0000 - val_loss: 2.0723 - val_acc: 0.3184\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.8821e-04 - acc: 1.0000 - val_loss: 2.0760 - val_acc: 0.3184\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.8155e-04 - acc: 1.0000 - val_loss: 2.0805 - val_acc: 0.3141\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.7479e-04 - acc: 1.0000 - val_loss: 2.0760 - val_acc: 0.3205\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6807e-04 - acc: 1.0000 - val_loss: 2.0789 - val_acc: 0.3162\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.6187e-04 - acc: 1.0000 - val_loss: 2.0809 - val_acc: 0.3184\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.5583e-04 - acc: 1.0000 - val_loss: 2.0831 - val_acc: 0.3184\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5012e-04 - acc: 1.0000 - val_loss: 2.0834 - val_acc: 0.3205\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.4456e-04 - acc: 1.0000 - val_loss: 2.0884 - val_acc: 0.3162\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.3934e-04 - acc: 1.0000 - val_loss: 2.0906 - val_acc: 0.3162\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.3430e-04 - acc: 1.0000 - val_loss: 2.0892 - val_acc: 0.3162\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.2928e-04 - acc: 1.0000 - val_loss: 2.0902 - val_acc: 0.3184\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2452e-04 - acc: 1.0000 - val_loss: 2.0909 - val_acc: 0.3205\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.2005e-04 - acc: 1.0000 - val_loss: 2.0947 - val_acc: 0.3184\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.1548e-04 - acc: 1.0000 - val_loss: 2.0948 - val_acc: 0.3205\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.1152e-04 - acc: 1.0000 - val_loss: 2.0974 - val_acc: 0.3184\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.0709e-04 - acc: 1.0000 - val_loss: 2.0987 - val_acc: 0.3162\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.0320e-04 - acc: 1.0000 - val_loss: 2.0981 - val_acc: 0.3184\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 9.9447e-05 - acc: 1.0000 - val_loss: 2.1021 - val_acc: 0.3162\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 9.5772e-05 - acc: 1.0000 - val_loss: 2.1037 - val_acc: 0.3162\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 9.2302e-05 - acc: 1.0000 - val_loss: 2.1031 - val_acc: 0.3269\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 8.8990e-05 - acc: 1.0000 - val_loss: 2.1054 - val_acc: 0.3184\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 8.6113e-05 - acc: 1.0000 - val_loss: 2.0997 - val_acc: 0.3248\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 8.2639e-05 - acc: 1.0000 - val_loss: 2.1051 - val_acc: 0.3269\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 7.9702e-05 - acc: 1.0000 - val_loss: 2.1074 - val_acc: 0.3184\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 7.6660e-05 - acc: 1.0000 - val_loss: 2.1104 - val_acc: 0.3205\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 7.3952e-05 - acc: 1.0000 - val_loss: 2.1155 - val_acc: 0.3226\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 7.1383e-05 - acc: 1.0000 - val_loss: 2.1141 - val_acc: 0.3226\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 6.8649e-05 - acc: 1.0000 - val_loss: 2.1154 - val_acc: 0.3184\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x160748908>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x175967320>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_data = pad_sequences(train_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "test_data = pad_sequences(test_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels,\n",
    "#                                                   test_size=len(set(train_labels)), random_state=2019,\n",
    "#                                                   stratify=train_labels)\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_val = to_categorical(y_val)\n",
    "# print(X_train.shape)\n",
    "\n",
    "word_input_tensor = Input(shape=(maxlen,) , name='words')\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# model.add(layers.Conv1D(32, 1, activation='relu'))\n",
    "# model.add(layers.MaxPooling1D(2))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "# model.add(layers.GlobalMaxPooling1D())\n",
    "# model.add(layers.SimpleRNN(embedding_dim, dropout=0.1, recurrent_dropout=0.5, return_sequences= True))\n",
    "# model.add(layers.SimpleRNN(embedding_dim))\n",
    "# model.add(Dense(embedding_dim, activation='relu'))\n",
    "# model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "x = layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen)(word_input_tensor)\n",
    "x = layers.Conv1D(32, 1, activation='relu', padding='same')(x)\n",
    "# x = layers.MaxPooling1D(2)(x)\n",
    "y = layers.Conv1D(32, 5, activation='relu', padding='same')(x)\n",
    "# y = layers.MaxPooling1D(2)(y)\n",
    "# y = layers.Conv1D(32, 3, activation='relu')(y)\n",
    "# y = layers.MaxPooling1D(2)(y)\n",
    "# y = layers.Conv1D(32, 5, activation='relu')(y)\n",
    "# y = layers.MaxPooling1D(2)(y)\n",
    "# y = layers.Conv1D(32, 5, activation='relu')(y)\n",
    "# y = layers.GlobalMaxPooling1D()(y)\n",
    "\n",
    "print('y:', y.shape, 'x:', x.shape)\n",
    "added = layers.add([y, x])\n",
    "answer = layers.GlobalMaxPooling1D()(added)\n",
    "answer = layers.Dense(len(set(train_labels)), activation='softmax')(answer)\n",
    "\n",
    "model = Model(word_input_tensor, answer)\n",
    "model.summary()\n",
    "\n",
    "# model.layers[0].set_weights([w2d.word_embedding])\n",
    "# model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=3e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=200,\n",
    "                    batch_size=1)\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              (None, 1019)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 32)           12083       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 32)           12065       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 32)           12074       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 96)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_11[1][0]              \n",
      "                                                                 sequential_12[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 9)            873         concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 37,095\n",
      "Trainable params: 1,887\n",
      "Non-trainable params: 35,208\n",
      "__________________________________________________________________________________________________\n",
      "Train on 63 samples, validate on 468 samples\n",
      "Epoch 1/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.1977 - acc: 0.1111 - val_loss: 0.1976 - val_acc: 0.0491\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1976 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1976 - acc: 0.1111 - val_loss: 0.1976 - val_acc: 0.0491\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1976 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1976 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1976 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1976 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1975 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1975 - acc: 0.1111 - val_loss: 0.1976 - val_acc: 0.0491\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1975 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1974 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1974 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1975 - acc: 0.1111 - val_loss: 0.1976 - val_acc: 0.0491\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1974 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1974 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1974 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1974 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1973 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1974 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1973 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1973 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1973 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1973 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1973 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1972 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1972 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1972 - acc: 0.1111 - val_loss: 0.1976 - val_acc: 0.0491\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1971 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1971 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1970 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1971 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1970 - acc: 0.1111 - val_loss: 0.1981 - val_acc: 0.0491\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1970 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 34/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1969 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1969 - acc: 0.1111 - val_loss: 0.1976 - val_acc: 0.0491\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1969 - acc: 0.1111 - val_loss: 0.1980 - val_acc: 0.0491\n",
      "Epoch 37/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1968 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1968 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1967 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1966 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1965 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1965 - acc: 0.1111 - val_loss: 0.1980 - val_acc: 0.0491\n",
      "Epoch 43/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1964 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 44/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1963 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1962 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 46/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1962 - acc: 0.1429 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1960 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 48/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1960 - acc: 0.1111 - val_loss: 0.1978 - val_acc: 0.0491\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1958 - acc: 0.1111 - val_loss: 0.1977 - val_acc: 0.0491\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1958 - acc: 0.1111 - val_loss: 0.1979 - val_acc: 0.0491\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1956 - acc: 0.1746 - val_loss: 0.1976 - val_acc: 0.0598\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1955 - acc: 0.1429 - val_loss: 0.1977 - val_acc: 0.0577\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1954 - acc: 0.1746 - val_loss: 0.1975 - val_acc: 0.0577\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1953 - acc: 0.1905 - val_loss: 0.1976 - val_acc: 0.0491\n",
      "Epoch 55/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1952 - acc: 0.1746 - val_loss: 0.1974 - val_acc: 0.0385\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1951 - acc: 0.1587 - val_loss: 0.1974 - val_acc: 0.0513\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1949 - acc: 0.2063 - val_loss: 0.1973 - val_acc: 0.0385\n",
      "Epoch 58/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1948 - acc: 0.1746 - val_loss: 0.1973 - val_acc: 0.0662\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1946 - acc: 0.2063 - val_loss: 0.1974 - val_acc: 0.0491\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1946 - acc: 0.2381 - val_loss: 0.1972 - val_acc: 0.0534\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1944 - acc: 0.1587 - val_loss: 0.1974 - val_acc: 0.0491\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1942 - acc: 0.1746 - val_loss: 0.1974 - val_acc: 0.0598\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1940 - acc: 0.1905 - val_loss: 0.1972 - val_acc: 0.0598\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1939 - acc: 0.2063 - val_loss: 0.1970 - val_acc: 0.0598\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1938 - acc: 0.2222 - val_loss: 0.1972 - val_acc: 0.0684\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1936 - acc: 0.2698 - val_loss: 0.1970 - val_acc: 0.0897\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1933 - acc: 0.2540 - val_loss: 0.1971 - val_acc: 0.0876\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1932 - acc: 0.2698 - val_loss: 0.1970 - val_acc: 0.0962\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1930 - acc: 0.2540 - val_loss: 0.1969 - val_acc: 0.0855\n",
      "Epoch 70/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1926 - acc: 0.2698 - val_loss: 0.1966 - val_acc: 0.0726\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1924 - acc: 0.3492 - val_loss: 0.1965 - val_acc: 0.1154\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1922 - acc: 0.3175 - val_loss: 0.1965 - val_acc: 0.1090\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1919 - acc: 0.3175 - val_loss: 0.1964 - val_acc: 0.1346\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1916 - acc: 0.3175 - val_loss: 0.1962 - val_acc: 0.1346\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1913 - acc: 0.3175 - val_loss: 0.1962 - val_acc: 0.1368\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1911 - acc: 0.3016 - val_loss: 0.1962 - val_acc: 0.1496\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1908 - acc: 0.3175 - val_loss: 0.1959 - val_acc: 0.1603\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1904 - acc: 0.3175 - val_loss: 0.1959 - val_acc: 0.1560\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.1900 - acc: 0.4127 - val_loss: 0.1960 - val_acc: 0.1624\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1897 - acc: 0.3333 - val_loss: 0.1958 - val_acc: 0.1688\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1893 - acc: 0.3175 - val_loss: 0.1954 - val_acc: 0.2415\n",
      "Epoch 82/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.1890 - acc: 0.3016 - val_loss: 0.1954 - val_acc: 0.2329\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1885 - acc: 0.3492 - val_loss: 0.1954 - val_acc: 0.2329\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1882 - acc: 0.3016 - val_loss: 0.1952 - val_acc: 0.2607\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1879 - acc: 0.2857 - val_loss: 0.1951 - val_acc: 0.2286\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1873 - acc: 0.3016 - val_loss: 0.1949 - val_acc: 0.2308\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1867 - acc: 0.2857 - val_loss: 0.1949 - val_acc: 0.2350\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1863 - acc: 0.2857 - val_loss: 0.1947 - val_acc: 0.2457\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1859 - acc: 0.2857 - val_loss: 0.1944 - val_acc: 0.2308\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1854 - acc: 0.3016 - val_loss: 0.1945 - val_acc: 0.2308\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1850 - acc: 0.3016 - val_loss: 0.1944 - val_acc: 0.2308\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1844 - acc: 0.2857 - val_loss: 0.1940 - val_acc: 0.2479\n",
      "Epoch 93/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1840 - acc: 0.2698 - val_loss: 0.1939 - val_acc: 0.2372\n",
      "Epoch 94/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1833 - acc: 0.2857 - val_loss: 0.1937 - val_acc: 0.2372\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1831 - acc: 0.2857 - val_loss: 0.1932 - val_acc: 0.2329\n",
      "Epoch 96/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1827 - acc: 0.3175 - val_loss: 0.1936 - val_acc: 0.2607\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1820 - acc: 0.2381 - val_loss: 0.1930 - val_acc: 0.2137\n",
      "Epoch 98/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1816 - acc: 0.2540 - val_loss: 0.1930 - val_acc: 0.2436\n",
      "Epoch 99/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1813 - acc: 0.2222 - val_loss: 0.1926 - val_acc: 0.2329\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1805 - acc: 0.2857 - val_loss: 0.1926 - val_acc: 0.2500\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1802 - acc: 0.2857 - val_loss: 0.1925 - val_acc: 0.2585\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1798 - acc: 0.3016 - val_loss: 0.1920 - val_acc: 0.2393\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1792 - acc: 0.2857 - val_loss: 0.1918 - val_acc: 0.2521\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1787 - acc: 0.2857 - val_loss: 0.1919 - val_acc: 0.2543\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1782 - acc: 0.3333 - val_loss: 0.1918 - val_acc: 0.2585\n",
      "Epoch 106/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1779 - acc: 0.2857 - val_loss: 0.1914 - val_acc: 0.2521\n",
      "Epoch 107/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1775 - acc: 0.3016 - val_loss: 0.1914 - val_acc: 0.2543\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1770 - acc: 0.3016 - val_loss: 0.1911 - val_acc: 0.2564\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1765 - acc: 0.3175 - val_loss: 0.1909 - val_acc: 0.2500\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1764 - acc: 0.3016 - val_loss: 0.1909 - val_acc: 0.2585\n",
      "Epoch 111/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1760 - acc: 0.3175 - val_loss: 0.1906 - val_acc: 0.2436\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1753 - acc: 0.3175 - val_loss: 0.1906 - val_acc: 0.2436\n",
      "Epoch 113/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1751 - acc: 0.3175 - val_loss: 0.1903 - val_acc: 0.2564\n",
      "Epoch 114/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1750 - acc: 0.2857 - val_loss: 0.1904 - val_acc: 0.2585\n",
      "Epoch 115/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1748 - acc: 0.2857 - val_loss: 0.1904 - val_acc: 0.2158\n",
      "Epoch 116/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1740 - acc: 0.3333 - val_loss: 0.1904 - val_acc: 0.2329\n",
      "Epoch 117/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1739 - acc: 0.3492 - val_loss: 0.1900 - val_acc: 0.2479\n",
      "Epoch 118/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1735 - acc: 0.3016 - val_loss: 0.1902 - val_acc: 0.1923\n",
      "Epoch 119/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1730 - acc: 0.3492 - val_loss: 0.1899 - val_acc: 0.2500\n",
      "Epoch 120/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1726 - acc: 0.3651 - val_loss: 0.1898 - val_acc: 0.2479\n",
      "Epoch 121/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1722 - acc: 0.3492 - val_loss: 0.1901 - val_acc: 0.2329\n",
      "Epoch 122/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1720 - acc: 0.3175 - val_loss: 0.1898 - val_acc: 0.2009\n",
      "Epoch 123/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1715 - acc: 0.3175 - val_loss: 0.1897 - val_acc: 0.2329\n",
      "Epoch 124/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1713 - acc: 0.3016 - val_loss: 0.1897 - val_acc: 0.2222\n",
      "Epoch 125/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1709 - acc: 0.3016 - val_loss: 0.1898 - val_acc: 0.2094\n",
      "Epoch 126/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1707 - acc: 0.3492 - val_loss: 0.1891 - val_acc: 0.2457\n",
      "Epoch 127/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1703 - acc: 0.4127 - val_loss: 0.1895 - val_acc: 0.2521\n",
      "Epoch 128/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1697 - acc: 0.3810 - val_loss: 0.1895 - val_acc: 0.2244\n",
      "Epoch 129/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1699 - acc: 0.3651 - val_loss: 0.1895 - val_acc: 0.2137\n",
      "Epoch 130/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1691 - acc: 0.3810 - val_loss: 0.1891 - val_acc: 0.2500\n",
      "Epoch 131/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1693 - acc: 0.4127 - val_loss: 0.1893 - val_acc: 0.2393\n",
      "Epoch 132/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1685 - acc: 0.3810 - val_loss: 0.1889 - val_acc: 0.2457\n",
      "Epoch 133/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1686 - acc: 0.3810 - val_loss: 0.1893 - val_acc: 0.2415\n",
      "Epoch 134/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1683 - acc: 0.3810 - val_loss: 0.1885 - val_acc: 0.2500\n",
      "Epoch 135/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1677 - acc: 0.4127 - val_loss: 0.1889 - val_acc: 0.2393\n",
      "Epoch 136/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1674 - acc: 0.4444 - val_loss: 0.1886 - val_acc: 0.2564\n",
      "Epoch 137/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1672 - acc: 0.4286 - val_loss: 0.1883 - val_acc: 0.2585\n",
      "Epoch 138/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1666 - acc: 0.4603 - val_loss: 0.1884 - val_acc: 0.2543\n",
      "Epoch 139/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1666 - acc: 0.4127 - val_loss: 0.1886 - val_acc: 0.2393\n",
      "Epoch 140/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1661 - acc: 0.4286 - val_loss: 0.1886 - val_acc: 0.2457\n",
      "Epoch 141/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1660 - acc: 0.4286 - val_loss: 0.1882 - val_acc: 0.2479\n",
      "Epoch 142/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1653 - acc: 0.4762 - val_loss: 0.1882 - val_acc: 0.2543\n",
      "Epoch 143/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1652 - acc: 0.4762 - val_loss: 0.1878 - val_acc: 0.2585\n",
      "Epoch 144/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1648 - acc: 0.4762 - val_loss: 0.1881 - val_acc: 0.2628\n",
      "Epoch 145/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1645 - acc: 0.4444 - val_loss: 0.1877 - val_acc: 0.2650\n",
      "Epoch 146/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1642 - acc: 0.4762 - val_loss: 0.1879 - val_acc: 0.2564\n",
      "Epoch 147/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1639 - acc: 0.4921 - val_loss: 0.1880 - val_acc: 0.2714\n",
      "Epoch 148/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1639 - acc: 0.4762 - val_loss: 0.1874 - val_acc: 0.2628\n",
      "Epoch 149/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1634 - acc: 0.4603 - val_loss: 0.1872 - val_acc: 0.2521\n",
      "Epoch 150/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1633 - acc: 0.4921 - val_loss: 0.1873 - val_acc: 0.2692\n",
      "Epoch 151/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1628 - acc: 0.4762 - val_loss: 0.1869 - val_acc: 0.2628\n",
      "Epoch 152/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1624 - acc: 0.4603 - val_loss: 0.1870 - val_acc: 0.2671\n",
      "Epoch 153/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1625 - acc: 0.4762 - val_loss: 0.1871 - val_acc: 0.2714\n",
      "Epoch 154/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1618 - acc: 0.5079 - val_loss: 0.1865 - val_acc: 0.2650\n",
      "Epoch 155/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1617 - acc: 0.4921 - val_loss: 0.1870 - val_acc: 0.2671\n",
      "Epoch 156/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1612 - acc: 0.5238 - val_loss: 0.1868 - val_acc: 0.2778\n",
      "Epoch 157/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1610 - acc: 0.5238 - val_loss: 0.1866 - val_acc: 0.2735\n",
      "Epoch 158/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1607 - acc: 0.5238 - val_loss: 0.1868 - val_acc: 0.2821\n",
      "Epoch 159/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1604 - acc: 0.5079 - val_loss: 0.1863 - val_acc: 0.2756\n",
      "Epoch 160/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1597 - acc: 0.5079 - val_loss: 0.1860 - val_acc: 0.2714\n",
      "Epoch 161/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1596 - acc: 0.5079 - val_loss: 0.1859 - val_acc: 0.2842\n",
      "Epoch 162/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1593 - acc: 0.5079 - val_loss: 0.1859 - val_acc: 0.2906\n",
      "Epoch 163/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1591 - acc: 0.5079 - val_loss: 0.1858 - val_acc: 0.2906\n",
      "Epoch 164/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1587 - acc: 0.4762 - val_loss: 0.1857 - val_acc: 0.2885\n",
      "Epoch 165/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1584 - acc: 0.5079 - val_loss: 0.1855 - val_acc: 0.2863\n",
      "Epoch 166/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1584 - acc: 0.5079 - val_loss: 0.1858 - val_acc: 0.2821\n",
      "Epoch 167/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1581 - acc: 0.4762 - val_loss: 0.1851 - val_acc: 0.2799\n",
      "Epoch 168/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1573 - acc: 0.5079 - val_loss: 0.1852 - val_acc: 0.2885\n",
      "Epoch 169/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1572 - acc: 0.5238 - val_loss: 0.1849 - val_acc: 0.2970\n",
      "Epoch 170/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1571 - acc: 0.5079 - val_loss: 0.1848 - val_acc: 0.2970\n",
      "Epoch 171/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1566 - acc: 0.5079 - val_loss: 0.1852 - val_acc: 0.3013\n",
      "Epoch 172/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1565 - acc: 0.5238 - val_loss: 0.1851 - val_acc: 0.3077\n",
      "Epoch 173/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1562 - acc: 0.5079 - val_loss: 0.1847 - val_acc: 0.2991\n",
      "Epoch 174/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1557 - acc: 0.4921 - val_loss: 0.1843 - val_acc: 0.2991\n",
      "Epoch 175/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1552 - acc: 0.5079 - val_loss: 0.1846 - val_acc: 0.3013\n",
      "Epoch 176/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1553 - acc: 0.4921 - val_loss: 0.1846 - val_acc: 0.2991\n",
      "Epoch 177/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1553 - acc: 0.5238 - val_loss: 0.1841 - val_acc: 0.2970\n",
      "Epoch 178/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1547 - acc: 0.5079 - val_loss: 0.1839 - val_acc: 0.2991\n",
      "Epoch 179/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1544 - acc: 0.4921 - val_loss: 0.1838 - val_acc: 0.2970\n",
      "Epoch 180/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1541 - acc: 0.4921 - val_loss: 0.1837 - val_acc: 0.3013\n",
      "Epoch 181/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1539 - acc: 0.4762 - val_loss: 0.1839 - val_acc: 0.3034\n",
      "Epoch 182/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1534 - acc: 0.5238 - val_loss: 0.1838 - val_acc: 0.3056\n",
      "Epoch 183/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1539 - acc: 0.5238 - val_loss: 0.1837 - val_acc: 0.3098\n",
      "Epoch 184/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1532 - acc: 0.5079 - val_loss: 0.1830 - val_acc: 0.2842\n",
      "Epoch 185/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1531 - acc: 0.4762 - val_loss: 0.1836 - val_acc: 0.3077\n",
      "Epoch 186/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1521 - acc: 0.5079 - val_loss: 0.1831 - val_acc: 0.3034\n",
      "Epoch 187/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1522 - acc: 0.5079 - val_loss: 0.1828 - val_acc: 0.2991\n",
      "Epoch 188/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1520 - acc: 0.5079 - val_loss: 0.1832 - val_acc: 0.3141\n",
      "Epoch 189/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1513 - acc: 0.5079 - val_loss: 0.1828 - val_acc: 0.3056\n",
      "Epoch 190/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1516 - acc: 0.5238 - val_loss: 0.1830 - val_acc: 0.3120\n",
      "Epoch 191/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1509 - acc: 0.4762 - val_loss: 0.1819 - val_acc: 0.2821\n",
      "Epoch 192/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1505 - acc: 0.4921 - val_loss: 0.1826 - val_acc: 0.3056\n",
      "Epoch 193/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1501 - acc: 0.5397 - val_loss: 0.1828 - val_acc: 0.3162\n",
      "Epoch 194/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1500 - acc: 0.5397 - val_loss: 0.1825 - val_acc: 0.3077\n",
      "Epoch 195/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1496 - acc: 0.5397 - val_loss: 0.1818 - val_acc: 0.2991\n",
      "Epoch 196/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1494 - acc: 0.4921 - val_loss: 0.1816 - val_acc: 0.2991\n",
      "Epoch 197/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1490 - acc: 0.5238 - val_loss: 0.1818 - val_acc: 0.3056\n",
      "Epoch 198/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1487 - acc: 0.5079 - val_loss: 0.1819 - val_acc: 0.3077\n",
      "Epoch 199/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1486 - acc: 0.5397 - val_loss: 0.1819 - val_acc: 0.3120\n",
      "Epoch 200/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1480 - acc: 0.5238 - val_loss: 0.1817 - val_acc: 0.3077\n",
      "Epoch 201/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1476 - acc: 0.5397 - val_loss: 0.1820 - val_acc: 0.3162\n",
      "Epoch 202/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1474 - acc: 0.5397 - val_loss: 0.1813 - val_acc: 0.3077\n",
      "Epoch 203/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1469 - acc: 0.5397 - val_loss: 0.1809 - val_acc: 0.3056\n",
      "Epoch 204/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1466 - acc: 0.5397 - val_loss: 0.1815 - val_acc: 0.3141\n",
      "Epoch 205/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1469 - acc: 0.5238 - val_loss: 0.1812 - val_acc: 0.3098\n",
      "Epoch 206/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1459 - acc: 0.5556 - val_loss: 0.1811 - val_acc: 0.3162\n",
      "Epoch 207/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1462 - acc: 0.5397 - val_loss: 0.1812 - val_acc: 0.3141\n",
      "Epoch 208/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1451 - acc: 0.5556 - val_loss: 0.1811 - val_acc: 0.3120\n",
      "Epoch 209/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1451 - acc: 0.5556 - val_loss: 0.1805 - val_acc: 0.3120\n",
      "Epoch 210/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1447 - acc: 0.5556 - val_loss: 0.1802 - val_acc: 0.3077\n",
      "Epoch 211/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1445 - acc: 0.5556 - val_loss: 0.1800 - val_acc: 0.3098\n",
      "Epoch 212/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1446 - acc: 0.5556 - val_loss: 0.1804 - val_acc: 0.3141\n",
      "Epoch 213/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1435 - acc: 0.5556 - val_loss: 0.1803 - val_acc: 0.3120\n",
      "Epoch 214/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1438 - acc: 0.5556 - val_loss: 0.1800 - val_acc: 0.3120\n",
      "Epoch 215/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1435 - acc: 0.5397 - val_loss: 0.1802 - val_acc: 0.3162\n",
      "Epoch 216/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1432 - acc: 0.5556 - val_loss: 0.1803 - val_acc: 0.3141\n",
      "Epoch 217/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1426 - acc: 0.5556 - val_loss: 0.1799 - val_acc: 0.3120\n",
      "Epoch 218/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1424 - acc: 0.5556 - val_loss: 0.1798 - val_acc: 0.3141\n",
      "Epoch 219/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1422 - acc: 0.5556 - val_loss: 0.1797 - val_acc: 0.3141\n",
      "Epoch 220/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1414 - acc: 0.5556 - val_loss: 0.1795 - val_acc: 0.3162\n",
      "Epoch 221/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1414 - acc: 0.5397 - val_loss: 0.1790 - val_acc: 0.3141\n",
      "Epoch 222/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1409 - acc: 0.5556 - val_loss: 0.1795 - val_acc: 0.3120\n",
      "Epoch 223/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1405 - acc: 0.5556 - val_loss: 0.1796 - val_acc: 0.3141\n",
      "Epoch 224/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1406 - acc: 0.5556 - val_loss: 0.1789 - val_acc: 0.3141\n",
      "Epoch 225/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1400 - acc: 0.5556 - val_loss: 0.1788 - val_acc: 0.3162\n",
      "Epoch 226/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1400 - acc: 0.5556 - val_loss: 0.1789 - val_acc: 0.3141\n",
      "Epoch 227/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1393 - acc: 0.5556 - val_loss: 0.1786 - val_acc: 0.3141\n",
      "Epoch 228/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1388 - acc: 0.5556 - val_loss: 0.1783 - val_acc: 0.3141\n",
      "Epoch 229/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1390 - acc: 0.5556 - val_loss: 0.1785 - val_acc: 0.3162\n",
      "Epoch 230/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1383 - acc: 0.5556 - val_loss: 0.1787 - val_acc: 0.3120\n",
      "Epoch 231/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1381 - acc: 0.5556 - val_loss: 0.1784 - val_acc: 0.3162\n",
      "Epoch 232/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1380 - acc: 0.5556 - val_loss: 0.1784 - val_acc: 0.3162\n",
      "Epoch 233/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1373 - acc: 0.5556 - val_loss: 0.1775 - val_acc: 0.3120\n",
      "Epoch 234/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1373 - acc: 0.5556 - val_loss: 0.1780 - val_acc: 0.3162\n",
      "Epoch 235/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1372 - acc: 0.5556 - val_loss: 0.1777 - val_acc: 0.3141\n",
      "Epoch 236/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1367 - acc: 0.5556 - val_loss: 0.1777 - val_acc: 0.3162\n",
      "Epoch 237/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1362 - acc: 0.5556 - val_loss: 0.1778 - val_acc: 0.3141\n",
      "Epoch 238/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1362 - acc: 0.5556 - val_loss: 0.1773 - val_acc: 0.3120\n",
      "Epoch 239/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1356 - acc: 0.5556 - val_loss: 0.1776 - val_acc: 0.3162\n",
      "Epoch 240/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1355 - acc: 0.5556 - val_loss: 0.1775 - val_acc: 0.3162\n",
      "Epoch 241/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1347 - acc: 0.5556 - val_loss: 0.1775 - val_acc: 0.3162\n",
      "Epoch 242/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1346 - acc: 0.5556 - val_loss: 0.1771 - val_acc: 0.3162\n",
      "Epoch 243/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1341 - acc: 0.5556 - val_loss: 0.1776 - val_acc: 0.3098\n",
      "Epoch 244/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1339 - acc: 0.5556 - val_loss: 0.1773 - val_acc: 0.3120\n",
      "Epoch 245/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1335 - acc: 0.5556 - val_loss: 0.1772 - val_acc: 0.3141\n",
      "Epoch 246/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1332 - acc: 0.5556 - val_loss: 0.1774 - val_acc: 0.3120\n",
      "Epoch 247/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1332 - acc: 0.5556 - val_loss: 0.1771 - val_acc: 0.3120\n",
      "Epoch 248/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1329 - acc: 0.5556 - val_loss: 0.1771 - val_acc: 0.3120\n",
      "Epoch 249/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1321 - acc: 0.5556 - val_loss: 0.1770 - val_acc: 0.3098\n",
      "Epoch 250/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1320 - acc: 0.5556 - val_loss: 0.1768 - val_acc: 0.3120\n",
      "Epoch 251/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1319 - acc: 0.5556 - val_loss: 0.1767 - val_acc: 0.3098\n",
      "Epoch 252/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1315 - acc: 0.5556 - val_loss: 0.1770 - val_acc: 0.3077\n",
      "Epoch 253/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1308 - acc: 0.5556 - val_loss: 0.1766 - val_acc: 0.3120\n",
      "Epoch 254/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1305 - acc: 0.5556 - val_loss: 0.1766 - val_acc: 0.3120\n",
      "Epoch 255/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1311 - acc: 0.5556 - val_loss: 0.1762 - val_acc: 0.3141\n",
      "Epoch 256/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1304 - acc: 0.5556 - val_loss: 0.1763 - val_acc: 0.3141\n",
      "Epoch 257/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1298 - acc: 0.5556 - val_loss: 0.1758 - val_acc: 0.3120\n",
      "Epoch 258/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1295 - acc: 0.5556 - val_loss: 0.1762 - val_acc: 0.3141\n",
      "Epoch 259/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1290 - acc: 0.5556 - val_loss: 0.1762 - val_acc: 0.3120\n",
      "Epoch 260/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1289 - acc: 0.5556 - val_loss: 0.1753 - val_acc: 0.3120\n",
      "Epoch 261/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1284 - acc: 0.5556 - val_loss: 0.1756 - val_acc: 0.3077\n",
      "Epoch 262/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1282 - acc: 0.5556 - val_loss: 0.1760 - val_acc: 0.3120\n",
      "Epoch 263/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1280 - acc: 0.5556 - val_loss: 0.1757 - val_acc: 0.3120\n",
      "Epoch 264/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1274 - acc: 0.5873 - val_loss: 0.1761 - val_acc: 0.3056\n",
      "Epoch 265/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1273 - acc: 0.5556 - val_loss: 0.1756 - val_acc: 0.3013\n",
      "Epoch 266/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1271 - acc: 0.5873 - val_loss: 0.1757 - val_acc: 0.3034\n",
      "Epoch 267/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1270 - acc: 0.5873 - val_loss: 0.1751 - val_acc: 0.3098\n",
      "Epoch 268/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1268 - acc: 0.5714 - val_loss: 0.1759 - val_acc: 0.3056\n",
      "Epoch 269/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1261 - acc: 0.5873 - val_loss: 0.1752 - val_acc: 0.3120\n",
      "Epoch 270/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1260 - acc: 0.5714 - val_loss: 0.1746 - val_acc: 0.3098\n",
      "Epoch 271/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1255 - acc: 0.5556 - val_loss: 0.1746 - val_acc: 0.3120\n",
      "Epoch 272/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1251 - acc: 0.5556 - val_loss: 0.1749 - val_acc: 0.3141\n",
      "Epoch 273/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1248 - acc: 0.5873 - val_loss: 0.1747 - val_acc: 0.3098\n",
      "Epoch 274/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1246 - acc: 0.5873 - val_loss: 0.1747 - val_acc: 0.3098\n",
      "Epoch 275/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1243 - acc: 0.5714 - val_loss: 0.1742 - val_acc: 0.3120\n",
      "Epoch 276/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1239 - acc: 0.5873 - val_loss: 0.1742 - val_acc: 0.3162\n",
      "Epoch 277/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1237 - acc: 0.5873 - val_loss: 0.1740 - val_acc: 0.3120\n",
      "Epoch 278/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1236 - acc: 0.5714 - val_loss: 0.1741 - val_acc: 0.3120\n",
      "Epoch 279/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1233 - acc: 0.5714 - val_loss: 0.1739 - val_acc: 0.3141\n",
      "Epoch 280/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1230 - acc: 0.5873 - val_loss: 0.1744 - val_acc: 0.3056\n",
      "Epoch 281/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1224 - acc: 0.5873 - val_loss: 0.1739 - val_acc: 0.3120\n",
      "Epoch 282/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1224 - acc: 0.5873 - val_loss: 0.1736 - val_acc: 0.3141\n",
      "Epoch 283/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1221 - acc: 0.5873 - val_loss: 0.1739 - val_acc: 0.3120\n",
      "Epoch 284/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1221 - acc: 0.5873 - val_loss: 0.1734 - val_acc: 0.3120\n",
      "Epoch 285/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1214 - acc: 0.5873 - val_loss: 0.1734 - val_acc: 0.3120\n",
      "Epoch 286/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1211 - acc: 0.5873 - val_loss: 0.1734 - val_acc: 0.3141\n",
      "Epoch 287/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1212 - acc: 0.5873 - val_loss: 0.1733 - val_acc: 0.3120\n",
      "Epoch 288/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1208 - acc: 0.5873 - val_loss: 0.1731 - val_acc: 0.3141\n",
      "Epoch 289/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1207 - acc: 0.6032 - val_loss: 0.1728 - val_acc: 0.3098\n",
      "Epoch 290/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1203 - acc: 0.5873 - val_loss: 0.1725 - val_acc: 0.3120\n",
      "Epoch 291/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1202 - acc: 0.5873 - val_loss: 0.1725 - val_acc: 0.3120\n",
      "Epoch 292/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1195 - acc: 0.5873 - val_loss: 0.1726 - val_acc: 0.3141\n",
      "Epoch 293/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1195 - acc: 0.5873 - val_loss: 0.1728 - val_acc: 0.3077\n",
      "Epoch 294/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1189 - acc: 0.5873 - val_loss: 0.1724 - val_acc: 0.3141\n",
      "Epoch 295/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1193 - acc: 0.5873 - val_loss: 0.1720 - val_acc: 0.3141\n",
      "Epoch 296/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1189 - acc: 0.6032 - val_loss: 0.1726 - val_acc: 0.3056\n",
      "Epoch 297/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1182 - acc: 0.5873 - val_loss: 0.1718 - val_acc: 0.3141\n",
      "Epoch 298/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1181 - acc: 0.5873 - val_loss: 0.1720 - val_acc: 0.3162\n",
      "Epoch 299/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1183 - acc: 0.6032 - val_loss: 0.1723 - val_acc: 0.3056\n",
      "Epoch 300/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1175 - acc: 0.5873 - val_loss: 0.1715 - val_acc: 0.3120\n",
      "Epoch 301/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1175 - acc: 0.5873 - val_loss: 0.1711 - val_acc: 0.3162\n",
      "Epoch 302/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1170 - acc: 0.5873 - val_loss: 0.1711 - val_acc: 0.3162\n",
      "Epoch 303/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1169 - acc: 0.6032 - val_loss: 0.1714 - val_acc: 0.3162\n",
      "Epoch 304/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1163 - acc: 0.5873 - val_loss: 0.1713 - val_acc: 0.3184\n",
      "Epoch 305/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1163 - acc: 0.6032 - val_loss: 0.1714 - val_acc: 0.3141\n",
      "Epoch 306/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1157 - acc: 0.6032 - val_loss: 0.1715 - val_acc: 0.3141\n",
      "Epoch 307/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1151 - acc: 0.6032 - val_loss: 0.1714 - val_acc: 0.3141\n",
      "Epoch 308/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1146 - acc: 0.5873 - val_loss: 0.1702 - val_acc: 0.3205\n",
      "Epoch 309/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1140 - acc: 0.6032 - val_loss: 0.1706 - val_acc: 0.3184\n",
      "Epoch 310/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1135 - acc: 0.6032 - val_loss: 0.1706 - val_acc: 0.3333\n",
      "Epoch 311/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1134 - acc: 0.6190 - val_loss: 0.1702 - val_acc: 0.3397\n",
      "Epoch 312/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1122 - acc: 0.6508 - val_loss: 0.1701 - val_acc: 0.3483\n",
      "Epoch 313/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1118 - acc: 0.6349 - val_loss: 0.1696 - val_acc: 0.3654\n",
      "Epoch 314/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1114 - acc: 0.6508 - val_loss: 0.1698 - val_acc: 0.3611\n",
      "Epoch 315/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1112 - acc: 0.6667 - val_loss: 0.1698 - val_acc: 0.3590\n",
      "Epoch 316/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1110 - acc: 0.6349 - val_loss: 0.1691 - val_acc: 0.3654\n",
      "Epoch 317/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1105 - acc: 0.6667 - val_loss: 0.1697 - val_acc: 0.3526\n",
      "Epoch 318/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1103 - acc: 0.6667 - val_loss: 0.1696 - val_acc: 0.3547\n",
      "Epoch 319/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1096 - acc: 0.6667 - val_loss: 0.1694 - val_acc: 0.3590\n",
      "Epoch 320/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1093 - acc: 0.6508 - val_loss: 0.1689 - val_acc: 0.3547\n",
      "Epoch 321/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1086 - acc: 0.6508 - val_loss: 0.1683 - val_acc: 0.3654\n",
      "Epoch 322/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1086 - acc: 0.6508 - val_loss: 0.1682 - val_acc: 0.3654\n",
      "Epoch 323/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1081 - acc: 0.6508 - val_loss: 0.1682 - val_acc: 0.3568\n",
      "Epoch 324/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1076 - acc: 0.6508 - val_loss: 0.1682 - val_acc: 0.3590\n",
      "Epoch 325/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1076 - acc: 0.6508 - val_loss: 0.1685 - val_acc: 0.3547\n",
      "Epoch 326/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1072 - acc: 0.6508 - val_loss: 0.1682 - val_acc: 0.3590\n",
      "Epoch 327/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1070 - acc: 0.6508 - val_loss: 0.1676 - val_acc: 0.3654\n",
      "Epoch 328/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1066 - acc: 0.6667 - val_loss: 0.1679 - val_acc: 0.3568\n",
      "Epoch 329/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1065 - acc: 0.6508 - val_loss: 0.1679 - val_acc: 0.3526\n",
      "Epoch 330/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1058 - acc: 0.6667 - val_loss: 0.1679 - val_acc: 0.3632\n",
      "Epoch 331/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1057 - acc: 0.6667 - val_loss: 0.1679 - val_acc: 0.3590\n",
      "Epoch 332/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1051 - acc: 0.6667 - val_loss: 0.1677 - val_acc: 0.3632\n",
      "Epoch 333/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1050 - acc: 0.6508 - val_loss: 0.1670 - val_acc: 0.3632\n",
      "Epoch 334/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1050 - acc: 0.6825 - val_loss: 0.1671 - val_acc: 0.3547\n",
      "Epoch 335/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1042 - acc: 0.6508 - val_loss: 0.1666 - val_acc: 0.3590\n",
      "Epoch 336/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1039 - acc: 0.6667 - val_loss: 0.1669 - val_acc: 0.3590\n",
      "Epoch 337/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1035 - acc: 0.6667 - val_loss: 0.1669 - val_acc: 0.3568\n",
      "Epoch 338/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1035 - acc: 0.6667 - val_loss: 0.1661 - val_acc: 0.3590\n",
      "Epoch 339/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1031 - acc: 0.6508 - val_loss: 0.1668 - val_acc: 0.3568\n",
      "Epoch 340/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1027 - acc: 0.6667 - val_loss: 0.1655 - val_acc: 0.3675\n",
      "Epoch 341/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1023 - acc: 0.6667 - val_loss: 0.1662 - val_acc: 0.3611\n",
      "Epoch 342/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1022 - acc: 0.6825 - val_loss: 0.1657 - val_acc: 0.3590\n",
      "Epoch 343/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1017 - acc: 0.6667 - val_loss: 0.1657 - val_acc: 0.3632\n",
      "Epoch 344/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1015 - acc: 0.6508 - val_loss: 0.1649 - val_acc: 0.3568\n",
      "Epoch 345/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1014 - acc: 0.6667 - val_loss: 0.1646 - val_acc: 0.3611\n",
      "Epoch 346/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1005 - acc: 0.6667 - val_loss: 0.1643 - val_acc: 0.3632\n",
      "Epoch 347/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1012 - acc: 0.6667 - val_loss: 0.1637 - val_acc: 0.3632\n",
      "Epoch 348/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1003 - acc: 0.6508 - val_loss: 0.1629 - val_acc: 0.3654\n",
      "Epoch 349/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0999 - acc: 0.6667 - val_loss: 0.1630 - val_acc: 0.3590\n",
      "Epoch 350/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0993 - acc: 0.6667 - val_loss: 0.1617 - val_acc: 0.3697\n",
      "Epoch 351/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0991 - acc: 0.6667 - val_loss: 0.1595 - val_acc: 0.3697\n",
      "Epoch 352/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0988 - acc: 0.6667 - val_loss: 0.1572 - val_acc: 0.3718\n",
      "Epoch 353/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0981 - acc: 0.6667 - val_loss: 0.1561 - val_acc: 0.3654\n",
      "Epoch 354/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0978 - acc: 0.6667 - val_loss: 0.1531 - val_acc: 0.3761\n",
      "Epoch 355/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0972 - acc: 0.6825 - val_loss: 0.1513 - val_acc: 0.3889\n",
      "Epoch 356/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0968 - acc: 0.6667 - val_loss: 0.1487 - val_acc: 0.4081\n",
      "Epoch 357/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0962 - acc: 0.6825 - val_loss: 0.1465 - val_acc: 0.4466\n",
      "Epoch 358/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0964 - acc: 0.6984 - val_loss: 0.1436 - val_acc: 0.4808\n",
      "Epoch 359/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0958 - acc: 0.6825 - val_loss: 0.1459 - val_acc: 0.4423\n",
      "Epoch 360/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0954 - acc: 0.6825 - val_loss: 0.1436 - val_acc: 0.4786\n",
      "Epoch 361/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0951 - acc: 0.6825 - val_loss: 0.1410 - val_acc: 0.5150\n",
      "Epoch 362/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0952 - acc: 0.6825 - val_loss: 0.1414 - val_acc: 0.5000\n",
      "Epoch 363/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0949 - acc: 0.6825 - val_loss: 0.1419 - val_acc: 0.4872\n",
      "Epoch 364/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0945 - acc: 0.6825 - val_loss: 0.1421 - val_acc: 0.4893\n",
      "Epoch 365/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0938 - acc: 0.7143 - val_loss: 0.1380 - val_acc: 0.5342\n",
      "Epoch 366/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0936 - acc: 0.7143 - val_loss: 0.1387 - val_acc: 0.5235\n",
      "Epoch 367/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0929 - acc: 0.7143 - val_loss: 0.1395 - val_acc: 0.5107\n",
      "Epoch 368/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0932 - acc: 0.7619 - val_loss: 0.1370 - val_acc: 0.5385\n",
      "Epoch 369/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0925 - acc: 0.7302 - val_loss: 0.1378 - val_acc: 0.5256\n",
      "Epoch 370/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0919 - acc: 0.7460 - val_loss: 0.1373 - val_acc: 0.5363\n",
      "Epoch 371/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0918 - acc: 0.6984 - val_loss: 0.1394 - val_acc: 0.5021\n",
      "Epoch 372/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0914 - acc: 0.7143 - val_loss: 0.1378 - val_acc: 0.5235\n",
      "Epoch 373/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0911 - acc: 0.7302 - val_loss: 0.1377 - val_acc: 0.5256\n",
      "Epoch 374/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0908 - acc: 0.7460 - val_loss: 0.1340 - val_acc: 0.5556\n",
      "Epoch 375/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0903 - acc: 0.7460 - val_loss: 0.1361 - val_acc: 0.5427\n",
      "Epoch 376/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0900 - acc: 0.7460 - val_loss: 0.1340 - val_acc: 0.5577\n",
      "Epoch 377/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0901 - acc: 0.7143 - val_loss: 0.1328 - val_acc: 0.5705\n",
      "Epoch 378/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0895 - acc: 0.7619 - val_loss: 0.1343 - val_acc: 0.5470\n",
      "Epoch 379/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0898 - acc: 0.7460 - val_loss: 0.1316 - val_acc: 0.5641\n",
      "Epoch 380/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0892 - acc: 0.7460 - val_loss: 0.1328 - val_acc: 0.5598\n",
      "Epoch 381/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0894 - acc: 0.7143 - val_loss: 0.1345 - val_acc: 0.5449\n",
      "Epoch 382/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0884 - acc: 0.7619 - val_loss: 0.1317 - val_acc: 0.5598\n",
      "Epoch 383/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0883 - acc: 0.7778 - val_loss: 0.1309 - val_acc: 0.5726\n",
      "Epoch 384/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0878 - acc: 0.7460 - val_loss: 0.1323 - val_acc: 0.5556\n",
      "Epoch 385/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0879 - acc: 0.7460 - val_loss: 0.1310 - val_acc: 0.5684\n",
      "Epoch 386/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0869 - acc: 0.7460 - val_loss: 0.1306 - val_acc: 0.5726\n",
      "Epoch 387/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0871 - acc: 0.7460 - val_loss: 0.1306 - val_acc: 0.5769\n",
      "Epoch 388/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0869 - acc: 0.7460 - val_loss: 0.1301 - val_acc: 0.5662\n",
      "Epoch 389/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0865 - acc: 0.7778 - val_loss: 0.1299 - val_acc: 0.5662\n",
      "Epoch 390/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0865 - acc: 0.7460 - val_loss: 0.1290 - val_acc: 0.5748\n",
      "Epoch 391/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0863 - acc: 0.7460 - val_loss: 0.1301 - val_acc: 0.5662\n",
      "Epoch 392/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0859 - acc: 0.7619 - val_loss: 0.1304 - val_acc: 0.5620\n",
      "Epoch 393/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0855 - acc: 0.7460 - val_loss: 0.1287 - val_acc: 0.5748\n",
      "Epoch 394/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0855 - acc: 0.7460 - val_loss: 0.1282 - val_acc: 0.5726\n",
      "Epoch 395/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0853 - acc: 0.7619 - val_loss: 0.1285 - val_acc: 0.5748\n",
      "Epoch 396/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0847 - acc: 0.7460 - val_loss: 0.1279 - val_acc: 0.5705\n",
      "Epoch 397/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0844 - acc: 0.7460 - val_loss: 0.1293 - val_acc: 0.5641\n",
      "Epoch 398/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0844 - acc: 0.7460 - val_loss: 0.1266 - val_acc: 0.5769\n",
      "Epoch 399/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0840 - acc: 0.7460 - val_loss: 0.1251 - val_acc: 0.5876\n",
      "Epoch 400/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0846 - acc: 0.7778 - val_loss: 0.1264 - val_acc: 0.5791\n",
      "Epoch 401/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0836 - acc: 0.7460 - val_loss: 0.1240 - val_acc: 0.5855\n",
      "Epoch 402/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0837 - acc: 0.7619 - val_loss: 0.1252 - val_acc: 0.5919\n",
      "Epoch 403/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0831 - acc: 0.7460 - val_loss: 0.1241 - val_acc: 0.5855\n",
      "Epoch 404/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0834 - acc: 0.7619 - val_loss: 0.1238 - val_acc: 0.5919\n",
      "Epoch 405/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0831 - acc: 0.7778 - val_loss: 0.1250 - val_acc: 0.5684\n",
      "Epoch 406/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0827 - acc: 0.7460 - val_loss: 0.1246 - val_acc: 0.5769\n",
      "Epoch 407/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0824 - acc: 0.7460 - val_loss: 0.1231 - val_acc: 0.5876\n",
      "Epoch 408/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0821 - acc: 0.7460 - val_loss: 0.1265 - val_acc: 0.5620\n",
      "Epoch 409/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0817 - acc: 0.7619 - val_loss: 0.1228 - val_acc: 0.5897\n",
      "Epoch 410/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0816 - acc: 0.7619 - val_loss: 0.1219 - val_acc: 0.5983\n",
      "Epoch 411/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0812 - acc: 0.7460 - val_loss: 0.1229 - val_acc: 0.5897\n",
      "Epoch 412/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0809 - acc: 0.7619 - val_loss: 0.1229 - val_acc: 0.5833\n",
      "Epoch 413/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0807 - acc: 0.7778 - val_loss: 0.1223 - val_acc: 0.5940\n",
      "Epoch 414/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0803 - acc: 0.7619 - val_loss: 0.1236 - val_acc: 0.5876\n",
      "Epoch 415/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0805 - acc: 0.7460 - val_loss: 0.1220 - val_acc: 0.5897\n",
      "Epoch 416/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0799 - acc: 0.7619 - val_loss: 0.1228 - val_acc: 0.5812\n",
      "Epoch 417/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0798 - acc: 0.7778 - val_loss: 0.1213 - val_acc: 0.5897\n",
      "Epoch 418/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0797 - acc: 0.7460 - val_loss: 0.1217 - val_acc: 0.6090\n",
      "Epoch 419/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0798 - acc: 0.7619 - val_loss: 0.1221 - val_acc: 0.5855\n",
      "Epoch 420/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0793 - acc: 0.7778 - val_loss: 0.1217 - val_acc: 0.5855\n",
      "Epoch 421/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0793 - acc: 0.7778 - val_loss: 0.1186 - val_acc: 0.6154\n",
      "Epoch 422/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0791 - acc: 0.7778 - val_loss: 0.1197 - val_acc: 0.6068\n",
      "Epoch 423/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0789 - acc: 0.7619 - val_loss: 0.1205 - val_acc: 0.5876\n",
      "Epoch 424/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0788 - acc: 0.7778 - val_loss: 0.1213 - val_acc: 0.5791\n",
      "Epoch 425/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0780 - acc: 0.7778 - val_loss: 0.1183 - val_acc: 0.6047\n",
      "Epoch 426/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0780 - acc: 0.7619 - val_loss: 0.1184 - val_acc: 0.6111\n",
      "Epoch 427/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0775 - acc: 0.7778 - val_loss: 0.1184 - val_acc: 0.6154\n",
      "Epoch 428/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0779 - acc: 0.7619 - val_loss: 0.1197 - val_acc: 0.5919\n",
      "Epoch 429/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0779 - acc: 0.7619 - val_loss: 0.1177 - val_acc: 0.6175\n",
      "Epoch 430/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0773 - acc: 0.7778 - val_loss: 0.1196 - val_acc: 0.5962\n",
      "Epoch 431/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0771 - acc: 0.7778 - val_loss: 0.1168 - val_acc: 0.6154\n",
      "Epoch 432/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0771 - acc: 0.7460 - val_loss: 0.1184 - val_acc: 0.6090\n",
      "Epoch 433/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0772 - acc: 0.7460 - val_loss: 0.1174 - val_acc: 0.6175\n",
      "Epoch 434/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0764 - acc: 0.7778 - val_loss: 0.1184 - val_acc: 0.6047\n",
      "Epoch 435/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0773 - acc: 0.7619 - val_loss: 0.1155 - val_acc: 0.6132\n",
      "Epoch 436/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0762 - acc: 0.7778 - val_loss: 0.1153 - val_acc: 0.6154\n",
      "Epoch 437/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0771 - acc: 0.7778 - val_loss: 0.1172 - val_acc: 0.5962\n",
      "Epoch 438/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0757 - acc: 0.7778 - val_loss: 0.1171 - val_acc: 0.6090\n",
      "Epoch 439/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0755 - acc: 0.7619 - val_loss: 0.1138 - val_acc: 0.6218\n",
      "Epoch 440/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0753 - acc: 0.7619 - val_loss: 0.1159 - val_acc: 0.6090\n",
      "Epoch 441/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0758 - acc: 0.7619 - val_loss: 0.1160 - val_acc: 0.6004\n",
      "Epoch 442/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0748 - acc: 0.7778 - val_loss: 0.1165 - val_acc: 0.6111\n",
      "Epoch 443/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0749 - acc: 0.7778 - val_loss: 0.1141 - val_acc: 0.6154\n",
      "Epoch 444/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0748 - acc: 0.7778 - val_loss: 0.1144 - val_acc: 0.6090\n",
      "Epoch 445/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0746 - acc: 0.7778 - val_loss: 0.1126 - val_acc: 0.6261\n",
      "Epoch 446/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0753 - acc: 0.7460 - val_loss: 0.1153 - val_acc: 0.6111\n",
      "Epoch 447/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0740 - acc: 0.7778 - val_loss: 0.1130 - val_acc: 0.6197\n",
      "Epoch 448/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0741 - acc: 0.7778 - val_loss: 0.1146 - val_acc: 0.6004\n",
      "Epoch 449/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0740 - acc: 0.7778 - val_loss: 0.1108 - val_acc: 0.6261\n",
      "Epoch 450/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0739 - acc: 0.7778 - val_loss: 0.1133 - val_acc: 0.6175\n",
      "Epoch 451/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0740 - acc: 0.7778 - val_loss: 0.1152 - val_acc: 0.6090\n",
      "Epoch 452/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0732 - acc: 0.7778 - val_loss: 0.1134 - val_acc: 0.6197\n",
      "Epoch 453/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0732 - acc: 0.7778 - val_loss: 0.1129 - val_acc: 0.6111\n",
      "Epoch 454/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0741 - acc: 0.7778 - val_loss: 0.1128 - val_acc: 0.6132\n",
      "Epoch 455/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0729 - acc: 0.7619 - val_loss: 0.1129 - val_acc: 0.6154\n",
      "Epoch 456/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0727 - acc: 0.7778 - val_loss: 0.1118 - val_acc: 0.6197\n",
      "Epoch 457/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0723 - acc: 0.7778 - val_loss: 0.1134 - val_acc: 0.6239\n",
      "Epoch 458/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0722 - acc: 0.7778 - val_loss: 0.1137 - val_acc: 0.6154\n",
      "Epoch 459/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0723 - acc: 0.7778 - val_loss: 0.1100 - val_acc: 0.6239\n",
      "Epoch 460/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0718 - acc: 0.7778 - val_loss: 0.1106 - val_acc: 0.6239\n",
      "Epoch 461/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0717 - acc: 0.7778 - val_loss: 0.1108 - val_acc: 0.6239\n",
      "Epoch 462/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0717 - acc: 0.7778 - val_loss: 0.1103 - val_acc: 0.6218\n",
      "Epoch 463/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0714 - acc: 0.7778 - val_loss: 0.1113 - val_acc: 0.6175\n",
      "Epoch 464/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0712 - acc: 0.7778 - val_loss: 0.1125 - val_acc: 0.6090\n",
      "Epoch 465/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0711 - acc: 0.7778 - val_loss: 0.1103 - val_acc: 0.6261\n",
      "Epoch 466/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0710 - acc: 0.7778 - val_loss: 0.1092 - val_acc: 0.6239\n",
      "Epoch 467/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0707 - acc: 0.7778 - val_loss: 0.1099 - val_acc: 0.6197\n",
      "Epoch 468/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0710 - acc: 0.7778 - val_loss: 0.1089 - val_acc: 0.6239\n",
      "Epoch 469/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0704 - acc: 0.7778 - val_loss: 0.1089 - val_acc: 0.6218\n",
      "Epoch 470/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0707 - acc: 0.7619 - val_loss: 0.1083 - val_acc: 0.6239\n",
      "Epoch 471/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0702 - acc: 0.7778 - val_loss: 0.1079 - val_acc: 0.6239\n",
      "Epoch 472/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0701 - acc: 0.7778 - val_loss: 0.1098 - val_acc: 0.6154\n",
      "Epoch 473/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0700 - acc: 0.7778 - val_loss: 0.1099 - val_acc: 0.6175\n",
      "Epoch 474/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0697 - acc: 0.7778 - val_loss: 0.1087 - val_acc: 0.6175\n",
      "Epoch 475/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0695 - acc: 0.7778 - val_loss: 0.1094 - val_acc: 0.6197\n",
      "Epoch 476/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0695 - acc: 0.7778 - val_loss: 0.1080 - val_acc: 0.6261\n",
      "Epoch 477/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0693 - acc: 0.7778 - val_loss: 0.1086 - val_acc: 0.6218\n",
      "Epoch 478/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0693 - acc: 0.7778 - val_loss: 0.1090 - val_acc: 0.6239\n",
      "Epoch 479/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0691 - acc: 0.7778 - val_loss: 0.1076 - val_acc: 0.6261\n",
      "Epoch 480/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0692 - acc: 0.7778 - val_loss: 0.1076 - val_acc: 0.6154\n",
      "Epoch 481/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0688 - acc: 0.7778 - val_loss: 0.1059 - val_acc: 0.6303\n",
      "Epoch 482/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0688 - acc: 0.7778 - val_loss: 0.1074 - val_acc: 0.6239\n",
      "Epoch 483/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0685 - acc: 0.7778 - val_loss: 0.1082 - val_acc: 0.6239\n",
      "Epoch 484/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0684 - acc: 0.7778 - val_loss: 0.1071 - val_acc: 0.6239\n",
      "Epoch 485/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0685 - acc: 0.7778 - val_loss: 0.1060 - val_acc: 0.6282\n",
      "Epoch 486/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0679 - acc: 0.7778 - val_loss: 0.1062 - val_acc: 0.6261\n",
      "Epoch 487/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0679 - acc: 0.7778 - val_loss: 0.1053 - val_acc: 0.6325\n",
      "Epoch 488/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0681 - acc: 0.7778 - val_loss: 0.1051 - val_acc: 0.6239\n",
      "Epoch 489/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0678 - acc: 0.7778 - val_loss: 0.1043 - val_acc: 0.6303\n",
      "Epoch 490/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0678 - acc: 0.7778 - val_loss: 0.1056 - val_acc: 0.6239\n",
      "Epoch 491/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0677 - acc: 0.7778 - val_loss: 0.1065 - val_acc: 0.6197\n",
      "Epoch 492/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0671 - acc: 0.7778 - val_loss: 0.1041 - val_acc: 0.6303\n",
      "Epoch 493/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0675 - acc: 0.7778 - val_loss: 0.1047 - val_acc: 0.6303\n",
      "Epoch 494/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0669 - acc: 0.7778 - val_loss: 0.1040 - val_acc: 0.6239\n",
      "Epoch 495/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0666 - acc: 0.7778 - val_loss: 0.1045 - val_acc: 0.6282\n",
      "Epoch 496/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0668 - acc: 0.7778 - val_loss: 0.1040 - val_acc: 0.6261\n",
      "Epoch 497/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0665 - acc: 0.7778 - val_loss: 0.1024 - val_acc: 0.6325\n",
      "Epoch 498/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0665 - acc: 0.7778 - val_loss: 0.1025 - val_acc: 0.6303\n",
      "Epoch 499/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0663 - acc: 0.7778 - val_loss: 0.1033 - val_acc: 0.6325\n",
      "Epoch 500/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0663 - acc: 0.7778 - val_loss: 0.1043 - val_acc: 0.6239\n",
      "Epoch 501/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0663 - acc: 0.7778 - val_loss: 0.1037 - val_acc: 0.6282\n",
      "Epoch 502/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0659 - acc: 0.7778 - val_loss: 0.1047 - val_acc: 0.6197\n",
      "Epoch 503/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0660 - acc: 0.7778 - val_loss: 0.1027 - val_acc: 0.6325\n",
      "Epoch 504/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0654 - acc: 0.7778 - val_loss: 0.1044 - val_acc: 0.6197\n",
      "Epoch 505/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0659 - acc: 0.7937 - val_loss: 0.1031 - val_acc: 0.6261\n",
      "Epoch 506/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0654 - acc: 0.7778 - val_loss: 0.1022 - val_acc: 0.6346\n",
      "Epoch 507/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0654 - acc: 0.7778 - val_loss: 0.1041 - val_acc: 0.6218\n",
      "Epoch 508/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0649 - acc: 0.7778 - val_loss: 0.1032 - val_acc: 0.6261\n",
      "Epoch 509/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0649 - acc: 0.7778 - val_loss: 0.1026 - val_acc: 0.6303\n",
      "Epoch 510/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0648 - acc: 0.7778 - val_loss: 0.1024 - val_acc: 0.6282\n",
      "Epoch 511/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0647 - acc: 0.7778 - val_loss: 0.1028 - val_acc: 0.6239\n",
      "Epoch 512/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0643 - acc: 0.7778 - val_loss: 0.1036 - val_acc: 0.6218\n",
      "Epoch 513/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0646 - acc: 0.7778 - val_loss: 0.1013 - val_acc: 0.6303\n",
      "Epoch 514/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0644 - acc: 0.7778 - val_loss: 0.1012 - val_acc: 0.6282\n",
      "Epoch 515/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0642 - acc: 0.7778 - val_loss: 0.1014 - val_acc: 0.6282\n",
      "Epoch 516/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0644 - acc: 0.7778 - val_loss: 0.1003 - val_acc: 0.6325\n",
      "Epoch 517/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0645 - acc: 0.7778 - val_loss: 0.1009 - val_acc: 0.6282\n",
      "Epoch 518/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0639 - acc: 0.7778 - val_loss: 0.0997 - val_acc: 0.6303\n",
      "Epoch 519/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0639 - acc: 0.7937 - val_loss: 0.1023 - val_acc: 0.6239\n",
      "Epoch 520/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0639 - acc: 0.7778 - val_loss: 0.1002 - val_acc: 0.6261\n",
      "Epoch 521/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0634 - acc: 0.7778 - val_loss: 0.1004 - val_acc: 0.6282\n",
      "Epoch 522/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0634 - acc: 0.7778 - val_loss: 0.1003 - val_acc: 0.6303\n",
      "Epoch 523/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0634 - acc: 0.7778 - val_loss: 0.1017 - val_acc: 0.6239\n",
      "Epoch 524/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0628 - acc: 0.7778 - val_loss: 0.1008 - val_acc: 0.6282\n",
      "Epoch 525/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0628 - acc: 0.7778 - val_loss: 0.1003 - val_acc: 0.6261\n",
      "Epoch 526/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0625 - acc: 0.7937 - val_loss: 0.0996 - val_acc: 0.6303\n",
      "Epoch 527/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0625 - acc: 0.7778 - val_loss: 0.1004 - val_acc: 0.6325\n",
      "Epoch 528/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0626 - acc: 0.7937 - val_loss: 0.1001 - val_acc: 0.6346\n",
      "Epoch 529/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0624 - acc: 0.7937 - val_loss: 0.0998 - val_acc: 0.6261\n",
      "Epoch 530/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0619 - acc: 0.7778 - val_loss: 0.0982 - val_acc: 0.6325\n",
      "Epoch 531/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0620 - acc: 0.7937 - val_loss: 0.0984 - val_acc: 0.6303\n",
      "Epoch 532/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0621 - acc: 0.8095 - val_loss: 0.0987 - val_acc: 0.6303\n",
      "Epoch 533/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0617 - acc: 0.7778 - val_loss: 0.0978 - val_acc: 0.6368\n",
      "Epoch 534/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0615 - acc: 0.8095 - val_loss: 0.0976 - val_acc: 0.6410\n",
      "Epoch 535/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0616 - acc: 0.7937 - val_loss: 0.0991 - val_acc: 0.6325\n",
      "Epoch 536/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0627 - acc: 0.7937 - val_loss: 0.0985 - val_acc: 0.6282\n",
      "Epoch 537/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0616 - acc: 0.8095 - val_loss: 0.0970 - val_acc: 0.6432\n",
      "Epoch 538/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0615 - acc: 0.8095 - val_loss: 0.0982 - val_acc: 0.6389\n",
      "Epoch 539/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0611 - acc: 0.7937 - val_loss: 0.0970 - val_acc: 0.6432\n",
      "Epoch 540/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0607 - acc: 0.8095 - val_loss: 0.0970 - val_acc: 0.6474\n",
      "Epoch 541/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0606 - acc: 0.8095 - val_loss: 0.0978 - val_acc: 0.6410\n",
      "Epoch 542/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0607 - acc: 0.7937 - val_loss: 0.0984 - val_acc: 0.6325\n",
      "Epoch 543/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0603 - acc: 0.8095 - val_loss: 0.0972 - val_acc: 0.6453\n",
      "Epoch 544/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0604 - acc: 0.8095 - val_loss: 0.0972 - val_acc: 0.6474\n",
      "Epoch 545/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0602 - acc: 0.8095 - val_loss: 0.0985 - val_acc: 0.6389\n",
      "Epoch 546/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0600 - acc: 0.8095 - val_loss: 0.0953 - val_acc: 0.6603\n",
      "Epoch 547/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0599 - acc: 0.8095 - val_loss: 0.0950 - val_acc: 0.6603\n",
      "Epoch 548/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0599 - acc: 0.8095 - val_loss: 0.0957 - val_acc: 0.6496\n",
      "Epoch 549/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0600 - acc: 0.8095 - val_loss: 0.0960 - val_acc: 0.6581\n",
      "Epoch 550/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0597 - acc: 0.8095 - val_loss: 0.0955 - val_acc: 0.6603\n",
      "Epoch 551/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0596 - acc: 0.8095 - val_loss: 0.0955 - val_acc: 0.6581\n",
      "Epoch 552/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0593 - acc: 0.8095 - val_loss: 0.0955 - val_acc: 0.6538\n",
      "Epoch 553/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0590 - acc: 0.8095 - val_loss: 0.0959 - val_acc: 0.6538\n",
      "Epoch 554/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0591 - acc: 0.8095 - val_loss: 0.0941 - val_acc: 0.6581\n",
      "Epoch 555/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0591 - acc: 0.8095 - val_loss: 0.0931 - val_acc: 0.6774\n",
      "Epoch 556/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0587 - acc: 0.8095 - val_loss: 0.0946 - val_acc: 0.6624\n",
      "Epoch 557/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0589 - acc: 0.8095 - val_loss: 0.0946 - val_acc: 0.6645\n",
      "Epoch 558/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0586 - acc: 0.8095 - val_loss: 0.0937 - val_acc: 0.6709\n",
      "Epoch 559/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0581 - acc: 0.8095 - val_loss: 0.0944 - val_acc: 0.6603\n",
      "Epoch 560/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0586 - acc: 0.8095 - val_loss: 0.0946 - val_acc: 0.6581\n",
      "Epoch 561/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0584 - acc: 0.8095 - val_loss: 0.0949 - val_acc: 0.6560\n",
      "Epoch 562/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0583 - acc: 0.8095 - val_loss: 0.0949 - val_acc: 0.6624\n",
      "Epoch 563/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0578 - acc: 0.8095 - val_loss: 0.0935 - val_acc: 0.6624\n",
      "Epoch 564/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0579 - acc: 0.8095 - val_loss: 0.0947 - val_acc: 0.6581\n",
      "Epoch 565/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0576 - acc: 0.8095 - val_loss: 0.0933 - val_acc: 0.6667\n",
      "Epoch 566/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0579 - acc: 0.8095 - val_loss: 0.0921 - val_acc: 0.6688\n",
      "Epoch 567/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0576 - acc: 0.7937 - val_loss: 0.0921 - val_acc: 0.6752\n",
      "Epoch 568/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0578 - acc: 0.8095 - val_loss: 0.0920 - val_acc: 0.6731\n",
      "Epoch 569/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0573 - acc: 0.8095 - val_loss: 0.0931 - val_acc: 0.6667\n",
      "Epoch 570/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0575 - acc: 0.8095 - val_loss: 0.0912 - val_acc: 0.6731\n",
      "Epoch 571/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0570 - acc: 0.8095 - val_loss: 0.0930 - val_acc: 0.6667\n",
      "Epoch 572/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0567 - acc: 0.8095 - val_loss: 0.0921 - val_acc: 0.6709\n",
      "Epoch 573/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0571 - acc: 0.8095 - val_loss: 0.0919 - val_acc: 0.6709\n",
      "Epoch 574/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0569 - acc: 0.8095 - val_loss: 0.0920 - val_acc: 0.6667\n",
      "Epoch 575/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0571 - acc: 0.8095 - val_loss: 0.0924 - val_acc: 0.6688\n",
      "Epoch 576/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0565 - acc: 0.8095 - val_loss: 0.0923 - val_acc: 0.6688\n",
      "Epoch 577/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0562 - acc: 0.8095 - val_loss: 0.0937 - val_acc: 0.6538\n",
      "Epoch 578/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0562 - acc: 0.8095 - val_loss: 0.0923 - val_acc: 0.6624\n",
      "Epoch 579/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0560 - acc: 0.8095 - val_loss: 0.0919 - val_acc: 0.6688\n",
      "Epoch 580/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0562 - acc: 0.8095 - val_loss: 0.0930 - val_acc: 0.6517\n",
      "Epoch 581/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0563 - acc: 0.8095 - val_loss: 0.0912 - val_acc: 0.6731\n",
      "Epoch 582/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0559 - acc: 0.8095 - val_loss: 0.0924 - val_acc: 0.6581\n",
      "Epoch 583/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0560 - acc: 0.8095 - val_loss: 0.0914 - val_acc: 0.6667\n",
      "Epoch 584/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0556 - acc: 0.8095 - val_loss: 0.0915 - val_acc: 0.6688\n",
      "Epoch 585/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0559 - acc: 0.8095 - val_loss: 0.0903 - val_acc: 0.6752\n",
      "Epoch 586/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0555 - acc: 0.8095 - val_loss: 0.0909 - val_acc: 0.6731\n",
      "Epoch 587/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0554 - acc: 0.8095 - val_loss: 0.0906 - val_acc: 0.6709\n",
      "Epoch 588/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0555 - acc: 0.8095 - val_loss: 0.0896 - val_acc: 0.6752\n",
      "Epoch 589/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0553 - acc: 0.8095 - val_loss: 0.0899 - val_acc: 0.6731\n",
      "Epoch 590/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0550 - acc: 0.8095 - val_loss: 0.0918 - val_acc: 0.6560\n",
      "Epoch 591/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0551 - acc: 0.8095 - val_loss: 0.0898 - val_acc: 0.6731\n",
      "Epoch 592/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0549 - acc: 0.8095 - val_loss: 0.0907 - val_acc: 0.6731\n",
      "Epoch 593/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0547 - acc: 0.8095 - val_loss: 0.0892 - val_acc: 0.6752\n",
      "Epoch 594/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0549 - acc: 0.8095 - val_loss: 0.0882 - val_acc: 0.6774\n",
      "Epoch 595/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0548 - acc: 0.8095 - val_loss: 0.0888 - val_acc: 0.6795\n",
      "Epoch 596/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0544 - acc: 0.8095 - val_loss: 0.0907 - val_acc: 0.6667\n",
      "Epoch 597/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0544 - acc: 0.8095 - val_loss: 0.0908 - val_acc: 0.6603\n",
      "Epoch 598/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0542 - acc: 0.8095 - val_loss: 0.0902 - val_acc: 0.6688\n",
      "Epoch 599/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0543 - acc: 0.8095 - val_loss: 0.0910 - val_acc: 0.6603\n",
      "Epoch 600/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0544 - acc: 0.8095 - val_loss: 0.0903 - val_acc: 0.6667\n",
      "Epoch 601/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0545 - acc: 0.8095 - val_loss: 0.0889 - val_acc: 0.6752\n",
      "Epoch 602/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0539 - acc: 0.8095 - val_loss: 0.0889 - val_acc: 0.6752\n",
      "Epoch 603/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0538 - acc: 0.8095 - val_loss: 0.0884 - val_acc: 0.6774\n",
      "Epoch 604/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0538 - acc: 0.8095 - val_loss: 0.0882 - val_acc: 0.6795\n",
      "Epoch 605/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0541 - acc: 0.8095 - val_loss: 0.0904 - val_acc: 0.6667\n",
      "Epoch 606/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0537 - acc: 0.8095 - val_loss: 0.0874 - val_acc: 0.6838\n",
      "Epoch 607/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0535 - acc: 0.8095 - val_loss: 0.0878 - val_acc: 0.6859\n",
      "Epoch 608/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0537 - acc: 0.8095 - val_loss: 0.0880 - val_acc: 0.6795\n",
      "Epoch 609/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0532 - acc: 0.8095 - val_loss: 0.0881 - val_acc: 0.6795\n",
      "Epoch 610/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0531 - acc: 0.8095 - val_loss: 0.0880 - val_acc: 0.6774\n",
      "Epoch 611/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0529 - acc: 0.8095 - val_loss: 0.0878 - val_acc: 0.6859\n",
      "Epoch 612/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0528 - acc: 0.8095 - val_loss: 0.0883 - val_acc: 0.6774\n",
      "Epoch 613/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0529 - acc: 0.8095 - val_loss: 0.0883 - val_acc: 0.6795\n",
      "Epoch 614/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0529 - acc: 0.8095 - val_loss: 0.0876 - val_acc: 0.6816\n",
      "Epoch 615/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0527 - acc: 0.8095 - val_loss: 0.0883 - val_acc: 0.6752\n",
      "Epoch 616/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0528 - acc: 0.8095 - val_loss: 0.0874 - val_acc: 0.6816\n",
      "Epoch 617/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0528 - acc: 0.8095 - val_loss: 0.0877 - val_acc: 0.6752\n",
      "Epoch 618/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0526 - acc: 0.8095 - val_loss: 0.0873 - val_acc: 0.6816\n",
      "Epoch 619/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0525 - acc: 0.8095 - val_loss: 0.0875 - val_acc: 0.6795\n",
      "Epoch 620/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0523 - acc: 0.8095 - val_loss: 0.0882 - val_acc: 0.6795\n",
      "Epoch 621/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0524 - acc: 0.8095 - val_loss: 0.0881 - val_acc: 0.6752\n",
      "Epoch 622/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0521 - acc: 0.8095 - val_loss: 0.0872 - val_acc: 0.6838\n",
      "Epoch 623/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0523 - acc: 0.8095 - val_loss: 0.0883 - val_acc: 0.6752\n",
      "Epoch 624/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0522 - acc: 0.8095 - val_loss: 0.0873 - val_acc: 0.6795\n",
      "Epoch 625/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0521 - acc: 0.8095 - val_loss: 0.0859 - val_acc: 0.6880\n",
      "Epoch 626/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0521 - acc: 0.8095 - val_loss: 0.0851 - val_acc: 0.6902\n",
      "Epoch 627/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0520 - acc: 0.8095 - val_loss: 0.0856 - val_acc: 0.6902\n",
      "Epoch 628/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0516 - acc: 0.8095 - val_loss: 0.0860 - val_acc: 0.6859\n",
      "Epoch 629/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0517 - acc: 0.8095 - val_loss: 0.0867 - val_acc: 0.6859\n",
      "Epoch 630/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0515 - acc: 0.8095 - val_loss: 0.0859 - val_acc: 0.6859\n",
      "Epoch 631/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0515 - acc: 0.8095 - val_loss: 0.0861 - val_acc: 0.6859\n",
      "Epoch 632/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0515 - acc: 0.8095 - val_loss: 0.0872 - val_acc: 0.6795\n",
      "Epoch 633/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0513 - acc: 0.8095 - val_loss: 0.0863 - val_acc: 0.6816\n",
      "Epoch 634/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0513 - acc: 0.8095 - val_loss: 0.0862 - val_acc: 0.6880\n",
      "Epoch 635/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0514 - acc: 0.8095 - val_loss: 0.0853 - val_acc: 0.6838\n",
      "Epoch 636/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0512 - acc: 0.8095 - val_loss: 0.0857 - val_acc: 0.6880\n",
      "Epoch 637/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0511 - acc: 0.8095 - val_loss: 0.0858 - val_acc: 0.6880\n",
      "Epoch 638/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0509 - acc: 0.8095 - val_loss: 0.0856 - val_acc: 0.6923\n",
      "Epoch 639/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0510 - acc: 0.8095 - val_loss: 0.0866 - val_acc: 0.6688\n",
      "Epoch 640/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0508 - acc: 0.8095 - val_loss: 0.0842 - val_acc: 0.6902\n",
      "Epoch 641/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0507 - acc: 0.8095 - val_loss: 0.0856 - val_acc: 0.6859\n",
      "Epoch 642/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0507 - acc: 0.8095 - val_loss: 0.0843 - val_acc: 0.6923\n",
      "Epoch 643/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0506 - acc: 0.8095 - val_loss: 0.0850 - val_acc: 0.6880\n",
      "Epoch 644/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0507 - acc: 0.8095 - val_loss: 0.0856 - val_acc: 0.6795\n",
      "Epoch 645/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0504 - acc: 0.8095 - val_loss: 0.0851 - val_acc: 0.6859\n",
      "Epoch 646/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0505 - acc: 0.8095 - val_loss: 0.0844 - val_acc: 0.6923\n",
      "Epoch 647/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0507 - acc: 0.8095 - val_loss: 0.0837 - val_acc: 0.6902\n",
      "Epoch 648/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0505 - acc: 0.8095 - val_loss: 0.0842 - val_acc: 0.6923\n",
      "Epoch 649/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0504 - acc: 0.8095 - val_loss: 0.0860 - val_acc: 0.6816\n",
      "Epoch 650/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0505 - acc: 0.8095 - val_loss: 0.0846 - val_acc: 0.6816\n",
      "Epoch 651/2000\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0502 - acc: 0.8095 - val_loss: 0.0835 - val_acc: 0.6923\n",
      "Epoch 652/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0500 - acc: 0.8095 - val_loss: 0.0829 - val_acc: 0.6944\n",
      "Epoch 653/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0500 - acc: 0.8095 - val_loss: 0.0824 - val_acc: 0.6923\n",
      "Epoch 654/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0500 - acc: 0.8095 - val_loss: 0.0837 - val_acc: 0.6902\n",
      "Epoch 655/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0500 - acc: 0.8095 - val_loss: 0.0835 - val_acc: 0.6944\n",
      "Epoch 656/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0498 - acc: 0.8095 - val_loss: 0.0834 - val_acc: 0.6923\n",
      "Epoch 657/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0496 - acc: 0.8095 - val_loss: 0.0841 - val_acc: 0.6923\n",
      "Epoch 658/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0497 - acc: 0.8095 - val_loss: 0.0825 - val_acc: 0.6987\n",
      "Epoch 659/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0494 - acc: 0.8095 - val_loss: 0.0835 - val_acc: 0.6923\n",
      "Epoch 660/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0496 - acc: 0.8095 - val_loss: 0.0846 - val_acc: 0.6859\n",
      "Epoch 661/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0496 - acc: 0.8095 - val_loss: 0.0849 - val_acc: 0.6731\n",
      "Epoch 662/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0492 - acc: 0.8095 - val_loss: 0.0839 - val_acc: 0.6944\n",
      "Epoch 663/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0494 - acc: 0.8095 - val_loss: 0.0850 - val_acc: 0.6731\n",
      "Epoch 664/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0494 - acc: 0.8095 - val_loss: 0.0831 - val_acc: 0.6944\n",
      "Epoch 665/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0495 - acc: 0.8095 - val_loss: 0.0821 - val_acc: 0.6944\n",
      "Epoch 666/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0491 - acc: 0.8095 - val_loss: 0.0826 - val_acc: 0.6880\n",
      "Epoch 667/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0491 - acc: 0.8095 - val_loss: 0.0822 - val_acc: 0.6966\n",
      "Epoch 668/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0491 - acc: 0.8095 - val_loss: 0.0816 - val_acc: 0.6944\n",
      "Epoch 669/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0489 - acc: 0.8095 - val_loss: 0.0823 - val_acc: 0.6923\n",
      "Epoch 670/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0489 - acc: 0.8095 - val_loss: 0.0817 - val_acc: 0.6944\n",
      "Epoch 671/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0488 - acc: 0.8095 - val_loss: 0.0810 - val_acc: 0.7009\n",
      "Epoch 672/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0488 - acc: 0.8095 - val_loss: 0.0828 - val_acc: 0.6923\n",
      "Epoch 673/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0487 - acc: 0.8095 - val_loss: 0.0817 - val_acc: 0.6944\n",
      "Epoch 674/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0487 - acc: 0.8095 - val_loss: 0.0815 - val_acc: 0.7051\n",
      "Epoch 675/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0487 - acc: 0.8095 - val_loss: 0.0817 - val_acc: 0.6944\n",
      "Epoch 676/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0485 - acc: 0.8095 - val_loss: 0.0817 - val_acc: 0.6944\n",
      "Epoch 677/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0485 - acc: 0.8095 - val_loss: 0.0830 - val_acc: 0.6923\n",
      "Epoch 678/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0484 - acc: 0.8095 - val_loss: 0.0801 - val_acc: 0.7073\n",
      "Epoch 679/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0484 - acc: 0.8095 - val_loss: 0.0809 - val_acc: 0.6966\n",
      "Epoch 680/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0484 - acc: 0.8095 - val_loss: 0.0808 - val_acc: 0.6966\n",
      "Epoch 681/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0485 - acc: 0.8095 - val_loss: 0.0809 - val_acc: 0.7030\n",
      "Epoch 682/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0483 - acc: 0.8095 - val_loss: 0.0797 - val_acc: 0.7073\n",
      "Epoch 683/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0481 - acc: 0.8095 - val_loss: 0.0787 - val_acc: 0.7073\n",
      "Epoch 684/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0481 - acc: 0.8095 - val_loss: 0.0821 - val_acc: 0.6944\n",
      "Epoch 685/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0483 - acc: 0.8095 - val_loss: 0.0805 - val_acc: 0.7051\n",
      "Epoch 686/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0479 - acc: 0.8095 - val_loss: 0.0818 - val_acc: 0.6944\n",
      "Epoch 687/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0481 - acc: 0.8095 - val_loss: 0.0801 - val_acc: 0.7051\n",
      "Epoch 688/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0481 - acc: 0.8095 - val_loss: 0.0813 - val_acc: 0.6944\n",
      "Epoch 689/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0478 - acc: 0.8095 - val_loss: 0.0809 - val_acc: 0.6966\n",
      "Epoch 690/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0480 - acc: 0.8095 - val_loss: 0.0804 - val_acc: 0.7073\n",
      "Epoch 691/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0478 - acc: 0.8095 - val_loss: 0.0783 - val_acc: 0.7094\n",
      "Epoch 692/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0476 - acc: 0.8095 - val_loss: 0.0800 - val_acc: 0.6987\n",
      "Epoch 693/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0476 - acc: 0.8095 - val_loss: 0.0804 - val_acc: 0.7030\n",
      "Epoch 694/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0476 - acc: 0.8095 - val_loss: 0.0807 - val_acc: 0.6923\n",
      "Epoch 695/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0476 - acc: 0.8095 - val_loss: 0.0808 - val_acc: 0.6923\n",
      "Epoch 696/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0475 - acc: 0.8095 - val_loss: 0.0798 - val_acc: 0.7051\n",
      "Epoch 697/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0476 - acc: 0.8095 - val_loss: 0.0789 - val_acc: 0.7030\n",
      "Epoch 698/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0473 - acc: 0.8095 - val_loss: 0.0798 - val_acc: 0.7051\n",
      "Epoch 699/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0473 - acc: 0.8095 - val_loss: 0.0798 - val_acc: 0.7030\n",
      "Epoch 700/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0473 - acc: 0.8095 - val_loss: 0.0806 - val_acc: 0.7009\n",
      "Epoch 701/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0471 - acc: 0.8095 - val_loss: 0.0796 - val_acc: 0.7094\n",
      "Epoch 702/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0472 - acc: 0.8095 - val_loss: 0.0794 - val_acc: 0.7030\n",
      "Epoch 703/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0471 - acc: 0.8095 - val_loss: 0.0790 - val_acc: 0.7051\n",
      "Epoch 704/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0472 - acc: 0.8095 - val_loss: 0.0805 - val_acc: 0.7051\n",
      "Epoch 705/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0471 - acc: 0.8095 - val_loss: 0.0797 - val_acc: 0.7009\n",
      "Epoch 706/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0470 - acc: 0.8095 - val_loss: 0.0796 - val_acc: 0.7030\n",
      "Epoch 707/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0469 - acc: 0.8095 - val_loss: 0.0795 - val_acc: 0.7094\n",
      "Epoch 708/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0469 - acc: 0.8095 - val_loss: 0.0786 - val_acc: 0.7115\n",
      "Epoch 709/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0470 - acc: 0.8095 - val_loss: 0.0790 - val_acc: 0.7094\n",
      "Epoch 710/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0469 - acc: 0.8095 - val_loss: 0.0784 - val_acc: 0.7073\n",
      "Epoch 711/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0469 - acc: 0.8095 - val_loss: 0.0790 - val_acc: 0.7030\n",
      "Epoch 712/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0468 - acc: 0.8095 - val_loss: 0.0788 - val_acc: 0.7030\n",
      "Epoch 713/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0466 - acc: 0.8095 - val_loss: 0.0778 - val_acc: 0.7073\n",
      "Epoch 714/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0466 - acc: 0.8095 - val_loss: 0.0777 - val_acc: 0.7073\n",
      "Epoch 715/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0467 - acc: 0.8095 - val_loss: 0.0775 - val_acc: 0.7073\n",
      "Epoch 716/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0466 - acc: 0.8095 - val_loss: 0.0771 - val_acc: 0.7051\n",
      "Epoch 717/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0466 - acc: 0.8095 - val_loss: 0.0770 - val_acc: 0.7115\n",
      "Epoch 718/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0463 - acc: 0.8095 - val_loss: 0.0775 - val_acc: 0.7073\n",
      "Epoch 719/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0465 - acc: 0.8095 - val_loss: 0.0771 - val_acc: 0.7094\n",
      "Epoch 720/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0465 - acc: 0.8095 - val_loss: 0.0779 - val_acc: 0.7030\n",
      "Epoch 721/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0461 - acc: 0.8095 - val_loss: 0.0765 - val_acc: 0.7115\n",
      "Epoch 722/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0461 - acc: 0.8095 - val_loss: 0.0775 - val_acc: 0.7094\n",
      "Epoch 723/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0462 - acc: 0.8095 - val_loss: 0.0781 - val_acc: 0.7094\n",
      "Epoch 724/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0461 - acc: 0.8095 - val_loss: 0.0785 - val_acc: 0.7051\n",
      "Epoch 725/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0463 - acc: 0.8095 - val_loss: 0.0770 - val_acc: 0.7051\n",
      "Epoch 726/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0460 - acc: 0.8095 - val_loss: 0.0788 - val_acc: 0.7030\n",
      "Epoch 727/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0463 - acc: 0.8095 - val_loss: 0.0773 - val_acc: 0.7094\n",
      "Epoch 728/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0461 - acc: 0.8095 - val_loss: 0.0765 - val_acc: 0.7073\n",
      "Epoch 729/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0461 - acc: 0.8095 - val_loss: 0.0762 - val_acc: 0.7073\n",
      "Epoch 730/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0460 - acc: 0.8095 - val_loss: 0.0760 - val_acc: 0.7094\n",
      "Epoch 731/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0459 - acc: 0.8095 - val_loss: 0.0779 - val_acc: 0.7073\n",
      "Epoch 732/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0458 - acc: 0.8095 - val_loss: 0.0765 - val_acc: 0.7115\n",
      "Epoch 733/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0457 - acc: 0.8095 - val_loss: 0.0767 - val_acc: 0.7094\n",
      "Epoch 734/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0456 - acc: 0.8095 - val_loss: 0.0770 - val_acc: 0.7115\n",
      "Epoch 735/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0456 - acc: 0.8095 - val_loss: 0.0752 - val_acc: 0.7094\n",
      "Epoch 736/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0456 - acc: 0.8095 - val_loss: 0.0777 - val_acc: 0.7030\n",
      "Epoch 737/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0455 - acc: 0.8095 - val_loss: 0.0780 - val_acc: 0.7030\n",
      "Epoch 738/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0455 - acc: 0.8095 - val_loss: 0.0770 - val_acc: 0.7073\n",
      "Epoch 739/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0455 - acc: 0.8095 - val_loss: 0.0756 - val_acc: 0.7115\n",
      "Epoch 740/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0454 - acc: 0.8095 - val_loss: 0.0760 - val_acc: 0.7137\n",
      "Epoch 741/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0453 - acc: 0.8095 - val_loss: 0.0755 - val_acc: 0.7094\n",
      "Epoch 742/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0452 - acc: 0.8095 - val_loss: 0.0764 - val_acc: 0.7094\n",
      "Epoch 743/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0454 - acc: 0.8095 - val_loss: 0.0773 - val_acc: 0.7030\n",
      "Epoch 744/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0453 - acc: 0.8095 - val_loss: 0.0763 - val_acc: 0.7094\n",
      "Epoch 745/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0450 - acc: 0.8095 - val_loss: 0.0769 - val_acc: 0.7030\n",
      "Epoch 746/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0452 - acc: 0.8095 - val_loss: 0.0777 - val_acc: 0.7073\n",
      "Epoch 747/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0451 - acc: 0.8095 - val_loss: 0.0772 - val_acc: 0.7073\n",
      "Epoch 748/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0451 - acc: 0.8095 - val_loss: 0.0767 - val_acc: 0.7115\n",
      "Epoch 749/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0451 - acc: 0.8095 - val_loss: 0.0773 - val_acc: 0.7051\n",
      "Epoch 750/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0450 - acc: 0.8095 - val_loss: 0.0765 - val_acc: 0.7073\n",
      "Epoch 751/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0449 - acc: 0.8095 - val_loss: 0.0770 - val_acc: 0.7030\n",
      "Epoch 752/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0447 - acc: 0.8095 - val_loss: 0.0768 - val_acc: 0.7030\n",
      "Epoch 753/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0450 - acc: 0.8095 - val_loss: 0.0764 - val_acc: 0.7051\n",
      "Epoch 754/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0448 - acc: 0.8095 - val_loss: 0.0770 - val_acc: 0.7030\n",
      "Epoch 755/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0446 - acc: 0.8095 - val_loss: 0.0757 - val_acc: 0.7073\n",
      "Epoch 756/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0446 - acc: 0.8095 - val_loss: 0.0763 - val_acc: 0.7073\n",
      "Epoch 757/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0448 - acc: 0.8095 - val_loss: 0.0749 - val_acc: 0.7094\n",
      "Epoch 758/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0444 - acc: 0.8095 - val_loss: 0.0778 - val_acc: 0.6966\n",
      "Epoch 759/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0446 - acc: 0.8095 - val_loss: 0.0759 - val_acc: 0.7051\n",
      "Epoch 760/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0447 - acc: 0.8095 - val_loss: 0.0758 - val_acc: 0.7051\n",
      "Epoch 761/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0446 - acc: 0.8095 - val_loss: 0.0786 - val_acc: 0.6987\n",
      "Epoch 762/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0446 - acc: 0.8095 - val_loss: 0.0785 - val_acc: 0.6987\n",
      "Epoch 763/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0444 - acc: 0.8254 - val_loss: 0.0784 - val_acc: 0.6966\n",
      "Epoch 764/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0443 - acc: 0.8095 - val_loss: 0.0770 - val_acc: 0.6966\n",
      "Epoch 765/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0445 - acc: 0.8095 - val_loss: 0.0792 - val_acc: 0.6902\n",
      "Epoch 766/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0445 - acc: 0.8095 - val_loss: 0.0798 - val_acc: 0.6880\n",
      "Epoch 767/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0441 - acc: 0.8095 - val_loss: 0.0769 - val_acc: 0.6966\n",
      "Epoch 768/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0441 - acc: 0.8254 - val_loss: 0.0772 - val_acc: 0.7009\n",
      "Epoch 769/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0441 - acc: 0.8095 - val_loss: 0.0770 - val_acc: 0.6966\n",
      "Epoch 770/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0438 - acc: 0.8095 - val_loss: 0.0770 - val_acc: 0.6987\n",
      "Epoch 771/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0438 - acc: 0.8095 - val_loss: 0.0764 - val_acc: 0.7009\n",
      "Epoch 772/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0439 - acc: 0.8095 - val_loss: 0.0763 - val_acc: 0.6987\n",
      "Epoch 773/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0438 - acc: 0.8254 - val_loss: 0.0779 - val_acc: 0.6966\n",
      "Epoch 774/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0438 - acc: 0.8095 - val_loss: 0.0741 - val_acc: 0.7094\n",
      "Epoch 775/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0437 - acc: 0.8095 - val_loss: 0.0770 - val_acc: 0.6966\n",
      "Epoch 776/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0436 - acc: 0.8254 - val_loss: 0.0781 - val_acc: 0.6923\n",
      "Epoch 777/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0436 - acc: 0.8254 - val_loss: 0.0755 - val_acc: 0.7009\n",
      "Epoch 778/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0439 - acc: 0.8095 - val_loss: 0.0772 - val_acc: 0.6966\n",
      "Epoch 779/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0434 - acc: 0.8254 - val_loss: 0.0761 - val_acc: 0.7009\n",
      "Epoch 780/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0435 - acc: 0.8095 - val_loss: 0.0764 - val_acc: 0.7009\n",
      "Epoch 781/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0437 - acc: 0.8095 - val_loss: 0.0762 - val_acc: 0.7009\n",
      "Epoch 782/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0435 - acc: 0.8095 - val_loss: 0.0741 - val_acc: 0.7115\n",
      "Epoch 783/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0431 - acc: 0.8254 - val_loss: 0.0750 - val_acc: 0.7009\n",
      "Epoch 784/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0432 - acc: 0.8254 - val_loss: 0.0754 - val_acc: 0.7009\n",
      "Epoch 785/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0433 - acc: 0.8254 - val_loss: 0.0774 - val_acc: 0.6944\n",
      "Epoch 786/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0433 - acc: 0.8254 - val_loss: 0.0773 - val_acc: 0.6966\n",
      "Epoch 787/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0432 - acc: 0.8254 - val_loss: 0.0770 - val_acc: 0.6966\n",
      "Epoch 788/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0432 - acc: 0.8254 - val_loss: 0.0745 - val_acc: 0.7073\n",
      "Epoch 789/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0429 - acc: 0.8254 - val_loss: 0.0754 - val_acc: 0.7009\n",
      "Epoch 790/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0430 - acc: 0.8254 - val_loss: 0.0752 - val_acc: 0.7009\n",
      "Epoch 791/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0428 - acc: 0.8254 - val_loss: 0.0762 - val_acc: 0.6987\n",
      "Epoch 792/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0431 - acc: 0.8254 - val_loss: 0.0744 - val_acc: 0.7051\n",
      "Epoch 793/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0427 - acc: 0.8254 - val_loss: 0.0764 - val_acc: 0.7009\n",
      "Epoch 794/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0430 - acc: 0.8254 - val_loss: 0.0762 - val_acc: 0.6966\n",
      "Epoch 795/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0428 - acc: 0.8254 - val_loss: 0.0756 - val_acc: 0.7009\n",
      "Epoch 796/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0427 - acc: 0.8254 - val_loss: 0.0770 - val_acc: 0.6923\n",
      "Epoch 797/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0427 - acc: 0.8254 - val_loss: 0.0736 - val_acc: 0.7051\n",
      "Epoch 798/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0427 - acc: 0.8254 - val_loss: 0.0761 - val_acc: 0.7009\n",
      "Epoch 799/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0426 - acc: 0.8254 - val_loss: 0.0742 - val_acc: 0.7051\n",
      "Epoch 800/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0425 - acc: 0.8254 - val_loss: 0.0750 - val_acc: 0.7030\n",
      "Epoch 801/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0424 - acc: 0.8254 - val_loss: 0.0744 - val_acc: 0.7030\n",
      "Epoch 802/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0424 - acc: 0.8254 - val_loss: 0.0740 - val_acc: 0.6987\n",
      "Epoch 803/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0425 - acc: 0.8254 - val_loss: 0.0748 - val_acc: 0.7030\n",
      "Epoch 804/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0425 - acc: 0.8254 - val_loss: 0.0739 - val_acc: 0.7073\n",
      "Epoch 805/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0423 - acc: 0.8254 - val_loss: 0.0751 - val_acc: 0.7009\n",
      "Epoch 806/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0423 - acc: 0.8254 - val_loss: 0.0743 - val_acc: 0.7051\n",
      "Epoch 807/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0423 - acc: 0.8254 - val_loss: 0.0737 - val_acc: 0.7051\n",
      "Epoch 808/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0422 - acc: 0.8254 - val_loss: 0.0730 - val_acc: 0.7094\n",
      "Epoch 809/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0423 - acc: 0.8254 - val_loss: 0.0737 - val_acc: 0.7030\n",
      "Epoch 810/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0421 - acc: 0.8254 - val_loss: 0.0742 - val_acc: 0.7073\n",
      "Epoch 811/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0423 - acc: 0.8254 - val_loss: 0.0739 - val_acc: 0.7094\n",
      "Epoch 812/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0425 - acc: 0.8413 - val_loss: 0.0737 - val_acc: 0.7094\n",
      "Epoch 813/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0418 - acc: 0.8254 - val_loss: 0.0751 - val_acc: 0.7030\n",
      "Epoch 814/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0419 - acc: 0.8254 - val_loss: 0.0748 - val_acc: 0.7030\n",
      "Epoch 815/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0419 - acc: 0.8413 - val_loss: 0.0734 - val_acc: 0.7115\n",
      "Epoch 816/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0420 - acc: 0.8254 - val_loss: 0.0746 - val_acc: 0.7030\n",
      "Epoch 817/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0416 - acc: 0.8254 - val_loss: 0.0733 - val_acc: 0.7115\n",
      "Epoch 818/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0418 - acc: 0.8413 - val_loss: 0.0734 - val_acc: 0.7073\n",
      "Epoch 819/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0421 - acc: 0.8413 - val_loss: 0.0734 - val_acc: 0.7094\n",
      "Epoch 820/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0418 - acc: 0.8254 - val_loss: 0.0749 - val_acc: 0.7030\n",
      "Epoch 821/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0418 - acc: 0.8254 - val_loss: 0.0768 - val_acc: 0.6923\n",
      "Epoch 822/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0418 - acc: 0.8254 - val_loss: 0.0733 - val_acc: 0.7115\n",
      "Epoch 823/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0418 - acc: 0.8254 - val_loss: 0.0729 - val_acc: 0.7115\n",
      "Epoch 824/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0416 - acc: 0.8413 - val_loss: 0.0737 - val_acc: 0.7073\n",
      "Epoch 825/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0417 - acc: 0.8413 - val_loss: 0.0745 - val_acc: 0.7051\n",
      "Epoch 826/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0416 - acc: 0.8254 - val_loss: 0.0737 - val_acc: 0.7094\n",
      "Epoch 827/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0415 - acc: 0.8254 - val_loss: 0.0742 - val_acc: 0.7030\n",
      "Epoch 828/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0415 - acc: 0.8413 - val_loss: 0.0723 - val_acc: 0.7094\n",
      "Epoch 829/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0413 - acc: 0.8413 - val_loss: 0.0725 - val_acc: 0.7115\n",
      "Epoch 830/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0417 - acc: 0.8254 - val_loss: 0.0735 - val_acc: 0.7094\n",
      "Epoch 831/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0414 - acc: 0.8413 - val_loss: 0.0725 - val_acc: 0.7094\n",
      "Epoch 832/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0411 - acc: 0.8413 - val_loss: 0.0735 - val_acc: 0.7073\n",
      "Epoch 833/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0412 - acc: 0.8413 - val_loss: 0.0731 - val_acc: 0.7073\n",
      "Epoch 834/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0411 - acc: 0.8413 - val_loss: 0.0730 - val_acc: 0.7115\n",
      "Epoch 835/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0411 - acc: 0.8413 - val_loss: 0.0735 - val_acc: 0.7094\n",
      "Epoch 836/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0414 - acc: 0.8413 - val_loss: 0.0742 - val_acc: 0.7073\n",
      "Epoch 837/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0412 - acc: 0.8254 - val_loss: 0.0729 - val_acc: 0.7115\n",
      "Epoch 838/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0413 - acc: 0.8254 - val_loss: 0.0734 - val_acc: 0.7094\n",
      "Epoch 839/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0411 - acc: 0.8413 - val_loss: 0.0725 - val_acc: 0.7115\n",
      "Epoch 840/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0414 - acc: 0.8413 - val_loss: 0.0711 - val_acc: 0.7137\n",
      "Epoch 841/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0414 - acc: 0.8413 - val_loss: 0.0716 - val_acc: 0.7137\n",
      "Epoch 842/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0409 - acc: 0.8413 - val_loss: 0.0723 - val_acc: 0.7115\n",
      "Epoch 843/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0408 - acc: 0.8413 - val_loss: 0.0734 - val_acc: 0.7115\n",
      "Epoch 844/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0409 - acc: 0.8413 - val_loss: 0.0725 - val_acc: 0.7115\n",
      "Epoch 845/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0409 - acc: 0.8254 - val_loss: 0.0737 - val_acc: 0.7073\n",
      "Epoch 846/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0409 - acc: 0.8413 - val_loss: 0.0731 - val_acc: 0.7094\n",
      "Epoch 847/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0409 - acc: 0.8254 - val_loss: 0.0746 - val_acc: 0.6987\n",
      "Epoch 848/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0406 - acc: 0.8413 - val_loss: 0.0722 - val_acc: 0.7094\n",
      "Epoch 849/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0406 - acc: 0.8413 - val_loss: 0.0722 - val_acc: 0.7094\n",
      "Epoch 850/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0410 - acc: 0.8413 - val_loss: 0.0720 - val_acc: 0.7094\n",
      "Epoch 851/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0406 - acc: 0.8254 - val_loss: 0.0749 - val_acc: 0.6966\n",
      "Epoch 852/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0407 - acc: 0.8413 - val_loss: 0.0731 - val_acc: 0.7094\n",
      "Epoch 853/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0407 - acc: 0.8413 - val_loss: 0.0734 - val_acc: 0.7073\n",
      "Epoch 854/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0407 - acc: 0.8413 - val_loss: 0.0726 - val_acc: 0.7115\n",
      "Epoch 855/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0405 - acc: 0.8413 - val_loss: 0.0732 - val_acc: 0.7073\n",
      "Epoch 856/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0404 - acc: 0.8413 - val_loss: 0.0726 - val_acc: 0.7115\n",
      "Epoch 857/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0403 - acc: 0.8413 - val_loss: 0.0720 - val_acc: 0.7137\n",
      "Epoch 858/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0405 - acc: 0.8413 - val_loss: 0.0731 - val_acc: 0.7073\n",
      "Epoch 859/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0403 - acc: 0.8413 - val_loss: 0.0724 - val_acc: 0.7115\n",
      "Epoch 860/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0402 - acc: 0.8413 - val_loss: 0.0719 - val_acc: 0.7158\n",
      "Epoch 861/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0407 - acc: 0.8413 - val_loss: 0.0729 - val_acc: 0.7094\n",
      "Epoch 862/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0406 - acc: 0.8413 - val_loss: 0.0725 - val_acc: 0.7137\n",
      "Epoch 863/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0402 - acc: 0.8413 - val_loss: 0.0720 - val_acc: 0.7115\n",
      "Epoch 864/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0406 - acc: 0.8413 - val_loss: 0.0722 - val_acc: 0.7073\n",
      "Epoch 865/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0404 - acc: 0.8413 - val_loss: 0.0737 - val_acc: 0.7030\n",
      "Epoch 866/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0404 - acc: 0.8254 - val_loss: 0.0738 - val_acc: 0.7009\n",
      "Epoch 867/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0401 - acc: 0.8413 - val_loss: 0.0712 - val_acc: 0.7137\n",
      "Epoch 868/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0400 - acc: 0.8413 - val_loss: 0.0716 - val_acc: 0.7094\n",
      "Epoch 869/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0402 - acc: 0.8413 - val_loss: 0.0737 - val_acc: 0.7009\n",
      "Epoch 870/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0402 - acc: 0.8413 - val_loss: 0.0731 - val_acc: 0.7030\n",
      "Epoch 871/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0401 - acc: 0.8413 - val_loss: 0.0721 - val_acc: 0.7073\n",
      "Epoch 872/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0402 - acc: 0.8413 - val_loss: 0.0735 - val_acc: 0.7030\n",
      "Epoch 873/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0404 - acc: 0.8413 - val_loss: 0.0714 - val_acc: 0.7094\n",
      "Epoch 874/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0405 - acc: 0.8413 - val_loss: 0.0741 - val_acc: 0.7009\n",
      "Epoch 875/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0401 - acc: 0.8413 - val_loss: 0.0725 - val_acc: 0.7030\n",
      "Epoch 876/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0401 - acc: 0.8413 - val_loss: 0.0706 - val_acc: 0.7158\n",
      "Epoch 877/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0398 - acc: 0.8413 - val_loss: 0.0714 - val_acc: 0.7094\n",
      "Epoch 878/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0400 - acc: 0.8413 - val_loss: 0.0723 - val_acc: 0.7094\n",
      "Epoch 879/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0401 - acc: 0.8413 - val_loss: 0.0713 - val_acc: 0.7094\n",
      "Epoch 880/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0401 - acc: 0.8413 - val_loss: 0.0722 - val_acc: 0.7051\n",
      "Epoch 881/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0398 - acc: 0.8413 - val_loss: 0.0719 - val_acc: 0.7115\n",
      "Epoch 882/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0397 - acc: 0.8413 - val_loss: 0.0709 - val_acc: 0.7094\n",
      "Epoch 883/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0397 - acc: 0.8413 - val_loss: 0.0720 - val_acc: 0.7094\n",
      "Epoch 884/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0399 - acc: 0.8413 - val_loss: 0.0716 - val_acc: 0.7158\n",
      "Epoch 885/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0400 - acc: 0.8413 - val_loss: 0.0729 - val_acc: 0.7051\n",
      "Epoch 886/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0399 - acc: 0.8413 - val_loss: 0.0724 - val_acc: 0.7009\n",
      "Epoch 887/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0398 - acc: 0.8413 - val_loss: 0.0728 - val_acc: 0.7030\n",
      "Epoch 888/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0399 - acc: 0.8413 - val_loss: 0.0726 - val_acc: 0.7009\n",
      "Epoch 889/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0397 - acc: 0.8413 - val_loss: 0.0720 - val_acc: 0.7094\n",
      "Epoch 890/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0396 - acc: 0.8413 - val_loss: 0.0715 - val_acc: 0.7115\n",
      "Epoch 891/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0396 - acc: 0.8413 - val_loss: 0.0733 - val_acc: 0.6987\n",
      "Epoch 892/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0396 - acc: 0.8413 - val_loss: 0.0721 - val_acc: 0.7009\n",
      "Epoch 893/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0395 - acc: 0.8413 - val_loss: 0.0723 - val_acc: 0.7051\n",
      "Epoch 894/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0397 - acc: 0.8413 - val_loss: 0.0709 - val_acc: 0.7094\n",
      "Epoch 895/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0394 - acc: 0.8413 - val_loss: 0.0710 - val_acc: 0.7115\n",
      "Epoch 896/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0394 - acc: 0.8413 - val_loss: 0.0724 - val_acc: 0.7030\n",
      "Epoch 897/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0393 - acc: 0.8413 - val_loss: 0.0714 - val_acc: 0.7115\n",
      "Epoch 898/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0392 - acc: 0.8413 - val_loss: 0.0714 - val_acc: 0.7094\n",
      "Epoch 899/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0394 - acc: 0.8413 - val_loss: 0.0708 - val_acc: 0.7115\n",
      "Epoch 900/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0396 - acc: 0.8413 - val_loss: 0.0703 - val_acc: 0.7115\n",
      "Epoch 901/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0394 - acc: 0.8413 - val_loss: 0.0725 - val_acc: 0.7009\n",
      "Epoch 902/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0394 - acc: 0.8413 - val_loss: 0.0708 - val_acc: 0.7115\n",
      "Epoch 903/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0392 - acc: 0.8413 - val_loss: 0.0719 - val_acc: 0.7030\n",
      "Epoch 904/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0394 - acc: 0.8413 - val_loss: 0.0710 - val_acc: 0.7115\n",
      "Epoch 905/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0391 - acc: 0.8413 - val_loss: 0.0721 - val_acc: 0.7073\n",
      "Epoch 906/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0394 - acc: 0.8413 - val_loss: 0.0713 - val_acc: 0.7073\n",
      "Epoch 907/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0395 - acc: 0.8413 - val_loss: 0.0726 - val_acc: 0.7009\n",
      "Epoch 908/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0393 - acc: 0.8413 - val_loss: 0.0714 - val_acc: 0.7115\n",
      "Epoch 909/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0392 - acc: 0.8413 - val_loss: 0.0727 - val_acc: 0.7030\n",
      "Epoch 910/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0393 - acc: 0.8413 - val_loss: 0.0715 - val_acc: 0.7051\n",
      "Epoch 911/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0392 - acc: 0.8413 - val_loss: 0.0705 - val_acc: 0.7115\n",
      "Epoch 912/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0390 - acc: 0.8413 - val_loss: 0.0719 - val_acc: 0.7030\n",
      "Epoch 913/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0392 - acc: 0.8413 - val_loss: 0.0710 - val_acc: 0.7115\n",
      "Epoch 914/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0390 - acc: 0.8413 - val_loss: 0.0709 - val_acc: 0.7115\n",
      "Epoch 915/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0389 - acc: 0.8413 - val_loss: 0.0703 - val_acc: 0.7115\n",
      "Epoch 916/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0390 - acc: 0.8413 - val_loss: 0.0721 - val_acc: 0.7009\n",
      "Epoch 917/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0389 - acc: 0.8413 - val_loss: 0.0711 - val_acc: 0.7073\n",
      "Epoch 918/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0389 - acc: 0.8413 - val_loss: 0.0708 - val_acc: 0.7115\n",
      "Epoch 919/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0390 - acc: 0.8413 - val_loss: 0.0709 - val_acc: 0.7115\n",
      "Epoch 920/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0389 - acc: 0.8413 - val_loss: 0.0703 - val_acc: 0.7115\n",
      "Epoch 921/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0391 - acc: 0.8413 - val_loss: 0.0721 - val_acc: 0.7009\n",
      "Epoch 922/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0387 - acc: 0.8413 - val_loss: 0.0695 - val_acc: 0.7179\n",
      "Epoch 923/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0389 - acc: 0.8413 - val_loss: 0.0703 - val_acc: 0.7094\n",
      "Epoch 924/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0389 - acc: 0.8413 - val_loss: 0.0706 - val_acc: 0.7115\n",
      "Epoch 925/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0389 - acc: 0.8413 - val_loss: 0.0693 - val_acc: 0.7158\n",
      "Epoch 926/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0391 - acc: 0.8413 - val_loss: 0.0715 - val_acc: 0.7073\n",
      "Epoch 927/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0388 - acc: 0.8413 - val_loss: 0.0696 - val_acc: 0.7137\n",
      "Epoch 928/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0385 - acc: 0.8413 - val_loss: 0.0706 - val_acc: 0.7115\n",
      "Epoch 929/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0388 - acc: 0.8413 - val_loss: 0.0706 - val_acc: 0.7115\n",
      "Epoch 930/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0390 - acc: 0.8413 - val_loss: 0.0703 - val_acc: 0.7137\n",
      "Epoch 931/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0389 - acc: 0.8413 - val_loss: 0.0716 - val_acc: 0.7030\n",
      "Epoch 932/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0387 - acc: 0.8413 - val_loss: 0.0720 - val_acc: 0.7030\n",
      "Epoch 933/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0387 - acc: 0.8413 - val_loss: 0.0700 - val_acc: 0.7115\n",
      "Epoch 934/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0385 - acc: 0.8413 - val_loss: 0.0701 - val_acc: 0.7115\n",
      "Epoch 935/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0389 - acc: 0.8413 - val_loss: 0.0712 - val_acc: 0.7051\n",
      "Epoch 936/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0386 - acc: 0.8413 - val_loss: 0.0703 - val_acc: 0.7115\n",
      "Epoch 937/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0386 - acc: 0.8413 - val_loss: 0.0703 - val_acc: 0.7073\n",
      "Epoch 938/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0387 - acc: 0.8413 - val_loss: 0.0707 - val_acc: 0.7073\n",
      "Epoch 939/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0387 - acc: 0.8413 - val_loss: 0.0710 - val_acc: 0.7094\n",
      "Epoch 940/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0385 - acc: 0.8413 - val_loss: 0.0703 - val_acc: 0.7094\n",
      "Epoch 941/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0385 - acc: 0.8413 - val_loss: 0.0718 - val_acc: 0.6987\n",
      "Epoch 942/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0384 - acc: 0.8413 - val_loss: 0.0701 - val_acc: 0.7115\n",
      "Epoch 943/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0385 - acc: 0.8413 - val_loss: 0.0701 - val_acc: 0.7137\n",
      "Epoch 944/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0385 - acc: 0.8413 - val_loss: 0.0694 - val_acc: 0.7158\n",
      "Epoch 945/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0384 - acc: 0.8413 - val_loss: 0.0709 - val_acc: 0.7094\n",
      "Epoch 946/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0384 - acc: 0.8413 - val_loss: 0.0705 - val_acc: 0.7094\n",
      "Epoch 947/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0385 - acc: 0.8413 - val_loss: 0.0719 - val_acc: 0.7030\n",
      "Epoch 948/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0384 - acc: 0.8413 - val_loss: 0.0712 - val_acc: 0.7030\n",
      "Epoch 949/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0382 - acc: 0.8413 - val_loss: 0.0698 - val_acc: 0.7158\n",
      "Epoch 950/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0383 - acc: 0.8413 - val_loss: 0.0694 - val_acc: 0.7179\n",
      "Epoch 951/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0385 - acc: 0.8413 - val_loss: 0.0692 - val_acc: 0.7201\n",
      "Epoch 952/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0385 - acc: 0.8413 - val_loss: 0.0704 - val_acc: 0.7094\n",
      "Epoch 953/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0382 - acc: 0.8413 - val_loss: 0.0711 - val_acc: 0.7030\n",
      "Epoch 954/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0383 - acc: 0.8413 - val_loss: 0.0704 - val_acc: 0.7115\n",
      "Epoch 955/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0382 - acc: 0.8413 - val_loss: 0.0703 - val_acc: 0.7115\n",
      "Epoch 956/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0381 - acc: 0.8413 - val_loss: 0.0708 - val_acc: 0.7137\n",
      "Epoch 957/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0380 - acc: 0.8413 - val_loss: 0.0717 - val_acc: 0.7030\n",
      "Epoch 958/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0380 - acc: 0.8413 - val_loss: 0.0706 - val_acc: 0.7137\n",
      "Epoch 959/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0384 - acc: 0.8413 - val_loss: 0.0690 - val_acc: 0.7222\n",
      "Epoch 960/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0381 - acc: 0.8413 - val_loss: 0.0694 - val_acc: 0.7222\n",
      "Epoch 961/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0382 - acc: 0.8413 - val_loss: 0.0718 - val_acc: 0.7030\n",
      "Epoch 962/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0381 - acc: 0.8413 - val_loss: 0.0696 - val_acc: 0.7222\n",
      "Epoch 963/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0387 - acc: 0.8413 - val_loss: 0.0708 - val_acc: 0.7158\n",
      "Epoch 964/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0382 - acc: 0.8413 - val_loss: 0.0711 - val_acc: 0.7073\n",
      "Epoch 965/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0380 - acc: 0.8413 - val_loss: 0.0720 - val_acc: 0.7030\n",
      "Epoch 966/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0378 - acc: 0.8413 - val_loss: 0.0699 - val_acc: 0.7222\n",
      "Epoch 967/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0378 - acc: 0.8413 - val_loss: 0.0709 - val_acc: 0.7201\n",
      "Epoch 968/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0379 - acc: 0.8413 - val_loss: 0.0720 - val_acc: 0.7115\n",
      "Epoch 969/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0384 - acc: 0.8413 - val_loss: 0.0696 - val_acc: 0.7222\n",
      "Epoch 970/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0380 - acc: 0.8413 - val_loss: 0.0701 - val_acc: 0.7201\n",
      "Epoch 971/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0377 - acc: 0.8413 - val_loss: 0.0697 - val_acc: 0.7222\n",
      "Epoch 972/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0377 - acc: 0.8413 - val_loss: 0.0700 - val_acc: 0.7350\n",
      "Epoch 973/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0377 - acc: 0.8413 - val_loss: 0.0698 - val_acc: 0.7329\n",
      "Epoch 974/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0379 - acc: 0.8413 - val_loss: 0.0704 - val_acc: 0.7265\n",
      "Epoch 975/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0377 - acc: 0.8413 - val_loss: 0.0712 - val_acc: 0.7179\n",
      "Epoch 976/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0378 - acc: 0.8413 - val_loss: 0.0703 - val_acc: 0.7308\n",
      "Epoch 977/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0373 - acc: 0.8413 - val_loss: 0.0717 - val_acc: 0.7201\n",
      "Epoch 978/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0377 - acc: 0.8413 - val_loss: 0.0712 - val_acc: 0.7286\n",
      "Epoch 979/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0375 - acc: 0.8413 - val_loss: 0.0702 - val_acc: 0.7286\n",
      "Epoch 980/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0373 - acc: 0.8413 - val_loss: 0.0711 - val_acc: 0.7265\n",
      "Epoch 981/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0375 - acc: 0.8413 - val_loss: 0.0720 - val_acc: 0.7201\n",
      "Epoch 982/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0372 - acc: 0.8413 - val_loss: 0.0711 - val_acc: 0.7265\n",
      "Epoch 983/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0376 - acc: 0.8413 - val_loss: 0.0712 - val_acc: 0.7308\n",
      "Epoch 984/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0374 - acc: 0.8413 - val_loss: 0.0717 - val_acc: 0.7244\n",
      "Epoch 985/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0370 - acc: 0.8571 - val_loss: 0.0707 - val_acc: 0.7286\n",
      "Epoch 986/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0374 - acc: 0.8413 - val_loss: 0.0711 - val_acc: 0.7286\n",
      "Epoch 987/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0370 - acc: 0.8413 - val_loss: 0.0707 - val_acc: 0.7244\n",
      "Epoch 988/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0366 - acc: 0.8413 - val_loss: 0.0731 - val_acc: 0.7201\n",
      "Epoch 989/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0369 - acc: 0.8571 - val_loss: 0.0706 - val_acc: 0.7308\n",
      "Epoch 990/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0367 - acc: 0.8571 - val_loss: 0.0730 - val_acc: 0.7158\n",
      "Epoch 991/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0370 - acc: 0.8571 - val_loss: 0.0727 - val_acc: 0.7158\n",
      "Epoch 992/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0366 - acc: 0.8571 - val_loss: 0.0708 - val_acc: 0.7244\n",
      "Epoch 993/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0369 - acc: 0.8413 - val_loss: 0.0717 - val_acc: 0.7201\n",
      "Epoch 994/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0368 - acc: 0.8571 - val_loss: 0.0710 - val_acc: 0.7286\n",
      "Epoch 995/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0368 - acc: 0.8571 - val_loss: 0.0713 - val_acc: 0.7222\n",
      "Epoch 996/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0364 - acc: 0.8730 - val_loss: 0.0702 - val_acc: 0.7329\n",
      "Epoch 997/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0366 - acc: 0.8413 - val_loss: 0.0712 - val_acc: 0.7244\n",
      "Epoch 998/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0363 - acc: 0.8730 - val_loss: 0.0716 - val_acc: 0.7222\n",
      "Epoch 999/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0365 - acc: 0.8413 - val_loss: 0.0709 - val_acc: 0.7244\n",
      "Epoch 1000/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0364 - acc: 0.8730 - val_loss: 0.0703 - val_acc: 0.7329\n",
      "Epoch 1001/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0364 - acc: 0.8571 - val_loss: 0.0725 - val_acc: 0.7179\n",
      "Epoch 1002/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0361 - acc: 0.8413 - val_loss: 0.0715 - val_acc: 0.7201\n",
      "Epoch 1003/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0360 - acc: 0.8571 - val_loss: 0.0714 - val_acc: 0.7201\n",
      "Epoch 1004/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0359 - acc: 0.8730 - val_loss: 0.0712 - val_acc: 0.7222\n",
      "Epoch 1005/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0362 - acc: 0.8571 - val_loss: 0.0702 - val_acc: 0.7308\n",
      "Epoch 1006/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0360 - acc: 0.8730 - val_loss: 0.0717 - val_acc: 0.7179\n",
      "Epoch 1007/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0362 - acc: 0.8571 - val_loss: 0.0724 - val_acc: 0.7158\n",
      "Epoch 1008/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0360 - acc: 0.8571 - val_loss: 0.0706 - val_acc: 0.7286\n",
      "Epoch 1009/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0362 - acc: 0.8730 - val_loss: 0.0710 - val_acc: 0.7244\n",
      "Epoch 1010/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0355 - acc: 0.8730 - val_loss: 0.0710 - val_acc: 0.7244\n",
      "Epoch 1011/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0359 - acc: 0.8730 - val_loss: 0.0702 - val_acc: 0.7308\n",
      "Epoch 1012/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0355 - acc: 0.8730 - val_loss: 0.0708 - val_acc: 0.7201\n",
      "Epoch 1013/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0356 - acc: 0.8730 - val_loss: 0.0714 - val_acc: 0.7179\n",
      "Epoch 1014/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0362 - acc: 0.8413 - val_loss: 0.0716 - val_acc: 0.7179\n",
      "Epoch 1015/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0360 - acc: 0.8730 - val_loss: 0.0721 - val_acc: 0.7179\n",
      "Epoch 1016/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0360 - acc: 0.8730 - val_loss: 0.0705 - val_acc: 0.7286\n",
      "Epoch 1017/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0356 - acc: 0.8730 - val_loss: 0.0709 - val_acc: 0.7244\n",
      "Epoch 1018/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0355 - acc: 0.8730 - val_loss: 0.0715 - val_acc: 0.7222\n",
      "Epoch 1019/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0356 - acc: 0.8730 - val_loss: 0.0705 - val_acc: 0.7244\n",
      "Epoch 1020/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0353 - acc: 0.8730 - val_loss: 0.0710 - val_acc: 0.7265\n",
      "Epoch 1021/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0354 - acc: 0.8730 - val_loss: 0.0704 - val_acc: 0.7265\n",
      "Epoch 1022/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0353 - acc: 0.8730 - val_loss: 0.0707 - val_acc: 0.7244\n",
      "Epoch 1023/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0353 - acc: 0.8730 - val_loss: 0.0710 - val_acc: 0.7222\n",
      "Epoch 1024/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0353 - acc: 0.8730 - val_loss: 0.0720 - val_acc: 0.7137\n",
      "Epoch 1025/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0354 - acc: 0.8730 - val_loss: 0.0705 - val_acc: 0.7222\n",
      "Epoch 1026/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0353 - acc: 0.8730 - val_loss: 0.0719 - val_acc: 0.7179\n",
      "Epoch 1027/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0351 - acc: 0.8730 - val_loss: 0.0702 - val_acc: 0.7244\n",
      "Epoch 1028/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0352 - acc: 0.8730 - val_loss: 0.0700 - val_acc: 0.7265\n",
      "Epoch 1029/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0352 - acc: 0.8730 - val_loss: 0.0697 - val_acc: 0.7286\n",
      "Epoch 1030/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0351 - acc: 0.8730 - val_loss: 0.0715 - val_acc: 0.7222\n",
      "Epoch 1031/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0350 - acc: 0.8730 - val_loss: 0.0700 - val_acc: 0.7265\n",
      "Epoch 1032/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0352 - acc: 0.8730 - val_loss: 0.0696 - val_acc: 0.7265\n",
      "Epoch 1033/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0352 - acc: 0.8730 - val_loss: 0.0696 - val_acc: 0.7265\n",
      "Epoch 1034/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0345 - acc: 0.8730 - val_loss: 0.0716 - val_acc: 0.7137\n",
      "Epoch 1035/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0350 - acc: 0.8730 - val_loss: 0.0705 - val_acc: 0.7265\n",
      "Epoch 1036/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0346 - acc: 0.8730 - val_loss: 0.0699 - val_acc: 0.7286\n",
      "Epoch 1037/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0348 - acc: 0.8730 - val_loss: 0.0696 - val_acc: 0.7286\n",
      "Epoch 1038/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0347 - acc: 0.8730 - val_loss: 0.0704 - val_acc: 0.7265\n",
      "Epoch 1039/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0348 - acc: 0.8730 - val_loss: 0.0701 - val_acc: 0.7244\n",
      "Epoch 1040/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0351 - acc: 0.8730 - val_loss: 0.0697 - val_acc: 0.7308\n",
      "Epoch 1041/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0347 - acc: 0.8730 - val_loss: 0.0714 - val_acc: 0.7179\n",
      "Epoch 1042/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0348 - acc: 0.8730 - val_loss: 0.0703 - val_acc: 0.7286\n",
      "Epoch 1043/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0346 - acc: 0.8730 - val_loss: 0.0694 - val_acc: 0.7265\n",
      "Epoch 1044/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0346 - acc: 0.8730 - val_loss: 0.0696 - val_acc: 0.7265\n",
      "Epoch 1045/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0344 - acc: 0.8730 - val_loss: 0.0701 - val_acc: 0.7244\n",
      "Epoch 1046/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0346 - acc: 0.8730 - val_loss: 0.0695 - val_acc: 0.7244\n",
      "Epoch 1047/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0346 - acc: 0.8730 - val_loss: 0.0695 - val_acc: 0.7265\n",
      "Epoch 1048/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0345 - acc: 0.8730 - val_loss: 0.0694 - val_acc: 0.7286\n",
      "Epoch 1049/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0344 - acc: 0.8730 - val_loss: 0.0694 - val_acc: 0.7286\n",
      "Epoch 1050/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0346 - acc: 0.8730 - val_loss: 0.0685 - val_acc: 0.7329\n",
      "Epoch 1051/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0346 - acc: 0.8730 - val_loss: 0.0707 - val_acc: 0.7265\n",
      "Epoch 1052/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0343 - acc: 0.8730 - val_loss: 0.0716 - val_acc: 0.7201\n",
      "Epoch 1053/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0344 - acc: 0.8730 - val_loss: 0.0709 - val_acc: 0.7222\n",
      "Epoch 1054/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0344 - acc: 0.8730 - val_loss: 0.0702 - val_acc: 0.7265\n",
      "Epoch 1055/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0343 - acc: 0.8730 - val_loss: 0.0686 - val_acc: 0.7308\n",
      "Epoch 1056/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0341 - acc: 0.8730 - val_loss: 0.0703 - val_acc: 0.7244\n",
      "Epoch 1057/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0342 - acc: 0.8730 - val_loss: 0.0696 - val_acc: 0.7286\n",
      "Epoch 1058/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0342 - acc: 0.8730 - val_loss: 0.0699 - val_acc: 0.7244\n",
      "Epoch 1059/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0340 - acc: 0.8730 - val_loss: 0.0700 - val_acc: 0.7286\n",
      "Epoch 1060/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0343 - acc: 0.8730 - val_loss: 0.0697 - val_acc: 0.7244\n",
      "Epoch 1061/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0340 - acc: 0.8730 - val_loss: 0.0699 - val_acc: 0.7222\n",
      "Epoch 1062/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0341 - acc: 0.8730 - val_loss: 0.0687 - val_acc: 0.7286\n",
      "Epoch 1063/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0339 - acc: 0.8730 - val_loss: 0.0706 - val_acc: 0.7222\n",
      "Epoch 1064/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0340 - acc: 0.8730 - val_loss: 0.0700 - val_acc: 0.7244\n",
      "Epoch 1065/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0338 - acc: 0.8730 - val_loss: 0.0688 - val_acc: 0.7329\n",
      "Epoch 1066/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0340 - acc: 0.8730 - val_loss: 0.0705 - val_acc: 0.7244\n",
      "Epoch 1067/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0339 - acc: 0.8730 - val_loss: 0.0711 - val_acc: 0.7222\n",
      "Epoch 1068/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0338 - acc: 0.8730 - val_loss: 0.0692 - val_acc: 0.7286\n",
      "Epoch 1069/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0339 - acc: 0.8730 - val_loss: 0.0699 - val_acc: 0.7222\n",
      "Epoch 1070/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0341 - acc: 0.8730 - val_loss: 0.0684 - val_acc: 0.7329\n",
      "Epoch 1071/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0340 - acc: 0.8730 - val_loss: 0.0691 - val_acc: 0.7265\n",
      "Epoch 1072/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0339 - acc: 0.8730 - val_loss: 0.0693 - val_acc: 0.7308\n",
      "Epoch 1073/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0340 - acc: 0.8730 - val_loss: 0.0680 - val_acc: 0.7329\n",
      "Epoch 1074/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0337 - acc: 0.8730 - val_loss: 0.0682 - val_acc: 0.7329\n",
      "Epoch 1075/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0335 - acc: 0.8730 - val_loss: 0.0690 - val_acc: 0.7244\n",
      "Epoch 1076/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0336 - acc: 0.8730 - val_loss: 0.0688 - val_acc: 0.7265\n",
      "Epoch 1077/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0335 - acc: 0.8730 - val_loss: 0.0695 - val_acc: 0.7286\n",
      "Epoch 1078/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0334 - acc: 0.8730 - val_loss: 0.0687 - val_acc: 0.7244\n",
      "Epoch 1079/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0335 - acc: 0.8730 - val_loss: 0.0689 - val_acc: 0.7308\n",
      "Epoch 1080/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0335 - acc: 0.8730 - val_loss: 0.0700 - val_acc: 0.7222\n",
      "Epoch 1081/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0336 - acc: 0.8730 - val_loss: 0.0691 - val_acc: 0.7286\n",
      "Epoch 1082/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0338 - acc: 0.8730 - val_loss: 0.0689 - val_acc: 0.7244\n",
      "Epoch 1083/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0337 - acc: 0.8730 - val_loss: 0.0691 - val_acc: 0.7244\n",
      "Epoch 1084/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0336 - acc: 0.8730 - val_loss: 0.0690 - val_acc: 0.7265\n",
      "Epoch 1085/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0333 - acc: 0.8730 - val_loss: 0.0676 - val_acc: 0.7308\n",
      "Epoch 1086/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0334 - acc: 0.8730 - val_loss: 0.0711 - val_acc: 0.7137\n",
      "Epoch 1087/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0336 - acc: 0.8730 - val_loss: 0.0678 - val_acc: 0.7329\n",
      "Epoch 1088/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0332 - acc: 0.8730 - val_loss: 0.0682 - val_acc: 0.7286\n",
      "Epoch 1089/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0335 - acc: 0.8730 - val_loss: 0.0695 - val_acc: 0.7244\n",
      "Epoch 1090/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0331 - acc: 0.8730 - val_loss: 0.0706 - val_acc: 0.7201\n",
      "Epoch 1091/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0332 - acc: 0.8730 - val_loss: 0.0681 - val_acc: 0.7286\n",
      "Epoch 1092/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0332 - acc: 0.8730 - val_loss: 0.0690 - val_acc: 0.7265\n",
      "Epoch 1093/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0332 - acc: 0.8730 - val_loss: 0.0691 - val_acc: 0.7244\n",
      "Epoch 1094/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0333 - acc: 0.8730 - val_loss: 0.0671 - val_acc: 0.7329\n",
      "Epoch 1095/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0332 - acc: 0.8730 - val_loss: 0.0682 - val_acc: 0.7286\n",
      "Epoch 1096/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0331 - acc: 0.8730 - val_loss: 0.0680 - val_acc: 0.7308\n",
      "Epoch 1097/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0332 - acc: 0.8730 - val_loss: 0.0676 - val_acc: 0.7286\n",
      "Epoch 1098/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0332 - acc: 0.8730 - val_loss: 0.0679 - val_acc: 0.7265\n",
      "Epoch 1099/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0331 - acc: 0.8730 - val_loss: 0.0683 - val_acc: 0.7265\n",
      "Epoch 1100/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0329 - acc: 0.8730 - val_loss: 0.0684 - val_acc: 0.7286\n",
      "Epoch 1101/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0334 - acc: 0.8730 - val_loss: 0.0684 - val_acc: 0.7265\n",
      "Epoch 1102/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0330 - acc: 0.8730 - val_loss: 0.0685 - val_acc: 0.7265\n",
      "Epoch 1103/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0328 - acc: 0.8730 - val_loss: 0.0690 - val_acc: 0.7265\n",
      "Epoch 1104/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0331 - acc: 0.8730 - val_loss: 0.0690 - val_acc: 0.7286\n",
      "Epoch 1105/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0332 - acc: 0.8730 - val_loss: 0.0685 - val_acc: 0.7222\n",
      "Epoch 1106/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0328 - acc: 0.8730 - val_loss: 0.0678 - val_acc: 0.7308\n",
      "Epoch 1107/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0330 - acc: 0.8730 - val_loss: 0.0675 - val_acc: 0.7286\n",
      "Epoch 1108/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0330 - acc: 0.8730 - val_loss: 0.0676 - val_acc: 0.7286\n",
      "Epoch 1109/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0327 - acc: 0.8730 - val_loss: 0.0693 - val_acc: 0.7201\n",
      "Epoch 1110/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0331 - acc: 0.8730 - val_loss: 0.0692 - val_acc: 0.7201\n",
      "Epoch 1111/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0327 - acc: 0.8730 - val_loss: 0.0681 - val_acc: 0.7308\n",
      "Epoch 1112/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0326 - acc: 0.8730 - val_loss: 0.0697 - val_acc: 0.7158\n",
      "Epoch 1113/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0329 - acc: 0.8730 - val_loss: 0.0686 - val_acc: 0.7286\n",
      "Epoch 1114/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0328 - acc: 0.8730 - val_loss: 0.0689 - val_acc: 0.7265\n",
      "Epoch 1115/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0328 - acc: 0.8730 - val_loss: 0.0672 - val_acc: 0.7308\n",
      "Epoch 1116/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0327 - acc: 0.8730 - val_loss: 0.0674 - val_acc: 0.7329\n",
      "Epoch 1117/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0326 - acc: 0.8730 - val_loss: 0.0684 - val_acc: 0.7244\n",
      "Epoch 1118/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0328 - acc: 0.8730 - val_loss: 0.0693 - val_acc: 0.7201\n",
      "Epoch 1119/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0325 - acc: 0.8730 - val_loss: 0.0693 - val_acc: 0.7265\n",
      "Epoch 1120/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0325 - acc: 0.8730 - val_loss: 0.0670 - val_acc: 0.7308\n",
      "Epoch 1121/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0324 - acc: 0.8730 - val_loss: 0.0681 - val_acc: 0.7244\n",
      "Epoch 1122/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0325 - acc: 0.8730 - val_loss: 0.0676 - val_acc: 0.7308\n",
      "Epoch 1123/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0326 - acc: 0.8730 - val_loss: 0.0696 - val_acc: 0.7222\n",
      "Epoch 1124/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0328 - acc: 0.8730 - val_loss: 0.0689 - val_acc: 0.7222\n",
      "Epoch 1125/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0324 - acc: 0.8730 - val_loss: 0.0695 - val_acc: 0.7265\n",
      "Epoch 1126/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0324 - acc: 0.8730 - val_loss: 0.0664 - val_acc: 0.7415\n",
      "Epoch 1127/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0322 - acc: 0.8730 - val_loss: 0.0685 - val_acc: 0.7308\n",
      "Epoch 1128/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0324 - acc: 0.8730 - val_loss: 0.0682 - val_acc: 0.7265\n",
      "Epoch 1129/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0323 - acc: 0.8730 - val_loss: 0.0673 - val_acc: 0.7329\n",
      "Epoch 1130/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0323 - acc: 0.8730 - val_loss: 0.0692 - val_acc: 0.7265\n",
      "Epoch 1131/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0326 - acc: 0.8730 - val_loss: 0.0678 - val_acc: 0.7286\n",
      "Epoch 1132/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0325 - acc: 0.8730 - val_loss: 0.0666 - val_acc: 0.7329\n",
      "Epoch 1133/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0327 - acc: 0.8730 - val_loss: 0.0683 - val_acc: 0.7265\n",
      "Epoch 1134/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0322 - acc: 0.8730 - val_loss: 0.0672 - val_acc: 0.7308\n",
      "Epoch 1135/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0323 - acc: 0.8730 - val_loss: 0.0676 - val_acc: 0.7244\n",
      "Epoch 1136/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0322 - acc: 0.8730 - val_loss: 0.0671 - val_acc: 0.7308\n",
      "Epoch 1137/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0321 - acc: 0.8730 - val_loss: 0.0688 - val_acc: 0.7201\n",
      "Epoch 1138/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0321 - acc: 0.8730 - val_loss: 0.0685 - val_acc: 0.7308\n",
      "Epoch 1139/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0321 - acc: 0.8730 - val_loss: 0.0679 - val_acc: 0.7286\n",
      "Epoch 1140/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0320 - acc: 0.8730 - val_loss: 0.0676 - val_acc: 0.7265\n",
      "Epoch 1141/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0321 - acc: 0.8730 - val_loss: 0.0678 - val_acc: 0.7265\n",
      "Epoch 1142/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0320 - acc: 0.8730 - val_loss: 0.0683 - val_acc: 0.7222\n",
      "Epoch 1143/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0321 - acc: 0.8730 - val_loss: 0.0689 - val_acc: 0.7244\n",
      "Epoch 1144/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0321 - acc: 0.8730 - val_loss: 0.0676 - val_acc: 0.7308\n",
      "Epoch 1145/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0321 - acc: 0.8730 - val_loss: 0.0662 - val_acc: 0.7329\n",
      "Epoch 1146/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0321 - acc: 0.8730 - val_loss: 0.0673 - val_acc: 0.7286\n",
      "Epoch 1147/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0318 - acc: 0.8730 - val_loss: 0.0685 - val_acc: 0.7286\n",
      "Epoch 1148/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0321 - acc: 0.8730 - val_loss: 0.0677 - val_acc: 0.7308\n",
      "Epoch 1149/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0318 - acc: 0.8730 - val_loss: 0.0683 - val_acc: 0.7265\n",
      "Epoch 1150/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0318 - acc: 0.8730 - val_loss: 0.0679 - val_acc: 0.7244\n",
      "Epoch 1151/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0672 - val_acc: 0.7329\n",
      "Epoch 1152/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0319 - acc: 0.8730 - val_loss: 0.0673 - val_acc: 0.7308\n",
      "Epoch 1153/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0321 - acc: 0.8730 - val_loss: 0.0668 - val_acc: 0.7372\n",
      "Epoch 1154/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0319 - acc: 0.8730 - val_loss: 0.0671 - val_acc: 0.7308\n",
      "Epoch 1155/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0320 - acc: 0.8730 - val_loss: 0.0664 - val_acc: 0.7350\n",
      "Epoch 1156/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0318 - acc: 0.8730 - val_loss: 0.0668 - val_acc: 0.7308\n",
      "Epoch 1157/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0664 - val_acc: 0.7308\n",
      "Epoch 1158/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0320 - acc: 0.8730 - val_loss: 0.0657 - val_acc: 0.7329\n",
      "Epoch 1159/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0318 - acc: 0.8730 - val_loss: 0.0689 - val_acc: 0.7201\n",
      "Epoch 1160/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0662 - val_acc: 0.7372\n",
      "Epoch 1161/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0316 - acc: 0.8730 - val_loss: 0.0676 - val_acc: 0.7265\n",
      "Epoch 1162/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0318 - acc: 0.8730 - val_loss: 0.0670 - val_acc: 0.7308\n",
      "Epoch 1163/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0316 - acc: 0.8730 - val_loss: 0.0687 - val_acc: 0.7201\n",
      "Epoch 1164/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0666 - val_acc: 0.7329\n",
      "Epoch 1165/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0674 - val_acc: 0.7308\n",
      "Epoch 1166/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0683 - val_acc: 0.7286\n",
      "Epoch 1167/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0316 - acc: 0.8730 - val_loss: 0.0682 - val_acc: 0.7265\n",
      "Epoch 1168/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0315 - acc: 0.8730 - val_loss: 0.0680 - val_acc: 0.7244\n",
      "Epoch 1169/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0663 - val_acc: 0.7350\n",
      "Epoch 1170/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0315 - acc: 0.8730 - val_loss: 0.0664 - val_acc: 0.7329\n",
      "Epoch 1171/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0316 - acc: 0.8730 - val_loss: 0.0675 - val_acc: 0.7308\n",
      "Epoch 1172/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0674 - val_acc: 0.7265\n",
      "Epoch 1173/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0690 - val_acc: 0.7201\n",
      "Epoch 1174/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0314 - acc: 0.8730 - val_loss: 0.0672 - val_acc: 0.7286\n",
      "Epoch 1175/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0315 - acc: 0.8730 - val_loss: 0.0672 - val_acc: 0.7286\n",
      "Epoch 1176/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0313 - acc: 0.8730 - val_loss: 0.0678 - val_acc: 0.7244\n",
      "Epoch 1177/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0314 - acc: 0.8730 - val_loss: 0.0672 - val_acc: 0.7265\n",
      "Epoch 1178/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0315 - acc: 0.8730 - val_loss: 0.0662 - val_acc: 0.7350\n",
      "Epoch 1179/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0315 - acc: 0.8730 - val_loss: 0.0687 - val_acc: 0.7244\n",
      "Epoch 1180/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0313 - acc: 0.8730 - val_loss: 0.0665 - val_acc: 0.7329\n",
      "Epoch 1181/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0312 - acc: 0.8730 - val_loss: 0.0675 - val_acc: 0.7244\n",
      "Epoch 1182/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0312 - acc: 0.8730 - val_loss: 0.0665 - val_acc: 0.7329\n",
      "Epoch 1183/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0675 - val_acc: 0.7286\n",
      "Epoch 1184/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0313 - acc: 0.8730 - val_loss: 0.0663 - val_acc: 0.7286\n",
      "Epoch 1185/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0314 - acc: 0.8730 - val_loss: 0.0686 - val_acc: 0.7244\n",
      "Epoch 1186/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0311 - acc: 0.8730 - val_loss: 0.0662 - val_acc: 0.7308\n",
      "Epoch 1187/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0313 - acc: 0.8730 - val_loss: 0.0680 - val_acc: 0.7222\n",
      "Epoch 1188/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0312 - acc: 0.8730 - val_loss: 0.0675 - val_acc: 0.7286\n",
      "Epoch 1189/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0312 - acc: 0.8730 - val_loss: 0.0660 - val_acc: 0.7286\n",
      "Epoch 1190/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0317 - acc: 0.8730 - val_loss: 0.0682 - val_acc: 0.7265\n",
      "Epoch 1191/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0312 - acc: 0.8730 - val_loss: 0.0666 - val_acc: 0.7286\n",
      "Epoch 1192/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0313 - acc: 0.8730 - val_loss: 0.0676 - val_acc: 0.7265\n",
      "Epoch 1193/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0310 - acc: 0.8730 - val_loss: 0.0666 - val_acc: 0.7308\n",
      "Epoch 1194/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0309 - acc: 0.8730 - val_loss: 0.0669 - val_acc: 0.7286\n",
      "Epoch 1195/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0311 - acc: 0.8730 - val_loss: 0.0674 - val_acc: 0.7308\n",
      "Epoch 1196/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0310 - acc: 0.8730 - val_loss: 0.0663 - val_acc: 0.7350\n",
      "Epoch 1197/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0308 - acc: 0.8730 - val_loss: 0.0676 - val_acc: 0.7265\n",
      "Epoch 1198/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0309 - acc: 0.8730 - val_loss: 0.0660 - val_acc: 0.7350\n",
      "Epoch 1199/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0310 - acc: 0.8730 - val_loss: 0.0670 - val_acc: 0.7308\n",
      "Epoch 1200/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0309 - acc: 0.8730 - val_loss: 0.0671 - val_acc: 0.7286\n",
      "Epoch 1201/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0309 - acc: 0.8730 - val_loss: 0.0674 - val_acc: 0.7265\n",
      "Epoch 1202/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0308 - acc: 0.8730 - val_loss: 0.0658 - val_acc: 0.7372\n",
      "Epoch 1203/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0309 - acc: 0.8730 - val_loss: 0.0662 - val_acc: 0.7329\n",
      "Epoch 1204/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0309 - acc: 0.8730 - val_loss: 0.0659 - val_acc: 0.7350\n",
      "Epoch 1205/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0309 - acc: 0.8730 - val_loss: 0.0665 - val_acc: 0.7372\n",
      "Epoch 1206/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0308 - acc: 0.8730 - val_loss: 0.0659 - val_acc: 0.7350\n",
      "Epoch 1207/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0308 - acc: 0.8730 - val_loss: 0.0656 - val_acc: 0.7350\n",
      "Epoch 1208/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0310 - acc: 0.8730 - val_loss: 0.0670 - val_acc: 0.7286\n",
      "Epoch 1209/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0309 - acc: 0.8730 - val_loss: 0.0670 - val_acc: 0.7286\n",
      "Epoch 1210/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0308 - acc: 0.8730 - val_loss: 0.0661 - val_acc: 0.7350\n",
      "Epoch 1211/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0307 - acc: 0.8730 - val_loss: 0.0671 - val_acc: 0.7308\n",
      "Epoch 1212/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0307 - acc: 0.8730 - val_loss: 0.0672 - val_acc: 0.7265\n",
      "Epoch 1213/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0308 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7372\n",
      "Epoch 1214/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0308 - acc: 0.8730 - val_loss: 0.0671 - val_acc: 0.7265\n",
      "Epoch 1215/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0307 - acc: 0.8730 - val_loss: 0.0669 - val_acc: 0.7308\n",
      "Epoch 1216/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0310 - acc: 0.8730 - val_loss: 0.0673 - val_acc: 0.7265\n",
      "Epoch 1217/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0307 - acc: 0.8730 - val_loss: 0.0672 - val_acc: 0.7265\n",
      "Epoch 1218/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0307 - acc: 0.8730 - val_loss: 0.0675 - val_acc: 0.7222\n",
      "Epoch 1219/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0308 - acc: 0.8730 - val_loss: 0.0656 - val_acc: 0.7329\n",
      "Epoch 1220/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0307 - acc: 0.8730 - val_loss: 0.0673 - val_acc: 0.7329\n",
      "Epoch 1221/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0307 - acc: 0.8730 - val_loss: 0.0661 - val_acc: 0.7329\n",
      "Epoch 1222/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0306 - acc: 0.8730 - val_loss: 0.0667 - val_acc: 0.7329\n",
      "Epoch 1223/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0306 - acc: 0.8730 - val_loss: 0.0650 - val_acc: 0.7372\n",
      "Epoch 1224/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0307 - acc: 0.8730 - val_loss: 0.0654 - val_acc: 0.7372\n",
      "Epoch 1225/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0306 - acc: 0.8730 - val_loss: 0.0645 - val_acc: 0.7393\n",
      "Epoch 1226/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0306 - acc: 0.8730 - val_loss: 0.0656 - val_acc: 0.7286\n",
      "Epoch 1227/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0305 - acc: 0.8730 - val_loss: 0.0670 - val_acc: 0.7244\n",
      "Epoch 1228/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0306 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7350\n",
      "Epoch 1229/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0306 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7329\n",
      "Epoch 1230/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0304 - acc: 0.8730 - val_loss: 0.0661 - val_acc: 0.7286\n",
      "Epoch 1231/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0306 - acc: 0.8730 - val_loss: 0.0661 - val_acc: 0.7265\n",
      "Epoch 1232/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0305 - acc: 0.8730 - val_loss: 0.0657 - val_acc: 0.7308\n",
      "Epoch 1233/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0304 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7308\n",
      "Epoch 1234/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0304 - acc: 0.8730 - val_loss: 0.0654 - val_acc: 0.7329\n",
      "Epoch 1235/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0304 - acc: 0.8730 - val_loss: 0.0652 - val_acc: 0.7308\n",
      "Epoch 1236/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0305 - acc: 0.8730 - val_loss: 0.0653 - val_acc: 0.7329\n",
      "Epoch 1237/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0308 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7350\n",
      "Epoch 1238/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0306 - acc: 0.8730 - val_loss: 0.0665 - val_acc: 0.7286\n",
      "Epoch 1239/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0303 - acc: 0.8730 - val_loss: 0.0645 - val_acc: 0.7350\n",
      "Epoch 1240/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0304 - acc: 0.8730 - val_loss: 0.0650 - val_acc: 0.7393\n",
      "Epoch 1241/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0304 - acc: 0.8730 - val_loss: 0.0640 - val_acc: 0.7372\n",
      "Epoch 1242/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0304 - acc: 0.8730 - val_loss: 0.0658 - val_acc: 0.7329\n",
      "Epoch 1243/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0303 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7329\n",
      "Epoch 1244/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0303 - acc: 0.8730 - val_loss: 0.0662 - val_acc: 0.7329\n",
      "Epoch 1245/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0304 - acc: 0.8730 - val_loss: 0.0662 - val_acc: 0.7308\n",
      "Epoch 1246/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0303 - acc: 0.8730 - val_loss: 0.0656 - val_acc: 0.7265\n",
      "Epoch 1247/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0303 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7350\n",
      "Epoch 1248/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0302 - acc: 0.8730 - val_loss: 0.0672 - val_acc: 0.7286\n",
      "Epoch 1249/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0303 - acc: 0.8730 - val_loss: 0.0669 - val_acc: 0.7286\n",
      "Epoch 1250/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0302 - acc: 0.8730 - val_loss: 0.0669 - val_acc: 0.7265\n",
      "Epoch 1251/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0302 - acc: 0.8730 - val_loss: 0.0652 - val_acc: 0.7308\n",
      "Epoch 1252/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0672 - val_acc: 0.7265\n",
      "Epoch 1253/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0654 - val_acc: 0.7372\n",
      "Epoch 1254/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0302 - acc: 0.8730 - val_loss: 0.0659 - val_acc: 0.7308\n",
      "Epoch 1255/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0656 - val_acc: 0.7308\n",
      "Epoch 1256/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0662 - val_acc: 0.7308\n",
      "Epoch 1257/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0300 - acc: 0.8730 - val_loss: 0.0663 - val_acc: 0.7286\n",
      "Epoch 1258/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0662 - val_acc: 0.7286\n",
      "Epoch 1259/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0658 - val_acc: 0.7286\n",
      "Epoch 1260/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0300 - acc: 0.8730 - val_loss: 0.0664 - val_acc: 0.7265\n",
      "Epoch 1261/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0653 - val_acc: 0.7329\n",
      "Epoch 1262/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0669 - val_acc: 0.7244\n",
      "Epoch 1263/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0665 - val_acc: 0.7265\n",
      "Epoch 1264/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0302 - acc: 0.8730 - val_loss: 0.0646 - val_acc: 0.7329\n",
      "Epoch 1265/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0654 - val_acc: 0.7308\n",
      "Epoch 1266/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0665 - val_acc: 0.7265\n",
      "Epoch 1267/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0300 - acc: 0.8730 - val_loss: 0.0661 - val_acc: 0.7308\n",
      "Epoch 1268/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0299 - acc: 0.8730 - val_loss: 0.0651 - val_acc: 0.7393\n",
      "Epoch 1269/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0300 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7350\n",
      "Epoch 1270/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0299 - acc: 0.8730 - val_loss: 0.0659 - val_acc: 0.7308\n",
      "Epoch 1271/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0301 - acc: 0.8730 - val_loss: 0.0651 - val_acc: 0.7350\n",
      "Epoch 1272/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0300 - acc: 0.8730 - val_loss: 0.0648 - val_acc: 0.7350\n",
      "Epoch 1273/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0299 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7286\n",
      "Epoch 1274/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0300 - acc: 0.8730 - val_loss: 0.0665 - val_acc: 0.7265\n",
      "Epoch 1275/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0299 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7329\n",
      "Epoch 1276/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0299 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7308\n",
      "Epoch 1277/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0299 - acc: 0.8730 - val_loss: 0.0660 - val_acc: 0.7265\n",
      "Epoch 1278/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0299 - acc: 0.8730 - val_loss: 0.0657 - val_acc: 0.7308\n",
      "Epoch 1279/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0299 - acc: 0.8730 - val_loss: 0.0643 - val_acc: 0.7415\n",
      "Epoch 1280/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0298 - acc: 0.8730 - val_loss: 0.0658 - val_acc: 0.7308\n",
      "Epoch 1281/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0298 - acc: 0.8730 - val_loss: 0.0657 - val_acc: 0.7308\n",
      "Epoch 1282/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0299 - acc: 0.8730 - val_loss: 0.0638 - val_acc: 0.7372\n",
      "Epoch 1283/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0298 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7329\n",
      "Epoch 1284/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0646 - val_acc: 0.7372\n",
      "Epoch 1285/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0298 - acc: 0.8730 - val_loss: 0.0642 - val_acc: 0.7372\n",
      "Epoch 1286/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0298 - acc: 0.8730 - val_loss: 0.0656 - val_acc: 0.7286\n",
      "Epoch 1287/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0300 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7308\n",
      "Epoch 1288/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0657 - val_acc: 0.7308\n",
      "Epoch 1289/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0298 - acc: 0.8730 - val_loss: 0.0646 - val_acc: 0.7308\n",
      "Epoch 1290/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0298 - acc: 0.8730 - val_loss: 0.0665 - val_acc: 0.7244\n",
      "Epoch 1291/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0298 - acc: 0.8730 - val_loss: 0.0646 - val_acc: 0.7308\n",
      "Epoch 1292/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0657 - val_acc: 0.7308\n",
      "Epoch 1293/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0298 - acc: 0.8730 - val_loss: 0.0653 - val_acc: 0.7329\n",
      "Epoch 1294/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0659 - val_acc: 0.7265\n",
      "Epoch 1295/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0639 - val_acc: 0.7436\n",
      "Epoch 1296/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0651 - val_acc: 0.7286\n",
      "Epoch 1297/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0659 - val_acc: 0.7286\n",
      "Epoch 1298/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0657 - val_acc: 0.7308\n",
      "Epoch 1299/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0657 - val_acc: 0.7329\n",
      "Epoch 1300/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0296 - acc: 0.8730 - val_loss: 0.0647 - val_acc: 0.7350\n",
      "Epoch 1301/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0652 - val_acc: 0.7308\n",
      "Epoch 1302/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0296 - acc: 0.8730 - val_loss: 0.0657 - val_acc: 0.7308\n",
      "Epoch 1303/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0296 - acc: 0.8730 - val_loss: 0.0652 - val_acc: 0.7329\n",
      "Epoch 1304/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0296 - acc: 0.8730 - val_loss: 0.0651 - val_acc: 0.7329\n",
      "Epoch 1305/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0643 - val_acc: 0.7329\n",
      "Epoch 1306/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0662 - val_acc: 0.7329\n",
      "Epoch 1307/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0297 - acc: 0.8730 - val_loss: 0.0647 - val_acc: 0.7308\n",
      "Epoch 1308/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0296 - acc: 0.8730 - val_loss: 0.0650 - val_acc: 0.7308\n",
      "Epoch 1309/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0296 - acc: 0.8730 - val_loss: 0.0640 - val_acc: 0.7350\n",
      "Epoch 1310/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0645 - val_acc: 0.7329\n",
      "Epoch 1311/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0652 - val_acc: 0.7308\n",
      "Epoch 1312/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0659 - val_acc: 0.7308\n",
      "Epoch 1313/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0657 - val_acc: 0.7308\n",
      "Epoch 1314/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0296 - acc: 0.8730 - val_loss: 0.0641 - val_acc: 0.7329\n",
      "Epoch 1315/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0658 - val_acc: 0.7329\n",
      "Epoch 1316/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0647 - val_acc: 0.7308\n",
      "Epoch 1317/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0649 - val_acc: 0.7265\n",
      "Epoch 1318/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0296 - acc: 0.8730 - val_loss: 0.0648 - val_acc: 0.7350\n",
      "Epoch 1319/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0650 - val_acc: 0.7286\n",
      "Epoch 1320/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0639 - val_acc: 0.7329\n",
      "Epoch 1321/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0647 - val_acc: 0.7350\n",
      "Epoch 1322/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0645 - val_acc: 0.7372\n",
      "Epoch 1323/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0639 - val_acc: 0.7329\n",
      "Epoch 1324/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0644 - val_acc: 0.7308\n",
      "Epoch 1325/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0652 - val_acc: 0.7286\n",
      "Epoch 1326/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0641 - val_acc: 0.7308\n",
      "Epoch 1327/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0644 - val_acc: 0.7308\n",
      "Epoch 1328/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7350\n",
      "Epoch 1329/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0640 - val_acc: 0.7244\n",
      "Epoch 1330/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0296 - acc: 0.8730 - val_loss: 0.0652 - val_acc: 0.7286\n",
      "Epoch 1331/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0647 - val_acc: 0.7308\n",
      "Epoch 1332/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0635 - val_acc: 0.7393\n",
      "Epoch 1333/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0658 - val_acc: 0.7329\n",
      "Epoch 1334/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0295 - acc: 0.8730 - val_loss: 0.0643 - val_acc: 0.7265\n",
      "Epoch 1335/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0650 - val_acc: 0.7308\n",
      "Epoch 1336/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0639 - val_acc: 0.7415\n",
      "Epoch 1337/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0649 - val_acc: 0.7265\n",
      "Epoch 1338/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0636 - val_acc: 0.7436\n",
      "Epoch 1339/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0646 - val_acc: 0.7244\n",
      "Epoch 1340/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0643 - val_acc: 0.7244\n",
      "Epoch 1341/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0294 - acc: 0.8730 - val_loss: 0.0644 - val_acc: 0.7286\n",
      "Epoch 1342/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0636 - val_acc: 0.7308\n",
      "Epoch 1343/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0642 - val_acc: 0.7286\n",
      "Epoch 1344/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0635 - val_acc: 0.7372\n",
      "Epoch 1345/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0640 - val_acc: 0.7265\n",
      "Epoch 1346/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0640 - val_acc: 0.7244\n",
      "Epoch 1347/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0647 - val_acc: 0.7286\n",
      "Epoch 1348/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0643 - val_acc: 0.7265\n",
      "Epoch 1349/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0643 - val_acc: 0.7244\n",
      "Epoch 1350/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0643 - val_acc: 0.7265\n",
      "Epoch 1351/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0641 - val_acc: 0.7265\n",
      "Epoch 1352/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7286\n",
      "Epoch 1353/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0640 - val_acc: 0.7222\n",
      "Epoch 1354/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7286\n",
      "Epoch 1355/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0656 - val_acc: 0.7265\n",
      "Epoch 1356/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0647 - val_acc: 0.7286\n",
      "Epoch 1357/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0639 - val_acc: 0.7265\n",
      "Epoch 1358/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0293 - acc: 0.8730 - val_loss: 0.0646 - val_acc: 0.7350\n",
      "Epoch 1359/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7329\n",
      "Epoch 1360/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0636 - val_acc: 0.7329\n",
      "Epoch 1361/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0640 - val_acc: 0.7244\n",
      "Epoch 1362/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0640 - val_acc: 0.7244\n",
      "Epoch 1363/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0648 - val_acc: 0.7244\n",
      "Epoch 1364/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0648 - val_acc: 0.7265\n",
      "Epoch 1365/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0647 - val_acc: 0.7286\n",
      "Epoch 1366/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0638 - val_acc: 0.7244\n",
      "Epoch 1367/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0640 - val_acc: 0.7350\n",
      "Epoch 1368/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7308\n",
      "Epoch 1369/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0642 - val_acc: 0.7244\n",
      "Epoch 1370/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0644 - val_acc: 0.7286\n",
      "Epoch 1371/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0641 - val_acc: 0.7286\n",
      "Epoch 1372/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7308\n",
      "Epoch 1373/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7415\n",
      "Epoch 1374/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.8730 - val_loss: 0.0647 - val_acc: 0.7329\n",
      "Epoch 1375/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0636 - val_acc: 0.7372\n",
      "Epoch 1376/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0652 - val_acc: 0.7286\n",
      "Epoch 1377/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0632 - val_acc: 0.7372\n",
      "Epoch 1378/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0638 - val_acc: 0.7244\n",
      "Epoch 1379/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0641 - val_acc: 0.7393\n",
      "Epoch 1380/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0636 - val_acc: 0.7308\n",
      "Epoch 1381/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0642 - val_acc: 0.7244\n",
      "Epoch 1382/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0638 - val_acc: 0.7286\n",
      "Epoch 1383/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0638 - val_acc: 0.7308\n",
      "Epoch 1384/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0631 - val_acc: 0.7393\n",
      "Epoch 1385/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0291 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.7286\n",
      "Epoch 1386/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0631 - val_acc: 0.7393\n",
      "Epoch 1387/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0653 - val_acc: 0.7201\n",
      "Epoch 1388/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0644 - val_acc: 0.7244\n",
      "Epoch 1389/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0651 - val_acc: 0.7286\n",
      "Epoch 1390/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0626 - val_acc: 0.7436\n",
      "Epoch 1391/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0639 - val_acc: 0.7265\n",
      "Epoch 1392/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0644 - val_acc: 0.7222\n",
      "Epoch 1393/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0631 - val_acc: 0.7286\n",
      "Epoch 1394/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0639 - val_acc: 0.7222\n",
      "Epoch 1395/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0639 - val_acc: 0.7222\n",
      "Epoch 1396/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0638 - val_acc: 0.7244\n",
      "Epoch 1397/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0643 - val_acc: 0.7222\n",
      "Epoch 1398/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0638 - val_acc: 0.7286\n",
      "Epoch 1399/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7265\n",
      "Epoch 1400/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0635 - val_acc: 0.7265\n",
      "Epoch 1401/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0634 - val_acc: 0.7350\n",
      "Epoch 1402/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0639 - val_acc: 0.7201\n",
      "Epoch 1403/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0635 - val_acc: 0.7286\n",
      "Epoch 1404/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0634 - val_acc: 0.7265\n",
      "Epoch 1405/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0290 - acc: 0.8730 - val_loss: 0.0655 - val_acc: 0.7222\n",
      "Epoch 1406/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0631 - val_acc: 0.7265\n",
      "Epoch 1407/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0639 - val_acc: 0.7244\n",
      "Epoch 1408/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0631 - val_acc: 0.7286\n",
      "Epoch 1409/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7329\n",
      "Epoch 1410/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0641 - val_acc: 0.7265\n",
      "Epoch 1411/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0635 - val_acc: 0.7244\n",
      "Epoch 1412/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7329\n",
      "Epoch 1413/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0625 - val_acc: 0.7372\n",
      "Epoch 1414/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0632 - val_acc: 0.7329\n",
      "Epoch 1415/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0629 - val_acc: 0.7308\n",
      "Epoch 1416/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0633 - val_acc: 0.7286\n",
      "Epoch 1417/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0627 - val_acc: 0.7244\n",
      "Epoch 1418/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0644 - val_acc: 0.7222\n",
      "Epoch 1419/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0627 - val_acc: 0.7329\n",
      "Epoch 1420/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0645 - val_acc: 0.7222\n",
      "Epoch 1421/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0633 - val_acc: 0.7286\n",
      "Epoch 1422/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0289 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7329\n",
      "Epoch 1423/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7350\n",
      "Epoch 1424/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7329\n",
      "Epoch 1425/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0635 - val_acc: 0.7244\n",
      "Epoch 1426/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7179\n",
      "Epoch 1427/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0647 - val_acc: 0.7265\n",
      "Epoch 1428/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.7244\n",
      "Epoch 1429/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0636 - val_acc: 0.7244\n",
      "Epoch 1430/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0623 - val_acc: 0.7372\n",
      "Epoch 1431/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.7265\n",
      "Epoch 1432/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0629 - val_acc: 0.7329\n",
      "Epoch 1433/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.7265\n",
      "Epoch 1434/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0643 - val_acc: 0.7201\n",
      "Epoch 1435/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7222\n",
      "Epoch 1436/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0643 - val_acc: 0.7222\n",
      "Epoch 1437/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0623 - val_acc: 0.7329\n",
      "Epoch 1438/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7393\n",
      "Epoch 1439/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0627 - val_acc: 0.7350\n",
      "Epoch 1440/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7308\n",
      "Epoch 1441/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0641 - val_acc: 0.7222\n",
      "Epoch 1442/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0627 - val_acc: 0.7308\n",
      "Epoch 1443/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0640 - val_acc: 0.7222\n",
      "Epoch 1444/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0634 - val_acc: 0.7179\n",
      "Epoch 1445/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0627 - val_acc: 0.7286\n",
      "Epoch 1446/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0632 - val_acc: 0.7244\n",
      "Epoch 1447/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0638 - val_acc: 0.7222\n",
      "Epoch 1448/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0288 - acc: 0.8730 - val_loss: 0.0629 - val_acc: 0.7308\n",
      "Epoch 1449/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0627 - val_acc: 0.7265\n",
      "Epoch 1450/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.7308\n",
      "Epoch 1451/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0625 - val_acc: 0.7350\n",
      "Epoch 1452/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0623 - val_acc: 0.7372\n",
      "Epoch 1453/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0621 - val_acc: 0.7393\n",
      "Epoch 1454/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0629 - val_acc: 0.7308\n",
      "Epoch 1455/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0621 - val_acc: 0.7308\n",
      "Epoch 1456/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0629 - val_acc: 0.7222\n",
      "Epoch 1457/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0636 - val_acc: 0.7265\n",
      "Epoch 1458/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7265\n",
      "Epoch 1459/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7372\n",
      "Epoch 1460/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.7244\n",
      "Epoch 1461/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0619 - val_acc: 0.7350\n",
      "Epoch 1462/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0619 - val_acc: 0.7436\n",
      "Epoch 1463/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0631 - val_acc: 0.7286\n",
      "Epoch 1464/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7372\n",
      "Epoch 1465/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7265\n",
      "Epoch 1466/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0622 - val_acc: 0.7350\n",
      "Epoch 1467/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0635 - val_acc: 0.7222\n",
      "Epoch 1468/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0638 - val_acc: 0.7179\n",
      "Epoch 1469/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0625 - val_acc: 0.7372\n",
      "Epoch 1470/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0634 - val_acc: 0.7222\n",
      "Epoch 1471/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.7265\n",
      "Epoch 1472/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0626 - val_acc: 0.7286\n",
      "Epoch 1473/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0634 - val_acc: 0.7222\n",
      "Epoch 1474/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0625 - val_acc: 0.7265\n",
      "Epoch 1475/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0287 - acc: 0.8730 - val_loss: 0.0627 - val_acc: 0.7286\n",
      "Epoch 1476/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0627 - val_acc: 0.7265\n",
      "Epoch 1477/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0631 - val_acc: 0.7222\n",
      "Epoch 1478/2000\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0631 - val_acc: 0.7222\n",
      "Epoch 1479/2000\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0631 - val_acc: 0.7286\n",
      "Epoch 1480/2000\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7201\n",
      "Epoch 1481/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7286\n",
      "Epoch 1482/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7286\n",
      "Epoch 1483/2000\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0625 - val_acc: 0.7286\n",
      "Epoch 1484/2000\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7222\n",
      "Epoch 1485/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0618 - val_acc: 0.7350\n",
      "Epoch 1486/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7244\n",
      "Epoch 1487/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0625 - val_acc: 0.7265\n",
      "Epoch 1488/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.7265\n",
      "Epoch 1489/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7308\n",
      "Epoch 1490/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0623 - val_acc: 0.7329\n",
      "Epoch 1491/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7286\n",
      "Epoch 1492/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0622 - val_acc: 0.7308\n",
      "Epoch 1493/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.7222\n",
      "Epoch 1494/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0621 - val_acc: 0.7286\n",
      "Epoch 1495/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0626 - val_acc: 0.7286\n",
      "Epoch 1496/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7201\n",
      "Epoch 1497/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0617 - val_acc: 0.7329\n",
      "Epoch 1498/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0622 - val_acc: 0.7372\n",
      "Epoch 1499/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0632 - val_acc: 0.7201\n",
      "Epoch 1500/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0625 - val_acc: 0.7265\n",
      "Epoch 1501/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0619 - val_acc: 0.7393\n",
      "Epoch 1502/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.7244\n",
      "Epoch 1503/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7286\n",
      "Epoch 1504/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0626 - val_acc: 0.7244\n",
      "Epoch 1505/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0629 - val_acc: 0.7201\n",
      "Epoch 1506/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7244\n",
      "Epoch 1507/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0622 - val_acc: 0.7329\n",
      "Epoch 1508/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0633 - val_acc: 0.7286\n",
      "Epoch 1509/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0286 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7265\n",
      "Epoch 1510/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0616 - val_acc: 0.7350\n",
      "Epoch 1511/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0621 - val_acc: 0.7350\n",
      "Epoch 1512/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0621 - val_acc: 0.7350\n",
      "Epoch 1513/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0617 - val_acc: 0.7329\n",
      "Epoch 1514/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0621 - val_acc: 0.7265\n",
      "Epoch 1515/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0622 - val_acc: 0.7265\n",
      "Epoch 1516/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0632 - val_acc: 0.7265\n",
      "Epoch 1517/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0615 - val_acc: 0.7372\n",
      "Epoch 1518/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0622 - val_acc: 0.7308\n",
      "Epoch 1519/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0623 - val_acc: 0.7244\n",
      "Epoch 1520/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0633 - val_acc: 0.7201\n",
      "Epoch 1521/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0611 - val_acc: 0.7393\n",
      "Epoch 1522/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0617 - val_acc: 0.7329\n",
      "Epoch 1523/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0619 - val_acc: 0.7393\n",
      "Epoch 1524/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0627 - val_acc: 0.7222\n",
      "Epoch 1525/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0619 - val_acc: 0.7265\n",
      "Epoch 1526/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0626 - val_acc: 0.7286\n",
      "Epoch 1527/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0621 - val_acc: 0.7329\n",
      "Epoch 1528/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0626 - val_acc: 0.7244\n",
      "Epoch 1529/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0619 - val_acc: 0.7329\n",
      "Epoch 1530/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0612 - val_acc: 0.7372\n",
      "Epoch 1531/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0619 - val_acc: 0.7329\n",
      "Epoch 1532/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0621 - val_acc: 0.7286\n",
      "Epoch 1533/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0624 - val_acc: 0.7244\n",
      "Epoch 1534/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0613 - val_acc: 0.7372\n",
      "Epoch 1535/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0628 - val_acc: 0.7222\n",
      "Epoch 1536/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0616 - val_acc: 0.7265\n",
      "Epoch 1537/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0627 - val_acc: 0.7222\n",
      "Epoch 1538/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0612 - val_acc: 0.7372\n",
      "Epoch 1539/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0613 - val_acc: 0.7350\n",
      "Epoch 1540/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0284 - acc: 0.8730 - val_loss: 0.0626 - val_acc: 0.7201\n",
      "Epoch 1541/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0620 - val_acc: 0.7308\n",
      "Epoch 1542/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0621 - val_acc: 0.7286\n",
      "Epoch 1543/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0619 - val_acc: 0.7329\n",
      "Epoch 1544/2000\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0637 - val_acc: 0.7244\n",
      "Epoch 1545/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0285 - acc: 0.8730 - val_loss: 0.0619 - val_acc: 0.7350\n",
      "Epoch 1546/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0284 - acc: 0.8730 - val_loss: 0.0623 - val_acc: 0.7244\n",
      "Epoch 1547/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0284 - acc: 0.8730 - val_loss: 0.0614 - val_acc: 0.7350\n",
      "Epoch 1548/2000\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0284 - acc: 0.8730 - val_loss: 0.0614 - val_acc: 0.7350\n",
      "Epoch 1549/2000\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0271 - acc: 0.8793"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "train_data = pad_sequences(train_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "test_data = pad_sequences(test_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels,\n",
    "#                                                   test_size=0.28, random_state=2019,\n",
    "#                                                   stratify=train_labels)\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_val = to_categorical(y_val)\n",
    "word_input_tensor = Input(shape=(maxlen,) , name='words')\n",
    "\n",
    "conv_1d_s3_model = Sequential()\n",
    "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_s3_model.add(layers.Dropout(0.1))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s3_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "conv_1d_s3_model.layers[0].set_weights([w2d.word_embedding])\n",
    "conv_1d_s3_model.layers[0].trainable = False\n",
    "word_output_tensor_0 = conv_1d_s3_model(word_input_tensor)\n",
    "\n",
    "\n",
    "conv_1d_s1_model = Sequential()\n",
    "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s1_model.add(layers.SeparableConv1D(32, 1, activation='relu'))\n",
    "# conv_1d_s1_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "conv_1d_s1_model.layers[0].set_weights([w2d.word_embedding])\n",
    "conv_1d_s1_model.layers[0].trainable = False\n",
    "word_output_tensor_1 = conv_1d_s1_model(word_input_tensor)\n",
    "\n",
    "conv_1d_complex_model = Sequential()\n",
    "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.GlobalAveragePooling1D())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "conv_1d_complex_model.layers[0].set_weights([w2d.word_embedding])\n",
    "conv_1d_complex_model.layers[0].trainable = False\n",
    "word_output_tensor_2 = conv_1d_complex_model(word_input_tensor)\n",
    "\n",
    "concatenated = layers.concatenate([word_output_tensor_0, word_output_tensor_1\n",
    "                                   , word_output_tensor_2\n",
    "                                  ], axis=-1)\n",
    "# concatenated = layers.Dense(32, activation='relu')(concatenated)\n",
    "answer = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model(word_input_tensor, answer)\n",
    "model.summary()\n",
    "\n",
    "# model.layers[0].set_weights([w2d.word_embedding])\n",
    "# model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=3e-4),\n",
    "              loss='mae',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=2000,\n",
    "                    batch_size=1)\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12785f1d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1416118d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
