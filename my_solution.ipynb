{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Reading general data of the problems\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Reading general data of the problems, done!\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# coding\u003dutf-8\nimport json\nimport os\n\nfrom MyUtils import clean_folder, read_files\nfrom Word2Dim import Word2Dim\n\ndataset_path \u003d \u0027.\u0027 + os.sep + \u0027pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23\u0027\noutpath \u003d \u0027.\u0027 + os.sep + \u0027dev_out\u0027\n\nclean_folder(outpath)\n\ninfocollection \u003d dataset_path + os.sep + \u0027collection-info.json\u0027\nproblems \u003d []\nlanguage \u003d []\nwith open(infocollection, \u0027r\u0027) as f:\n    for attrib in json.load(f):\n        problems.append(attrib[\u0027problem-name\u0027])\n        language.append(attrib[\u0027language\u0027])\nprint(\u0027Reading general data of the problems, done!\u0027)\n"
    },
    {
      "cell_type": "markdown",
      "source": "# Reading problem 1",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "doc count to process:  63\n",
            "process_doc, done!\n",
            "doc count to process:  468\n",
            "Reading problem 1, done!\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "problem \u003d problems[0]\nindex \u003d 0\ninfoproblem \u003d dataset_path + os.sep + problem + os.sep + \u0027problem-info.json\u0027\ncandidates \u003d []\nwith open(infoproblem, \u0027r\u0027) as f:\n    fj \u003d json.load(f)\n    unk_folder \u003d fj[\u0027unknown-folder\u0027]\n    for attrib in fj[\u0027candidate-authors\u0027]:\n        candidates.append(attrib[\u0027author-name\u0027])\n\ncandidates.sort()\n# Building training set\ntrain_docs \u003d []\nfor candidate in candidates:\n    train_docs.extend(read_files(dataset_path + os.sep + problem, candidate))\ntrain_texts \u003d [text for i, (text, label) in enumerate(train_docs)]\ntrain_labels \u003d [label for i, (text, label) in enumerate(train_docs)]\nindex_2_label_dict \u003d {i: l for i, l in enumerate(set(train_labels))}\nlabel_2_index_dict \u003d {l: i for i, l in enumerate(set(train_labels))}\ntrain_labels \u003d sorted([label_2_index_dict[v] for v in train_labels])\nw2d \u003d Word2Dim()\ntrain_tokenized_with_pos, train_tokenized_indexed \u003d w2d.fit_transform_texts(train_texts, train_labels,\n                                                                            language[index])\n\nmaxlen \u003d len(max(train_tokenized_indexed, key\u003dlen))  # We will cut the texts after # words\nembedding_dim \u003d w2d.word_embedding.shape[1]\n\n# preparing test set\nground_truth_file \u003d dataset_path + os.sep + problem + os.sep + \u0027ground-truth.json\u0027\ngt \u003d {}\nwith open(ground_truth_file, \u0027r\u0027) as f:\n    for attrib in json.load(f)[\u0027ground_truth\u0027]:\n        gt[attrib[\u0027unknown-text\u0027]] \u003d attrib[\u0027true-author\u0027]\n\ntest_docs \u003d read_files(dataset_path + os.sep + problem, unk_folder, gt)\ntest_texts \u003d [text for i, (text, label) in enumerate(test_docs)]\ntest_labels \u003d [label for i, (text, label) in enumerate(test_docs)]\n\n# Filter validation to known authors\ntest_texts \u003d [text for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\ntest_labels \u003d [label for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n\ntest_labels \u003d sorted([label_2_index_dict[v] for v in test_labels])\n\ntest_tokenized_with_pos, test_tokenized_indexed \u003d w2d.transform(test_texts)\nprint(\"Reading problem 1, done!\")",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Keras Stuff\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
            "WARNING:tensorflow:From c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate \u003d 1 - keep_prob`.\n",
            "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nembedding_1 (Embedding)      (None, 1020, 9)           68607     \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 18)                1368      \n_________________________________________________________________\ndense_1 (Dense)              (None, 9)                 171       \n_________________________________________________________________\ndense_2 (Dense)              (None, 9)                 90        \n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTotal params: 70,236\nTrainable params: 70,236\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:From c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n",
            "Train on 63 samples, validate on 468 samples\nEpoch 1/120\n",
            "\r 1/63 [..............................] - ETA: 2:50 - loss: 2.1863 - acc: 1.0000",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2/63 [..............................] - ETA: 1:52 - loss: 2.1849 - acc: 1.0000",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3/63 [\u003e.............................] - ETA: 1:33 - loss: 2.1885 - acc: 0.6667",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4/63 [\u003e.............................] - ETA: 1:23 - loss: 2.1929 - acc: 0.5000",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5/63 [\u003d\u003e............................] - ETA: 1:16 - loss: 2.1948 - acc: 0.4000",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6/63 [\u003d\u003e............................] - ETA: 1:11 - loss: 2.1971 - acc: 0.3333",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7/63 [\u003d\u003d\u003e...........................] - ETA: 1:08 - loss: 2.1980 - acc: 0.2857",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8/63 [\u003d\u003d\u003e...........................] - ETA: 1:04 - loss: 2.1990 - acc: 0.2500",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9/63 [\u003d\u003d\u003d\u003e..........................] - ETA: 1:02 - loss: 2.2012 - acc: 0.2222",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/63 [\u003d\u003d\u003d\u003e..........................] - ETA: 1:00 - loss: 2.2016 - acc: 0.2000",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r11/63 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 58s - loss: 2.2022 - acc: 0.1818 ",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/63 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 56s - loss: 2.2022 - acc: 0.1667",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r13/63 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 54s - loss: 2.2018 - acc: 0.1538",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/63 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 53s - loss: 2.2010 - acc: 0.1429",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 51s - loss: 2.2018 - acc: 0.1333",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r16/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 50s - loss: 2.2019 - acc: 0.1250",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r17/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 48s - loss: 2.2013 - acc: 0.1176",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 47s - loss: 2.2020 - acc: 0.1111",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r19/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 45s - loss: 2.2006 - acc: 0.1579",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r20/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 44s - loss: 2.2008 - acc: 0.1500",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r21/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 43s - loss: 2.2011 - acc: 0.1429",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r22/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 42s - loss: 2.2005 - acc: 0.1364",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 41s - loss: 2.1999 - acc: 0.1304",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r24/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 39s - loss: 2.2001 - acc: 0.1250",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r25/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 38s - loss: 2.1990 - acc: 0.1600",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r26/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 37s - loss: 2.1993 - acc: 0.1538",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r27/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 36s - loss: 2.1991 - acc: 0.1481",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 35s - loss: 2.1998 - acc: 0.1429",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 34s - loss: 2.1986 - acc: 0.1724",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r30/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 33s - loss: 2.1986 - acc: 0.1667",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 32s - loss: 2.1972 - acc: 0.1935",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...............] - ETA: 31s - loss: 2.1974 - acc: 0.1875",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r33/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...............] - ETA: 30s - loss: 2.1982 - acc: 0.1818",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r34/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 29s - loss: 2.1987 - acc: 0.1765",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r35/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 28s - loss: 2.1990 - acc: 0.1714",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r36/63 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.............] - ETA: 27s - loss: 2.1996 - acc: 0.1667"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\nfrom keras.layers import Embedding, Flatten, Dense, LSTM, Bidirectional\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop\nfrom keras.utils import to_categorical\nfrom keras_preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\n# from sklearn.model_selection import train_test_split\n\ntrain_data \u003d pad_sequences(train_tokenized_indexed, maxlen\u003dmaxlen)\n\ntest_data \u003d pad_sequences(test_tokenized_indexed, maxlen\u003dmaxlen)\n\nX_train, X_val, y_train, y_val \u003d train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n# X_train, X_val, y_train, y_val \u003d train_test_split(train_data, train_labels,\n#                                                   test_size\u003d0.28, random_state\u003d2019,\n#                                                   stratify\u003dtrain_labels)\n\n# y_train \u003d to_categorical(y_train)\n# y_val \u003d to_categorical(y_val)\n\nmodel \u003d Sequential()\nmodel.add(Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length\u003dmaxlen))\nmodel.add(Bidirectional(LSTM(embedding_dim, dropout\u003d0.2, recurrent_dropout\u003d0.2)))\nmodel.add(Dense(embedding_dim, activation\u003d\u0027relu\u0027))\nmodel.add(Dense(len(set(train_labels)), activation\u003d\u0027softmax\u0027))\nmodel.summary()\n\n# model.layers[0].set_weights([w2d.word_embedding])\n# model.layers[0].trainable \u003d False\n\nmodel.compile(optimizer\u003dRMSprop(lr\u003d0.001),\n              loss\u003d\u0027categorical_crossentropy\u0027,\n              metrics\u003d[\u0027acc\u0027])\nhistory \u003d model.fit(X_train, y_train,\n                    validation_data\u003d(X_val, y_val),\n                    epochs\u003d120,\n                    batch_size\u003d1)\n\nacc \u003d history.history[\u0027acc\u0027]\nval_acc \u003d history.history[\u0027val_acc\u0027]\nloss \u003d history.history[\u0027loss\u0027]\nval_loss \u003d history.history[\u0027val_loss\u0027]\n\nepochs \u003d range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, \u0027bo\u0027, label\u003d\u0027Training acc\u0027)\nplt.plot(epochs, val_acc, \u0027b\u0027, label\u003d\u0027Validation acc\u0027)\nplt.title(\u0027Training and validation accuracy\u0027)\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, \u0027bo\u0027, label\u003d\u0027Training loss\u0027)\nplt.plot(epochs, val_loss, \u0027b\u0027, label\u003d\u0027Validation loss\u0027)\nplt.title(\u0027Training and validation loss\u0027)\nplt.legend()\n\nplt.show()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}