{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Reading general data of the problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading general data of the problems, done!\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import division\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from MyUtils import clean_folder, read_files\n",
    "from Word2Dim import Word2Dim\n",
    "\n",
    "dataset_path = '.' + os.sep + 'pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23'\n",
    "outpath = '.' + os.sep + 'dev_out'\n",
    "\n",
    "clean_folder(outpath)\n",
    "\n",
    "infocollection = dataset_path + os.sep + 'collection-info.json'\n",
    "problems = []\n",
    "language = []\n",
    "with open(infocollection, 'r') as f:\n",
    "    for attrib in json.load(f):\n",
    "        problems.append(attrib['problem-name'])\n",
    "        language.append(attrib['language'])\n",
    "print('Reading general data of the problems, done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reading problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc count to process:  63\n",
      "process_doc, done!\n",
      "doc count to process:  468\n",
      "Reading problem 1, done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "problem = problems[0]\n",
    "index = 0\n",
    "\n",
    "# used for n_gram extraction and word indexing, a threshold which prevent words appearing lower than this value to be counted in calculations\n",
    "tf = 5\n",
    "\n",
    "\n",
    "infoproblem = dataset_path + os.sep + problem + os.sep + 'problem-info.json'\n",
    "candidates = []\n",
    "with open(infoproblem, 'r') as f:\n",
    "    fj = json.load(f)\n",
    "    unk_folder = fj['unknown-folder']\n",
    "    for attrib in fj['candidate-authors']:\n",
    "        candidates.append(attrib['author-name'])\n",
    "\n",
    "candidates.sort()\n",
    "# Building training set\n",
    "train_docs = []\n",
    "for candidate in candidates:\n",
    "    train_docs.extend(read_files(dataset_path + os.sep + problem, candidate))\n",
    "train_texts = [text for i, (text, label) in enumerate(train_docs)]\n",
    "train_labels = [label for i, (text, label) in enumerate(train_docs)]\n",
    "index_2_label_dict = {i: l for i, l in enumerate(set(train_labels))}\n",
    "label_2_index_dict = {l: i for i, l in enumerate(set(train_labels))}\n",
    "train_labels = [label_2_index_dict[v] for v in train_labels]\n",
    "w2d = Word2Dim()\n",
    "train_tokenized_with_pos, train_tokenized_indexed = w2d.fit_transform_texts(train_texts, train_labels,\n",
    "                                                                            language[index], tf= tf)\n",
    "\n",
    "maxlen = len(max(train_tokenized_indexed, key=len))  # We will cut the texts after # words\n",
    "embedding_dim = w2d.word_embedding.shape[1]\n",
    "\n",
    "# preparing test set\n",
    "ground_truth_file = dataset_path + os.sep + problem + os.sep + 'ground-truth.json'\n",
    "gt = {}\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    for attrib in json.load(f)['ground_truth']:\n",
    "        gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "test_docs = read_files(dataset_path + os.sep + problem, unk_folder, gt)\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs)]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs)]\n",
    "\n",
    "# Filter validation to known authors\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "\n",
    "test_labels = [label_2_index_dict[v] for v in test_labels]\n",
    "\n",
    "test_tokenized_with_pos, test_tokenized_indexed = w2d.transform(test_texts)\n",
    "print(\"Reading problem 1, done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Extraction for Neural Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from MyUtils import extract_n_grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "n = 3\n",
    "vocabulary = extract_n_grams(train_docs, n, tf)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(n, n), lowercase=False, vocabulary=vocabulary)\n",
    "n_gram_train_data = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "n_gram_train_data = n_gram_train_data.astype(float)\n",
    "\n",
    "for i, v in enumerate(train_texts):\n",
    "    n_gram_train_data[i] = n_gram_train_data[i] / len(train_texts[i])\n",
    "n_gram_test_data = vectorizer.transform(test_texts)\n",
    "n_gram_test_data = n_gram_test_data.astype(float)\n",
    "for i, v in enumerate(test_texts):\n",
    "    n_gram_test_data[i] = n_gram_test_data[i] / len(test_texts[i])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_ngrams = max_abs_scaler.fit_transform(n_gram_train_data)\n",
    "scaled_test_data_ngrams = max_abs_scaler.transform(n_gram_test_data)\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_words = max_abs_scaler.fit_transform(w2d.get_texts_vectorized_and_normalized(train_tokenized_indexed)[:, 1:])\n",
    "scaled_test_data_words = max_abs_scaler.transform(w2d.get_texts_vectorized_and_normalized(test_tokenized_indexed)[:, 1:])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 1302)\n",
      "(468, 1302)\n",
      "1302\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train_data_words.shape)\n",
    "print(scaled_test_data_words.shape)\n",
    "print(len(w2d.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 4152)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input, callbacks\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=40,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "train_data = pad_sequences(train_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "test_data = pad_sequences(test_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels,\n",
    "#                                                   test_size=len(set(train_labels)), random_state=2019,\n",
    "#                                                   stratify=train_labels)\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_val = to_categorical(y_val)\n",
    "# print(X_train.shape)\n",
    "\n",
    "print(scaled_train_data_ngrams.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "n_grams (InputLayer)            (None, 4152)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "words (InputLayer)              (None, 1302)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_21 (Sequential)      (None, 64)           139168      n_grams[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_22 (Sequential)      (None, 64)           47968       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           sequential_21[1][0]              \n",
      "                                                                 sequential_22[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 128)          16512       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 128)          0           dense_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 9)            1161        dropout_63[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 204,809\n",
      "Trainable params: 204,809\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 63 samples, validate on 468 samples\n",
      "Epoch 1/2000\n",
      "63/63 [==============================] - 4s 57ms/step - loss: 8.8954 - acc: 0.1111 - val_loss: 8.6654 - val_acc: 0.1068\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 8.5115 - acc: 0.1270 - val_loss: 8.3995 - val_acc: 0.1026\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 8.2787 - acc: 0.1905 - val_loss: 8.1769 - val_acc: 0.1154\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 8.0545 - acc: 0.1905 - val_loss: 7.9874 - val_acc: 0.1197\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 7.8767 - acc: 0.1429 - val_loss: 7.8213 - val_acc: 0.1111\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 7.6793 - acc: 0.2540 - val_loss: 7.6897 - val_acc: 0.1175\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 7.5847 - acc: 0.2063 - val_loss: 7.5581 - val_acc: 0.1261\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 7.4502 - acc: 0.2063 - val_loss: 7.4407 - val_acc: 0.1303\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 7.2878 - acc: 0.3016 - val_loss: 7.3107 - val_acc: 0.2009\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 7.1705 - acc: 0.3333 - val_loss: 7.1621 - val_acc: 0.4509\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 7.0356 - acc: 0.3333 - val_loss: 7.0819 - val_acc: 0.3269\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 6.8812 - acc: 0.3968 - val_loss: 6.9468 - val_acc: 0.4615\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 6.6862 - acc: 0.4921 - val_loss: 6.8286 - val_acc: 0.5128\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 6.6015 - acc: 0.3651 - val_loss: 6.6599 - val_acc: 0.6560\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 6.4618 - acc: 0.5397 - val_loss: 6.6178 - val_acc: 0.5064\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 6.2491 - acc: 0.5397 - val_loss: 6.5042 - val_acc: 0.4573\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 6.0670 - acc: 0.5556 - val_loss: 6.3346 - val_acc: 0.5598\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 6.0410 - acc: 0.5397 - val_loss: 6.1744 - val_acc: 0.6902\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 5.8602 - acc: 0.6032 - val_loss: 6.0868 - val_acc: 0.6838\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 5.8392 - acc: 0.6032 - val_loss: 6.0047 - val_acc: 0.6432\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 5.7040 - acc: 0.5079 - val_loss: 5.7262 - val_acc: 0.7735\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 5.5034 - acc: 0.6825 - val_loss: 5.7070 - val_acc: 0.7564\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 5.4280 - acc: 0.6667 - val_loss: 5.5906 - val_acc: 0.7500\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 5.2604 - acc: 0.7143 - val_loss: 5.4327 - val_acc: 0.8098\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 5.3216 - acc: 0.6825 - val_loss: 5.5551 - val_acc: 0.6731\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 5.0514 - acc: 0.7937 - val_loss: 5.2336 - val_acc: 0.8141\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.9794 - acc: 0.7460 - val_loss: 5.3029 - val_acc: 0.7927\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.8074 - acc: 0.8413 - val_loss: 5.1656 - val_acc: 0.8077\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.7834 - acc: 0.8413 - val_loss: 4.9909 - val_acc: 0.8397\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.7264 - acc: 0.8095 - val_loss: 4.8238 - val_acc: 0.8697\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.6372 - acc: 0.8095 - val_loss: 4.8638 - val_acc: 0.8462\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.5938 - acc: 0.8254 - val_loss: 5.0426 - val_acc: 0.7286\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.4517 - acc: 0.9048 - val_loss: 4.7714 - val_acc: 0.8547\n",
      "Epoch 34/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.4650 - acc: 0.7778 - val_loss: 4.6861 - val_acc: 0.8590\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.3203 - acc: 0.9048 - val_loss: 4.5737 - val_acc: 0.8803\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.1926 - acc: 0.9206 - val_loss: 4.5182 - val_acc: 0.8761\n",
      "Epoch 37/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.3165 - acc: 0.9206 - val_loss: 4.3880 - val_acc: 0.8889\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.2801 - acc: 0.8095 - val_loss: 4.4210 - val_acc: 0.8761\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.0752 - acc: 0.9206 - val_loss: 4.4430 - val_acc: 0.8590\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.0332 - acc: 0.9048 - val_loss: 4.2694 - val_acc: 0.8996\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.0322 - acc: 0.8889 - val_loss: 4.1846 - val_acc: 0.8910\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 4.0160 - acc: 0.8889 - val_loss: 4.5323 - val_acc: 0.7521\n",
      "Epoch 43/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.9530 - acc: 0.9048 - val_loss: 4.2145 - val_acc: 0.8611\n",
      "Epoch 44/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.8098 - acc: 0.9524 - val_loss: 4.1601 - val_acc: 0.8782\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.7011 - acc: 0.9683 - val_loss: 4.0569 - val_acc: 0.8868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.7244 - acc: 0.9683 - val_loss: 4.0127 - val_acc: 0.8953\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.6547 - acc: 0.9524 - val_loss: 3.9969 - val_acc: 0.8761\n",
      "Epoch 48/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.6078 - acc: 0.9683 - val_loss: 4.0246 - val_acc: 0.8632\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.6077 - acc: 0.9524 - val_loss: 3.9242 - val_acc: 0.8761\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.5902 - acc: 0.9365 - val_loss: 3.8390 - val_acc: 0.8974\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.6547 - acc: 0.9206 - val_loss: 4.0794 - val_acc: 0.8120\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.5365 - acc: 0.9524 - val_loss: 3.8680 - val_acc: 0.8718\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.4618 - acc: 0.9365 - val_loss: 3.7661 - val_acc: 0.8889\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.4012 - acc: 0.9683 - val_loss: 3.8087 - val_acc: 0.8590\n",
      "Epoch 55/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.4413 - acc: 0.9841 - val_loss: 3.7773 - val_acc: 0.8654\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.4134 - acc: 0.9365 - val_loss: 3.6377 - val_acc: 0.8953\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.3101 - acc: 0.9524 - val_loss: 3.6102 - val_acc: 0.8932\n",
      "Epoch 58/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.3200 - acc: 0.9365 - val_loss: 3.6339 - val_acc: 0.8846\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.1975 - acc: 0.9841 - val_loss: 3.6282 - val_acc: 0.8825\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.2552 - acc: 0.9524 - val_loss: 3.4618 - val_acc: 0.9209\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.2995 - acc: 0.9365 - val_loss: 3.5281 - val_acc: 0.8889\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.1197 - acc: 1.0000 - val_loss: 3.5005 - val_acc: 0.8974\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.0894 - acc: 0.9841 - val_loss: 3.4343 - val_acc: 0.8868\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.0725 - acc: 0.9683 - val_loss: 3.4031 - val_acc: 0.8654\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.0811 - acc: 0.9524 - val_loss: 3.4043 - val_acc: 0.8739\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.0665 - acc: 0.9365 - val_loss: 3.3651 - val_acc: 0.8846\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.0224 - acc: 0.9841 - val_loss: 3.3226 - val_acc: 0.8761\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.0340 - acc: 0.9683 - val_loss: 3.3622 - val_acc: 0.8675\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.9011 - acc: 1.0000 - val_loss: 3.2566 - val_acc: 0.8889\n",
      "Epoch 70/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.9816 - acc: 0.9365 - val_loss: 3.1943 - val_acc: 0.8974\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.0028 - acc: 0.9524 - val_loss: 3.2523 - val_acc: 0.8739\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.0678 - acc: 0.9048 - val_loss: 3.2493 - val_acc: 0.8782\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.8689 - acc: 0.9841 - val_loss: 3.1692 - val_acc: 0.9017\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.8358 - acc: 0.9841 - val_loss: 3.1856 - val_acc: 0.8868\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.8029 - acc: 0.9841 - val_loss: 3.1959 - val_acc: 0.8803\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.7708 - acc: 0.9841 - val_loss: 3.0460 - val_acc: 0.9167\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.7989 - acc: 0.9841 - val_loss: 3.0679 - val_acc: 0.9017\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.7833 - acc: 0.9683 - val_loss: 3.2009 - val_acc: 0.8504\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.7265 - acc: 0.9841 - val_loss: 2.9911 - val_acc: 0.9017\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.6956 - acc: 0.9841 - val_loss: 3.0626 - val_acc: 0.8675\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.6910 - acc: 0.9524 - val_loss: 3.0012 - val_acc: 0.8974\n",
      "Epoch 82/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.6395 - acc: 1.0000 - val_loss: 3.0673 - val_acc: 0.8611\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.7651 - acc: 0.9048 - val_loss: 2.9301 - val_acc: 0.8996\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.6734 - acc: 0.9683 - val_loss: 3.0109 - val_acc: 0.8611\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.5900 - acc: 1.0000 - val_loss: 2.9892 - val_acc: 0.8675\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.6183 - acc: 0.9841 - val_loss: 3.0308 - val_acc: 0.8376\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.5731 - acc: 0.9841 - val_loss: 2.8512 - val_acc: 0.9188\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.5240 - acc: 1.0000 - val_loss: 2.7538 - val_acc: 0.9060\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.5347 - acc: 0.9683 - val_loss: 2.8843 - val_acc: 0.8846\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.5192 - acc: 0.9683 - val_loss: 2.8522 - val_acc: 0.8974\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.5224 - acc: 0.9365 - val_loss: 2.8792 - val_acc: 0.8782\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.4370 - acc: 1.0000 - val_loss: 2.7048 - val_acc: 0.9167\n",
      "Epoch 93/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.5403 - acc: 0.9365 - val_loss: 2.8464 - val_acc: 0.8782\n",
      "Epoch 94/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.4511 - acc: 0.9683 - val_loss: 2.7651 - val_acc: 0.8889\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.4623 - acc: 0.9524 - val_loss: 2.7504 - val_acc: 0.9017\n",
      "Epoch 96/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.4000 - acc: 0.9683 - val_loss: 2.7176 - val_acc: 0.9081\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.3938 - acc: 0.9683 - val_loss: 2.6822 - val_acc: 0.8932\n",
      "Epoch 98/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.3892 - acc: 0.9683 - val_loss: 2.7041 - val_acc: 0.8974\n",
      "Epoch 99/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.3985 - acc: 0.9683 - val_loss: 2.6377 - val_acc: 0.9103\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.3037 - acc: 1.0000 - val_loss: 2.7188 - val_acc: 0.8825\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.3249 - acc: 0.9841 - val_loss: 2.6007 - val_acc: 0.8974\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.3746 - acc: 0.9683 - val_loss: 2.6999 - val_acc: 0.8803\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.3894 - acc: 0.9683 - val_loss: 2.6462 - val_acc: 0.8675\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.3640 - acc: 0.9683 - val_loss: 2.6239 - val_acc: 0.8953\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.3206 - acc: 0.9683 - val_loss: 2.5523 - val_acc: 0.8932\n",
      "Epoch 106/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.2874 - acc: 0.9683 - val_loss: 2.7615 - val_acc: 0.8397\n",
      "Epoch 107/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 16ms/step - loss: 2.2681 - acc: 0.9683 - val_loss: 2.5671 - val_acc: 0.9038\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.3109 - acc: 0.9841 - val_loss: 2.6314 - val_acc: 0.8803\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.2191 - acc: 0.9841 - val_loss: 2.5422 - val_acc: 0.8974\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.2150 - acc: 1.0000 - val_loss: 2.6867 - val_acc: 0.8376\n",
      "Epoch 111/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.1733 - acc: 1.0000 - val_loss: 2.5018 - val_acc: 0.9081\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.2068 - acc: 0.9683 - val_loss: 2.5570 - val_acc: 0.8654\n",
      "Epoch 113/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.2215 - acc: 0.9841 - val_loss: 2.4321 - val_acc: 0.9231\n",
      "Epoch 114/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.1693 - acc: 1.0000 - val_loss: 2.5953 - val_acc: 0.8376\n",
      "Epoch 115/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.1403 - acc: 0.9524 - val_loss: 2.6303 - val_acc: 0.8483\n",
      "Epoch 116/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.1190 - acc: 1.0000 - val_loss: 2.4164 - val_acc: 0.8974\n",
      "Epoch 117/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0636 - acc: 0.9841 - val_loss: 2.4145 - val_acc: 0.9017\n",
      "Epoch 118/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0571 - acc: 1.0000 - val_loss: 2.5682 - val_acc: 0.8419\n",
      "Epoch 119/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0940 - acc: 0.9841 - val_loss: 2.5302 - val_acc: 0.8419\n",
      "Epoch 120/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0801 - acc: 0.9841 - val_loss: 2.4979 - val_acc: 0.8590\n",
      "Epoch 121/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0638 - acc: 0.9841 - val_loss: 2.4254 - val_acc: 0.8889\n",
      "Epoch 122/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.1221 - acc: 0.9683 - val_loss: 2.4508 - val_acc: 0.8846\n",
      "Epoch 123/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0458 - acc: 1.0000 - val_loss: 2.3303 - val_acc: 0.9038\n",
      "Epoch 124/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0340 - acc: 0.9683 - val_loss: 2.4031 - val_acc: 0.8782\n",
      "Epoch 125/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0351 - acc: 0.9683 - val_loss: 2.2937 - val_acc: 0.9124\n",
      "Epoch 126/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0351 - acc: 0.9683 - val_loss: 2.4080 - val_acc: 0.8803\n",
      "Epoch 127/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0129 - acc: 0.9683 - val_loss: 2.3184 - val_acc: 0.8974\n",
      "Epoch 128/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0093 - acc: 0.9841 - val_loss: 2.3661 - val_acc: 0.8889\n",
      "Epoch 129/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.9714 - acc: 0.9683 - val_loss: 2.2234 - val_acc: 0.9081\n",
      "Epoch 130/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.9430 - acc: 1.0000 - val_loss: 2.2596 - val_acc: 0.9124\n",
      "Epoch 131/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.9497 - acc: 0.9841 - val_loss: 2.3264 - val_acc: 0.8974\n",
      "Epoch 132/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0101 - acc: 0.9524 - val_loss: 2.3259 - val_acc: 0.8675\n",
      "Epoch 133/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.9371 - acc: 0.9841 - val_loss: 2.2199 - val_acc: 0.9209\n",
      "Epoch 134/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0466 - acc: 0.9524 - val_loss: 2.3133 - val_acc: 0.8974\n",
      "Epoch 135/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.9059 - acc: 1.0000 - val_loss: 2.2623 - val_acc: 0.8803\n",
      "Epoch 136/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8993 - acc: 0.9841 - val_loss: 2.2923 - val_acc: 0.8825\n",
      "Epoch 137/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8347 - acc: 0.9841 - val_loss: 2.2778 - val_acc: 0.8590\n",
      "Epoch 138/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 2.0490 - acc: 0.9048 - val_loss: 2.2621 - val_acc: 0.8910\n",
      "Epoch 139/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8775 - acc: 1.0000 - val_loss: 2.1651 - val_acc: 0.8996\n",
      "Epoch 140/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.8375 - acc: 1.0000 - val_loss: 2.1286 - val_acc: 0.9209\n",
      "Epoch 141/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8541 - acc: 0.9683 - val_loss: 2.0946 - val_acc: 0.9209\n",
      "Epoch 142/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.7852 - acc: 1.0000 - val_loss: 2.0958 - val_acc: 0.9188\n",
      "Epoch 143/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8298 - acc: 0.9841 - val_loss: 2.1458 - val_acc: 0.9124\n",
      "Epoch 144/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8015 - acc: 0.9841 - val_loss: 2.3060 - val_acc: 0.8291\n",
      "Epoch 145/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8364 - acc: 0.9841 - val_loss: 2.0855 - val_acc: 0.9081\n",
      "Epoch 146/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8614 - acc: 0.9683 - val_loss: 2.1075 - val_acc: 0.9081\n",
      "Epoch 147/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8214 - acc: 0.9841 - val_loss: 2.2474 - val_acc: 0.8590\n",
      "Epoch 148/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8212 - acc: 0.9683 - val_loss: 2.2135 - val_acc: 0.8910\n",
      "Epoch 149/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.8131 - acc: 0.9683 - val_loss: 2.1225 - val_acc: 0.8974\n",
      "Epoch 150/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.7429 - acc: 1.0000 - val_loss: 2.0038 - val_acc: 0.9167\n",
      "Epoch 151/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.7878 - acc: 0.9841 - val_loss: 2.0333 - val_acc: 0.8953\n",
      "Epoch 152/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.7817 - acc: 0.9841 - val_loss: 2.0731 - val_acc: 0.8932\n",
      "Epoch 153/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.7186 - acc: 1.0000 - val_loss: 2.0266 - val_acc: 0.9017\n",
      "Epoch 154/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6749 - acc: 1.0000 - val_loss: 1.9889 - val_acc: 0.9145\n",
      "Epoch 155/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.7572 - acc: 0.9683 - val_loss: 2.0238 - val_acc: 0.9060\n",
      "Epoch 156/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.7405 - acc: 0.9841 - val_loss: 1.9782 - val_acc: 0.8932\n",
      "Epoch 157/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.7383 - acc: 0.9683 - val_loss: 2.0545 - val_acc: 0.8974\n",
      "Epoch 158/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6968 - acc: 1.0000 - val_loss: 1.9783 - val_acc: 0.9188\n",
      "Epoch 159/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6983 - acc: 0.9683 - val_loss: 2.0818 - val_acc: 0.8675\n",
      "Epoch 160/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.7029 - acc: 0.9841 - val_loss: 2.1834 - val_acc: 0.8440\n",
      "Epoch 161/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.7273 - acc: 0.9683 - val_loss: 2.2457 - val_acc: 0.7991\n",
      "Epoch 162/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6649 - acc: 0.9683 - val_loss: 2.1210 - val_acc: 0.8547\n",
      "Epoch 163/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6893 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.8312\n",
      "Epoch 164/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6822 - acc: 0.9841 - val_loss: 1.9854 - val_acc: 0.8996\n",
      "Epoch 165/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6653 - acc: 0.9841 - val_loss: 2.1455 - val_acc: 0.8504\n",
      "Epoch 166/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6410 - acc: 0.9841 - val_loss: 2.0467 - val_acc: 0.8761\n",
      "Epoch 167/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6777 - acc: 0.9683 - val_loss: 1.9708 - val_acc: 0.8996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.6382 - acc: 1.0000 - val_loss: 1.9870 - val_acc: 0.8739\n",
      "Epoch 169/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5691 - acc: 1.0000 - val_loss: 1.9400 - val_acc: 0.8739\n",
      "Epoch 170/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5771 - acc: 1.0000 - val_loss: 1.9155 - val_acc: 0.8868\n",
      "Epoch 171/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5690 - acc: 1.0000 - val_loss: 1.9318 - val_acc: 0.9038\n",
      "Epoch 172/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5766 - acc: 1.0000 - val_loss: 1.9580 - val_acc: 0.8910\n",
      "Epoch 173/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5854 - acc: 0.9524 - val_loss: 1.9486 - val_acc: 0.9038\n",
      "Epoch 174/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5624 - acc: 0.9841 - val_loss: 2.0346 - val_acc: 0.8483\n",
      "Epoch 175/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5541 - acc: 0.9683 - val_loss: 2.0969 - val_acc: 0.8226\n",
      "Epoch 176/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5719 - acc: 0.9683 - val_loss: 1.9143 - val_acc: 0.8868\n",
      "Epoch 177/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5780 - acc: 1.0000 - val_loss: 1.8643 - val_acc: 0.9060\n",
      "Epoch 178/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4987 - acc: 1.0000 - val_loss: 1.9782 - val_acc: 0.8803\n",
      "Epoch 179/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5252 - acc: 0.9841 - val_loss: 1.9663 - val_acc: 0.8803\n",
      "Epoch 180/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5546 - acc: 0.9841 - val_loss: 1.9156 - val_acc: 0.8868\n",
      "Epoch 181/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5702 - acc: 0.9683 - val_loss: 1.8966 - val_acc: 0.8974\n",
      "Epoch 182/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5236 - acc: 1.0000 - val_loss: 1.8750 - val_acc: 0.8782\n",
      "Epoch 183/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5349 - acc: 1.0000 - val_loss: 1.9538 - val_acc: 0.8590\n",
      "Epoch 184/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5339 - acc: 0.9841 - val_loss: 1.8658 - val_acc: 0.8846\n",
      "Epoch 185/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5493 - acc: 0.9841 - val_loss: 1.8115 - val_acc: 0.9145\n",
      "Epoch 186/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.4747 - acc: 1.0000 - val_loss: 1.9207 - val_acc: 0.8440\n",
      "Epoch 187/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4524 - acc: 1.0000 - val_loss: 1.7891 - val_acc: 0.9145\n",
      "Epoch 188/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5133 - acc: 0.9524 - val_loss: 1.7993 - val_acc: 0.8803\n",
      "Epoch 189/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5490 - acc: 0.9365 - val_loss: 1.7772 - val_acc: 0.9103\n",
      "Epoch 190/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5073 - acc: 0.9683 - val_loss: 1.9067 - val_acc: 0.8761\n",
      "Epoch 191/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4605 - acc: 1.0000 - val_loss: 1.7760 - val_acc: 0.8953\n",
      "Epoch 192/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5149 - acc: 0.9524 - val_loss: 1.9732 - val_acc: 0.8376\n",
      "Epoch 193/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5252 - acc: 0.9841 - val_loss: 1.9513 - val_acc: 0.8376\n",
      "Epoch 194/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4990 - acc: 0.9683 - val_loss: 1.7428 - val_acc: 0.9017\n",
      "Epoch 195/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4688 - acc: 0.9841 - val_loss: 1.7526 - val_acc: 0.9167\n",
      "Epoch 196/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4468 - acc: 0.9841 - val_loss: 1.7519 - val_acc: 0.9167\n",
      "Epoch 197/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4110 - acc: 1.0000 - val_loss: 1.7122 - val_acc: 0.9124\n",
      "Epoch 198/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3764 - acc: 1.0000 - val_loss: 1.6885 - val_acc: 0.9252\n",
      "Epoch 199/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4463 - acc: 0.9524 - val_loss: 1.9418 - val_acc: 0.8355\n",
      "Epoch 200/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4424 - acc: 0.9841 - val_loss: 1.7568 - val_acc: 0.8825\n",
      "Epoch 201/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4329 - acc: 0.9683 - val_loss: 1.7837 - val_acc: 0.8868\n",
      "Epoch 202/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3753 - acc: 1.0000 - val_loss: 1.7798 - val_acc: 0.8632\n",
      "Epoch 203/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3761 - acc: 0.9841 - val_loss: 1.7766 - val_acc: 0.8697\n",
      "Epoch 204/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3724 - acc: 1.0000 - val_loss: 1.7403 - val_acc: 0.8846\n",
      "Epoch 205/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3329 - acc: 1.0000 - val_loss: 1.7008 - val_acc: 0.8996\n",
      "Epoch 206/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3490 - acc: 0.9841 - val_loss: 1.7491 - val_acc: 0.8825\n",
      "Epoch 207/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3635 - acc: 0.9683 - val_loss: 1.6624 - val_acc: 0.8974\n",
      "Epoch 208/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3492 - acc: 1.0000 - val_loss: 1.8998 - val_acc: 0.8291\n",
      "Epoch 209/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3558 - acc: 0.9841 - val_loss: 1.7585 - val_acc: 0.8611\n",
      "Epoch 210/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3986 - acc: 0.9524 - val_loss: 1.8730 - val_acc: 0.8462\n",
      "Epoch 211/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3281 - acc: 1.0000 - val_loss: 1.8022 - val_acc: 0.8376\n",
      "Epoch 212/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3756 - acc: 0.9841 - val_loss: 1.7588 - val_acc: 0.8739\n",
      "Epoch 213/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3743 - acc: 1.0000 - val_loss: 1.7119 - val_acc: 0.8889\n",
      "Epoch 214/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4021 - acc: 0.9683 - val_loss: 1.8975 - val_acc: 0.8312\n",
      "Epoch 215/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3382 - acc: 1.0000 - val_loss: 1.8458 - val_acc: 0.8611\n",
      "Epoch 216/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4039 - acc: 0.9524 - val_loss: 1.8118 - val_acc: 0.8504\n",
      "Epoch 217/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3800 - acc: 0.9683 - val_loss: 1.7443 - val_acc: 0.8697\n",
      "Epoch 218/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4111 - acc: 0.9683 - val_loss: 1.8560 - val_acc: 0.8397\n",
      "Epoch 219/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3211 - acc: 1.0000 - val_loss: 1.6862 - val_acc: 0.8953\n",
      "Epoch 220/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2730 - acc: 1.0000 - val_loss: 1.6730 - val_acc: 0.8825\n",
      "Epoch 221/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2950 - acc: 0.9841 - val_loss: 1.6237 - val_acc: 0.8803\n",
      "Epoch 222/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3360 - acc: 0.9683 - val_loss: 1.7268 - val_acc: 0.8632\n",
      "Epoch 223/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2646 - acc: 1.0000 - val_loss: 1.6527 - val_acc: 0.8846\n",
      "Epoch 224/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2571 - acc: 1.0000 - val_loss: 1.6159 - val_acc: 0.9060\n",
      "Epoch 225/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2564 - acc: 1.0000 - val_loss: 1.5999 - val_acc: 0.8889\n",
      "Epoch 226/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2835 - acc: 0.9683 - val_loss: 1.6555 - val_acc: 0.8910\n",
      "Epoch 227/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2841 - acc: 0.9683 - val_loss: 1.7888 - val_acc: 0.8291\n",
      "Epoch 228/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2734 - acc: 1.0000 - val_loss: 1.6577 - val_acc: 0.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.2777 - acc: 0.9841 - val_loss: 1.6585 - val_acc: 0.8611\n",
      "Epoch 230/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3065 - acc: 0.9683 - val_loss: 1.6719 - val_acc: 0.8932\n",
      "Epoch 231/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2744 - acc: 1.0000 - val_loss: 1.5731 - val_acc: 0.9038\n",
      "Epoch 232/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2586 - acc: 0.9683 - val_loss: 1.9201 - val_acc: 0.7479\n",
      "Epoch 233/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2708 - acc: 0.9683 - val_loss: 1.6365 - val_acc: 0.8846\n",
      "Epoch 234/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2059 - acc: 1.0000 - val_loss: 1.5557 - val_acc: 0.8974\n",
      "Epoch 235/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2032 - acc: 1.0000 - val_loss: 1.6214 - val_acc: 0.8868\n",
      "Epoch 236/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2761 - acc: 0.9683 - val_loss: 1.9689 - val_acc: 0.7799\n",
      "Epoch 237/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.3279 - acc: 0.9524 - val_loss: 1.7289 - val_acc: 0.8376\n",
      "Epoch 238/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2114 - acc: 1.0000 - val_loss: 1.5610 - val_acc: 0.9017\n",
      "Epoch 239/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2327 - acc: 0.9841 - val_loss: 1.6549 - val_acc: 0.8739\n",
      "Epoch 240/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2156 - acc: 0.9841 - val_loss: 1.5822 - val_acc: 0.8974\n",
      "Epoch 241/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1759 - acc: 1.0000 - val_loss: 1.5634 - val_acc: 0.8910\n",
      "Epoch 242/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2255 - acc: 1.0000 - val_loss: 1.5488 - val_acc: 0.8654\n",
      "Epoch 243/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2092 - acc: 0.9841 - val_loss: 1.5391 - val_acc: 0.8974\n",
      "Epoch 244/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2095 - acc: 0.9841 - val_loss: 1.5471 - val_acc: 0.8825\n",
      "Epoch 245/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2334 - acc: 1.0000 - val_loss: 1.5807 - val_acc: 0.9060\n",
      "Epoch 246/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2097 - acc: 0.9841 - val_loss: 1.5106 - val_acc: 0.9060\n",
      "Epoch 247/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1604 - acc: 1.0000 - val_loss: 1.6049 - val_acc: 0.8868\n",
      "Epoch 248/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1601 - acc: 1.0000 - val_loss: 1.5931 - val_acc: 0.8739\n",
      "Epoch 249/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1399 - acc: 1.0000 - val_loss: 1.5666 - val_acc: 0.8782\n",
      "Epoch 250/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1334 - acc: 1.0000 - val_loss: 1.5586 - val_acc: 0.8803\n",
      "Epoch 251/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1795 - acc: 0.9841 - val_loss: 1.4992 - val_acc: 0.9081\n",
      "Epoch 252/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1189 - acc: 1.0000 - val_loss: 1.5035 - val_acc: 0.8868\n",
      "Epoch 253/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2196 - acc: 0.9524 - val_loss: 1.8265 - val_acc: 0.7927\n",
      "Epoch 254/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2917 - acc: 0.9524 - val_loss: 1.6144 - val_acc: 0.8761\n",
      "Epoch 255/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2250 - acc: 0.9841 - val_loss: 1.5633 - val_acc: 0.8697\n",
      "Epoch 256/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1317 - acc: 1.0000 - val_loss: 1.6327 - val_acc: 0.8526\n",
      "Epoch 257/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1927 - acc: 0.9841 - val_loss: 1.6003 - val_acc: 0.8547\n",
      "Epoch 258/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2003 - acc: 0.9841 - val_loss: 1.5309 - val_acc: 0.8782\n",
      "Epoch 259/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2217 - acc: 0.9683 - val_loss: 1.5113 - val_acc: 0.8846\n",
      "Epoch 260/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1626 - acc: 1.0000 - val_loss: 1.5134 - val_acc: 0.8803\n",
      "Epoch 261/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1872 - acc: 0.9841 - val_loss: 1.5309 - val_acc: 0.8868\n",
      "Epoch 262/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1183 - acc: 1.0000 - val_loss: 1.4524 - val_acc: 0.8910\n",
      "Epoch 263/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1500 - acc: 0.9841 - val_loss: 1.7418 - val_acc: 0.8013\n",
      "Epoch 264/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1401 - acc: 1.0000 - val_loss: 1.6048 - val_acc: 0.8632\n",
      "Epoch 265/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2882 - acc: 0.9365 - val_loss: 1.5823 - val_acc: 0.8953\n",
      "Epoch 266/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1965 - acc: 0.9524 - val_loss: 1.5160 - val_acc: 0.8974\n",
      "Epoch 267/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1979 - acc: 0.9683 - val_loss: 1.4609 - val_acc: 0.9103\n",
      "Epoch 268/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1320 - acc: 0.9841 - val_loss: 1.4376 - val_acc: 0.9081\n",
      "Epoch 269/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1299 - acc: 1.0000 - val_loss: 1.4249 - val_acc: 0.9103\n",
      "Epoch 270/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1395 - acc: 1.0000 - val_loss: 1.4332 - val_acc: 0.9081\n",
      "Epoch 271/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0944 - acc: 1.0000 - val_loss: 1.4945 - val_acc: 0.8718\n",
      "Epoch 272/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1106 - acc: 0.9841 - val_loss: 1.4991 - val_acc: 0.8739\n",
      "Epoch 273/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1007 - acc: 1.0000 - val_loss: 1.4459 - val_acc: 0.8932\n",
      "Epoch 274/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1116 - acc: 0.9841 - val_loss: 1.6128 - val_acc: 0.8248\n",
      "Epoch 275/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1408 - acc: 0.9841 - val_loss: 1.4385 - val_acc: 0.8996\n",
      "Epoch 276/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0762 - acc: 0.9841 - val_loss: 1.4369 - val_acc: 0.9038\n",
      "Epoch 277/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1173 - acc: 1.0000 - val_loss: 1.5159 - val_acc: 0.8718\n",
      "Epoch 278/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1206 - acc: 0.9683 - val_loss: 1.5438 - val_acc: 0.8675\n",
      "Epoch 279/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0661 - acc: 1.0000 - val_loss: 1.5170 - val_acc: 0.8782\n",
      "Epoch 280/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0829 - acc: 0.9841 - val_loss: 1.5809 - val_acc: 0.8333\n",
      "Epoch 281/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1124 - acc: 0.9683 - val_loss: 1.8294 - val_acc: 0.7030\n",
      "Epoch 282/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0807 - acc: 1.0000 - val_loss: 1.5236 - val_acc: 0.8568\n",
      "Epoch 283/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0735 - acc: 1.0000 - val_loss: 1.4498 - val_acc: 0.8996\n",
      "Epoch 284/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0616 - acc: 0.9841 - val_loss: 1.3775 - val_acc: 0.8932\n",
      "Epoch 285/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1492 - acc: 0.9841 - val_loss: 1.5051 - val_acc: 0.8654\n",
      "Epoch 286/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0631 - acc: 1.0000 - val_loss: 1.4364 - val_acc: 0.8782\n",
      "Epoch 287/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0677 - acc: 1.0000 - val_loss: 1.5179 - val_acc: 0.8568\n",
      "Epoch 288/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0779 - acc: 0.9683 - val_loss: 1.4181 - val_acc: 0.8803\n",
      "Epoch 289/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0873 - acc: 0.9841 - val_loss: 1.4175 - val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0531 - acc: 1.0000 - val_loss: 1.3802 - val_acc: 0.8953\n",
      "Epoch 291/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0992 - acc: 0.9683 - val_loss: 1.3511 - val_acc: 0.9060\n",
      "Epoch 292/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0436 - acc: 0.9841 - val_loss: 1.3547 - val_acc: 0.9038\n",
      "Epoch 293/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0252 - acc: 0.9841 - val_loss: 1.3841 - val_acc: 0.8974\n",
      "Epoch 294/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0039 - acc: 1.0000 - val_loss: 1.3597 - val_acc: 0.8868\n",
      "Epoch 295/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.0191 - acc: 0.9841 - val_loss: 1.5244 - val_acc: 0.8440\n",
      "Epoch 296/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0359 - acc: 0.9841 - val_loss: 1.4357 - val_acc: 0.8654\n",
      "Epoch 297/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0492 - acc: 1.0000 - val_loss: 1.5423 - val_acc: 0.8376\n",
      "Epoch 298/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1145 - acc: 0.9524 - val_loss: 1.6034 - val_acc: 0.8568\n",
      "Epoch 299/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1049 - acc: 0.9683 - val_loss: 1.4191 - val_acc: 0.8953\n",
      "Epoch 300/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0915 - acc: 0.9683 - val_loss: 1.4343 - val_acc: 0.8825\n",
      "Epoch 301/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0569 - acc: 0.9841 - val_loss: 1.3927 - val_acc: 0.8654\n",
      "Epoch 302/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0046 - acc: 1.0000 - val_loss: 1.4122 - val_acc: 0.8761\n",
      "Epoch 303/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0614 - acc: 0.9841 - val_loss: 1.3872 - val_acc: 0.8803\n",
      "Epoch 304/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0166 - acc: 1.0000 - val_loss: 1.4529 - val_acc: 0.8739\n",
      "Epoch 305/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0150 - acc: 0.9841 - val_loss: 1.3932 - val_acc: 0.8782\n",
      "Epoch 306/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0345 - acc: 0.9683 - val_loss: 1.4416 - val_acc: 0.8868\n",
      "Epoch 307/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0745 - acc: 0.9524 - val_loss: 1.4884 - val_acc: 0.8483\n",
      "Epoch 308/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0242 - acc: 1.0000 - val_loss: 1.3725 - val_acc: 0.8953\n",
      "Epoch 309/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9595 - acc: 1.0000 - val_loss: 1.3379 - val_acc: 0.8910\n",
      "Epoch 310/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9549 - acc: 1.0000 - val_loss: 1.3089 - val_acc: 0.8846\n",
      "Epoch 311/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9354 - acc: 1.0000 - val_loss: 1.5438 - val_acc: 0.7970\n",
      "Epoch 312/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9983 - acc: 0.9683 - val_loss: 1.3339 - val_acc: 0.8889\n",
      "Epoch 313/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9859 - acc: 0.9841 - val_loss: 1.3305 - val_acc: 0.8782\n",
      "Epoch 314/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0089 - acc: 0.9841 - val_loss: 1.6583 - val_acc: 0.7906\n",
      "Epoch 315/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0116 - acc: 1.0000 - val_loss: 1.4245 - val_acc: 0.8675\n",
      "Epoch 316/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0001 - acc: 1.0000 - val_loss: 1.3133 - val_acc: 0.8889\n",
      "Epoch 317/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0262 - acc: 0.9841 - val_loss: 1.3166 - val_acc: 0.9103\n",
      "Epoch 318/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0530 - acc: 0.9841 - val_loss: 1.4502 - val_acc: 0.8504\n",
      "Epoch 319/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9810 - acc: 1.0000 - val_loss: 1.3394 - val_acc: 0.8825\n",
      "Epoch 320/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9801 - acc: 1.0000 - val_loss: 1.3636 - val_acc: 0.8846\n",
      "Epoch 321/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9805 - acc: 1.0000 - val_loss: 1.3945 - val_acc: 0.8526\n",
      "Epoch 322/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0468 - acc: 0.9841 - val_loss: 1.4198 - val_acc: 0.8718\n",
      "Epoch 323/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9596 - acc: 1.0000 - val_loss: 1.4377 - val_acc: 0.8440\n",
      "Epoch 324/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.9565 - acc: 1.0000 - val_loss: 1.4825 - val_acc: 0.8419\n",
      "Epoch 325/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.9689 - acc: 0.9841 - val_loss: 1.3807 - val_acc: 0.8739\n",
      "Epoch 326/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9542 - acc: 0.9841 - val_loss: 1.3096 - val_acc: 0.8868\n",
      "Epoch 327/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9380 - acc: 1.0000 - val_loss: 1.3538 - val_acc: 0.8718\n",
      "Epoch 328/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9581 - acc: 0.9841 - val_loss: 1.2997 - val_acc: 0.8803\n",
      "Epoch 329/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9981 - acc: 0.9841 - val_loss: 1.2785 - val_acc: 0.8996\n",
      "Epoch 330/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9733 - acc: 0.9841 - val_loss: 1.3163 - val_acc: 0.8932\n",
      "Epoch 331/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9729 - acc: 1.0000 - val_loss: 1.4144 - val_acc: 0.8376\n",
      "Epoch 332/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9982 - acc: 1.0000 - val_loss: 1.3632 - val_acc: 0.8868\n",
      "Epoch 333/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9436 - acc: 1.0000 - val_loss: 1.2960 - val_acc: 0.8932\n",
      "Epoch 334/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9663 - acc: 1.0000 - val_loss: 1.3250 - val_acc: 0.8846\n",
      "Epoch 335/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9985 - acc: 0.9841 - val_loss: 1.3163 - val_acc: 0.8825\n",
      "Epoch 336/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9239 - acc: 1.0000 - val_loss: 1.2914 - val_acc: 0.8889\n",
      "Epoch 337/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9267 - acc: 1.0000 - val_loss: 1.3481 - val_acc: 0.8654\n",
      "Epoch 338/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9243 - acc: 1.0000 - val_loss: 1.3643 - val_acc: 0.8611\n",
      "Epoch 339/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9881 - acc: 0.9841 - val_loss: 1.3258 - val_acc: 0.8697\n",
      "Epoch 340/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0045 - acc: 0.9683 - val_loss: 1.4747 - val_acc: 0.8440\n",
      "Epoch 341/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0029 - acc: 0.9841 - val_loss: 1.3559 - val_acc: 0.8718\n",
      "Epoch 342/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0013 - acc: 0.9841 - val_loss: 1.2725 - val_acc: 0.9103\n",
      "Epoch 343/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9365 - acc: 1.0000 - val_loss: 1.3187 - val_acc: 0.8739\n",
      "Epoch 344/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9274 - acc: 0.9841 - val_loss: 1.4294 - val_acc: 0.8269\n",
      "Epoch 345/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9750 - acc: 0.9841 - val_loss: 1.3126 - val_acc: 0.8889\n",
      "Epoch 346/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9980 - acc: 0.9683 - val_loss: 1.3658 - val_acc: 0.8803\n",
      "Epoch 347/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9427 - acc: 1.0000 - val_loss: 1.2524 - val_acc: 0.8868\n",
      "Epoch 348/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9676 - acc: 1.0000 - val_loss: 1.5315 - val_acc: 0.8056\n",
      "Epoch 349/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9443 - acc: 0.9841 - val_loss: 1.3751 - val_acc: 0.8611\n",
      "Epoch 350/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8872 - acc: 1.0000 - val_loss: 1.2795 - val_acc: 0.8932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9178 - acc: 0.9841 - val_loss: 1.2611 - val_acc: 0.8932\n",
      "Epoch 352/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9584 - acc: 0.9841 - val_loss: 1.6346 - val_acc: 0.7479\n",
      "Epoch 353/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9235 - acc: 1.0000 - val_loss: 1.3215 - val_acc: 0.8718\n",
      "Epoch 354/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9800 - acc: 0.9841 - val_loss: 1.3054 - val_acc: 0.8825\n",
      "Epoch 355/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9226 - acc: 0.9841 - val_loss: 1.4594 - val_acc: 0.7970\n",
      "Epoch 356/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8880 - acc: 1.0000 - val_loss: 1.2904 - val_acc: 0.8739\n",
      "Epoch 357/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9328 - acc: 1.0000 - val_loss: 1.4370 - val_acc: 0.8291\n",
      "Epoch 358/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8956 - acc: 1.0000 - val_loss: 1.3531 - val_acc: 0.8419\n",
      "Epoch 359/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8851 - acc: 1.0000 - val_loss: 1.2607 - val_acc: 0.8675\n",
      "Epoch 360/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8753 - acc: 1.0000 - val_loss: 1.2723 - val_acc: 0.8654\n",
      "Epoch 361/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9006 - acc: 1.0000 - val_loss: 1.3510 - val_acc: 0.8611\n",
      "Epoch 362/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8772 - acc: 1.0000 - val_loss: 1.2211 - val_acc: 0.8846\n",
      "Epoch 363/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8673 - acc: 1.0000 - val_loss: 1.3287 - val_acc: 0.8590\n",
      "Epoch 364/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8878 - acc: 1.0000 - val_loss: 1.3941 - val_acc: 0.8312\n",
      "Epoch 365/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9231 - acc: 0.9841 - val_loss: 1.2845 - val_acc: 0.8803\n",
      "Epoch 366/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9372 - acc: 0.9841 - val_loss: 1.6748 - val_acc: 0.7244\n",
      "Epoch 367/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8766 - acc: 1.0000 - val_loss: 1.3284 - val_acc: 0.8697\n",
      "Epoch 368/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9268 - acc: 0.9683 - val_loss: 1.2547 - val_acc: 0.8803\n",
      "Epoch 369/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8977 - acc: 1.0000 - val_loss: 1.4814 - val_acc: 0.7906\n",
      "Epoch 370/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8925 - acc: 0.9683 - val_loss: 1.2845 - val_acc: 0.8675\n",
      "Epoch 371/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.9430 - acc: 1.0000 - val_loss: 1.2636 - val_acc: 0.8782\n",
      "Epoch 372/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9650 - acc: 0.9841 - val_loss: 1.3242 - val_acc: 0.8761\n",
      "Epoch 373/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8696 - acc: 1.0000 - val_loss: 1.2881 - val_acc: 0.8675\n",
      "Epoch 374/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8781 - acc: 0.9841 - val_loss: 1.1960 - val_acc: 0.8996\n",
      "Epoch 375/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8974 - acc: 0.9841 - val_loss: 1.4801 - val_acc: 0.8162\n",
      "Epoch 376/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9699 - acc: 0.9524 - val_loss: 1.3990 - val_acc: 0.8355\n",
      "Epoch 377/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8821 - acc: 1.0000 - val_loss: 1.3752 - val_acc: 0.8312\n",
      "Epoch 378/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8755 - acc: 1.0000 - val_loss: 1.2760 - val_acc: 0.8846\n",
      "Epoch 379/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8286 - acc: 1.0000 - val_loss: 1.2350 - val_acc: 0.8846\n",
      "Epoch 380/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8252 - acc: 1.0000 - val_loss: 1.2160 - val_acc: 0.9038\n",
      "Epoch 381/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8624 - acc: 0.9841 - val_loss: 1.2367 - val_acc: 0.8761\n",
      "Epoch 382/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8753 - acc: 0.9841 - val_loss: 1.3976 - val_acc: 0.8034\n",
      "Epoch 383/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.9241 - acc: 0.9524 - val_loss: 1.3533 - val_acc: 0.8739\n",
      "Epoch 384/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8948 - acc: 1.0000 - val_loss: 1.3104 - val_acc: 0.8889\n",
      "Epoch 385/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.9200 - acc: 0.9841 - val_loss: 1.2904 - val_acc: 0.8697\n",
      "Epoch 386/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9414 - acc: 0.9683 - val_loss: 1.4738 - val_acc: 0.8077\n",
      "Epoch 387/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9871 - acc: 0.9524 - val_loss: 1.3773 - val_acc: 0.8590\n",
      "Epoch 388/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8770 - acc: 1.0000 - val_loss: 1.2504 - val_acc: 0.8910\n",
      "Epoch 389/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8603 - acc: 1.0000 - val_loss: 1.2245 - val_acc: 0.8910\n",
      "Epoch 390/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9099 - acc: 0.9524 - val_loss: 1.2150 - val_acc: 0.8974\n",
      "Epoch 391/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9022 - acc: 0.9841 - val_loss: 1.3759 - val_acc: 0.8312\n",
      "Epoch 392/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8592 - acc: 0.9841 - val_loss: 1.2278 - val_acc: 0.8868\n",
      "Epoch 393/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8628 - acc: 0.9841 - val_loss: 1.2486 - val_acc: 0.8910\n",
      "Epoch 394/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8272 - acc: 1.0000 - val_loss: 1.2301 - val_acc: 0.8932\n",
      "\n",
      "Epoch 00394: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 395/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8388 - acc: 1.0000 - val_loss: 1.2216 - val_acc: 0.8932\n",
      "Epoch 396/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8235 - acc: 1.0000 - val_loss: 1.2290 - val_acc: 0.8953\n",
      "Epoch 397/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8255 - acc: 1.0000 - val_loss: 1.2247 - val_acc: 0.8846\n",
      "Epoch 398/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8088 - acc: 1.0000 - val_loss: 1.2121 - val_acc: 0.8718\n",
      "Epoch 399/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8214 - acc: 0.9683 - val_loss: 1.2360 - val_acc: 0.8718\n",
      "Epoch 400/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7859 - acc: 1.0000 - val_loss: 1.1881 - val_acc: 0.8739\n",
      "Epoch 401/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7795 - acc: 1.0000 - val_loss: 1.1764 - val_acc: 0.8953\n",
      "Epoch 402/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7858 - acc: 1.0000 - val_loss: 1.1913 - val_acc: 0.8932\n",
      "Epoch 403/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7751 - acc: 1.0000 - val_loss: 1.2266 - val_acc: 0.8825\n",
      "Epoch 404/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7736 - acc: 1.0000 - val_loss: 1.1923 - val_acc: 0.8889\n",
      "Epoch 405/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8070 - acc: 1.0000 - val_loss: 1.1622 - val_acc: 0.8846\n",
      "Epoch 406/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8188 - acc: 0.9841 - val_loss: 1.2316 - val_acc: 0.8846\n",
      "Epoch 407/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8015 - acc: 1.0000 - val_loss: 1.1829 - val_acc: 0.8868\n",
      "Epoch 408/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8617 - acc: 0.9683 - val_loss: 1.1903 - val_acc: 0.8632\n",
      "Epoch 409/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8243 - acc: 1.0000 - val_loss: 1.1653 - val_acc: 0.8953\n",
      "Epoch 410/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7701 - acc: 1.0000 - val_loss: 1.1801 - val_acc: 0.8761\n",
      "Epoch 411/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8340 - acc: 1.0000 - val_loss: 1.1980 - val_acc: 0.8846\n",
      "Epoch 412/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7874 - acc: 1.0000 - val_loss: 1.2198 - val_acc: 0.8932\n",
      "Epoch 413/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8044 - acc: 1.0000 - val_loss: 1.2198 - val_acc: 0.8782\n",
      "Epoch 414/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8052 - acc: 0.9841 - val_loss: 1.3007 - val_acc: 0.8376\n",
      "Epoch 415/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7809 - acc: 1.0000 - val_loss: 1.1762 - val_acc: 0.8910\n",
      "Epoch 416/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8352 - acc: 1.0000 - val_loss: 1.2389 - val_acc: 0.8547\n",
      "Epoch 417/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7835 - acc: 1.0000 - val_loss: 1.2161 - val_acc: 0.8632\n",
      "Epoch 418/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7936 - acc: 1.0000 - val_loss: 1.2055 - val_acc: 0.8761\n",
      "Epoch 419/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8083 - acc: 0.9683 - val_loss: 1.2059 - val_acc: 0.8761\n",
      "Epoch 420/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8450 - acc: 0.9683 - val_loss: 1.2709 - val_acc: 0.8248\n",
      "Epoch 421/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8268 - acc: 0.9841 - val_loss: 1.1811 - val_acc: 0.8846\n",
      "Epoch 422/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8489 - acc: 0.9841 - val_loss: 1.2514 - val_acc: 0.8803\n",
      "Epoch 423/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8248 - acc: 1.0000 - val_loss: 1.2349 - val_acc: 0.8697\n",
      "Epoch 424/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8454 - acc: 0.9841 - val_loss: 1.1948 - val_acc: 0.8868\n",
      "Epoch 425/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7774 - acc: 1.0000 - val_loss: 1.1849 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00425: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 426/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8097 - acc: 0.9683 - val_loss: 1.1638 - val_acc: 0.8889\n",
      "Epoch 427/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.7662 - acc: 1.0000 - val_loss: 1.1560 - val_acc: 0.8761\n",
      "Epoch 428/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7540 - acc: 1.0000 - val_loss: 1.1657 - val_acc: 0.8782\n",
      "Epoch 429/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7974 - acc: 1.0000 - val_loss: 1.1648 - val_acc: 0.8782\n",
      "Epoch 430/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7927 - acc: 0.9841 - val_loss: 1.1541 - val_acc: 0.8761\n",
      "Epoch 431/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7707 - acc: 1.0000 - val_loss: 1.1495 - val_acc: 0.8782\n",
      "Epoch 432/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7511 - acc: 0.9841 - val_loss: 1.1468 - val_acc: 0.8718\n",
      "Epoch 433/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.7463 - acc: 1.0000 - val_loss: 1.1311 - val_acc: 0.8825\n",
      "Epoch 434/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7628 - acc: 1.0000 - val_loss: 1.1512 - val_acc: 0.8761\n",
      "Epoch 435/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8057 - acc: 0.9841 - val_loss: 1.1403 - val_acc: 0.8825\n",
      "Epoch 436/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.7668 - acc: 1.0000 - val_loss: 1.1290 - val_acc: 0.8889\n",
      "Epoch 437/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.7643 - acc: 1.0000 - val_loss: 1.1236 - val_acc: 0.8889\n",
      "Epoch 438/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8140 - acc: 0.9683 - val_loss: 1.1670 - val_acc: 0.8868\n",
      "Epoch 439/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.7797 - acc: 1.0000 - val_loss: 1.1468 - val_acc: 0.8910\n",
      "Epoch 440/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7424 - acc: 1.0000 - val_loss: 1.1343 - val_acc: 0.8932\n",
      "Epoch 441/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7790 - acc: 1.0000 - val_loss: 1.1437 - val_acc: 0.8803\n",
      "Epoch 442/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7467 - acc: 1.0000 - val_loss: 1.1451 - val_acc: 0.8846\n",
      "Epoch 443/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7544 - acc: 1.0000 - val_loss: 1.1503 - val_acc: 0.8846\n",
      "Epoch 444/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7763 - acc: 1.0000 - val_loss: 1.1529 - val_acc: 0.8868\n",
      "Epoch 445/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7599 - acc: 1.0000 - val_loss: 1.1658 - val_acc: 0.8868\n",
      "Epoch 446/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7666 - acc: 0.9841 - val_loss: 1.1462 - val_acc: 0.8782\n",
      "Epoch 447/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7904 - acc: 0.9841 - val_loss: 1.1733 - val_acc: 0.8782\n",
      "Epoch 448/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7562 - acc: 1.0000 - val_loss: 1.1551 - val_acc: 0.8718\n",
      "Epoch 449/2000\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.7501 - acc: 1.0000 - val_loss: 1.1267 - val_acc: 0.8868\n",
      "Epoch 450/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7547 - acc: 1.0000 - val_loss: 1.1461 - val_acc: 0.8910\n",
      "Epoch 451/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7911 - acc: 1.0000 - val_loss: 1.1564 - val_acc: 0.8675\n",
      "Epoch 452/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7516 - acc: 1.0000 - val_loss: 1.1820 - val_acc: 0.8654\n",
      "Epoch 453/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7570 - acc: 1.0000 - val_loss: 1.1536 - val_acc: 0.8761\n",
      "Epoch 454/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7713 - acc: 0.9841 - val_loss: 1.1388 - val_acc: 0.8782\n",
      "Epoch 455/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7980 - acc: 0.9524 - val_loss: 1.1309 - val_acc: 0.8803\n",
      "Epoch 456/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7372 - acc: 1.0000 - val_loss: 1.1446 - val_acc: 0.8782\n",
      "Epoch 457/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7400 - acc: 1.0000 - val_loss: 1.1348 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00457: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 458/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7133 - acc: 1.0000 - val_loss: 1.1307 - val_acc: 0.8825\n",
      "Epoch 459/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7519 - acc: 0.9841 - val_loss: 1.1334 - val_acc: 0.8825\n",
      "Epoch 460/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7931 - acc: 0.9683 - val_loss: 1.1422 - val_acc: 0.8739\n",
      "Epoch 461/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7524 - acc: 1.0000 - val_loss: 1.1578 - val_acc: 0.8718\n",
      "Epoch 462/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7553 - acc: 1.0000 - val_loss: 1.1482 - val_acc: 0.8782\n",
      "Epoch 463/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7898 - acc: 0.9841 - val_loss: 1.1497 - val_acc: 0.8825\n",
      "Epoch 464/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7448 - acc: 0.9841 - val_loss: 1.1516 - val_acc: 0.8761\n",
      "Epoch 465/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7461 - acc: 1.0000 - val_loss: 1.1532 - val_acc: 0.8782\n",
      "Epoch 466/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7804 - acc: 0.9841 - val_loss: 1.1352 - val_acc: 0.8761\n",
      "Epoch 467/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7318 - acc: 0.9841 - val_loss: 1.1345 - val_acc: 0.8718\n",
      "Epoch 468/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7507 - acc: 0.9841 - val_loss: 1.1522 - val_acc: 0.8697\n",
      "Epoch 469/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7456 - acc: 1.0000 - val_loss: 1.1508 - val_acc: 0.8782\n",
      "Epoch 470/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7304 - acc: 1.0000 - val_loss: 1.1633 - val_acc: 0.8761\n",
      "Epoch 471/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7385 - acc: 1.0000 - val_loss: 1.1657 - val_acc: 0.8718\n",
      "Epoch 472/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7237 - acc: 1.0000 - val_loss: 1.1577 - val_acc: 0.8697\n",
      "Epoch 473/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7484 - acc: 0.9841 - val_loss: 1.1432 - val_acc: 0.8675\n",
      "Epoch 474/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7536 - acc: 1.0000 - val_loss: 1.1513 - val_acc: 0.8675\n",
      "Epoch 475/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7563 - acc: 0.9841 - val_loss: 1.1581 - val_acc: 0.8739\n",
      "Epoch 476/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7773 - acc: 0.9683 - val_loss: 1.1487 - val_acc: 0.8761\n",
      "Epoch 477/2000\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7776 - acc: 0.9683 - val_loss: 1.1523 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00477: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9252136752136753\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_input_tensor = Input(shape=(scaled_train_data_words.shape[1],) , name='words')\n",
    "ngram_input_tensor = Input(shape=(scaled_train_data_ngrams.shape[1],) , name='n_grams')\n",
    "\n",
    "neu0 = Sequential()\n",
    "neu0.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "\n",
    "neu1 = Sequential()\n",
    "neu1.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "\n",
    "output_tensor_0 = neu0(ngram_input_tensor)\n",
    "output_tensor_1 = neu1(word_input_tensor)\n",
    "\n",
    "concatenated = layers.concatenate([output_tensor_0,\n",
    "                                   output_tensor_1\n",
    "                                  ], axis=-1)\n",
    "\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "concatenated = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model([ngram_input_tensor, word_input_tensor], concatenated)\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit([scaled_train_data_ngrams, scaled_train_data_words], y_train,\n",
    "                    validation_data=([scaled_test_data_ngrams, scaled_test_data_words], y_val),\n",
    "                    epochs=2000,\n",
    "                    batch_size=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "n_grams (InputLayer)            (None, 4152)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "words (InputLayer)              (None, 1302)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "convnets (InputLayer)           (None, 1020)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_38 (Sequential)      (None, 64)           139168      n_grams[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_39 (Sequential)      (None, 64)           47968       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_40 (Sequential)      (None, 32)           12202       convnets[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_41 (Sequential)      (None, 32)           12184       convnets[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_42 (Sequential)      (None, 32)           12193       convnets[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 224)          0           sequential_38[1][0]              \n",
      "                                                                 sequential_39[1][0]              \n",
      "                                                                 sequential_40[1][0]              \n",
      "                                                                 sequential_41[1][0]              \n",
      "                                                                 sequential_42[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_108 (Dense)               (None, 128)          28800       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 128)          0           dense_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_109 (Dense)               (None, 9)            1161        dropout_104[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 253,676\n",
      "Trainable params: 253,484\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "Train on 63 samples, validate on 468 samples\n",
      "Epoch 1/2000\n",
      "63/63 [==============================] - 9s 143ms/step - loss: 9.7193 - acc: 0.0635 - val_loss: 9.1561 - val_acc: 0.1581\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 9.1887 - acc: 0.1111 - val_loss: 8.8007 - val_acc: 0.4359\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 8.7800 - acc: 0.1429 - val_loss: 8.5408 - val_acc: 0.3376\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 8.4937 - acc: 0.0794 - val_loss: 8.2987 - val_acc: 0.2671\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 8.2218 - acc: 0.2063 - val_loss: 8.0776 - val_acc: 0.2692\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 7.9425 - acc: 0.2698 - val_loss: 7.8835 - val_acc: 0.3397\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 7.7279 - acc: 0.3016 - val_loss: 7.7249 - val_acc: 0.2585\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 7.5423 - acc: 0.2540 - val_loss: 7.5281 - val_acc: 0.3974\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 7.3504 - acc: 0.3175 - val_loss: 7.4385 - val_acc: 0.2415\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 7.1161 - acc: 0.4444 - val_loss: 7.2018 - val_acc: 0.5513\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 6.9844 - acc: 0.3810 - val_loss: 7.0800 - val_acc: 0.5064\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 6.7871 - acc: 0.4921 - val_loss: 6.9296 - val_acc: 0.5427\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 6.6068 - acc: 0.5079 - val_loss: 6.7454 - val_acc: 0.6111\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 6.4313 - acc: 0.5079 - val_loss: 6.6697 - val_acc: 0.5107\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 6.2721 - acc: 0.5873 - val_loss: 6.4959 - val_acc: 0.6709\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 6.1434 - acc: 0.5873 - val_loss: 6.4091 - val_acc: 0.6218\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 5.9497 - acc: 0.6667 - val_loss: 6.2586 - val_acc: 0.6282\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 5.7733 - acc: 0.7778 - val_loss: 6.1508 - val_acc: 0.6389\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 5.6586 - acc: 0.6667 - val_loss: 6.0413 - val_acc: 0.6453\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 5.4598 - acc: 0.7302 - val_loss: 5.9266 - val_acc: 0.6838\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 5.4098 - acc: 0.7778 - val_loss: 5.7956 - val_acc: 0.7308\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 5.2683 - acc: 0.7460 - val_loss: 5.7617 - val_acc: 0.6004\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 5.2012 - acc: 0.6667 - val_loss: 5.6881 - val_acc: 0.6197\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 4.9623 - acc: 0.8889 - val_loss: 5.6555 - val_acc: 0.5812\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 4.8009 - acc: 0.8730 - val_loss: 5.4010 - val_acc: 0.7244\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 4.8675 - acc: 0.7778 - val_loss: 5.2884 - val_acc: 0.7970\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 4.7079 - acc: 0.8730 - val_loss: 5.3299 - val_acc: 0.6560\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 4.7407 - acc: 0.8254 - val_loss: 5.2715 - val_acc: 0.6966\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 4.5182 - acc: 0.9206 - val_loss: 5.1397 - val_acc: 0.7329\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 4.4570 - acc: 0.9048 - val_loss: 5.0892 - val_acc: 0.6838\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 4.3447 - acc: 0.9206 - val_loss: 5.0861 - val_acc: 0.6410\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 4.2845 - acc: 0.9206 - val_loss: 5.0090 - val_acc: 0.6474\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 4.3449 - acc: 0.8730 - val_loss: 4.7509 - val_acc: 0.8141\n",
      "Epoch 34/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 4.1261 - acc: 0.9524 - val_loss: 4.8397 - val_acc: 0.7179\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 4.2292 - acc: 0.8889 - val_loss: 4.7497 - val_acc: 0.7521\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 4.0296 - acc: 0.9206 - val_loss: 4.6114 - val_acc: 0.8077\n",
      "Epoch 37/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 4s 63ms/step - loss: 4.1010 - acc: 0.8889 - val_loss: 4.5966 - val_acc: 0.7628\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 3.8367 - acc: 0.9841 - val_loss: 4.4122 - val_acc: 0.8355\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 3.8296 - acc: 0.9524 - val_loss: 4.4936 - val_acc: 0.7650\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 3.7610 - acc: 0.9365 - val_loss: 4.3377 - val_acc: 0.7970\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.7552 - acc: 0.9524 - val_loss: 4.4038 - val_acc: 0.7393\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.6121 - acc: 0.9841 - val_loss: 4.4218 - val_acc: 0.7094\n",
      "Epoch 43/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 3.6313 - acc: 0.9524 - val_loss: 4.3216 - val_acc: 0.7607\n",
      "Epoch 44/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 3.5974 - acc: 0.9524 - val_loss: 4.2782 - val_acc: 0.7585\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 3.5470 - acc: 0.9206 - val_loss: 4.1675 - val_acc: 0.7949\n",
      "Epoch 46/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 3.4867 - acc: 0.9365 - val_loss: 4.2206 - val_acc: 0.6944\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 3.4125 - acc: 0.9841 - val_loss: 4.0646 - val_acc: 0.7799\n",
      "Epoch 48/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.4120 - acc: 0.9841 - val_loss: 4.0157 - val_acc: 0.7949\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 3.3956 - acc: 0.9365 - val_loss: 4.0520 - val_acc: 0.7179\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 3.2615 - acc: 0.9683 - val_loss: 3.9246 - val_acc: 0.7885\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.2306 - acc: 0.9841 - val_loss: 3.8567 - val_acc: 0.8205\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 3.2419 - acc: 0.9683 - val_loss: 3.9199 - val_acc: 0.7543\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.1942 - acc: 0.9841 - val_loss: 4.1751 - val_acc: 0.5235\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 3.1108 - acc: 0.9841 - val_loss: 3.8687 - val_acc: 0.7329\n",
      "Epoch 55/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.0932 - acc: 0.9841 - val_loss: 3.7409 - val_acc: 0.7885\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 3.0566 - acc: 1.0000 - val_loss: 3.7830 - val_acc: 0.7457\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 3.0477 - acc: 1.0000 - val_loss: 3.9029 - val_acc: 0.6282\n",
      "Epoch 58/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.9552 - acc: 1.0000 - val_loss: 3.6391 - val_acc: 0.7885\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.9483 - acc: 0.9841 - val_loss: 3.5775 - val_acc: 0.8269\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.9116 - acc: 0.9683 - val_loss: 3.6595 - val_acc: 0.7137\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.8871 - acc: 0.9841 - val_loss: 3.5884 - val_acc: 0.7073\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 2.8245 - acc: 1.0000 - val_loss: 3.6015 - val_acc: 0.6944\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 2.8504 - acc: 0.9683 - val_loss: 3.4719 - val_acc: 0.8034\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 2.8280 - acc: 0.9841 - val_loss: 3.4158 - val_acc: 0.8269\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.7684 - acc: 0.9841 - val_loss: 3.3261 - val_acc: 0.8440\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 2.7297 - acc: 1.0000 - val_loss: 3.4404 - val_acc: 0.7479\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.6607 - acc: 1.0000 - val_loss: 3.4645 - val_acc: 0.7201\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 2.7489 - acc: 0.9524 - val_loss: 3.4278 - val_acc: 0.7350\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 2.6339 - acc: 1.0000 - val_loss: 3.3023 - val_acc: 0.7927\n",
      "Epoch 70/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 2.6366 - acc: 0.9841 - val_loss: 3.1989 - val_acc: 0.8376\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 2.5849 - acc: 1.0000 - val_loss: 3.2329 - val_acc: 0.8248\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 2.5870 - acc: 0.9841 - val_loss: 3.2764 - val_acc: 0.7607\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.5362 - acc: 0.9841 - val_loss: 3.3497 - val_acc: 0.6667\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 2.5314 - acc: 0.9841 - val_loss: 3.4624 - val_acc: 0.5620\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 2.4785 - acc: 1.0000 - val_loss: 3.1947 - val_acc: 0.7607\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 2.4299 - acc: 1.0000 - val_loss: 3.1616 - val_acc: 0.7607\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 2.4183 - acc: 0.9841 - val_loss: 3.0469 - val_acc: 0.8056\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 2.4166 - acc: 0.9841 - val_loss: 3.0742 - val_acc: 0.7927\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 2.3462 - acc: 1.0000 - val_loss: 2.9920 - val_acc: 0.7991\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 2.3572 - acc: 0.9841 - val_loss: 3.2187 - val_acc: 0.6816\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.3204 - acc: 1.0000 - val_loss: 3.0112 - val_acc: 0.7991\n",
      "Epoch 82/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 2.2851 - acc: 1.0000 - val_loss: 3.1889 - val_acc: 0.6346\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.3131 - acc: 1.0000 - val_loss: 3.0027 - val_acc: 0.7628\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 2.2866 - acc: 0.9841 - val_loss: 2.9332 - val_acc: 0.7885\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 2.1993 - acc: 1.0000 - val_loss: 2.8779 - val_acc: 0.7821\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.2552 - acc: 0.9841 - val_loss: 3.0082 - val_acc: 0.7372\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 2.1715 - acc: 1.0000 - val_loss: 2.8733 - val_acc: 0.7842\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 2.1345 - acc: 1.0000 - val_loss: 3.0100 - val_acc: 0.6795\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 2.1446 - acc: 0.9683 - val_loss: 3.0891 - val_acc: 0.6560\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 2.1404 - acc: 0.9841 - val_loss: 3.2183 - val_acc: 0.5577\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.0866 - acc: 1.0000 - val_loss: 2.9065 - val_acc: 0.7286\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.1065 - acc: 1.0000 - val_loss: 2.8393 - val_acc: 0.7585\n",
      "Epoch 93/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 2.0308 - acc: 1.0000 - val_loss: 2.9970 - val_acc: 0.6325\n",
      "Epoch 94/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 2.0414 - acc: 1.0000 - val_loss: 2.8694 - val_acc: 0.6688\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 2.0318 - acc: 0.9841 - val_loss: 2.8253 - val_acc: 0.7179\n",
      "Epoch 96/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 1.9916 - acc: 1.0000 - val_loss: 2.9127 - val_acc: 0.6303\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.9797 - acc: 0.9841 - val_loss: 2.6640 - val_acc: 0.7756\n",
      "Epoch 98/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 4s 64ms/step - loss: 1.9353 - acc: 1.0000 - val_loss: 2.6913 - val_acc: 0.7415\n",
      "Epoch 99/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 1.9460 - acc: 1.0000 - val_loss: 2.6519 - val_acc: 0.7585\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.8897 - acc: 1.0000 - val_loss: 2.7390 - val_acc: 0.7009\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 1.8552 - acc: 1.0000 - val_loss: 2.6496 - val_acc: 0.7201\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.8580 - acc: 1.0000 - val_loss: 2.5533 - val_acc: 0.7799\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.8327 - acc: 1.0000 - val_loss: 2.7985 - val_acc: 0.6667\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 1.8368 - acc: 1.0000 - val_loss: 2.5657 - val_acc: 0.7415\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.7969 - acc: 1.0000 - val_loss: 2.5162 - val_acc: 0.7735\n",
      "Epoch 106/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.8318 - acc: 0.9524 - val_loss: 2.5076 - val_acc: 0.7692\n",
      "Epoch 107/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 1.8257 - acc: 0.9841 - val_loss: 2.6705 - val_acc: 0.6859\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.7692 - acc: 0.9841 - val_loss: 2.5424 - val_acc: 0.7244\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 1.7352 - acc: 0.9841 - val_loss: 2.5032 - val_acc: 0.7564\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.7052 - acc: 1.0000 - val_loss: 2.5460 - val_acc: 0.7009\n",
      "Epoch 111/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.6902 - acc: 1.0000 - val_loss: 2.4662 - val_acc: 0.7372\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.6552 - acc: 1.0000 - val_loss: 2.6656 - val_acc: 0.6026\n",
      "Epoch 113/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 1.6414 - acc: 1.0000 - val_loss: 2.9829 - val_acc: 0.4338\n",
      "Epoch 114/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 1.6181 - acc: 1.0000 - val_loss: 2.3388 - val_acc: 0.7756\n",
      "Epoch 115/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.6322 - acc: 1.0000 - val_loss: 2.3284 - val_acc: 0.7585\n",
      "Epoch 116/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.5722 - acc: 1.0000 - val_loss: 2.3761 - val_acc: 0.7265\n",
      "Epoch 117/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.6361 - acc: 0.9683 - val_loss: 2.3790 - val_acc: 0.7350\n",
      "Epoch 118/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 1.5626 - acc: 1.0000 - val_loss: 2.4033 - val_acc: 0.6987\n",
      "Epoch 119/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 1.5247 - acc: 1.0000 - val_loss: 2.2560 - val_acc: 0.7650\n",
      "Epoch 120/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 1.5065 - acc: 1.0000 - val_loss: 2.3815 - val_acc: 0.6688\n",
      "Epoch 121/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.4885 - acc: 1.0000 - val_loss: 2.3438 - val_acc: 0.7051\n",
      "Epoch 122/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.4564 - acc: 1.0000 - val_loss: 2.2276 - val_acc: 0.7564\n",
      "Epoch 123/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.4775 - acc: 1.0000 - val_loss: 2.1393 - val_acc: 0.7799\n",
      "Epoch 124/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 1.4737 - acc: 0.9841 - val_loss: 2.4249 - val_acc: 0.6603\n",
      "Epoch 125/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.4791 - acc: 0.9841 - val_loss: 2.2938 - val_acc: 0.7201\n",
      "Epoch 126/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.4275 - acc: 1.0000 - val_loss: 2.2467 - val_acc: 0.7222\n",
      "Epoch 127/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.3731 - acc: 1.0000 - val_loss: 2.1148 - val_acc: 0.7521\n",
      "Epoch 128/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.3370 - acc: 1.0000 - val_loss: 2.1833 - val_acc: 0.7137\n",
      "Epoch 129/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.3430 - acc: 1.0000 - val_loss: 2.0853 - val_acc: 0.7543\n",
      "Epoch 130/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.3371 - acc: 1.0000 - val_loss: 2.2354 - val_acc: 0.6966\n",
      "Epoch 131/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.3467 - acc: 0.9841 - val_loss: 2.3774 - val_acc: 0.5983\n",
      "Epoch 132/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.3192 - acc: 0.9841 - val_loss: 2.0360 - val_acc: 0.7628\n",
      "Epoch 133/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.3011 - acc: 1.0000 - val_loss: 2.2959 - val_acc: 0.6474\n",
      "Epoch 134/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.2732 - acc: 1.0000 - val_loss: 2.1437 - val_acc: 0.6774\n",
      "Epoch 135/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.2452 - acc: 1.0000 - val_loss: 2.0321 - val_acc: 0.7244\n",
      "Epoch 136/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 1.2148 - acc: 1.0000 - val_loss: 2.1246 - val_acc: 0.6859\n",
      "Epoch 137/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.2286 - acc: 1.0000 - val_loss: 2.0949 - val_acc: 0.6902\n",
      "Epoch 138/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 1.1922 - acc: 1.0000 - val_loss: 2.0549 - val_acc: 0.7030\n",
      "Epoch 139/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.1709 - acc: 1.0000 - val_loss: 2.0744 - val_acc: 0.6709\n",
      "Epoch 140/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 1.1552 - acc: 1.0000 - val_loss: 2.1035 - val_acc: 0.6410\n",
      "Epoch 141/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 1.1441 - acc: 1.0000 - val_loss: 2.1000 - val_acc: 0.6432\n",
      "Epoch 142/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 1.1236 - acc: 1.0000 - val_loss: 2.0054 - val_acc: 0.6987\n",
      "Epoch 143/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.0881 - acc: 1.0000 - val_loss: 1.9726 - val_acc: 0.6944\n",
      "Epoch 144/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.1208 - acc: 0.9841 - val_loss: 2.2294 - val_acc: 0.5662\n",
      "Epoch 145/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.0812 - acc: 1.0000 - val_loss: 2.0099 - val_acc: 0.6453\n",
      "Epoch 146/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.0635 - acc: 1.0000 - val_loss: 2.0466 - val_acc: 0.6432\n",
      "Epoch 147/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 1.0278 - acc: 1.0000 - val_loss: 1.9507 - val_acc: 0.6838\n",
      "Epoch 148/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.0572 - acc: 0.9841 - val_loss: 2.0089 - val_acc: 0.6560\n",
      "Epoch 149/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.0217 - acc: 1.0000 - val_loss: 1.9554 - val_acc: 0.6667\n",
      "Epoch 150/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.0250 - acc: 1.0000 - val_loss: 1.7896 - val_acc: 0.7350\n",
      "Epoch 151/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.9963 - acc: 1.0000 - val_loss: 1.8082 - val_acc: 0.7265\n",
      "Epoch 152/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.9908 - acc: 0.9841 - val_loss: 1.9356 - val_acc: 0.6538\n",
      "Epoch 153/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.9477 - acc: 1.0000 - val_loss: 1.8782 - val_acc: 0.6731\n",
      "Epoch 154/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.9257 - acc: 1.0000 - val_loss: 1.9588 - val_acc: 0.6325\n",
      "Epoch 155/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.9149 - acc: 1.0000 - val_loss: 1.9452 - val_acc: 0.6111\n",
      "Epoch 156/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.8998 - acc: 1.0000 - val_loss: 1.7987 - val_acc: 0.7009\n",
      "Epoch 157/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.8639 - acc: 1.0000 - val_loss: 1.8805 - val_acc: 0.6282\n",
      "Epoch 158/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.8825 - acc: 1.0000 - val_loss: 1.9855 - val_acc: 0.6026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.8487 - acc: 1.0000 - val_loss: 1.8356 - val_acc: 0.6474\n",
      "Epoch 160/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.8792 - acc: 0.9841 - val_loss: 1.7990 - val_acc: 0.6560\n",
      "Epoch 161/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.8370 - acc: 1.0000 - val_loss: 2.0233 - val_acc: 0.5513\n",
      "Epoch 162/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.8124 - acc: 1.0000 - val_loss: 1.7764 - val_acc: 0.6581\n",
      "Epoch 163/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.7996 - acc: 1.0000 - val_loss: 1.9746 - val_acc: 0.5705\n",
      "Epoch 164/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.7932 - acc: 1.0000 - val_loss: 1.8169 - val_acc: 0.6026\n",
      "Epoch 165/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.7734 - acc: 1.0000 - val_loss: 1.8209 - val_acc: 0.6026\n",
      "Epoch 166/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.7457 - acc: 1.0000 - val_loss: 1.8119 - val_acc: 0.5876\n",
      "Epoch 167/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.7581 - acc: 0.9841 - val_loss: 1.9829 - val_acc: 0.5470\n",
      "Epoch 168/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.7319 - acc: 1.0000 - val_loss: 1.7425 - val_acc: 0.6346\n",
      "Epoch 169/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.7182 - acc: 1.0000 - val_loss: 1.7523 - val_acc: 0.6282\n",
      "Epoch 170/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.7338 - acc: 0.9841 - val_loss: 1.7390 - val_acc: 0.6282\n",
      "Epoch 171/2000\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.6782 - acc: 1.0000 - val_loss: 1.7376 - val_acc: 0.6261\n",
      "Epoch 172/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.6667 - acc: 1.0000 - val_loss: 1.7923 - val_acc: 0.5833\n",
      "Epoch 173/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.6591 - acc: 1.0000 - val_loss: 1.7291 - val_acc: 0.6004\n",
      "Epoch 174/2000\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.6359 - acc: 1.0000 - val_loss: 1.7113 - val_acc: 0.6111\n",
      "Epoch 175/2000\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.6273 - acc: 1.0000 - val_loss: 1.7047 - val_acc: 0.6090\n",
      "Epoch 176/2000\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.6425 - acc: 1.0000 - val_loss: 1.7219 - val_acc: 0.6090\n",
      "Epoch 177/2000\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.6170 - acc: 1.0000 - val_loss: 1.7179 - val_acc: 0.5940\n",
      "Epoch 178/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.6104 - acc: 1.0000 - val_loss: 1.6441 - val_acc: 0.6560\n",
      "Epoch 179/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.5883 - acc: 1.0000 - val_loss: 1.6418 - val_acc: 0.6346\n",
      "Epoch 180/2000\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.5886 - acc: 0.9841 - val_loss: 1.9547 - val_acc: 0.4915\n",
      "Epoch 181/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.5791 - acc: 1.0000 - val_loss: 1.7539 - val_acc: 0.5705\n",
      "Epoch 182/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.5639 - acc: 1.0000 - val_loss: 1.6418 - val_acc: 0.6068\n",
      "Epoch 183/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.5475 - acc: 1.0000 - val_loss: 1.6557 - val_acc: 0.5812\n",
      "Epoch 184/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.5232 - acc: 1.0000 - val_loss: 1.5926 - val_acc: 0.6047\n",
      "Epoch 185/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.5164 - acc: 1.0000 - val_loss: 1.6062 - val_acc: 0.6154\n",
      "Epoch 186/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.5243 - acc: 1.0000 - val_loss: 1.6598 - val_acc: 0.5769\n",
      "Epoch 187/2000\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.5015 - acc: 1.0000 - val_loss: 1.6848 - val_acc: 0.5427\n",
      "Epoch 188/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.4901 - acc: 1.0000 - val_loss: 1.7071 - val_acc: 0.5406\n",
      "Epoch 189/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.4883 - acc: 1.0000 - val_loss: 1.5306 - val_acc: 0.6453\n",
      "Epoch 190/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.4697 - acc: 1.0000 - val_loss: 1.6359 - val_acc: 0.5705\n",
      "Epoch 191/2000\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.4620 - acc: 1.0000 - val_loss: 1.5982 - val_acc: 0.5855\n",
      "Epoch 192/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.4577 - acc: 1.0000 - val_loss: 1.5841 - val_acc: 0.6047\n",
      "Epoch 193/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.4589 - acc: 1.0000 - val_loss: 1.9523 - val_acc: 0.4573\n",
      "Epoch 194/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.4478 - acc: 1.0000 - val_loss: 1.6785 - val_acc: 0.5470\n",
      "Epoch 195/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.4308 - acc: 1.0000 - val_loss: 1.6064 - val_acc: 0.5726\n",
      "Epoch 196/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.4257 - acc: 1.0000 - val_loss: 1.5263 - val_acc: 0.6004\n",
      "Epoch 197/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.4275 - acc: 1.0000 - val_loss: 1.6201 - val_acc: 0.5833\n",
      "Epoch 198/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.4122 - acc: 1.0000 - val_loss: 1.6990 - val_acc: 0.5278\n",
      "Epoch 199/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.4003 - acc: 1.0000 - val_loss: 1.6599 - val_acc: 0.5363\n",
      "Epoch 200/2000\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.4060 - acc: 1.0000 - val_loss: 1.6324 - val_acc: 0.5662\n",
      "Epoch 201/2000\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.4004 - acc: 1.0000 - val_loss: 1.4914 - val_acc: 0.6239\n",
      "Epoch 202/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.4002 - acc: 1.0000 - val_loss: 1.5523 - val_acc: 0.6004\n",
      "Epoch 203/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.3870 - acc: 1.0000 - val_loss: 1.8092 - val_acc: 0.4808\n",
      "Epoch 204/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.3747 - acc: 1.0000 - val_loss: 1.6274 - val_acc: 0.5385\n",
      "Epoch 205/2000\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.3759 - acc: 1.0000 - val_loss: 1.4878 - val_acc: 0.6175\n",
      "Epoch 206/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.3563 - acc: 1.0000 - val_loss: 1.5009 - val_acc: 0.5876\n",
      "Epoch 207/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.3536 - acc: 1.0000 - val_loss: 1.4878 - val_acc: 0.5983\n",
      "Epoch 208/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.3464 - acc: 1.0000 - val_loss: 1.5047 - val_acc: 0.5919\n",
      "Epoch 209/2000\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.3470 - acc: 1.0000 - val_loss: 1.4853 - val_acc: 0.5876\n",
      "Epoch 210/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.3343 - acc: 1.0000 - val_loss: 1.4886 - val_acc: 0.5983\n",
      "Epoch 211/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.3340 - acc: 1.0000 - val_loss: 1.4369 - val_acc: 0.6090\n",
      "Epoch 212/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.3265 - acc: 1.0000 - val_loss: 1.4799 - val_acc: 0.5962\n",
      "Epoch 213/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.3185 - acc: 1.0000 - val_loss: 1.4945 - val_acc: 0.5791\n",
      "Epoch 214/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.3200 - acc: 1.0000 - val_loss: 1.5199 - val_acc: 0.5534\n",
      "Epoch 215/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.3085 - acc: 1.0000 - val_loss: 1.5450 - val_acc: 0.5385\n",
      "Epoch 216/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.3174 - acc: 1.0000 - val_loss: 1.5657 - val_acc: 0.5385\n",
      "Epoch 217/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.3014 - acc: 1.0000 - val_loss: 1.5523 - val_acc: 0.5256\n",
      "Epoch 218/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.2974 - acc: 1.0000 - val_loss: 1.4889 - val_acc: 0.5705\n",
      "Epoch 219/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2971 - acc: 1.0000 - val_loss: 1.5113 - val_acc: 0.5748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.2902 - acc: 1.0000 - val_loss: 1.6326 - val_acc: 0.5342\n",
      "Epoch 221/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2806 - acc: 1.0000 - val_loss: 1.5845 - val_acc: 0.5342\n",
      "Epoch 222/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2782 - acc: 1.0000 - val_loss: 1.5856 - val_acc: 0.5449\n",
      "Epoch 223/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2802 - acc: 1.0000 - val_loss: 1.4892 - val_acc: 0.5855\n",
      "Epoch 224/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2733 - acc: 1.0000 - val_loss: 1.4518 - val_acc: 0.5833\n",
      "Epoch 225/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2838 - acc: 1.0000 - val_loss: 1.5542 - val_acc: 0.5449\n",
      "Epoch 226/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2703 - acc: 1.0000 - val_loss: 1.6476 - val_acc: 0.5043\n",
      "Epoch 227/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2590 - acc: 1.0000 - val_loss: 1.5832 - val_acc: 0.5342\n",
      "Epoch 228/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2588 - acc: 1.0000 - val_loss: 1.5370 - val_acc: 0.5342\n",
      "Epoch 229/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.2584 - acc: 1.0000 - val_loss: 1.5432 - val_acc: 0.5513\n",
      "Epoch 230/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.2507 - acc: 1.0000 - val_loss: 1.5488 - val_acc: 0.5406\n",
      "Epoch 231/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2467 - acc: 1.0000 - val_loss: 1.4538 - val_acc: 0.5684\n",
      "\n",
      "Epoch 00231: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 232/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2463 - acc: 1.0000 - val_loss: 1.4204 - val_acc: 0.5940\n",
      "Epoch 233/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2438 - acc: 1.0000 - val_loss: 1.4633 - val_acc: 0.5513\n",
      "Epoch 234/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2414 - acc: 1.0000 - val_loss: 1.5228 - val_acc: 0.5449\n",
      "Epoch 235/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2445 - acc: 1.0000 - val_loss: 1.5146 - val_acc: 0.5406\n",
      "Epoch 236/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2367 - acc: 1.0000 - val_loss: 1.4791 - val_acc: 0.5406\n",
      "Epoch 237/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2382 - acc: 1.0000 - val_loss: 1.4942 - val_acc: 0.5491\n",
      "Epoch 238/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2371 - acc: 1.0000 - val_loss: 1.5065 - val_acc: 0.5385\n",
      "Epoch 239/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2326 - acc: 1.0000 - val_loss: 1.4098 - val_acc: 0.5812\n",
      "Epoch 240/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2364 - acc: 1.0000 - val_loss: 1.4878 - val_acc: 0.5662\n",
      "Epoch 241/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2322 - acc: 1.0000 - val_loss: 1.4789 - val_acc: 0.5470\n",
      "Epoch 242/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2305 - acc: 1.0000 - val_loss: 1.5743 - val_acc: 0.5299\n",
      "Epoch 243/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2275 - acc: 1.0000 - val_loss: 1.4972 - val_acc: 0.5534\n",
      "Epoch 244/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2229 - acc: 1.0000 - val_loss: 1.4652 - val_acc: 0.5620\n",
      "Epoch 245/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2234 - acc: 1.0000 - val_loss: 1.4625 - val_acc: 0.5620\n",
      "Epoch 246/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2247 - acc: 1.0000 - val_loss: 1.5276 - val_acc: 0.5406\n",
      "Epoch 247/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2312 - acc: 1.0000 - val_loss: 1.4589 - val_acc: 0.5662\n",
      "Epoch 248/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2207 - acc: 1.0000 - val_loss: 1.4016 - val_acc: 0.5769\n",
      "Epoch 249/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2230 - acc: 1.0000 - val_loss: 1.4162 - val_acc: 0.5726\n",
      "Epoch 250/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2210 - acc: 1.0000 - val_loss: 1.4971 - val_acc: 0.5470\n",
      "Epoch 251/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2190 - acc: 1.0000 - val_loss: 1.4990 - val_acc: 0.5513\n",
      "Epoch 252/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2129 - acc: 1.0000 - val_loss: 1.4607 - val_acc: 0.5705\n",
      "Epoch 253/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2160 - acc: 1.0000 - val_loss: 1.5694 - val_acc: 0.5235\n",
      "Epoch 254/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2109 - acc: 1.0000 - val_loss: 1.5030 - val_acc: 0.5491\n",
      "Epoch 255/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2067 - acc: 1.0000 - val_loss: 1.4758 - val_acc: 0.5534\n",
      "Epoch 256/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2069 - acc: 1.0000 - val_loss: 1.4621 - val_acc: 0.5491\n",
      "Epoch 257/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2140 - acc: 1.0000 - val_loss: 1.4853 - val_acc: 0.5620\n",
      "Epoch 258/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2089 - acc: 1.0000 - val_loss: 1.4748 - val_acc: 0.5577\n",
      "Epoch 259/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.2196 - acc: 1.0000 - val_loss: 1.3595 - val_acc: 0.6218\n",
      "Epoch 260/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.2069 - acc: 1.0000 - val_loss: 1.3827 - val_acc: 0.5897\n",
      "Epoch 261/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2023 - acc: 1.0000 - val_loss: 1.3705 - val_acc: 0.5962\n",
      "Epoch 262/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.1997 - acc: 1.0000 - val_loss: 1.4120 - val_acc: 0.5705\n",
      "Epoch 263/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2016 - acc: 1.0000 - val_loss: 1.4538 - val_acc: 0.5620\n",
      "Epoch 264/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.1981 - acc: 1.0000 - val_loss: 1.4274 - val_acc: 0.5684\n",
      "Epoch 265/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1996 - acc: 1.0000 - val_loss: 1.4171 - val_acc: 0.5705\n",
      "Epoch 266/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.1966 - acc: 1.0000 - val_loss: 1.4134 - val_acc: 0.5705\n",
      "Epoch 267/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1962 - acc: 1.0000 - val_loss: 1.4680 - val_acc: 0.5577\n",
      "Epoch 268/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2000 - acc: 1.0000 - val_loss: 1.4675 - val_acc: 0.5577\n",
      "Epoch 269/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1971 - acc: 1.0000 - val_loss: 1.4778 - val_acc: 0.5556\n",
      "Epoch 270/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.1954 - acc: 1.0000 - val_loss: 1.4229 - val_acc: 0.5662\n",
      "Epoch 271/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1916 - acc: 1.0000 - val_loss: 1.3996 - val_acc: 0.5684\n",
      "Epoch 272/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1950 - acc: 1.0000 - val_loss: 1.4013 - val_acc: 0.5748\n",
      "Epoch 273/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1877 - acc: 1.0000 - val_loss: 1.4128 - val_acc: 0.5791\n",
      "Epoch 274/2000\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.1873 - acc: 1.0000 - val_loss: 1.4394 - val_acc: 0.5620\n",
      "Epoch 275/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1825 - acc: 1.0000 - val_loss: 1.4032 - val_acc: 0.5705\n",
      "Epoch 276/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1851 - acc: 1.0000 - val_loss: 1.4842 - val_acc: 0.5427\n",
      "Epoch 277/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1871 - acc: 1.0000 - val_loss: 1.4921 - val_acc: 0.5321\n",
      "Epoch 278/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1827 - acc: 1.0000 - val_loss: 1.4067 - val_acc: 0.5684\n",
      "Epoch 279/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.1807 - acc: 1.0000 - val_loss: 1.4103 - val_acc: 0.5641\n",
      "\n",
      "Epoch 00279: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 280/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1794 - acc: 1.0000 - val_loss: 1.4187 - val_acc: 0.5598\n",
      "Epoch 281/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1761 - acc: 1.0000 - val_loss: 1.4160 - val_acc: 0.5556\n",
      "Epoch 282/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.1797 - acc: 1.0000 - val_loss: 1.3869 - val_acc: 0.5726\n",
      "Epoch 283/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1780 - acc: 1.0000 - val_loss: 1.4172 - val_acc: 0.5534\n",
      "Epoch 284/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1801 - acc: 1.0000 - val_loss: 1.4308 - val_acc: 0.5534\n",
      "Epoch 285/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1804 - acc: 1.0000 - val_loss: 1.4326 - val_acc: 0.5598\n",
      "Epoch 286/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1783 - acc: 1.0000 - val_loss: 1.3562 - val_acc: 0.5983\n",
      "Epoch 287/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1764 - acc: 1.0000 - val_loss: 1.3698 - val_acc: 0.5876\n",
      "Epoch 288/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1770 - acc: 1.0000 - val_loss: 1.3938 - val_acc: 0.5769\n",
      "Epoch 289/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.1736 - acc: 1.0000 - val_loss: 1.4157 - val_acc: 0.5620\n",
      "Epoch 290/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1738 - acc: 1.0000 - val_loss: 1.3864 - val_acc: 0.5855\n",
      "Epoch 291/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1742 - acc: 1.0000 - val_loss: 1.4432 - val_acc: 0.5556\n",
      "Epoch 292/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1728 - acc: 1.0000 - val_loss: 1.4144 - val_acc: 0.5620\n",
      "Epoch 293/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1747 - acc: 1.0000 - val_loss: 1.4074 - val_acc: 0.5662\n",
      "Epoch 294/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1733 - acc: 1.0000 - val_loss: 1.3693 - val_acc: 0.5791\n",
      "Epoch 295/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.1680 - acc: 1.0000 - val_loss: 1.3916 - val_acc: 0.5705\n",
      "Epoch 296/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1710 - acc: 1.0000 - val_loss: 1.4013 - val_acc: 0.5748\n",
      "Epoch 297/2000\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.1681 - acc: 1.0000 - val_loss: 1.4003 - val_acc: 0.5769\n",
      "Epoch 298/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1688 - acc: 1.0000 - val_loss: 1.4198 - val_acc: 0.5620\n",
      "Epoch 299/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1677 - acc: 1.0000 - val_loss: 1.4086 - val_acc: 0.5684\n",
      "Epoch 300/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1727 - acc: 1.0000 - val_loss: 1.3709 - val_acc: 0.5919\n",
      "Epoch 301/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1650 - acc: 1.0000 - val_loss: 1.3737 - val_acc: 0.5855\n",
      "Epoch 302/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1715 - acc: 1.0000 - val_loss: 1.4026 - val_acc: 0.5705\n",
      "Epoch 303/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1673 - acc: 1.0000 - val_loss: 1.3487 - val_acc: 0.6047\n",
      "Epoch 304/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1678 - acc: 1.0000 - val_loss: 1.3948 - val_acc: 0.5812\n",
      "Epoch 305/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1691 - acc: 1.0000 - val_loss: 1.4356 - val_acc: 0.5598\n",
      "Epoch 306/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1649 - acc: 1.0000 - val_loss: 1.3930 - val_acc: 0.5791\n",
      "Epoch 307/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1713 - acc: 1.0000 - val_loss: 1.4239 - val_acc: 0.5556\n",
      "Epoch 308/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1661 - acc: 1.0000 - val_loss: 1.4201 - val_acc: 0.5620\n",
      "Epoch 309/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1651 - acc: 1.0000 - val_loss: 1.4173 - val_acc: 0.5556\n",
      "Epoch 310/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1643 - acc: 1.0000 - val_loss: 1.4176 - val_acc: 0.5598\n",
      "Epoch 311/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1619 - acc: 1.0000 - val_loss: 1.3969 - val_acc: 0.5705\n",
      "Epoch 312/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1585 - acc: 1.0000 - val_loss: 1.3905 - val_acc: 0.5769\n",
      "Epoch 313/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1592 - acc: 1.0000 - val_loss: 1.3937 - val_acc: 0.5726\n",
      "Epoch 314/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.1590 - acc: 1.0000 - val_loss: 1.4026 - val_acc: 0.5662\n",
      "Epoch 315/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1650 - acc: 1.0000 - val_loss: 1.3918 - val_acc: 0.5748\n",
      "Epoch 316/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1611 - acc: 1.0000 - val_loss: 1.4091 - val_acc: 0.5726\n",
      "Epoch 317/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1613 - acc: 1.0000 - val_loss: 1.4275 - val_acc: 0.5598\n",
      "Epoch 318/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1609 - acc: 1.0000 - val_loss: 1.4469 - val_acc: 0.5513\n",
      "Epoch 319/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1595 - acc: 1.0000 - val_loss: 1.4084 - val_acc: 0.5620\n",
      "Epoch 320/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1582 - acc: 1.0000 - val_loss: 1.4250 - val_acc: 0.5556\n",
      "Epoch 321/2000\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.1556 - acc: 1.0000 - val_loss: 1.3958 - val_acc: 0.5598\n",
      "Epoch 322/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1544 - acc: 1.0000 - val_loss: 1.4020 - val_acc: 0.5556\n",
      "Epoch 323/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1546 - acc: 1.0000 - val_loss: 1.4085 - val_acc: 0.5620\n",
      "\n",
      "Epoch 00323: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 324/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1565 - acc: 1.0000 - val_loss: 1.3846 - val_acc: 0.5684\n",
      "Epoch 325/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.1546 - acc: 1.0000 - val_loss: 1.4219 - val_acc: 0.5491\n",
      "Epoch 326/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.1535 - acc: 1.0000 - val_loss: 1.4270 - val_acc: 0.5491\n",
      "Epoch 327/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.1538 - acc: 1.0000 - val_loss: 1.4333 - val_acc: 0.5491\n",
      "Epoch 328/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1576 - acc: 1.0000 - val_loss: 1.3929 - val_acc: 0.5577\n",
      "Epoch 329/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1520 - acc: 1.0000 - val_loss: 1.3808 - val_acc: 0.5726\n",
      "Epoch 330/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1521 - acc: 1.0000 - val_loss: 1.3771 - val_acc: 0.5726\n",
      "Epoch 331/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1530 - acc: 1.0000 - val_loss: 1.3768 - val_acc: 0.5662\n",
      "Epoch 332/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1525 - acc: 1.0000 - val_loss: 1.3880 - val_acc: 0.5641\n",
      "Epoch 333/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1529 - acc: 1.0000 - val_loss: 1.3914 - val_acc: 0.5620\n",
      "Epoch 334/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1537 - acc: 1.0000 - val_loss: 1.3883 - val_acc: 0.5620\n",
      "Epoch 335/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.1517 - acc: 1.0000 - val_loss: 1.3862 - val_acc: 0.5620\n",
      "Epoch 336/2000\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.1501 - acc: 1.0000 - val_loss: 1.3801 - val_acc: 0.5684\n",
      "Epoch 337/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1507 - acc: 1.0000 - val_loss: 1.3974 - val_acc: 0.5577\n",
      "Epoch 338/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1500 - acc: 1.0000 - val_loss: 1.4043 - val_acc: 0.5534\n",
      "Epoch 339/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.1524 - acc: 1.0000 - val_loss: 1.4065 - val_acc: 0.5513\n",
      "Epoch 340/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 4s 61ms/step - loss: 0.1497 - acc: 1.0000 - val_loss: 1.3885 - val_acc: 0.5662\n",
      "Epoch 341/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.1518 - acc: 1.0000 - val_loss: 1.3998 - val_acc: 0.5556\n",
      "Epoch 342/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 0.1473 - acc: 1.0000 - val_loss: 1.3878 - val_acc: 0.5598\n",
      "Epoch 343/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.1497 - acc: 1.0000 - val_loss: 1.4001 - val_acc: 0.5577\n",
      "\n",
      "Epoch 00343: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844017094017094\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_input_tensor = Input(shape=(scaled_train_data_words.shape[1],) , name='words')\n",
    "ngram_input_tensor = Input(shape=(scaled_train_data_ngrams.shape[1],) , name='n_grams')\n",
    "\n",
    "neu0 = Sequential()\n",
    "neu0.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "\n",
    "neu1 = Sequential()\n",
    "neu1.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "\n",
    "output_tensor_0 = neu0(ngram_input_tensor)\n",
    "output_tensor_1 = neu1(word_input_tensor)\n",
    "\n",
    "\n",
    "conv_input_tensor = Input(shape=(maxlen,) , name='convnets')\n",
    "\n",
    "conv_1d_s3_model = Sequential()\n",
    "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s3_model.add(layers.Dropout(0.2))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "conv_output_tensor_0 = conv_1d_s3_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "conv_1d_s1_model = Sequential()\n",
    "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s1_model.add(layers.Dropout(0.2))\n",
    "conv_1d_s1_model.add(layers.SeparableConv1D(32, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "conv_output_tensor_1 = conv_1d_s1_model(conv_input_tensor)\n",
    "\n",
    "conv_1d_complex_model = Sequential()\n",
    "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_complex_model.add(layers.Dropout(0.2))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "conv_output_tensor_2 = conv_1d_complex_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "\n",
    "concatenated = layers.concatenate([output_tensor_0,\n",
    "                                   output_tensor_1,\n",
    "                                   conv_output_tensor_0,\n",
    "                                   conv_output_tensor_1,\n",
    "                                   conv_output_tensor_2,\n",
    "                                  ], axis=-1)\n",
    "\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "concatenated = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model([ngram_input_tensor, word_input_tensor, conv_input_tensor], concatenated)\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit([scaled_train_data_ngrams, scaled_train_data_words, X_train], y_train,\n",
    "                    validation_data=([scaled_test_data_ngrams, scaled_test_data_words, X_val], y_val),\n",
    "                    epochs=2000,\n",
    "                    batch_size=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "n_grams (InputLayer)            (None, 4152)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "words (InputLayer)              (None, 1302)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "convnets (InputLayer)           (None, 1020)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_43 (Sequential)      (None, 32)           68048       n_grams[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_44 (Sequential)      (None, 32)           22448       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_45 (Sequential)      (None, 32)           12202       convnets[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_46 (Sequential)      (None, 32)           12184       convnets[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_47 (Sequential)      (None, 32)           12193       convnets[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 160)          0           sequential_43[1][0]              \n",
      "                                                                 sequential_44[1][0]              \n",
      "                                                                 sequential_45[1][0]              \n",
      "                                                                 sequential_46[1][0]              \n",
      "                                                                 sequential_47[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_116 (Dense)               (None, 64)           10304       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_114 (Dropout)           (None, 64)           0           dense_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_117 (Dense)               (None, 9)            585         dropout_114[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 137,964\n",
      "Trainable params: 137,772\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "Train on 63 samples, validate on 468 samples\n",
      "Epoch 1/2000\n",
      "63/63 [==============================] - 9s 150ms/step - loss: 5.8693 - acc: 0.0794 - val_loss: 5.4950 - val_acc: 0.1111\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 5.4910 - acc: 0.2063 - val_loss: 5.3414 - val_acc: 0.1132\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 5.3241 - acc: 0.1746 - val_loss: 5.2467 - val_acc: 0.1132\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 5.2161 - acc: 0.1270 - val_loss: 5.1645 - val_acc: 0.1154\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 5.0721 - acc: 0.2222 - val_loss: 5.0775 - val_acc: 0.1410\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 4.9553 - acc: 0.1746 - val_loss: 5.0339 - val_acc: 0.1410\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 4.8279 - acc: 0.2540 - val_loss: 4.9381 - val_acc: 0.1410\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 4.7335 - acc: 0.3175 - val_loss: 4.8087 - val_acc: 0.2415\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 4.6781 - acc: 0.2540 - val_loss: 4.7432 - val_acc: 0.2436\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 4.5053 - acc: 0.3968 - val_loss: 4.6477 - val_acc: 0.3462\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 4.4459 - acc: 0.4286 - val_loss: 4.5737 - val_acc: 0.3932\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 4.4508 - acc: 0.3968 - val_loss: 4.5528 - val_acc: 0.3141\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 4.3244 - acc: 0.4127 - val_loss: 4.4542 - val_acc: 0.5278\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 4.2190 - acc: 0.4603 - val_loss: 4.4195 - val_acc: 0.5192\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 4.2496 - acc: 0.3492 - val_loss: 4.3816 - val_acc: 0.5064\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 4.1385 - acc: 0.4127 - val_loss: 4.3552 - val_acc: 0.3590\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.9903 - acc: 0.5397 - val_loss: 4.3007 - val_acc: 0.4081\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 3.9992 - acc: 0.5397 - val_loss: 4.2590 - val_acc: 0.4017\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.8313 - acc: 0.6032 - val_loss: 4.1882 - val_acc: 0.4893\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.7779 - acc: 0.7143 - val_loss: 4.1403 - val_acc: 0.5214\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.7541 - acc: 0.5873 - val_loss: 4.0801 - val_acc: 0.5833\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 3.6069 - acc: 0.7143 - val_loss: 4.0447 - val_acc: 0.5962\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 3.5293 - acc: 0.6508 - val_loss: 3.9628 - val_acc: 0.6538\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 3.5532 - acc: 0.6667 - val_loss: 3.9379 - val_acc: 0.6325\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 3.5028 - acc: 0.6190 - val_loss: 3.8381 - val_acc: 0.7030\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 3.3025 - acc: 0.7302 - val_loss: 3.8296 - val_acc: 0.5833\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 3.2830 - acc: 0.7778 - val_loss: 3.7426 - val_acc: 0.7115\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 3.3091 - acc: 0.6667 - val_loss: 3.6649 - val_acc: 0.7265\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 3.1270 - acc: 0.7778 - val_loss: 3.6771 - val_acc: 0.6966\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.0570 - acc: 0.7937 - val_loss: 3.5442 - val_acc: 0.7244\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 3.1990 - acc: 0.7143 - val_loss: 3.5288 - val_acc: 0.7607\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 3.0896 - acc: 0.7937 - val_loss: 3.5093 - val_acc: 0.7650\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 3.0026 - acc: 0.6984 - val_loss: 3.4215 - val_acc: 0.7799\n",
      "Epoch 34/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.8068 - acc: 0.8571 - val_loss: 3.4129 - val_acc: 0.7842\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 2.8616 - acc: 0.8254 - val_loss: 3.3697 - val_acc: 0.7692\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 2.8445 - acc: 0.7778 - val_loss: 3.2998 - val_acc: 0.7650\n",
      "Epoch 37/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 4s 60ms/step - loss: 2.7657 - acc: 0.8254 - val_loss: 3.3100 - val_acc: 0.7756\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 2.6519 - acc: 0.8095 - val_loss: 3.2236 - val_acc: 0.7778\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 2.5792 - acc: 0.9206 - val_loss: 3.1807 - val_acc: 0.8034\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 2.7127 - acc: 0.8413 - val_loss: 3.0942 - val_acc: 0.7863\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 2.6092 - acc: 0.8413 - val_loss: 3.1014 - val_acc: 0.8056\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 2.3919 - acc: 0.9206 - val_loss: 3.0442 - val_acc: 0.8205\n",
      "Epoch 43/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.4015 - acc: 0.9524 - val_loss: 3.0663 - val_acc: 0.7927\n",
      "Epoch 44/2000\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 2.5161 - acc: 0.8254 - val_loss: 3.0270 - val_acc: 0.7949\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 2.4611 - acc: 0.8413 - val_loss: 2.9948 - val_acc: 0.7991\n",
      "Epoch 46/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 2.3171 - acc: 0.9524 - val_loss: 2.9670 - val_acc: 0.7991\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 2.3596 - acc: 0.8730 - val_loss: 2.8392 - val_acc: 0.8162\n",
      "Epoch 48/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 2.3625 - acc: 0.9048 - val_loss: 2.9106 - val_acc: 0.8056\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 2.3188 - acc: 0.8889 - val_loss: 2.8490 - val_acc: 0.8333\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 2.2017 - acc: 0.9365 - val_loss: 2.7777 - val_acc: 0.8419\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 2.1811 - acc: 0.9206 - val_loss: 2.8310 - val_acc: 0.7970\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 2.2824 - acc: 0.8571 - val_loss: 2.8208 - val_acc: 0.7842\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 2.0782 - acc: 0.9683 - val_loss: 2.7881 - val_acc: 0.8098\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 2.0369 - acc: 0.9524 - val_loss: 2.7535 - val_acc: 0.8077\n",
      "Epoch 55/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 2.0214 - acc: 0.9683 - val_loss: 2.6844 - val_acc: 0.8162\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.9857 - acc: 0.9683 - val_loss: 2.6238 - val_acc: 0.8184\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - 4s 57ms/step - loss: 1.9562 - acc: 0.9683 - val_loss: 2.5912 - val_acc: 0.8184\n",
      "Epoch 58/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 2.0758 - acc: 0.8254 - val_loss: 2.5596 - val_acc: 0.8312\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.9148 - acc: 0.9524 - val_loss: 2.6122 - val_acc: 0.8248\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.9868 - acc: 0.9365 - val_loss: 2.6605 - val_acc: 0.7927\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.9356 - acc: 0.9683 - val_loss: 2.5746 - val_acc: 0.8120\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.8718 - acc: 0.9365 - val_loss: 2.5309 - val_acc: 0.8205\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 1.8294 - acc: 0.9841 - val_loss: 2.5651 - val_acc: 0.7970\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.8604 - acc: 0.9683 - val_loss: 2.5059 - val_acc: 0.7970\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.7887 - acc: 0.9683 - val_loss: 2.4482 - val_acc: 0.8077\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.8664 - acc: 0.9206 - val_loss: 2.4200 - val_acc: 0.8077\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.9098 - acc: 0.8889 - val_loss: 2.4071 - val_acc: 0.7863\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.7991 - acc: 0.9365 - val_loss: 2.3916 - val_acc: 0.7885\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - 4s 57ms/step - loss: 1.8292 - acc: 0.9206 - val_loss: 2.3849 - val_acc: 0.8205\n",
      "Epoch 70/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.7972 - acc: 0.9524 - val_loss: 2.3483 - val_acc: 0.8440\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 1.8285 - acc: 0.9524 - val_loss: 2.4137 - val_acc: 0.8141\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.6820 - acc: 0.9683 - val_loss: 2.3511 - val_acc: 0.8291\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.6716 - acc: 0.9365 - val_loss: 2.3172 - val_acc: 0.8077\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.6290 - acc: 0.9841 - val_loss: 2.2942 - val_acc: 0.8120\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.6324 - acc: 0.9841 - val_loss: 2.2913 - val_acc: 0.8419\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.6092 - acc: 1.0000 - val_loss: 2.2649 - val_acc: 0.8098\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.5720 - acc: 1.0000 - val_loss: 2.2709 - val_acc: 0.8291\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.5812 - acc: 1.0000 - val_loss: 2.2745 - val_acc: 0.8205\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.5890 - acc: 0.9841 - val_loss: 2.3000 - val_acc: 0.7991\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.5543 - acc: 1.0000 - val_loss: 2.2482 - val_acc: 0.7885\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.5449 - acc: 0.9841 - val_loss: 2.1850 - val_acc: 0.8034\n",
      "Epoch 82/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.5713 - acc: 0.9683 - val_loss: 2.2705 - val_acc: 0.7585\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.5036 - acc: 1.0000 - val_loss: 2.1813 - val_acc: 0.7863\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - 4s 57ms/step - loss: 1.4316 - acc: 1.0000 - val_loss: 2.1456 - val_acc: 0.7842\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 1.4557 - acc: 1.0000 - val_loss: 2.1367 - val_acc: 0.7970\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.4634 - acc: 1.0000 - val_loss: 2.1269 - val_acc: 0.7863\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.4386 - acc: 0.9683 - val_loss: 2.1496 - val_acc: 0.7735\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.4941 - acc: 0.9841 - val_loss: 2.1560 - val_acc: 0.7714\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.4299 - acc: 0.9841 - val_loss: 2.1675 - val_acc: 0.7650\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - 4s 57ms/step - loss: 1.4304 - acc: 1.0000 - val_loss: 2.0965 - val_acc: 0.7799\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.3540 - acc: 1.0000 - val_loss: 2.0229 - val_acc: 0.8056\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.4512 - acc: 1.0000 - val_loss: 2.1109 - val_acc: 0.7650\n",
      "Epoch 93/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 1.3674 - acc: 0.9841 - val_loss: 2.1286 - val_acc: 0.7415\n",
      "Epoch 94/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.3711 - acc: 1.0000 - val_loss: 2.1193 - val_acc: 0.7436\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.3672 - acc: 1.0000 - val_loss: 2.0653 - val_acc: 0.7756\n",
      "Epoch 96/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 1.3256 - acc: 1.0000 - val_loss: 2.0310 - val_acc: 0.7735\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.3492 - acc: 0.9841 - val_loss: 2.0033 - val_acc: 0.7906\n",
      "Epoch 98/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 4s 61ms/step - loss: 1.3525 - acc: 0.9841 - val_loss: 1.9998 - val_acc: 0.7650\n",
      "Epoch 99/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.3710 - acc: 0.9841 - val_loss: 2.0339 - val_acc: 0.7671\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.3109 - acc: 0.9841 - val_loss: 2.0458 - val_acc: 0.7585\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.2733 - acc: 1.0000 - val_loss: 1.9678 - val_acc: 0.7842\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.2666 - acc: 1.0000 - val_loss: 1.9904 - val_acc: 0.7543\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.2486 - acc: 0.9841 - val_loss: 1.9112 - val_acc: 0.7650\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.2767 - acc: 0.9683 - val_loss: 1.9952 - val_acc: 0.7393\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.2468 - acc: 1.0000 - val_loss: 1.9586 - val_acc: 0.7479\n",
      "Epoch 106/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 1.2328 - acc: 1.0000 - val_loss: 1.9807 - val_acc: 0.7521\n",
      "Epoch 107/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.2202 - acc: 1.0000 - val_loss: 2.0505 - val_acc: 0.6923\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.2438 - acc: 0.9841 - val_loss: 2.0271 - val_acc: 0.7051\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.1977 - acc: 1.0000 - val_loss: 2.0243 - val_acc: 0.7308\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.2054 - acc: 1.0000 - val_loss: 1.9527 - val_acc: 0.7521\n",
      "Epoch 111/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.1719 - acc: 1.0000 - val_loss: 1.9405 - val_acc: 0.7329\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.1876 - acc: 0.9841 - val_loss: 1.9603 - val_acc: 0.7158\n",
      "Epoch 113/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 1.1699 - acc: 0.9841 - val_loss: 1.9456 - val_acc: 0.7094\n",
      "Epoch 114/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.1315 - acc: 1.0000 - val_loss: 1.8919 - val_acc: 0.7308\n",
      "Epoch 115/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.1014 - acc: 1.0000 - val_loss: 1.9107 - val_acc: 0.7244\n",
      "Epoch 116/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.1352 - acc: 1.0000 - val_loss: 1.8644 - val_acc: 0.7436\n",
      "Epoch 117/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.1111 - acc: 1.0000 - val_loss: 1.8917 - val_acc: 0.7265\n",
      "Epoch 118/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.0970 - acc: 0.9841 - val_loss: 1.8961 - val_acc: 0.7350\n",
      "Epoch 119/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.0761 - acc: 1.0000 - val_loss: 1.8723 - val_acc: 0.7329\n",
      "Epoch 120/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 1.0630 - acc: 1.0000 - val_loss: 1.9219 - val_acc: 0.6902\n",
      "Epoch 121/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.1073 - acc: 0.9683 - val_loss: 1.9728 - val_acc: 0.6581\n",
      "Epoch 122/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.0720 - acc: 1.0000 - val_loss: 1.9149 - val_acc: 0.6752\n",
      "Epoch 123/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 1.0704 - acc: 1.0000 - val_loss: 1.9109 - val_acc: 0.6944\n",
      "Epoch 124/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.0648 - acc: 0.9841 - val_loss: 1.9147 - val_acc: 0.6816\n",
      "Epoch 125/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.0278 - acc: 1.0000 - val_loss: 1.8697 - val_acc: 0.7030\n",
      "Epoch 126/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.0399 - acc: 1.0000 - val_loss: 1.8332 - val_acc: 0.7222\n",
      "Epoch 127/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.0257 - acc: 1.0000 - val_loss: 1.8383 - val_acc: 0.7051\n",
      "Epoch 128/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 1.0030 - acc: 1.0000 - val_loss: 1.8069 - val_acc: 0.7073\n",
      "Epoch 129/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 1.0016 - acc: 1.0000 - val_loss: 1.9120 - val_acc: 0.6474\n",
      "Epoch 130/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 0.9979 - acc: 1.0000 - val_loss: 1.7951 - val_acc: 0.7115\n",
      "Epoch 131/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.9974 - acc: 1.0000 - val_loss: 1.8920 - val_acc: 0.6731\n",
      "Epoch 132/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.9804 - acc: 1.0000 - val_loss: 1.8367 - val_acc: 0.6966\n",
      "Epoch 133/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.9809 - acc: 1.0000 - val_loss: 1.8677 - val_acc: 0.6581\n",
      "Epoch 134/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.9931 - acc: 1.0000 - val_loss: 1.8701 - val_acc: 0.6795\n",
      "Epoch 135/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.9921 - acc: 0.9683 - val_loss: 1.9338 - val_acc: 0.6325\n",
      "Epoch 136/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.9522 - acc: 1.0000 - val_loss: 1.8585 - val_acc: 0.6560\n",
      "Epoch 137/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.9359 - acc: 1.0000 - val_loss: 1.8687 - val_acc: 0.6410\n",
      "Epoch 138/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.9249 - acc: 1.0000 - val_loss: 1.8808 - val_acc: 0.6389\n",
      "Epoch 139/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.9261 - acc: 1.0000 - val_loss: 1.8448 - val_acc: 0.6517\n",
      "Epoch 140/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.9045 - acc: 1.0000 - val_loss: 1.8125 - val_acc: 0.6603\n",
      "Epoch 141/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.8989 - acc: 1.0000 - val_loss: 1.7740 - val_acc: 0.6709\n",
      "Epoch 142/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8901 - acc: 0.9841 - val_loss: 1.7513 - val_acc: 0.6709\n",
      "Epoch 143/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.9021 - acc: 1.0000 - val_loss: 1.7846 - val_acc: 0.6688\n",
      "Epoch 144/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8886 - acc: 1.0000 - val_loss: 1.8281 - val_acc: 0.6410\n",
      "Epoch 145/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.9006 - acc: 1.0000 - val_loss: 1.9330 - val_acc: 0.6047\n",
      "Epoch 146/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8776 - acc: 1.0000 - val_loss: 1.8290 - val_acc: 0.6410\n",
      "Epoch 147/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8400 - acc: 1.0000 - val_loss: 1.8379 - val_acc: 0.6197\n",
      "Epoch 148/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8445 - acc: 0.9841 - val_loss: 1.7686 - val_acc: 0.6538\n",
      "Epoch 149/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8287 - acc: 1.0000 - val_loss: 1.7475 - val_acc: 0.6624\n",
      "Epoch 150/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8499 - acc: 1.0000 - val_loss: 1.7775 - val_acc: 0.6453\n",
      "Epoch 151/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8305 - acc: 1.0000 - val_loss: 1.7453 - val_acc: 0.6581\n",
      "Epoch 152/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8229 - acc: 1.0000 - val_loss: 1.7506 - val_acc: 0.6538\n",
      "Epoch 153/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.7989 - acc: 1.0000 - val_loss: 1.7410 - val_acc: 0.6496\n",
      "Epoch 154/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8019 - acc: 1.0000 - val_loss: 1.6879 - val_acc: 0.6645\n",
      "Epoch 155/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.7861 - acc: 1.0000 - val_loss: 1.8004 - val_acc: 0.6197\n",
      "Epoch 156/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.7699 - acc: 1.0000 - val_loss: 1.7358 - val_acc: 0.6410\n",
      "Epoch 157/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.7548 - acc: 1.0000 - val_loss: 1.7709 - val_acc: 0.6239\n",
      "Epoch 158/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.7492 - acc: 1.0000 - val_loss: 1.7472 - val_acc: 0.6261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.7403 - acc: 1.0000 - val_loss: 1.7537 - val_acc: 0.6132\n",
      "Epoch 160/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.7672 - acc: 1.0000 - val_loss: 1.7650 - val_acc: 0.5983\n",
      "Epoch 161/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.7516 - acc: 1.0000 - val_loss: 1.6932 - val_acc: 0.6453\n",
      "Epoch 162/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.7439 - acc: 1.0000 - val_loss: 1.8200 - val_acc: 0.5833\n",
      "Epoch 163/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.7500 - acc: 1.0000 - val_loss: 1.8928 - val_acc: 0.5577\n",
      "Epoch 164/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.7068 - acc: 1.0000 - val_loss: 1.8254 - val_acc: 0.5833\n",
      "Epoch 165/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.7197 - acc: 1.0000 - val_loss: 1.8302 - val_acc: 0.5919\n",
      "Epoch 166/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.7071 - acc: 1.0000 - val_loss: 1.8168 - val_acc: 0.5940\n",
      "Epoch 167/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.6804 - acc: 1.0000 - val_loss: 1.7572 - val_acc: 0.6026\n",
      "Epoch 168/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.6766 - acc: 1.0000 - val_loss: 1.7139 - val_acc: 0.6132\n",
      "Epoch 169/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.6579 - acc: 1.0000 - val_loss: 1.6660 - val_acc: 0.6261\n",
      "Epoch 170/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.6700 - acc: 1.0000 - val_loss: 1.7150 - val_acc: 0.6111\n",
      "Epoch 171/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.6501 - acc: 1.0000 - val_loss: 1.7199 - val_acc: 0.6026\n",
      "Epoch 172/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.6684 - acc: 1.0000 - val_loss: 1.8184 - val_acc: 0.5684\n",
      "Epoch 173/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.6458 - acc: 1.0000 - val_loss: 1.6584 - val_acc: 0.6090\n",
      "Epoch 174/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.6320 - acc: 1.0000 - val_loss: 1.6367 - val_acc: 0.6282\n",
      "Epoch 175/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.6197 - acc: 1.0000 - val_loss: 1.6883 - val_acc: 0.6132\n",
      "Epoch 176/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.6325 - acc: 1.0000 - val_loss: 1.7719 - val_acc: 0.5406\n",
      "Epoch 177/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.6166 - acc: 1.0000 - val_loss: 1.6220 - val_acc: 0.6282\n",
      "Epoch 178/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.5971 - acc: 1.0000 - val_loss: 1.6870 - val_acc: 0.6004\n",
      "Epoch 179/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.5823 - acc: 1.0000 - val_loss: 1.6312 - val_acc: 0.6239\n",
      "Epoch 180/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.6037 - acc: 0.9841 - val_loss: 1.7564 - val_acc: 0.5641\n",
      "Epoch 181/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.5726 - acc: 1.0000 - val_loss: 1.6587 - val_acc: 0.5983\n",
      "Epoch 182/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.5592 - acc: 1.0000 - val_loss: 1.6651 - val_acc: 0.6026\n",
      "Epoch 183/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.5605 - acc: 1.0000 - val_loss: 1.7226 - val_acc: 0.5812\n",
      "Epoch 184/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.5470 - acc: 1.0000 - val_loss: 1.6372 - val_acc: 0.6047\n",
      "Epoch 185/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.5439 - acc: 1.0000 - val_loss: 1.5622 - val_acc: 0.6218\n",
      "Epoch 186/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.5374 - acc: 1.0000 - val_loss: 1.7382 - val_acc: 0.5513\n",
      "Epoch 187/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.5252 - acc: 1.0000 - val_loss: 1.6903 - val_acc: 0.5662\n",
      "Epoch 188/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.5199 - acc: 1.0000 - val_loss: 1.6116 - val_acc: 0.6026\n",
      "Epoch 189/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.5212 - acc: 1.0000 - val_loss: 1.6636 - val_acc: 0.5662\n",
      "Epoch 190/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.5323 - acc: 1.0000 - val_loss: 1.5994 - val_acc: 0.5983\n",
      "Epoch 191/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.4971 - acc: 1.0000 - val_loss: 1.6444 - val_acc: 0.5641\n",
      "Epoch 192/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.5175 - acc: 0.9841 - val_loss: 1.7012 - val_acc: 0.5406\n",
      "Epoch 193/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.4793 - acc: 1.0000 - val_loss: 1.6373 - val_acc: 0.5620\n",
      "Epoch 194/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.4755 - acc: 1.0000 - val_loss: 1.5769 - val_acc: 0.5940\n",
      "Epoch 195/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.4655 - acc: 1.0000 - val_loss: 1.5712 - val_acc: 0.6004\n",
      "Epoch 196/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.4722 - acc: 1.0000 - val_loss: 1.5311 - val_acc: 0.6047\n",
      "Epoch 197/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.4690 - acc: 1.0000 - val_loss: 1.5565 - val_acc: 0.6068\n",
      "Epoch 198/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 0.4489 - acc: 1.0000 - val_loss: 1.5709 - val_acc: 0.5833\n",
      "Epoch 199/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.4422 - acc: 1.0000 - val_loss: 1.5330 - val_acc: 0.6111\n",
      "Epoch 200/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.4504 - acc: 1.0000 - val_loss: 1.7845 - val_acc: 0.5085\n",
      "Epoch 201/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.4318 - acc: 1.0000 - val_loss: 1.6098 - val_acc: 0.5684\n",
      "Epoch 202/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.4294 - acc: 1.0000 - val_loss: 1.6122 - val_acc: 0.5513\n",
      "Epoch 203/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.4281 - acc: 1.0000 - val_loss: 1.5791 - val_acc: 0.5513\n",
      "Epoch 204/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.4112 - acc: 1.0000 - val_loss: 1.5269 - val_acc: 0.6004\n",
      "Epoch 205/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.4240 - acc: 1.0000 - val_loss: 1.5526 - val_acc: 0.5812\n",
      "Epoch 206/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.4196 - acc: 1.0000 - val_loss: 1.6205 - val_acc: 0.5470\n",
      "Epoch 207/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.4081 - acc: 1.0000 - val_loss: 1.5442 - val_acc: 0.5598\n",
      "Epoch 208/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3937 - acc: 1.0000 - val_loss: 1.5442 - val_acc: 0.5577\n",
      "Epoch 209/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3918 - acc: 1.0000 - val_loss: 1.6186 - val_acc: 0.5449\n",
      "Epoch 210/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3958 - acc: 1.0000 - val_loss: 1.6889 - val_acc: 0.5278\n",
      "Epoch 211/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3801 - acc: 1.0000 - val_loss: 1.4969 - val_acc: 0.5791\n",
      "Epoch 212/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.3713 - acc: 1.0000 - val_loss: 1.5070 - val_acc: 0.5748\n",
      "Epoch 213/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.3684 - acc: 1.0000 - val_loss: 1.4919 - val_acc: 0.5876\n",
      "Epoch 214/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3586 - acc: 1.0000 - val_loss: 1.4820 - val_acc: 0.5855\n",
      "Epoch 215/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.3668 - acc: 1.0000 - val_loss: 1.5141 - val_acc: 0.5812\n",
      "Epoch 216/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3555 - acc: 1.0000 - val_loss: 1.5483 - val_acc: 0.5620\n",
      "Epoch 217/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3532 - acc: 1.0000 - val_loss: 1.5517 - val_acc: 0.5662\n",
      "Epoch 218/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3767 - acc: 0.9841 - val_loss: 1.4650 - val_acc: 0.6090\n",
      "Epoch 219/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3429 - acc: 1.0000 - val_loss: 1.4746 - val_acc: 0.5598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3381 - acc: 1.0000 - val_loss: 1.4476 - val_acc: 0.5919\n",
      "Epoch 221/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.3365 - acc: 1.0000 - val_loss: 1.4458 - val_acc: 0.5897\n",
      "Epoch 222/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3320 - acc: 1.0000 - val_loss: 1.5235 - val_acc: 0.5577\n",
      "Epoch 223/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3549 - acc: 1.0000 - val_loss: 1.3354 - val_acc: 0.6474\n",
      "Epoch 224/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3179 - acc: 1.0000 - val_loss: 1.4147 - val_acc: 0.6004\n",
      "Epoch 225/2000\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 0.3155 - acc: 1.0000 - val_loss: 1.4756 - val_acc: 0.5748\n",
      "Epoch 226/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.3171 - acc: 1.0000 - val_loss: 1.5496 - val_acc: 0.5513\n",
      "Epoch 227/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3281 - acc: 1.0000 - val_loss: 1.5870 - val_acc: 0.5321\n",
      "Epoch 228/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.3276 - acc: 1.0000 - val_loss: 1.5052 - val_acc: 0.5705\n",
      "Epoch 229/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.3074 - acc: 1.0000 - val_loss: 1.6394 - val_acc: 0.5214\n",
      "Epoch 230/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.3003 - acc: 1.0000 - val_loss: 1.5593 - val_acc: 0.5449\n",
      "Epoch 231/2000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.2989 - acc: 1.0000 - val_loss: 1.5295 - val_acc: 0.5534\n",
      "Epoch 232/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2935 - acc: 1.0000 - val_loss: 1.5781 - val_acc: 0.5235\n",
      "Epoch 233/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.3025 - acc: 1.0000 - val_loss: 1.4862 - val_acc: 0.5684\n",
      "Epoch 234/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2876 - acc: 1.0000 - val_loss: 1.4613 - val_acc: 0.5812\n",
      "Epoch 235/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2855 - acc: 1.0000 - val_loss: 1.4218 - val_acc: 0.5919\n",
      "Epoch 236/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2871 - acc: 1.0000 - val_loss: 1.5233 - val_acc: 0.5577\n",
      "Epoch 237/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2866 - acc: 1.0000 - val_loss: 1.5178 - val_acc: 0.5620\n",
      "Epoch 238/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2912 - acc: 1.0000 - val_loss: 1.6069 - val_acc: 0.5192\n",
      "Epoch 239/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2824 - acc: 1.0000 - val_loss: 1.6106 - val_acc: 0.5214\n",
      "Epoch 240/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.2788 - acc: 1.0000 - val_loss: 1.5503 - val_acc: 0.5256\n",
      "Epoch 241/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2744 - acc: 1.0000 - val_loss: 1.4607 - val_acc: 0.5726\n",
      "Epoch 242/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2676 - acc: 1.0000 - val_loss: 1.4571 - val_acc: 0.5641\n",
      "Epoch 243/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2719 - acc: 1.0000 - val_loss: 1.4186 - val_acc: 0.5769\n",
      "\n",
      "Epoch 00243: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 244/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.2620 - acc: 1.0000 - val_loss: 1.4193 - val_acc: 0.5833\n",
      "Epoch 245/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.2698 - acc: 1.0000 - val_loss: 1.4747 - val_acc: 0.5598\n",
      "Epoch 246/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 0.2586 - acc: 1.0000 - val_loss: 1.4847 - val_acc: 0.5577\n",
      "Epoch 247/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 0.2610 - acc: 1.0000 - val_loss: 1.4728 - val_acc: 0.5577\n",
      "Epoch 248/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.2589 - acc: 1.0000 - val_loss: 1.4855 - val_acc: 0.5513\n",
      "Epoch 249/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.2590 - acc: 1.0000 - val_loss: 1.4655 - val_acc: 0.5598\n",
      "Epoch 250/2000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2599 - acc: 1.0000 - val_loss: 1.4276 - val_acc: 0.5748\n",
      "Epoch 251/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.2557 - acc: 1.0000 - val_loss: 1.4379 - val_acc: 0.5684\n",
      "Epoch 252/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2573 - acc: 1.0000 - val_loss: 1.4279 - val_acc: 0.5684\n",
      "Epoch 253/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.2544 - acc: 1.0000 - val_loss: 1.4552 - val_acc: 0.5684\n",
      "Epoch 254/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.2600 - acc: 1.0000 - val_loss: 1.4380 - val_acc: 0.5769\n",
      "Epoch 255/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.2543 - acc: 1.0000 - val_loss: 1.4068 - val_acc: 0.5812\n",
      "Epoch 256/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.2522 - acc: 1.0000 - val_loss: 1.4403 - val_acc: 0.5791\n",
      "Epoch 257/2000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.2485 - acc: 1.0000 - val_loss: 1.4359 - val_acc: 0.5769\n",
      "Epoch 258/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 0.2463 - acc: 1.0000 - val_loss: 1.4231 - val_acc: 0.5791\n",
      "Epoch 259/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 0.2443 - acc: 1.0000 - val_loss: 1.4276 - val_acc: 0.5705\n",
      "Epoch 260/2000\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.2471 - acc: 1.0000 - val_loss: 1.4744 - val_acc: 0.5620\n",
      "Epoch 261/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.2480 - acc: 1.0000 - val_loss: 1.4244 - val_acc: 0.5748\n",
      "Epoch 262/2000\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 0.2424 - acc: 1.0000 - val_loss: 1.4147 - val_acc: 0.5726\n",
      "Epoch 263/2000\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.2444 - acc: 1.0000 - val_loss: 1.4937 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00263: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844017094017094\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_input_tensor = Input(shape=(scaled_train_data_words.shape[1],) , name='words')\n",
    "ngram_input_tensor = Input(shape=(scaled_train_data_ngrams.shape[1],) , name='n_grams')\n",
    "\n",
    "neu0 = Sequential()\n",
    "neu0.add(layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "\n",
    "neu1 = Sequential()\n",
    "neu1.add(layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "\n",
    "output_tensor_0 = neu0(ngram_input_tensor)\n",
    "output_tensor_1 = neu1(word_input_tensor)\n",
    "\n",
    "\n",
    "conv_input_tensor = Input(shape=(maxlen,) , name='convnets')\n",
    "\n",
    "conv_1d_s3_model = Sequential()\n",
    "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s3_model.add(layers.Dropout(0.2))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "conv_output_tensor_0 = conv_1d_s3_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "conv_1d_s1_model = Sequential()\n",
    "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s1_model.add(layers.Dropout(0.2))\n",
    "conv_1d_s1_model.add(layers.SeparableConv1D(32, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "conv_output_tensor_1 = conv_1d_s1_model(conv_input_tensor)\n",
    "\n",
    "conv_1d_complex_model = Sequential()\n",
    "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_complex_model.add(layers.Dropout(0.2))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "conv_output_tensor_2 = conv_1d_complex_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "\n",
    "concatenated = layers.concatenate([output_tensor_0,\n",
    "                                   output_tensor_1,\n",
    "                                   conv_output_tensor_0,\n",
    "                                   conv_output_tensor_1,\n",
    "                                   conv_output_tensor_2,\n",
    "                                  ], axis=-1)\n",
    "\n",
    "concatenated = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "concatenated = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model([ngram_input_tensor, word_input_tensor, conv_input_tensor], concatenated)\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit([scaled_train_data_ngrams, scaled_train_data_words, X_train], y_train,\n",
    "                    validation_data=([scaled_test_data_ngrams, scaled_test_data_words, X_val], y_val),\n",
    "                    epochs=2000,\n",
    "                    batch_size=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              (None, 1020)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_98 (Sequential)      (None, 32)           12202       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_99 (Sequential)      (None, 32)           12184       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_100 (Sequential)     (None, 32)           12193       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 96)           0           sequential_98[1][0]              \n",
      "                                                                 sequential_99[1][0]              \n",
      "                                                                 sequential_100[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 9)            873         concatenate_32[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 37,452\n",
      "Trainable params: 37,260\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "Train on 63 samples, validate on 468 samples\n",
      "Epoch 1/2000\n",
      " - 18s - loss: 2.6666 - acc: 0.1111 - val_loss: 2.3199 - val_acc: 0.0321\n",
      "Epoch 2/2000\n",
      " - 6s - loss: 2.3824 - acc: 0.1111 - val_loss: 2.1591 - val_acc: 0.2650\n",
      "Epoch 3/2000\n",
      " - 6s - loss: 2.2668 - acc: 0.1111 - val_loss: 2.0977 - val_acc: 0.4359\n",
      "Epoch 4/2000\n",
      " - 6s - loss: 2.1778 - acc: 0.2222 - val_loss: 2.1075 - val_acc: 0.3654\n",
      "Epoch 5/2000\n",
      " - 6s - loss: 2.1537 - acc: 0.1587 - val_loss: 2.0696 - val_acc: 0.4679\n",
      "Epoch 6/2000\n",
      " - 6s - loss: 2.0824 - acc: 0.3492 - val_loss: 2.0815 - val_acc: 0.4701\n",
      "Epoch 7/2000\n",
      " - 6s - loss: 2.0326 - acc: 0.4444 - val_loss: 2.1065 - val_acc: 0.3333\n",
      "Epoch 8/2000\n",
      " - 6s - loss: 2.0118 - acc: 0.4603 - val_loss: 2.0739 - val_acc: 0.4359\n",
      "Epoch 9/2000\n",
      " - 6s - loss: 1.9807 - acc: 0.6667 - val_loss: 2.0606 - val_acc: 0.4936\n",
      "Epoch 10/2000\n",
      " - 6s - loss: 1.9251 - acc: 0.7302 - val_loss: 2.0340 - val_acc: 0.4936\n",
      "Epoch 11/2000\n",
      " - 6s - loss: 1.9090 - acc: 0.7937 - val_loss: 2.0644 - val_acc: 0.3269\n",
      "Epoch 12/2000\n",
      " - 6s - loss: 1.8726 - acc: 0.7460 - val_loss: 2.0328 - val_acc: 0.4338\n",
      "Epoch 13/2000\n",
      " - 6s - loss: 1.8273 - acc: 0.8571 - val_loss: 2.0359 - val_acc: 0.3910\n",
      "Epoch 14/2000\n",
      " - 6s - loss: 1.7960 - acc: 0.9206 - val_loss: 2.0139 - val_acc: 0.4167\n",
      "Epoch 15/2000\n",
      " - 6s - loss: 1.7390 - acc: 0.8889 - val_loss: 1.9792 - val_acc: 0.5556\n",
      "Epoch 16/2000\n",
      " - 6s - loss: 1.7388 - acc: 0.8730 - val_loss: 1.9666 - val_acc: 0.4808\n",
      "Epoch 17/2000\n",
      " - 6s - loss: 1.6815 - acc: 0.8730 - val_loss: 1.9672 - val_acc: 0.4637\n",
      "Epoch 18/2000\n",
      " - 6s - loss: 1.6362 - acc: 0.9365 - val_loss: 1.9678 - val_acc: 0.4444\n",
      "Epoch 19/2000\n",
      " - 6s - loss: 1.5967 - acc: 0.9048 - val_loss: 1.9332 - val_acc: 0.5278\n",
      "Epoch 20/2000\n",
      " - 6s - loss: 1.5704 - acc: 0.9683 - val_loss: 1.9379 - val_acc: 0.4679\n",
      "Epoch 21/2000\n",
      " - 6s - loss: 1.5338 - acc: 0.9524 - val_loss: 1.8939 - val_acc: 0.5278\n",
      "Epoch 22/2000\n",
      " - 6s - loss: 1.4982 - acc: 0.9683 - val_loss: 1.8829 - val_acc: 0.5470\n",
      "Epoch 23/2000\n",
      " - 6s - loss: 1.4757 - acc: 0.9206 - val_loss: 1.8661 - val_acc: 0.5556\n",
      "Epoch 24/2000\n",
      " - 6s - loss: 1.4147 - acc: 0.9683 - val_loss: 1.8494 - val_acc: 0.5470\n",
      "Epoch 25/2000\n",
      " - 6s - loss: 1.3673 - acc: 0.9524 - val_loss: 1.8577 - val_acc: 0.5278\n",
      "Epoch 26/2000\n",
      " - 6s - loss: 1.3645 - acc: 0.9365 - val_loss: 1.8212 - val_acc: 0.5513\n",
      "Epoch 27/2000\n",
      " - 6s - loss: 1.3202 - acc: 0.9365 - val_loss: 1.8074 - val_acc: 0.5705\n",
      "Epoch 28/2000\n",
      " - 6s - loss: 1.2873 - acc: 0.9683 - val_loss: 1.7884 - val_acc: 0.5534\n",
      "Epoch 29/2000\n",
      " - 6s - loss: 1.2365 - acc: 0.9683 - val_loss: 1.7709 - val_acc: 0.5791\n",
      "Epoch 30/2000\n",
      " - 6s - loss: 1.1979 - acc: 0.9841 - val_loss: 1.7776 - val_acc: 0.5449\n",
      "Epoch 31/2000\n",
      " - 6s - loss: 1.1620 - acc: 1.0000 - val_loss: 1.7258 - val_acc: 0.6154\n",
      "Epoch 32/2000\n",
      " - 6s - loss: 1.0889 - acc: 1.0000 - val_loss: 1.6961 - val_acc: 0.6154\n",
      "Epoch 33/2000\n",
      " - 6s - loss: 1.0449 - acc: 0.9841 - val_loss: 1.7029 - val_acc: 0.6004\n",
      "Epoch 34/2000\n",
      " - 6s - loss: 1.0014 - acc: 0.9841 - val_loss: 1.7025 - val_acc: 0.5705\n",
      "Epoch 35/2000\n",
      " - 6s - loss: 0.9827 - acc: 0.9524 - val_loss: 1.6889 - val_acc: 0.5791\n",
      "Epoch 36/2000\n",
      " - 6s - loss: 0.9576 - acc: 0.9841 - val_loss: 1.6375 - val_acc: 0.6325\n",
      "Epoch 37/2000\n",
      " - 6s - loss: 0.9013 - acc: 1.0000 - val_loss: 1.6175 - val_acc: 0.6175\n",
      "Epoch 38/2000\n",
      " - 6s - loss: 0.8850 - acc: 0.9841 - val_loss: 1.6092 - val_acc: 0.6239\n",
      "Epoch 39/2000\n",
      " - 6s - loss: 0.8355 - acc: 1.0000 - val_loss: 1.5988 - val_acc: 0.6325\n",
      "Epoch 40/2000\n",
      " - 6s - loss: 0.8499 - acc: 1.0000 - val_loss: 1.5886 - val_acc: 0.6154\n",
      "Epoch 41/2000\n",
      " - 6s - loss: 0.7747 - acc: 1.0000 - val_loss: 1.5796 - val_acc: 0.6325\n",
      "Epoch 42/2000\n",
      " - 6s - loss: 0.7253 - acc: 1.0000 - val_loss: 1.5471 - val_acc: 0.6346\n",
      "Epoch 43/2000\n",
      " - 6s - loss: 0.7053 - acc: 1.0000 - val_loss: 1.5503 - val_acc: 0.6111\n",
      "Epoch 44/2000\n",
      " - 6s - loss: 0.6605 - acc: 1.0000 - val_loss: 1.5296 - val_acc: 0.6111\n",
      "Epoch 45/2000\n",
      " - 6s - loss: 0.6796 - acc: 1.0000 - val_loss: 1.5467 - val_acc: 0.5791\n",
      "Epoch 46/2000\n",
      " - 6s - loss: 0.6139 - acc: 1.0000 - val_loss: 1.5020 - val_acc: 0.6154\n",
      "Epoch 47/2000\n",
      " - 6s - loss: 0.6174 - acc: 1.0000 - val_loss: 1.5122 - val_acc: 0.6154\n",
      "Epoch 48/2000\n",
      " - 6s - loss: 0.5738 - acc: 1.0000 - val_loss: 1.4651 - val_acc: 0.6410\n",
      "Epoch 49/2000\n",
      " - 6s - loss: 0.5428 - acc: 1.0000 - val_loss: 1.4448 - val_acc: 0.6389\n",
      "Epoch 50/2000\n",
      " - 6s - loss: 0.5314 - acc: 1.0000 - val_loss: 1.4386 - val_acc: 0.6389\n",
      "Epoch 51/2000\n",
      " - 6s - loss: 0.5109 - acc: 1.0000 - val_loss: 1.4171 - val_acc: 0.6432\n",
      "Epoch 52/2000\n",
      " - 6s - loss: 0.4544 - acc: 0.9841 - val_loss: 1.4126 - val_acc: 0.6474\n",
      "Epoch 53/2000\n",
      " - 6s - loss: 0.4414 - acc: 1.0000 - val_loss: 1.4172 - val_acc: 0.6239\n",
      "Epoch 54/2000\n",
      " - 6s - loss: 0.4317 - acc: 1.0000 - val_loss: 1.4071 - val_acc: 0.6368\n",
      "Epoch 55/2000\n",
      " - 6s - loss: 0.3805 - acc: 1.0000 - val_loss: 1.3739 - val_acc: 0.6560\n",
      "Epoch 56/2000\n",
      " - 6s - loss: 0.3776 - acc: 1.0000 - val_loss: 1.3861 - val_acc: 0.6410\n",
      "Epoch 57/2000\n",
      " - 6s - loss: 0.3681 - acc: 1.0000 - val_loss: 1.3631 - val_acc: 0.6517\n",
      "Epoch 58/2000\n",
      " - 6s - loss: 0.3336 - acc: 1.0000 - val_loss: 1.3379 - val_acc: 0.6603\n",
      "Epoch 59/2000\n",
      " - 6s - loss: 0.3024 - acc: 1.0000 - val_loss: 1.3540 - val_acc: 0.6432\n",
      "Epoch 60/2000\n",
      " - 6s - loss: 0.3031 - acc: 1.0000 - val_loss: 1.3213 - val_acc: 0.6517\n",
      "Epoch 61/2000\n",
      " - 6s - loss: 0.3142 - acc: 1.0000 - val_loss: 1.2921 - val_acc: 0.6496\n",
      "Epoch 62/2000\n",
      " - 6s - loss: 0.2839 - acc: 1.0000 - val_loss: 1.2818 - val_acc: 0.6517\n",
      "Epoch 63/2000\n",
      " - 6s - loss: 0.2530 - acc: 1.0000 - val_loss: 1.3130 - val_acc: 0.6346\n",
      "Epoch 64/2000\n",
      " - 6s - loss: 0.2632 - acc: 1.0000 - val_loss: 1.2794 - val_acc: 0.6624\n",
      "Epoch 65/2000\n",
      " - 6s - loss: 0.2403 - acc: 1.0000 - val_loss: 1.2833 - val_acc: 0.6432\n",
      "Epoch 66/2000\n",
      " - 6s - loss: 0.2274 - acc: 1.0000 - val_loss: 1.2529 - val_acc: 0.6624\n",
      "Epoch 67/2000\n",
      " - 6s - loss: 0.2223 - acc: 1.0000 - val_loss: 1.2450 - val_acc: 0.6581\n",
      "Epoch 68/2000\n",
      " - 6s - loss: 0.1984 - acc: 1.0000 - val_loss: 1.2681 - val_acc: 0.6432\n",
      "Epoch 69/2000\n",
      " - 6s - loss: 0.2142 - acc: 1.0000 - val_loss: 1.2224 - val_acc: 0.6688\n",
      "Epoch 70/2000\n",
      " - 6s - loss: 0.1919 - acc: 1.0000 - val_loss: 1.2480 - val_acc: 0.6517\n",
      "Epoch 71/2000\n",
      " - 6s - loss: 0.1803 - acc: 1.0000 - val_loss: 1.2052 - val_acc: 0.6645\n",
      "Epoch 72/2000\n",
      " - 6s - loss: 0.1987 - acc: 1.0000 - val_loss: 1.2236 - val_acc: 0.6560\n",
      "Epoch 73/2000\n",
      " - 6s - loss: 0.1574 - acc: 1.0000 - val_loss: 1.2179 - val_acc: 0.6538\n",
      "Epoch 74/2000\n",
      " - 6s - loss: 0.1640 - acc: 1.0000 - val_loss: 1.1879 - val_acc: 0.6624\n",
      "Epoch 75/2000\n",
      " - 6s - loss: 0.1446 - acc: 1.0000 - val_loss: 1.1907 - val_acc: 0.6667\n",
      "Epoch 76/2000\n",
      " - 6s - loss: 0.1290 - acc: 1.0000 - val_loss: 1.2003 - val_acc: 0.6560\n",
      "Epoch 77/2000\n",
      " - 6s - loss: 0.1298 - acc: 1.0000 - val_loss: 1.1844 - val_acc: 0.6560\n",
      "Epoch 78/2000\n",
      " - 6s - loss: 0.1296 - acc: 1.0000 - val_loss: 1.1656 - val_acc: 0.6688\n",
      "Epoch 79/2000\n",
      " - 6s - loss: 0.1146 - acc: 1.0000 - val_loss: 1.1621 - val_acc: 0.6709\n",
      "Epoch 80/2000\n",
      " - 6s - loss: 0.1294 - acc: 1.0000 - val_loss: 1.1591 - val_acc: 0.6709\n",
      "Epoch 81/2000\n",
      " - 6s - loss: 0.1007 - acc: 1.0000 - val_loss: 1.1428 - val_acc: 0.6688\n",
      "Epoch 82/2000\n",
      " - 6s - loss: 0.1061 - acc: 1.0000 - val_loss: 1.1574 - val_acc: 0.6603\n",
      "Epoch 83/2000\n",
      " - 6s - loss: 0.0931 - acc: 1.0000 - val_loss: 1.1274 - val_acc: 0.6688\n",
      "Epoch 84/2000\n",
      " - 6s - loss: 0.1029 - acc: 1.0000 - val_loss: 1.1270 - val_acc: 0.6709\n",
      "Epoch 85/2000\n",
      " - 6s - loss: 0.0979 - acc: 1.0000 - val_loss: 1.1512 - val_acc: 0.6496\n",
      "Epoch 86/2000\n",
      " - 6s - loss: 0.0819 - acc: 1.0000 - val_loss: 1.1493 - val_acc: 0.6453\n",
      "Epoch 87/2000\n",
      " - 6s - loss: 0.0846 - acc: 1.0000 - val_loss: 1.1228 - val_acc: 0.6645\n",
      "Epoch 88/2000\n",
      " - 6s - loss: 0.0758 - acc: 1.0000 - val_loss: 1.1247 - val_acc: 0.6581\n",
      "Epoch 89/2000\n",
      " - 6s - loss: 0.0732 - acc: 1.0000 - val_loss: 1.1094 - val_acc: 0.6731\n",
      "Epoch 90/2000\n",
      " - 6s - loss: 0.0705 - acc: 1.0000 - val_loss: 1.1069 - val_acc: 0.6731\n",
      "Epoch 91/2000\n",
      " - 6s - loss: 0.0665 - acc: 1.0000 - val_loss: 1.0833 - val_acc: 0.6816\n",
      "Epoch 92/2000\n",
      " - 6s - loss: 0.0725 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.6859\n",
      "Epoch 93/2000\n",
      " - 6s - loss: 0.0591 - acc: 1.0000 - val_loss: 1.0676 - val_acc: 0.6795\n",
      "Epoch 94/2000\n",
      " - 6s - loss: 0.0574 - acc: 1.0000 - val_loss: 1.0793 - val_acc: 0.6731\n",
      "Epoch 95/2000\n",
      " - 6s - loss: 0.0595 - acc: 1.0000 - val_loss: 1.0915 - val_acc: 0.6752\n",
      "Epoch 96/2000\n",
      " - 6s - loss: 0.0590 - acc: 1.0000 - val_loss: 1.0946 - val_acc: 0.6645\n",
      "Epoch 97/2000\n",
      " - 6s - loss: 0.0621 - acc: 1.0000 - val_loss: 1.1002 - val_acc: 0.6560\n",
      "Epoch 98/2000\n",
      " - 6s - loss: 0.0501 - acc: 1.0000 - val_loss: 1.0711 - val_acc: 0.6752\n",
      "Epoch 99/2000\n",
      " - 6s - loss: 0.0449 - acc: 1.0000 - val_loss: 1.0693 - val_acc: 0.6688\n",
      "Epoch 100/2000\n",
      " - 6s - loss: 0.0387 - acc: 1.0000 - val_loss: 1.0675 - val_acc: 0.6688\n",
      "Epoch 101/2000\n",
      " - 6s - loss: 0.0443 - acc: 1.0000 - val_loss: 1.0448 - val_acc: 0.6795\n",
      "Epoch 102/2000\n",
      " - 6s - loss: 0.0446 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.6688\n",
      "Epoch 103/2000\n",
      " - 6s - loss: 0.0477 - acc: 1.0000 - val_loss: 1.0334 - val_acc: 0.6838\n",
      "Epoch 104/2000\n",
      " - 6s - loss: 0.0420 - acc: 1.0000 - val_loss: 1.0410 - val_acc: 0.6709\n",
      "Epoch 105/2000\n",
      " - 6s - loss: 0.0395 - acc: 1.0000 - val_loss: 1.0415 - val_acc: 0.6774\n",
      "Epoch 106/2000\n",
      " - 6s - loss: 0.0420 - acc: 1.0000 - val_loss: 1.0356 - val_acc: 0.6838\n",
      "Epoch 107/2000\n",
      " - 6s - loss: 0.0360 - acc: 1.0000 - val_loss: 1.0236 - val_acc: 0.6774\n",
      "Epoch 108/2000\n",
      " - 6s - loss: 0.0369 - acc: 1.0000 - val_loss: 1.0139 - val_acc: 0.6923\n",
      "Epoch 109/2000\n",
      " - 6s - loss: 0.0336 - acc: 1.0000 - val_loss: 1.0217 - val_acc: 0.6838\n",
      "Epoch 110/2000\n",
      " - 6s - loss: 0.0290 - acc: 1.0000 - val_loss: 1.0002 - val_acc: 0.6902\n",
      "Epoch 111/2000\n",
      " - 6s - loss: 0.0258 - acc: 1.0000 - val_loss: 0.9997 - val_acc: 0.6923\n",
      "Epoch 112/2000\n",
      " - 6s - loss: 0.0282 - acc: 1.0000 - val_loss: 0.9815 - val_acc: 0.6902\n",
      "Epoch 113/2000\n",
      " - 6s - loss: 0.0340 - acc: 1.0000 - val_loss: 1.0224 - val_acc: 0.6774\n",
      "Epoch 114/2000\n",
      " - 6s - loss: 0.0258 - acc: 1.0000 - val_loss: 1.0011 - val_acc: 0.6838\n",
      "Epoch 115/2000\n",
      " - 6s - loss: 0.0240 - acc: 1.0000 - val_loss: 1.0043 - val_acc: 0.6880\n",
      "Epoch 116/2000\n",
      " - 6s - loss: 0.0245 - acc: 1.0000 - val_loss: 1.0079 - val_acc: 0.6795\n",
      "Epoch 117/2000\n",
      " - 6s - loss: 0.0233 - acc: 1.0000 - val_loss: 1.0064 - val_acc: 0.6774\n",
      "Epoch 118/2000\n",
      " - 6s - loss: 0.0225 - acc: 1.0000 - val_loss: 0.9907 - val_acc: 0.6880\n",
      "Epoch 119/2000\n",
      " - 6s - loss: 0.0201 - acc: 1.0000 - val_loss: 0.9892 - val_acc: 0.6816\n",
      "Epoch 120/2000\n",
      " - 6s - loss: 0.0185 - acc: 1.0000 - val_loss: 1.0029 - val_acc: 0.6731\n",
      "Epoch 121/2000\n",
      " - 6s - loss: 0.0168 - acc: 1.0000 - val_loss: 0.9850 - val_acc: 0.6859\n",
      "Epoch 122/2000\n",
      " - 6s - loss: 0.0221 - acc: 1.0000 - val_loss: 0.9648 - val_acc: 0.6902\n",
      "Epoch 123/2000\n",
      " - 6s - loss: 0.0178 - acc: 1.0000 - val_loss: 0.9658 - val_acc: 0.6859\n",
      "Epoch 124/2000\n",
      " - 6s - loss: 0.0164 - acc: 1.0000 - val_loss: 0.9728 - val_acc: 0.6902\n",
      "Epoch 125/2000\n",
      " - 6s - loss: 0.0159 - acc: 1.0000 - val_loss: 0.9779 - val_acc: 0.6923\n",
      "Epoch 126/2000\n",
      " - 6s - loss: 0.0153 - acc: 1.0000 - val_loss: 0.9792 - val_acc: 0.6838\n",
      "Epoch 127/2000\n",
      " - 6s - loss: 0.0156 - acc: 1.0000 - val_loss: 0.9782 - val_acc: 0.6902\n",
      "Epoch 128/2000\n",
      " - 6s - loss: 0.0140 - acc: 1.0000 - val_loss: 0.9611 - val_acc: 0.6944\n",
      "Epoch 129/2000\n",
      " - 6s - loss: 0.0127 - acc: 1.0000 - val_loss: 0.9652 - val_acc: 0.6816\n",
      "Epoch 130/2000\n",
      " - 6s - loss: 0.0145 - acc: 1.0000 - val_loss: 0.9776 - val_acc: 0.6859\n",
      "Epoch 131/2000\n",
      " - 6s - loss: 0.0115 - acc: 1.0000 - val_loss: 0.9813 - val_acc: 0.6902\n",
      "Epoch 132/2000\n",
      " - 6s - loss: 0.0138 - acc: 1.0000 - val_loss: 0.9775 - val_acc: 0.6859\n",
      "Epoch 133/2000\n",
      " - 6s - loss: 0.0138 - acc: 1.0000 - val_loss: 0.9780 - val_acc: 0.6902\n",
      "Epoch 134/2000\n",
      " - 6s - loss: 0.0102 - acc: 1.0000 - val_loss: 0.9858 - val_acc: 0.6880\n",
      "Epoch 135/2000\n",
      " - 6s - loss: 0.0125 - acc: 1.0000 - val_loss: 0.9778 - val_acc: 0.6923\n",
      "Epoch 136/2000\n",
      " - 6s - loss: 0.0108 - acc: 1.0000 - val_loss: 0.9654 - val_acc: 0.6902\n",
      "Epoch 137/2000\n",
      " - 6s - loss: 0.0124 - acc: 1.0000 - val_loss: 0.9672 - val_acc: 0.6923\n",
      "Epoch 138/2000\n",
      " - 6s - loss: 0.0100 - acc: 1.0000 - val_loss: 0.9758 - val_acc: 0.6838\n",
      "Epoch 139/2000\n",
      " - 6s - loss: 0.0100 - acc: 1.0000 - val_loss: 0.9537 - val_acc: 0.6944\n",
      "Epoch 140/2000\n",
      " - 6s - loss: 0.0089 - acc: 1.0000 - val_loss: 0.9651 - val_acc: 0.6966\n",
      "Epoch 141/2000\n",
      " - 6s - loss: 0.0090 - acc: 1.0000 - val_loss: 0.9523 - val_acc: 0.6966\n",
      "Epoch 142/2000\n",
      " - 6s - loss: 0.0079 - acc: 1.0000 - val_loss: 0.9561 - val_acc: 0.6923\n",
      "Epoch 143/2000\n",
      " - 6s - loss: 0.0074 - acc: 1.0000 - val_loss: 0.9546 - val_acc: 0.6902\n",
      "Epoch 144/2000\n",
      " - 6s - loss: 0.0076 - acc: 1.0000 - val_loss: 0.9778 - val_acc: 0.6774\n",
      "Epoch 145/2000\n",
      " - 6s - loss: 0.0078 - acc: 1.0000 - val_loss: 0.9849 - val_acc: 0.6795\n",
      "Epoch 146/2000\n",
      " - 6s - loss: 0.0080 - acc: 1.0000 - val_loss: 0.9485 - val_acc: 0.6880\n",
      "Epoch 147/2000\n",
      " - 6s - loss: 0.0084 - acc: 1.0000 - val_loss: 0.9359 - val_acc: 0.7051\n",
      "Epoch 148/2000\n",
      " - 6s - loss: 0.0063 - acc: 1.0000 - val_loss: 0.9330 - val_acc: 0.7030\n",
      "Epoch 149/2000\n",
      " - 6s - loss: 0.0076 - acc: 1.0000 - val_loss: 0.9322 - val_acc: 0.7009\n",
      "Epoch 150/2000\n",
      " - 6s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.9272 - val_acc: 0.7009\n",
      "Epoch 151/2000\n",
      " - 6s - loss: 0.0069 - acc: 1.0000 - val_loss: 0.9374 - val_acc: 0.7009\n",
      "Epoch 152/2000\n",
      " - 6s - loss: 0.0072 - acc: 1.0000 - val_loss: 0.9558 - val_acc: 0.6944\n",
      "Epoch 153/2000\n",
      " - 6s - loss: 0.0056 - acc: 1.0000 - val_loss: 0.9518 - val_acc: 0.6987\n",
      "Epoch 154/2000\n",
      " - 6s - loss: 0.0069 - acc: 1.0000 - val_loss: 0.9310 - val_acc: 0.6923\n",
      "Epoch 155/2000\n",
      " - 6s - loss: 0.0060 - acc: 1.0000 - val_loss: 0.9379 - val_acc: 0.7030\n",
      "Epoch 156/2000\n",
      " - 6s - loss: 0.0061 - acc: 1.0000 - val_loss: 0.9388 - val_acc: 0.6987\n",
      "Epoch 157/2000\n",
      " - 6s - loss: 0.0050 - acc: 1.0000 - val_loss: 0.9374 - val_acc: 0.6944\n",
      "Epoch 158/2000\n",
      " - 6s - loss: 0.0055 - acc: 1.0000 - val_loss: 0.9245 - val_acc: 0.6966\n",
      "Epoch 159/2000\n",
      " - 6s - loss: 0.0047 - acc: 1.0000 - val_loss: 0.9211 - val_acc: 0.6987\n",
      "Epoch 160/2000\n",
      " - 6s - loss: 0.0044 - acc: 1.0000 - val_loss: 0.9333 - val_acc: 0.6966\n",
      "Epoch 161/2000\n",
      " - 6s - loss: 0.0043 - acc: 1.0000 - val_loss: 0.9268 - val_acc: 0.7009\n",
      "Epoch 162/2000\n",
      " - 6s - loss: 0.0038 - acc: 1.0000 - val_loss: 0.9137 - val_acc: 0.7009\n",
      "Epoch 163/2000\n",
      " - 6s - loss: 0.0044 - acc: 1.0000 - val_loss: 0.9197 - val_acc: 0.7073\n",
      "Epoch 164/2000\n",
      " - 6s - loss: 0.0043 - acc: 1.0000 - val_loss: 0.9135 - val_acc: 0.7094\n",
      "Epoch 165/2000\n",
      " - 6s - loss: 0.0050 - acc: 1.0000 - val_loss: 0.9249 - val_acc: 0.7030\n",
      "Epoch 166/2000\n",
      " - 6s - loss: 0.0040 - acc: 1.0000 - val_loss: 0.9177 - val_acc: 0.7051\n",
      "Epoch 167/2000\n",
      " - 6s - loss: 0.0038 - acc: 1.0000 - val_loss: 0.9147 - val_acc: 0.7073\n",
      "Epoch 168/2000\n",
      " - 6s - loss: 0.0038 - acc: 1.0000 - val_loss: 0.9293 - val_acc: 0.7073\n",
      "Epoch 169/2000\n",
      " - 6s - loss: 0.0030 - acc: 1.0000 - val_loss: 0.9256 - val_acc: 0.7051\n",
      "Epoch 170/2000\n",
      " - 6s - loss: 0.0041 - acc: 1.0000 - val_loss: 0.9380 - val_acc: 0.6880\n",
      "Epoch 171/2000\n",
      " - 6s - loss: 0.0033 - acc: 1.0000 - val_loss: 0.9402 - val_acc: 0.6923\n",
      "Epoch 172/2000\n",
      " - 6s - loss: 0.0026 - acc: 1.0000 - val_loss: 0.9316 - val_acc: 0.6923\n",
      "Epoch 173/2000\n",
      " - 6s - loss: 0.0028 - acc: 1.0000 - val_loss: 0.9216 - val_acc: 0.6987\n",
      "Epoch 174/2000\n",
      " - 6s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.9416 - val_acc: 0.6987\n",
      "Epoch 175/2000\n",
      " - 6s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.9386 - val_acc: 0.6923\n",
      "Epoch 176/2000\n",
      " - 6s - loss: 0.0029 - acc: 1.0000 - val_loss: 0.9022 - val_acc: 0.7009\n",
      "Epoch 177/2000\n",
      " - 6s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.9224 - val_acc: 0.6987\n",
      "Epoch 178/2000\n",
      " - 6s - loss: 0.0039 - acc: 1.0000 - val_loss: 0.9387 - val_acc: 0.6923\n",
      "Epoch 179/2000\n",
      " - 6s - loss: 0.0028 - acc: 1.0000 - val_loss: 0.9515 - val_acc: 0.6923\n",
      "Epoch 180/2000\n",
      " - 6s - loss: 0.0024 - acc: 1.0000 - val_loss: 0.9393 - val_acc: 0.6923\n",
      "Epoch 181/2000\n",
      " - 6s - loss: 0.0019 - acc: 1.0000 - val_loss: 0.9302 - val_acc: 0.6880\n",
      "Epoch 182/2000\n",
      " - 6s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.9236 - val_acc: 0.6902\n",
      "Epoch 183/2000\n",
      " - 6s - loss: 0.0020 - acc: 1.0000 - val_loss: 0.9316 - val_acc: 0.6944\n",
      "Epoch 184/2000\n",
      " - 6s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.9194 - val_acc: 0.6944\n",
      "Epoch 185/2000\n",
      " - 6s - loss: 0.0021 - acc: 1.0000 - val_loss: 0.9081 - val_acc: 0.7030\n",
      "Epoch 186/2000\n",
      " - 6s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.9079 - val_acc: 0.7009\n",
      "Epoch 187/2000\n",
      " - 6s - loss: 0.0019 - acc: 1.0000 - val_loss: 0.9017 - val_acc: 0.7009\n",
      "Epoch 188/2000\n",
      " - 6s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.9154 - val_acc: 0.6923\n",
      "Epoch 189/2000\n",
      " - 6s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.9250 - val_acc: 0.6966\n",
      "Epoch 190/2000\n",
      " - 6s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.9251 - val_acc: 0.6987\n",
      "Epoch 191/2000\n",
      " - 6s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.9220 - val_acc: 0.7030\n",
      "Epoch 192/2000\n",
      " - 6s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.9129 - val_acc: 0.7030\n",
      "Epoch 193/2000\n",
      " - 6s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.9126 - val_acc: 0.7030\n",
      "Epoch 194/2000\n",
      " - 6s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.9240 - val_acc: 0.6923\n",
      "Epoch 195/2000\n",
      " - 6s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.9152 - val_acc: 0.7009\n",
      "Epoch 196/2000\n",
      " - 6s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.9229 - val_acc: 0.7009\n",
      "Epoch 197/2000\n",
      " - 6s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.9117 - val_acc: 0.7051\n",
      "Epoch 198/2000\n",
      " - 6s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.8984 - val_acc: 0.7137\n",
      "Epoch 199/2000\n",
      " - 6s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.9164 - val_acc: 0.7094\n",
      "Epoch 200/2000\n",
      " - 6s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.9200 - val_acc: 0.6987\n",
      "Epoch 201/2000\n",
      " - 6s - loss: 0.0019 - acc: 1.0000 - val_loss: 0.9415 - val_acc: 0.6966\n",
      "Epoch 202/2000\n",
      " - 6s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.9338 - val_acc: 0.6966\n",
      "Epoch 203/2000\n",
      " - 6s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.9088 - val_acc: 0.7030\n",
      "Epoch 204/2000\n",
      " - 6s - loss: 9.1017e-04 - acc: 1.0000 - val_loss: 0.9079 - val_acc: 0.7009\n",
      "Epoch 205/2000\n",
      " - 6s - loss: 7.6896e-04 - acc: 1.0000 - val_loss: 0.9023 - val_acc: 0.7051\n",
      "Epoch 206/2000\n",
      " - 6s - loss: 9.0620e-04 - acc: 1.0000 - val_loss: 0.9020 - val_acc: 0.7094\n",
      "Epoch 207/2000\n",
      " - 6s - loss: 8.0993e-04 - acc: 1.0000 - val_loss: 0.8998 - val_acc: 0.7094\n",
      "Epoch 208/2000\n",
      " - 6s - loss: 7.9132e-04 - acc: 1.0000 - val_loss: 0.9081 - val_acc: 0.7051\n",
      "Epoch 209/2000\n",
      " - 6s - loss: 8.1058e-04 - acc: 1.0000 - val_loss: 0.9041 - val_acc: 0.7051\n",
      "Epoch 210/2000\n",
      " - 6s - loss: 9.4997e-04 - acc: 1.0000 - val_loss: 0.9080 - val_acc: 0.7051\n",
      "Epoch 211/2000\n",
      " - 6s - loss: 9.6516e-04 - acc: 1.0000 - val_loss: 0.9113 - val_acc: 0.6987\n",
      "Epoch 212/2000\n",
      " - 6s - loss: 8.6966e-04 - acc: 1.0000 - val_loss: 0.9174 - val_acc: 0.7094\n",
      "Epoch 213/2000\n",
      " - 6s - loss: 6.3080e-04 - acc: 1.0000 - val_loss: 0.9152 - val_acc: 0.7051\n",
      "Epoch 214/2000\n",
      " - 6s - loss: 6.6598e-04 - acc: 1.0000 - val_loss: 0.8965 - val_acc: 0.7137\n",
      "Epoch 215/2000\n",
      " - 6s - loss: 6.7351e-04 - acc: 1.0000 - val_loss: 0.9089 - val_acc: 0.7073\n",
      "Epoch 216/2000\n",
      " - 6s - loss: 6.4013e-04 - acc: 1.0000 - val_loss: 0.9144 - val_acc: 0.7073\n",
      "Epoch 217/2000\n",
      " - 6s - loss: 4.2710e-04 - acc: 1.0000 - val_loss: 0.9164 - val_acc: 0.7030\n",
      "Epoch 218/2000\n",
      " - 6s - loss: 6.6944e-04 - acc: 1.0000 - val_loss: 0.9110 - val_acc: 0.7094\n",
      "Epoch 219/2000\n",
      " - 6s - loss: 6.1483e-04 - acc: 1.0000 - val_loss: 0.9237 - val_acc: 0.6987\n",
      "Epoch 220/2000\n",
      " - 6s - loss: 3.9071e-04 - acc: 1.0000 - val_loss: 0.9181 - val_acc: 0.6987\n",
      "Epoch 221/2000\n",
      " - 6s - loss: 5.1268e-04 - acc: 1.0000 - val_loss: 0.9259 - val_acc: 0.7009\n",
      "Epoch 222/2000\n",
      " - 6s - loss: 5.4005e-04 - acc: 1.0000 - val_loss: 0.9368 - val_acc: 0.7009\n",
      "Epoch 223/2000\n",
      " - 6s - loss: 5.2575e-04 - acc: 1.0000 - val_loss: 0.9206 - val_acc: 0.7009\n",
      "Epoch 224/2000\n",
      " - 6s - loss: 4.9019e-04 - acc: 1.0000 - val_loss: 0.9187 - val_acc: 0.7009\n",
      "Epoch 225/2000\n",
      " - 6s - loss: 4.6339e-04 - acc: 1.0000 - val_loss: 0.9102 - val_acc: 0.7009\n",
      "Epoch 226/2000\n",
      " - 6s - loss: 5.3189e-04 - acc: 1.0000 - val_loss: 0.9099 - val_acc: 0.7094\n",
      "Epoch 227/2000\n",
      " - 6s - loss: 4.4237e-04 - acc: 1.0000 - val_loss: 0.9128 - val_acc: 0.7073\n",
      "Epoch 228/2000\n",
      " - 6s - loss: 4.9187e-04 - acc: 1.0000 - val_loss: 0.9058 - val_acc: 0.7115\n",
      "Epoch 229/2000\n",
      " - 6s - loss: 3.4127e-04 - acc: 1.0000 - val_loss: 0.9208 - val_acc: 0.7094\n",
      "Epoch 230/2000\n",
      " - 6s - loss: 7.3036e-04 - acc: 1.0000 - val_loss: 0.9559 - val_acc: 0.7051\n",
      "Epoch 231/2000\n",
      " - 6s - loss: 4.7057e-04 - acc: 1.0000 - val_loss: 0.9506 - val_acc: 0.6944\n",
      "Epoch 232/2000\n",
      " - 6s - loss: 4.6728e-04 - acc: 1.0000 - val_loss: 0.9412 - val_acc: 0.6987\n",
      "Epoch 233/2000\n",
      " - 6s - loss: 4.7666e-04 - acc: 1.0000 - val_loss: 0.9294 - val_acc: 0.7030\n",
      "Epoch 234/2000\n",
      " - 6s - loss: 3.8640e-04 - acc: 1.0000 - val_loss: 0.9601 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00234: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 235/2000\n",
      " - 6s - loss: 3.2772e-04 - acc: 1.0000 - val_loss: 0.9518 - val_acc: 0.6944\n",
      "Epoch 236/2000\n",
      " - 6s - loss: 3.8809e-04 - acc: 1.0000 - val_loss: 0.9445 - val_acc: 0.6966\n",
      "Epoch 237/2000\n",
      " - 6s - loss: 3.2558e-04 - acc: 1.0000 - val_loss: 0.9392 - val_acc: 0.6987\n",
      "Epoch 238/2000\n",
      " - 6s - loss: 3.4285e-04 - acc: 1.0000 - val_loss: 0.9384 - val_acc: 0.6966\n",
      "Epoch 239/2000\n",
      " - 6s - loss: 2.7965e-04 - acc: 1.0000 - val_loss: 0.9352 - val_acc: 0.6987\n",
      "Epoch 240/2000\n",
      " - 6s - loss: 2.7694e-04 - acc: 1.0000 - val_loss: 0.9295 - val_acc: 0.6987\n",
      "Epoch 241/2000\n",
      " - 6s - loss: 6.7584e-04 - acc: 1.0000 - val_loss: 0.9682 - val_acc: 0.6902\n",
      "Epoch 242/2000\n",
      " - 6s - loss: 2.9183e-04 - acc: 1.0000 - val_loss: 0.9658 - val_acc: 0.6902\n",
      "Epoch 243/2000\n",
      " - 6s - loss: 3.2122e-04 - acc: 1.0000 - val_loss: 0.9580 - val_acc: 0.6902\n",
      "Epoch 244/2000\n",
      " - 6s - loss: 3.9586e-04 - acc: 1.0000 - val_loss: 0.9401 - val_acc: 0.6923\n",
      "Epoch 245/2000\n",
      " - 6s - loss: 2.3898e-04 - acc: 1.0000 - val_loss: 0.9435 - val_acc: 0.6944\n",
      "Epoch 246/2000\n",
      " - 6s - loss: 2.6869e-04 - acc: 1.0000 - val_loss: 0.9392 - val_acc: 0.6966\n",
      "Epoch 247/2000\n",
      " - 6s - loss: 2.6526e-04 - acc: 1.0000 - val_loss: 0.9354 - val_acc: 0.7009\n",
      "Epoch 248/2000\n",
      " - 6s - loss: 2.8482e-04 - acc: 1.0000 - val_loss: 0.9388 - val_acc: 0.6966\n",
      "Epoch 249/2000\n",
      " - 6s - loss: 3.2101e-04 - acc: 1.0000 - val_loss: 0.9299 - val_acc: 0.7051\n",
      "Epoch 250/2000\n",
      " - 6s - loss: 3.0868e-04 - acc: 1.0000 - val_loss: 0.9102 - val_acc: 0.7137\n",
      "Epoch 251/2000\n",
      " - 6s - loss: 1.6442e-04 - acc: 1.0000 - val_loss: 0.9088 - val_acc: 0.7137\n",
      "Epoch 252/2000\n",
      " - 6s - loss: 2.0253e-04 - acc: 1.0000 - val_loss: 0.9107 - val_acc: 0.7115\n",
      "Epoch 253/2000\n",
      " - 6s - loss: 3.6802e-04 - acc: 1.0000 - val_loss: 0.9236 - val_acc: 0.7009\n",
      "Epoch 254/2000\n",
      " - 6s - loss: 2.3720e-04 - acc: 1.0000 - val_loss: 0.9240 - val_acc: 0.7030\n",
      "\n",
      "Epoch 00254: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xl8U2UW8PHfaS2UHSwoCEhBHZSdWhEHFFzGAUbcF7Aoog7iNqijrwzuKO+4IuCC4sIooOirog6CuOEAOoMWZBURla2ACFXWqlB63j+ehKYlN03bpGmS8/188mmSe3Pvc5v25Mm5zz2PqCrGGGMSS0qsG2CMMSbyLLgbY0wCsuBujDEJyIK7McYkIAvuxhiTgCy4G2NMArLgboISkVQR2S0iR0Zy3VgSkaNFJOJjf0XkDBFZG/B4lYicHM66FdjX8yIysqKvD7HdB0TkX5HeromdQ2LdABMZIrI74GFt4Hdgv+/xNao6tTzbU9X9QN1Ir5sMVLVtJLYjIlcDg1S1d8C2r47Etk3is+CeIFT1QHD19QyvVtWPvNYXkUNUtbAq2maMqXqWlkkSvq/dr4nIqyKyCxgkIieJyP9EZLuIbBaR8SKS5lv/EBFREcn0PZ7iWz5LRHaJyH9FpHV51/Ut7ysi34rIDhF5QkQ+E5ErPNodThuvEZHvROQXERkf8NpUEXlcRPJF5HugT4jfz50iMq3Uc0+JyBjf/atFZKXveL739aq9tpUnIr1992uLyGRf21YAxwfZ7w++7a4QkbN9z3cEngRO9qW8tgX8bu8NeP0w37Hni8jbItIsnN9NWUTkXF97tovIJyLSNmDZSBHZJCI7ReSbgGPtLiKLfM9vEZFHwt2fiQJVtVuC3YC1wBmlnnsA2Av0x32o1wJOAE7EfYNrA3wL3OBb/xBAgUzf4ynANiAbSANeA6ZUYN3DgF3AOb5ltwD7gCs8jiWcNr4DNAAygZ/9xw7cAKwAWgAZwFz3Jx90P22A3UCdgG3/BGT7Hvf3rSPAacCvQCffsjOAtQHbygN6++4/CnwKNAJaAV+XWvdioJnvPbnU14bDfcuuBj4t1c4pwL2++2f62tgFSAeeBj4J53cT5PgfAP7lu3+crx2n+d6jkb7fexrQHlgHNPWt2xpo47v/JTDQd78ecGKs/xeS+WY99+QyX1X/rapFqvqrqn6pqgtUtVBVfwAmAr1CvP4NVc1V1X3AVFxQKe+6ZwGLVfUd37LHcR8EQYXZxn+q6g5VXYsLpP59XQw8rqp5qpoPPBhiPz8Ay3EfOgB/Araraq5v+b9V9Qd1PgE+BoKeNC3lYuABVf1FVdfheuOB+31dVTf73pNXcB/M2WFsFyAHeF5VF6vqb8AIoJeItAhYx+t3E8oA4F1V/cT3Hj0I1Md9yBbiPkja+1J7a3y/O3Af0seISIaq7lLVBWEeh4kCC+7JZUPgAxE5VkTeE5EfRWQnMApoHOL1PwbcLyD0SVSvdY8IbIeqKq6nG1SYbQxrX7geZyivAAN99y/FfSj523GWiCwQkZ9FZDuu1xzqd+XXLFQbROQKEVniS39sB44Nc7vgju/A9lR1J/AL0DxgnfK8Z17bLcK9R81VdRXwd9z78JMvzdfUt+oQoB2wSkS+EJF+YR6HiQIL7sml9DDAZ3G91aNVtT5wNy7tEE2bcWkSAEREKBmMSqtMGzcDLQMelzVU8zXgDF/P9xxcsEdEagFvAP/EpUwaAh+E2Y4fvdogIm2ACcC1QIZvu98EbLesYZubcKke//bq4dI/G8NoV3m2m4J7zzYCqOoUVe2BS8mk4n4vqOoqVR2AS709BrwpIumVbIupIAvuya0esAPYIyLHAddUwT5nAFki0l9EDgGGA02i1MbXgZtEpLmIZAC3h1pZVbcA84FJwCpVXe1bVBOoAWwF9ovIWcDp5WjDSBFpKO46gBsCltXFBfCtuM+5q3E9d78tQAv/CeQgXgWuEpFOIlITF2TnqarnN6FytPlsEent2/dtuPMkC0TkOBE51be/X323/bgDuExEGvt6+jt8x1ZUybaYCrLgntz+DgzG/eM+i+u5RpUvgF4CjAHygaOAr3Dj8iPdxgm43Pgy3Mm+N8J4zSu4E6SvBLR5O3AzMB13UvJC3IdUOO7BfYNYC8wCXg7Y7lJgPPCFb51jgcA89YfAamCLiASmV/yvfx+XHpnue/2RuDx8pajqCtzvfALug6cPcLYv/14TeBh3nuRH3DeFO30v7QesFDca61HgElXdW9n2mIoRl/I0JjZEJBWXBrhQVefFuj3GJArruZsqJyJ9RKSB76v9XbgRGF/EuFnGJBQL7iYWegI/4L7a9wHOVVWvtIwxpgIsLWOMMQnIeu7GGJOAYlY4rHHjxpqZmRmr3RtjTFxauHDhNlUNNXwYiGFwz8zMJDc3N1a7N8aYuCQiZV1pDVhaxhhjEpIFd2OMSUAW3I0xJgHZTEzGJIl9+/aRl5fHb7/9FuummDCkp6fTokUL0tK8SguFZsHdmCSRl5dHvXr1yMzMxBXjNNWVqpKfn09eXh6tW7cu+wVBxFVaZupUyMyElBT3c2q5pnw2Jrn99ttvZGRkWGCPAyJCRkZGpb5lxU3PfepUGDoUCgrc43Xr3GOAnErXwTMmOVhgjx+Vfa/ipud+xx3Fgd2voMA9b4wxpqS4Ce7r15fveWNM9ZKfn0+XLl3o0qULTZs2pXnz5gce790bXtn3IUOGsGrVqpDrPPXUU0yNUM62Z8+eLF68OCLbqmpxk5Y58kiXign2vDEm8qZOdd+M1693/2ejR1cuBZqRkXEgUN57773UrVuXW2+9tcQ6qoqqkpISvN85adKkMvdz/fXXV7yRCSRueu6jR0Pt2iWfq13bPW+MiSz/Oa5160C1+BxXNAYxfPfdd3To0IFhw4aRlZXF5s2bGTp0KNnZ2bRv355Ro0YdWNffky4sLKRhw4aMGDGCzp07c9JJJ/HTTz8BcOeddzJ27NgD648YMYJu3brRtm1bPv/8cwD27NnDBRdcQOfOnRk4cCDZ2dll9tCnTJlCx44d6dChAyNHjgSgsLCQyy677MDz48ePB+Dxxx+nXbt2dO7cmUGDBkX8dxaOuAnuOTkwcSK0agUi7ufEiXYy1ZhoqOpzXF9//TVXXXUVX331Fc2bN+fBBx8kNzeXJUuW8OGHH/L1118f9JodO3bQq1cvlixZwkknncSLL74YdNuqyhdffMEjjzxy4IPiiSeeoGnTpixZsoQRI0bw1VdfhWxfXl4ed955J3PmzOGrr77is88+Y8aMGSxcuJBt27axbNkyli9fzuWXXw7Aww8/zOLFi1myZAlPPvlkJX87FRM3wR1cIF+7FoqK3E8L7MZER1Wf4zrqqKM44YQTDjx+9dVXycrKIisri5UrVwYN7rVq1aJv374AHH/88axduzbots8///yD1pk/fz4DBgwAoHPnzrRv3z5k+xYsWMBpp51G48aNSUtL49JLL2Xu3LkcffTRrFq1iuHDhzN79mwaNGgAQPv27Rk0aBBTp06t8EVIlRVXwd0YUzW8zmVF6xxXnTp1DtxfvXo148aN45NPPmHp0qX06dMn6HjvGjVqHLifmppKYWFh0G3XrFnzoHXKO0mR1/oZGRksXbqUnj17Mn78eK655hoAZs+ezbBhw/jiiy/Izs5m//795dpfJFhwN8YcJJbnuHbu3Em9evWoX78+mzdvZvbs2RHfR8+ePXn99dcBWLZsWdBvBoG6d+/OnDlzyM/Pp7CwkGnTptGrVy+2bt2KqnLRRRdx3333sWjRIvbv309eXh6nnXYajzzyCFu3bqWgdI6rCsTNaBljTNXxpzwjOVomXFlZWbRr144OHTrQpk0bevToEfF93HjjjVx++eV06tSJrKwsOnTocCClEkyLFi0YNWoUvXv3RlXp378/f/nLX1i0aBFXXXUVqoqI8NBDD1FYWMill17Krl27KCoq4vbbb6devXoRP4ayxGwO1ezsbLXJOoypOitXruS4446LdTOqhcLCQgoLC0lPT2f16tWceeaZrF69mkMOqV793WDvmYgsVNXssl5b5pGISEvgZaApUARMVNVxpdbpDbwDrPE99ZaqjsIYY6qh3bt3c/rpp1NYWIiq8uyzz1a7wF5Z4RxNIfB3VV0kIvWAhSLyoaqWTlLNU9WzIt9EY4yJrIYNG7Jw4cJYNyOqyjyhqqqbVXWR7/4uYCXQPNoNM8YYU3HlGi0jIplAV2BBkMUnicgSEZklIqEHjRpjjImqsJNMIlIXeBO4SVV3llq8CGilqrtFpB/wNnBMkG0MBYYCHGlFYYwxJmrC6rmLSBousE9V1bdKL1fVnaq623d/JpAmIo2DrDdRVbNVNbtJkyaVbLoxxhgvZQZ3cRXjXwBWquoYj3Wa+tZDRLr5tpsfyYYaY+Jb7969D7ogaezYsVx33XUhX1e3bl0ANm3axIUXXui57bKGVo8dO7bExUT9+vVj+/bt4TQ9pHvvvZdHH3200tuJtHB67j2Ay4DTRGSx79ZPRIaJyDDfOhcCy0VkCTAeGKCxGkBvjKmWBg4cyLRp00o8N23aNAYOHBjW64844gjeeOONCu+/dHCfOXMmDRs2rPD2qrtwRsvMV1VR1U6q2sV3m6mqz6jqM751nlTV9qraWVW7q+rn0W+6MSaeXHjhhcyYMYPff/8dgLVr17Jp0yZ69ux5YNx5VlYWHTt25J133jno9WvXrqVDhw4A/PrrrwwYMIBOnTpxySWX8Ouvvx5Y79prrz1QLviee+4BYPz48WzatIlTTz2VU089FYDMzEy2bdsGwJgxY+jQoQMdOnQ4UC547dq1HHfccfz1r3+lffv2nHnmmSX2E8zixYvp3r07nTp14rzzzuOXX345sP927drRqVOnAwXL/vOf/xyYrKRr167s2rWrwr/bYOJy1H6kJxEwJtncdBNEeoKhLl3AFxeDysjIoFu3brz//vucc845TJs2jUsuuQQRIT09nenTp1O/fn22bdtG9+7dOfvssz3nEZ0wYQK1a9dm6dKlLF26lKysrAPLRo8ezaGHHsr+/fs5/fTTWbp0KX/7298YM2YMc+bMoXHjkqcDFy5cyKRJk1iwYAGqyoknnkivXr1o1KgRq1ev5tVXX+W5557j4osv5s033wxZn/3yyy/niSeeoFevXtx9993cd999jB07lgcffJA1a9ZQs2bNA6mgRx99lKeeeooePXqwe/du0tPTy/HbLlvcFQ6rykkEjDGRFZiaCUzJqCojR46kU6dOnHHGGWzcuJEtW7Z4bmfu3LkHgmynTp3o1KnTgWWvv/46WVlZdO3alRUrVpRZFGz+/Pmcd9551KlTh7p163L++eczb948AFq3bk2XLl2A0GWFwdWX3759O7169QJg8ODBzJ0790Abc3JymDJlyoErYXv06MEtt9zC+PHj2b59e8SvkI27nnuoSQSs925MeEL1sKPp3HPP5ZZbbmHRokX8+uuvB3rcU6dOZevWrSxcuJC0tDQyMzODlvkNFKxXv2bNGh599FG+/PJLGjVqxBVXXFHmdkKdHvSXCwZXMristIyX9957j7lz5/Luu+9y//33s2LFCkaMGMFf/vIXZs6cSffu3fnoo4849thjK7T9YOKu524TZRsTv+rWrUvv3r258sorS5xI3bFjB4cddhhpaWnMmTOHdcEmTA5wyimnHJgEe/ny5SxduhRw5YLr1KlDgwYN2LJlC7NmzTrwmnr16gXNa59yyim8/fbbFBQUsGfPHqZPn87JJ59c7mNr0KABjRo1OtDrnzx5Mr169aKoqIgNGzZw6qmn8vDDD7N9+3Z2797N999/T8eOHbn99tvJzs7mm2++Kfc+Q4m7nrtNlG1MfBs4cCDnn39+iZEzOTk59O/fn+zsbLp06VJmD/baa69lyJAhdOrUiS5dutCtWzfAzarUtWtX2rdvf1C54KFDh9K3b1+aNWvGnDlzDjyflZXFFVdccWAbV199NV27dg2ZgvHy0ksvMWzYMAoKCmjTpg2TJk1i//79DBo0iB07dqCq3HzzzTRs2JC77rqLOXPmkJqaSrt27Q7MKhUpcVfy159zD0zN1K5t86kaUxYr+Rt/KlPyN+7SMjZRtjHGlC3u0jKzZ8PDD8PcuZaKMcYYL3HXc1eFpUshLy/WLTEm/tiF4/Gjsu9V3AX3I45wP197DTIzISXF/bRx7saElp6eTn5+vgX4OKCq5OfnV+rCprhLy/iD+4QJsG+fu++/kAks926MlxYtWpCXl8fWrVtj3RQThvT0dFq0aFHh18fdaBlV11sPplUrqMDoJWOMiRsJO1rGo9QEYBcyGWOMX9wFd4AaNYI/b6NnjDHGicvg3rnzwT342rVddUhjjDFxGtxPPBFq1YKMjOLnatWKXXuMMaa6ibvRMuBGzBQUuJOrfvn5NmLGGGP84rLn3qyZ+1m6+mZBAQwfXvXtMcaY6iYug7t/rHsw+fl2QZMxxiRccAc3cYcxxiSzuAzuRx8NaWney228uzEm2cVlcE9Ph969ITU1+HIb726MSXZxGdwBTj8d9u8/eAikjXc3xpg4Du5nnOF+pqaWTNH8/jssXhybNhljTHURt8G9SxeXftm3r7g6JLje/GOPweTJsWubMcbEWtwG99RUWLYMmjQ5eJkq3Hxz1bfJGGOqi7gN7gD168PGjcGX5efD1Ve7WZt69oQNG6q2bcYYE0txHdwh9MiYF16As8+Gzz6DUaOqrk3GGBNrcR/cR492I2S8rFvnJvGYNAnOPRdeeaXq2maMMbFSZnAXkZYiMkdEVorIChE5qHqLOONF5DsRWSoiWdFp7sFycmDixNDr3HwzNG8O8+bBlVfCkiVV0zZjjImVcHruhcDfVfU4oDtwvYi0K7VOX+AY320oMCGirSxDTo7rnXu5/37Xg//mG1cmePDgkhUljTEm0ZQZ3FV1s6ou8t3fBawEmpda7RzgZXX+BzQUkWYRb20IoS5c8hcTa9IE/vlP13OfNavq2maMMVWtXDl3EckEugILSi1qDgSOR8nj4A8ARGSoiOSKSG6kZ2DPySk5eUdp/mJiAwdCy5bwwAMlx8cbY0wiCTu4i0hd4E3gJlXdWXpxkJcclPhQ1Ymqmq2q2U2CDVCvpHHjvJf5i4mlpcG998J//wvdu7tgf+658O67EW+OMcbETFjBXUTScIF9qqq+FWSVPKBlwOMWwKbKN698QvXeDz20+P6VV8K0abB7NyxcCPPnw/XXw969VdNOY4yJtnBGywjwArBSVcd4rPYucLlv1Ex3YIeqbo5gO8M2blzwcsD5+XDddcWPL7kEVq2Cb791+fi8PBgxAl57reraaowx0SJaxrAREekJzAOWAUW+p0cCRwKo6jO+D4AngT5AATBEVXNDbTc7O1tzc0OuUmGNG7tgXpqIqzlTeo5VVTj+ePjqK/d4xQpoV3o8kDHGVAMislBVs8tcr6zgHi3RDO4pKd5DHVu1grVrD35+/XoX3C+4AG69FR58MCpNM8aYSgk3uMf9FarBhCpJsG5d8DlWjzwSzjkH+vSBKVNcdUljjIlXCRncR492KRgvQ4d6T6I9ZIgrRnb++bBtW3TaZ4wx0ZaQwT0nB4YN8w7wBQXek2iffz48+ijMng1/+hP8/HP02mmMMdGSkMEd4OmnQ0/YsW5d8OdF4O9/d+Pev/7apWk++ACef95KFhhj4kfCBncIXXNGxDs1A3DmmfDGG+4k65//DH/9qxsPb4wx8SChgzt4599VvVMzfv37w3vvwdix7iKoMV6j/I0xpppJ+OCek+OdTvFKzQQ680wYPhyuvRbeeQc+/TSizTPGmKhI+OAOFU/NBLrpJvjDH1ywnz49cm0zxphoSIrgXpnUjF/jxq7Y2PHHu2Jjln83xlRnSRHcQ6Vm/NUiw9GoEcyYAZmZbm7WZ55xwyZtFI0xprpJiuAOkUnNgKs6OWsW1Kjh8vC33QaLFkWmjcYYEylJE9xHjw5eLbKoyJUALk+Ab90aPvsM3nrL1bF5++3ItdMYYyIhIQuHefGqFgneBcXKcuqprkzBsmWVapoxxoQlqQuHeQlVSqA8ufdA554Ly5e72vDGGFNdJFVwD1UtMiWlfKkZv4svhvR0uP9+KCyEXbtcqscYY2IpqYK7V94dXInfUNUivTRrBjff7F53+OFQvz4ce6yrS2OMMbGSVME9JwcmTfKeZzVUtchQbr8dsrJc/v3BB2HnTujZE3bsqFx7jTGmopLqhGogr9maRCqfVpk/H04+2fXmL720ctsyxphAdkK1DF7590MPrfy2//hHaNrUyhQYY2InaYO7V/59166KnVgNlJLipuybNQt+/bVy2zLGmIpI2uCek+NOfpa2d2/F8u6lXXAB7NnjygUbY0xVOyTWDYglr3Hv4ZQCLssZZ8All7gPirQ0uOEGN2TSGGOqQtL23ME7717eejNe23jhBTeL0223Qbt2buIPY4ypCkkd3EOVAh4+vPLbr1PH5d0//ND12s86C847LzLfDIwxJpSkDu6hSgHn51e+9+53xhmweLEbA//BB64Xb5UkjTHRlNTBHbxLAUNkeu9+NWq4i52+/hpq1YI774zcto0xprSkD+6jR3svi2Tv3a9VK7j1Vpeu6d8fnnwSfvrJlQ+2ST+MMZGStFeoBopGKeBQdu2C7Gw3WmfbNndid/16d2Vrjx6R3ZcxJrFE7ApVEXlRRH4SkeUey3uLyA4RWey73V2RBsfSuHHey9ati3zvvV49VyJ4/XpXk+ann9wJ18mTI7sfY0zyCict8y+gTxnrzFPVLr7bqMo3q2rl5HgXE4OKVYsMR61a8OmnLg9/wQUwbRqccAK8/nrk92WMSS5lBndVnQuEmOYiMYwbB7VrB19WUBDZk6uB6tVz0/YNHuyqSObmwpgx0dmXMSZ5ROqE6kkiskREZolIe6+VRGSoiOSKSO7WrVsjtOvIyMmBiRO9l0fj5GqgP/0J/vc/GDUKFiyANWuity9jTOKLRHBfBLRS1c7AE4DndNGqOlFVs1U1u0mTJhHYdWTl5IQeGhmJmjOhnHgiXHaZuz9tWnT3ZYxJbJUO7qq6U1V3++7PBNJEpHGlWxYjoYZGVnSe1fLIzITeveHhh+H776O/P2NMYqp0cBeRpiLuIn4R6ebbpsfAwuov1MnVSNR6D8eLL7qywb17w2OP2ZysxpjyC2co5KvAf4G2IpInIleJyDARGeZb5UJguYgsAcYDAzRWg+cjZNy46NV6D0fr1q7IWOvW7oKnhx+O/j6NMYnFLmLy4HVhUzQuavKi6qbpe/11N1Ry40Zo0QIeecS7oqUxJrGFexFTUtdzD8Wr1ntV5N39RNwInrp14f333dR9M2bAvHmwerWrOmmMMcEkfW0ZL9Gs9V4e9erBc8/Bhg3w5ZcuyG/e7CpM/vOf7upWY4wpzYK7B685VouK4MorqzbABzr5ZDjtNHjgARg5EoYMsYJjxpiDWXD34DXHKkRuntWKeuQR6NMHbrwRZs6Ef/0LCgu9U0nGmORjJ1RDSEkJ3SueMsV9CMRKUZHrxX/1FRx7LHz3nbs1ahS7NhljoitiVSGTWVkjUqJVUCxcKSluTPz+/W5mp59/Dl3h0hiTPCy4h+CVd/eLZkGxcLVpA7Nnu+qS550HY8fCL7+4ZQUF7maMST4W3EPIyYFJk0KXA452QbFw9OjhbvfcAzt3wv/9vy6ddPrpbv5WO+FqTPKx4F6GnBw3W1IsC4qFq3NnVzp4/Hi4/35XZfK//3WTchtjkosF9zDFuqBYuB54wH3TuOce+MMf3BWtAwe6UgZWRtiY5GHBPUzVoaBYOJo3hxUr3LeJSZNc4bEOHeDHH+Guu9w6d98N990X23YaY6LLhkKWw9Sp7qKhfftKPl+jhhu1EsthkWUZMcIVILvpJnj8cahZ09WqWbECTjkl1q0zxoQr3KGQFtzLqToUFKuI7dvdCdZFi1zvfuNG6NgRli2Djz5yy4wx1Z8VDosSr6tA162r2naUV8OGrjbNnDnQrp0rY7BsmVs2aZKrV3PIIXD22d5zyRpj4ofl3MupuhQUq4iUFNdDb9YMhg2Dli3hkkvg1Vfd9H4DB7orXm1yEGPinwX3cho92gXy0lSrz5DIcNx6q0sj3XqrC+b9+7shlAsWuPlbP/kE+vaFWbNi3VJjTEVYzr0CggV3v1jXm6mIJUtcbZq0NMjKco8BUlNdb/+tt+Css2LbRmOMY7VloijUBU2xrjdTEZ07u9EzKSku//63vxXXkO/cGS6+2OXq9++PdUuNMeGynnsFTJ3qgrhX3ZbqPnKmPLZsge7d3fHUrQsnngjPPOPmeD3+eOjZ0623YYOrRlm3bkyba0zCs9EyUeRPuwwaFHx5dbpitbIOPxy++MLVjV+40H2wdegAv//urn799ls3x+sVVxRXqRw8ONatNsZYz70SMjODD4FMpJ57aYsXw4ABcNJJbpKQIUPgzTfd8EoR+PprdzL26KO9JzsxxlSc9dyrwOjRB6dnRKBfv9i1Kdq6dIFvvnH39+xxOfpGjdyJ5MJC6NTJpWsyMtx8r9ll/gkaY6LBTqhWQk6OS0EEjp5RhZdeir+TqhUxbZrLta9fD0cdBW3buqGTEye63PsJJ7jiZStWBH/9vn3u6thE/ZZjTCxZWqaSkjE1E47Nm13a5vHHXbmDBQtcDR5wV8r+4x/u586d0L69S/ccYt8jjSmTDYWsIl4nT6t7OYJoa9bMBfDnnnOBe/BgdxL2scfgj390qZ2cHLjzTtezf/xx2LrVJhYxJlKs515JXj13EZg8Of4uaIqGhx5yVSn9zjsPXnjB5epVoXdvmDvXLWvUCKZPd73+du3gttti0WJjqi+rCllFpk51dVmC/RqTPTUTaMYMN6Ty2GNdDZvA8xS7d7vZovLyXAmEDRtg7153YdUPP0DTpu4CqlDz2RqTLCy4VyGvcgQiVoSrvL780s0He8YZLuC3bOlSX/45Ye+7z6V1jElWEcu5i8iLIvKTiCz3WC4iMl5EvhORpSKSVZEGxzOvcgTVaYameHHCCS6Yz5jhhpnu2AHDh7sCZytXuolF7r4bPv4YHnnEDb91z0sDAAASiElEQVT0W7TIjdQxxoTRcxeRU4DdwMuq2iHI8n7AjUA/4ERgnKqeWNaOE6nn7jVDE8C118LTT1d9mxKBqrul+LogO3bA9deXHGY6cmTx/LbZ2e4q2vHjoU0bF+xTU911B126uHXy872nSzQmHkQ0LSMimcAMj+D+LPCpqr7qe7wK6K2qm0NtM5GCO3jP0GQnViNv3jx3LuOTT9yJ14YNXXGziRPdCdlffim5fnq6O4H7zTdw//0utXP33bFouTGVV5VXqDYHNgQ8zvM9d1BwF5GhwFCAI71mvYhTXjM0qRbXWrEAHxknn+xuF17oTtB++KEL7OnpLmf/wQeu/k3Xru4q2j//ufh336YN3HOPK49w6aXuuT173GidTZvcWPtTToHWrd179/HH7sMkI8ONjJo3z+3v0ktd9cz0dJca2rMHGjQ4uK3797sPlldegQcfdEXYjKkSqlrmDcgElnssew/oGfD4Y+D4srZ5/PHHayJp1cqfRAh+q11bdcqUWLcyMf3+u+rAgar33BN8+Z49qh9+qPrxx6q//abaq5dqjRqqY8aoXnGFanp6yfeqYUPVd9912wRVkeJlKSmqXbu6+126qN5/v+pRR6nWr6/6zTeq77+vunNn8b7vuqv4daefXhW/DRNtn32metttqpMmub+tqgbkajhxO6yVQgf3Z4GBAY9XAc3K2maiBfcpU0oGgWC3Vq1i3Uqjqrptm2rbtu49qVNHddgwF/jXrlVduFC1RYvigPzAA6q7d6uuW6f65ZeqGze6bfz736qNGhUH+UMPVa1Vyz1u21Z16FDVCy5w27j8ctUHH3TLFi8O3qbJk1073nuv6n4PgQoLVWfPVv3119jsP1bGj1cdMUJ16VL3+OefVY85RrVfP9XPPz94/aIi1Y4di/+nDztMdf78qm1zuME9Ejn3vwA3UHxCdbyqditrm4mWcwe47jpX69zrV2pDI6uPvXtdGubww6FWrZLLNm6E+fPdkMuWLb238dtvLu1Sp44riTxsmCt9/Pzzrpjc4YfDEUfA22+7971lS5cOevlllzbynyh+80246CJ38rew0J0wrl/ftaNWLejWzaWfKuKDD9yJ5Y4d3Ynl7dvdKKNGjdxopAYN3DmKSy5x6abOnd3+27ev2P7C8cYb7pzJ1Ve78yXRUFDgrpdo29Z7nR9+gGOOce+NiGtPejo88YRLw+Xnu7+B/v3de9m+vbsm4/TTXRrw2GPhqqvcldW9e7vX9urlnt+wwU1T2bhx5I8t3Jx7OL32V3H58324fPpVwDBgmG+5AE8B3wPLgOxwPlUSrefuN2WKampq8J57RkasW2eqwv79ridc2syZLuUDqocc4np9p53mvvF17676yy+qJ5108N9NWprqo4+qvvKK+/bXq5f7puFXVKT6ySeqa9a4x3PmqF52mer555fczmmnqfbuXfwN87LLVPPzVbOyXJrq//wf9w0kNVV18GDVm29WPeUUt73SfvtN9dVXVZ99VnXWLNUdO4rbMneuS1ds3eqOKdDnn7tjB9UjjnDrRMrSpW57s2e7VJmI6osvujYFc9117riXLVO95Rb3LQtUL73UfVsbM6ZkL92fomvYsDgds2aNW+cPf1Bt3rzkug0aqE6fXrw//+/mrbeKvylUBJFMy0TjlqjBXdUF+LS04AH+2mtj3ToTS3l5qs89p/qPf7jgetRRqjfdpLprl1u+caMLOv/+t1v3m29U+/Yt/vvp2NEF+JQU1exs12HIzi5OI/3xj+5vr25d9/Ouu1yAffJJF7hB9fnnVW+80QXZbt1cgJs50+1/61bV4cPd61NSVJs2dQHtjjtUv/tOtaBAdcECl7oI/Ltu1Mh9OJx5pnvcpInbf/36qi+/rPrjj6p9+rjzG61bq86Y4fY/YIDq99+XDMBFRao//eR+/vyz+/D461/dcV52meqWLaobNqg+9JBLfZ1zjmpOTnHwBbePk0929+vWda+fMcO15emnVd94Q7VmTdUrryze77x57gNx7dqS79n27e7YX3vN7ee554K/t0VFqqtXu9/lZ5+59taqpfr//p/qp58Wn8MB1dtvr/jfkAX3GMvICB7cRezEqimfoiLV3FzX4/vtN9erHDzY5fovvth9QDzyiOrIkao9eqhedJEL6KW/PeTnq/7nP+7+998XB8JXXjl4n7t2uSC6a5fqkCHFf7/16rkAnZnpguWGDaoffVQc1OvUUb33XtX+/V1Hxh9gDzvMDSoYPlz122/dPvwnm8Ft76yzXPD2v8b/Lcc/IOHUU92+27Vzj8F9yGRmuvvDh6vefbfqxInu91RQoPrUU679wTpbnTq5Y4yWH39UPfLIkv/7o0a58y6bN1d8u+EGdys/ECUpKd65d6s5Y6qD0aPd+YKbbip73f/8x/3NfvqpO18xdiw0aVJyHf8E6qmpxc8VFrqicRMmuFx7377Fy4qKXD3/NWvcFckbN8K2be5/Z/Bgd06kVSs361f37q6t06fDBRe4obAvvODOYai6XHi9et7t//FHV+CvXj3XptmzXY69UaNwf1sVs2MHfPWVu8CxbVuIxAhwqy0TY17VIv2mTLFx7yZ5FBZGrl5/Xp4rKR34IZJMrJ57jI0e7V1QDNxIhWSYrckYiOxELC1aJG9gLw8L7lGSk+OGxnkpKIA77qi69hhjkosF9yh6+unQRaq8ZnEyxpjKsuAeZV41ZyAyJ1eMMSYYC+5RFiqA795teXdjTHRYcI+y0aOhdu3gy/Lz7cSqMSY6LLhHWU6Oq0PhNVuTnVg1xkSDBfcqkJPjLgDxGhppJ1aNMZFmwb0KeeXfba5VY0ykWXCvQqNHQ1rawc/v2mV5d2NMZFlwr0I5Oa5Od2l798Lw4VXfHmNM4rLgXsW8xr3n51vv3RgTORbcq1ioce+DB1uAN8ZEhgX3KjZ6tPey/ftt3LsxJjIsuFexnJzQ9WZs3LsxJhIsuMfAuHGhywHbuHdjTGVZcI+BnBzvWZrALWvc2NIzxpiKs+AeI17lCPzy8+HKKy3AG2MqxoJ7jIQqKOa3d6/l340xFWPBPUbKKijmF2oeVmOM8WLBPYb8BcVCBXgRS80YY8rPgns14FVzBtzJVUvNGGPKK4JzkpuKyslxPwcNCr7cUjPGmPKynns1kZMTOj1z3XVV1xZjTPyz4F6NjB7tfXHThAk29t0YE76wgruI9BGRVSLynYiMCLL8ChHZKiKLfberI9/UxFfWxU0256oxJlxlBncRSQWeAvoC7YCBItIuyKqvqWoX3+35CLczaZQ1NNJqzxhjwhFOz70b8J2q/qCqe4FpwDnRbVbyCpWa8bPaM8aYsoQT3JsDGwIe5/meK+0CEVkqIm+ISMtgGxKRoSKSKyK5W7durUBzE19ODgwbFjrA25yrxpiyhBPcg4WZ0pnhfwOZqtoJ+Ah4KdiGVHWiqmaranaTJk3K19Ik8vTTMHmyd2ng/HwbPWOMCS2c4J4HBPbEWwCbAldQ1XxV/d338Dng+Mg0L3nl5MC2bd4BfsIEC/DGGG/hBPcvgWNEpLWI1AAGAO8GriAizQIeng2sjFwTk5vXnKsAzzxjI2eMMcGVGdxVtRC4AZiNC9qvq+oKERklImf7VvubiKwQkSXA34ArotXgZBNqzlUrTWCM8SIaamB1FGVnZ2tubm5M9h1Ppk6Fyy7zHv8uAkVFVdsmY0zsiMhCVc0uaz27QrWa84+e8aIKmZmWnjHGlGTBPQ48/TRce6338nXrXO/eTrAaY/wsuMeJp5/2HjkDrgdvJ1iNMX4W3ONIqJEz4AL88OFV0xZjTPVmwT2OhBo545efb713Y4wF97gSzqTa4Cb9ELETrcYkMwvucSTcSbX91q2zEsHGJCsL7nHGP6m2augTrH4FBZaHNyYZWXCPY+PGlV0eGFwe3mZxMia5WHCPY2XN3BTIZnEyJrlYcI9z4ebfwVI0xiQTC+5xLtwRNH75+S6VY2kaYxKbBfc4FziCRiS8k6zggvygQRbkjUlUFtwTgH8ETVFR6Ak+gsnPhyFDXJBPSbGx8cYkCgvuCWjcuPKlavbtc0Fe1cbGG5MoLLgnIH+qpjw9+EAFBTYJiDHxzoJ7gvLPwTplSsWC/Lp11ns3Jp5ZcE9wgUE+pZzv9qBBViPemHhlwT1J5OTAyy+XLxcPMGGCG4UjAqmpVpDMmHhhwT2JVDYX75+r1U66GlP9WXBPMpXNxfsVFLi0jfXmjameLLgnqUgF+cDevM3jakz1YcE9yQUG+Tp1Krct1ZI5eitzYEzsWHA3gAvyu3dXvidfmr/MQbBgP3WqS+fYlbHGRJ4Fd1NCpNI1XgKD/aBBLp3jvzI2MK1jgd+YyrHgboLyB3lVF+jLU1q4ogLTOqUDf2DvP1jaxz4MjClJNNzZHiIsOztbc3NzY7JvU3FTp8I118CePbFuScVkZMDFF8PMmbB+PRx5pCubnJMT65YZEx4RWaiq2WWtZz13Uy6BuXl/bz6cqf6qi/x89+0gnG8FVXGrW9fdSl8olpJiJ6VN5YQV3EWkj4isEpHvRGREkOU1ReQ13/IFIpIZ6Yaa6iVwou6ioujl6BPdnj0lvwX5h5YGfqEufVLabolxi/aHdpnBXURSgaeAvkA7YKCItCu12lXAL6p6NPA48FCkG2qqt8AcfVXm6Y2JV/n5cOWV0Qvw4fTcuwHfqeoPqroXmAacU2qdc4CXfPffAE4XEYlcM028CezZ+4O99eyNKWnv3uiV1w4nuDcHNgQ8zvM9F3QdVS0EdgAH/SuLyFARyRWR3K1bt1asxSYule7ZB95C5e/9lSytq2AS1fr10dluOME92L9V6SE24ayDqk5U1WxVzW7SpEk47TNJoHT+PjDw798f/HmvbwP2YWDizZFHRme74QT3PKBlwOMWwCavdUTkEKAB8HMkGmiMl2DfBsr7YWBMLNWo4YbiRkM4wf1L4BgRaS0iNYABwLul1nkXGOy7fyHwicZqAL0xHkKlhmJx86ejRNyHTkaGu9+qlVtmH0aJLSMDXnwxetdYhHURk4j0A8YCqcCLqjpaREYBuar6roikA5OBrrge+wBV/SHUNu0iJmOMKb9wL2I6JJyNqepMYGap5+4OuP8bcFF5G2mMMSY67ApVY4xJQBbcjTEmAVlwN8aYBGTB3RhjElDMSv6KyFZgXQVe2hjYFuHmVGfJdryQfMdsx5vYIn28rVS1zKtAYxbcK0pEcsMZBpQoku14IfmO2Y43scXqeC0tY4wxCciCuzHGJKB4DO4TY92AKpZsxwvJd8x2vIktJscbdzl3Y4wxZYvHnrsxxpgyWHA3xpgEFFfBvayJuhOBiKwVkWUislhEcn3PHSoiH4rIat/PRrFuZ0WJyIsi8pOILA94LujxiTPe934vFZGs2LW8YjyO914R2eh7jxf7qq76l/3Dd7yrROTPsWl1xYlISxGZIyIrRWSFiAz3PZ+Q73GI4439e6yqcXHDlRv+HmgD1ACWAO1i3a4oHOdaoHGp5x4GRvjujwAeinU7K3F8pwBZwPKyjg/oB8zCzfTVHVgQ6/ZH6HjvBW4Nsm473991TaC17+89NdbHUM7jbQZk+e7XA771HVdCvschjjfm73E89dzDmag7UQVOQP4ScG4M21IpqjqXg2fp8jq+c4CX1fkf0FBEmlVNSyPD43i9nANMU9XfVXUN8B3u7z5uqOpmVV3ku78LWImbYzkh3+MQx+ulyt7jeAru4UzUnQgU+EBEForIUN9zh6vqZnB/TMBhMWtddHgdXyK/5zf40hAvBqTZEup4RSQTN4HPApLgPS51vBDj9ziegntYk3AngB6qmgX0Ba4XkVNi3aAYStT3fAJwFNAF2Aw85ns+YY5XROoCbwI3qerOUKsGeS7ujjnI8cb8PY6n4B7ORN1xT1U3+X7+BEzHfWXb4v+q6vv5U+xaGBVex5eQ77mqblHV/apaBDxH8dfyhDheEUnDBbqpqvqW7+mEfY+DHW91eI/jKbiHM1F3XBOROiJSz38fOBNYTskJyAcD78SmhVHjdXzvApf7RlR0B3b4v9rHs1I55fNw7zG44x0gIjVFpDVwDPBFVbevMkREgBeAlao6JmBRQr7HXsdbLd7jWJ9tLueZ6X64s9HfA3fEuj1ROL42uDPpS4AV/mMEMoCPgdW+n4fGuq2VOMZXcV9T9+F6MVd5HR/uK+xTvvd7GZAd6/ZH6Hgn+45nKe6fvVnA+nf4jncV0DfW7a/A8fbEpRmWAot9t36J+h6HON6Yv8dWfsAYYxJQPKVljDHGhMmCuzHGJCAL7sYYk4AsuBtjTAKy4G6MMQnIgrsxxiQgC+7GGJOA/j+lrY91t9jAJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "train_data = pad_sequences(train_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "test_data = pad_sequences(test_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels,\n",
    "#                                                   test_size=0.28, random_state=2019,\n",
    "#                                                   stratify=train_labels)\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_val = to_categorical(y_val)\n",
    "word_input_tensor = Input(shape=(maxlen,) , name='words')\n",
    "\n",
    "conv_1d_s3_model = Sequential()\n",
    "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s3_model.add(layers.Dropout(0.2))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s3_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s3_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s3_model.layers[0].trainable = False\n",
    "word_output_tensor_0 = conv_1d_s3_model(word_input_tensor)\n",
    "\n",
    "\n",
    "conv_1d_s1_model = Sequential()\n",
    "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s1_model.add(layers.Dropout(0.2))\n",
    "conv_1d_s1_model.add(layers.SeparableConv1D(32, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s1_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s1_model.layers[0].trainable = False\n",
    "word_output_tensor_1 = conv_1d_s1_model(word_input_tensor)\n",
    "\n",
    "conv_1d_complex_model = Sequential()\n",
    "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_complex_model.add(layers.Dropout(0.2))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_complex_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_complex_model.layers[0].trainable = False\n",
    "word_output_tensor_2 = conv_1d_complex_model(word_input_tensor)\n",
    "\n",
    "# x = layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen)(word_input_tensor)\n",
    "# x = layers.Conv1D(128, 5, activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# y = layers.Conv1D(128, 10, activation='relu', padding='same')(x)\n",
    "# added = layers.add([y, x])\n",
    "# added = layers.GlobalMaxPooling1D()(added)\n",
    "\n",
    "concatenated = layers.concatenate([word_output_tensor_0,\n",
    "                                   word_output_tensor_1,\n",
    "                                   word_output_tensor_2,\n",
    "#                                    ,added\n",
    "                                  ], axis=-1)\n",
    "# concatenated = layers.Dense(32, activation='relu')(concatenated)\n",
    "answer = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model(word_input_tensor, answer)\n",
    "model.summary()\n",
    "\n",
    "# model.layers[0].set_weights([w2d.word_embedding])\n",
    "# model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=2000,\n",
    "                    batch_size=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose= 2\n",
    "                   )\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7136752136752137\n"
     ]
    }
   ],
   "source": [
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
