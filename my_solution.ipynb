{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Reading general data of the problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading general data of the problems, done!\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import division\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "# multiprocessing.set_start_method('spawn')\n",
    "from MyUtils import clean_folder, read_files, shuffle_docs, shuffle_docs2\n",
    "from Word2Dim import Word2Dim\n",
    "\n",
    "dataset_path = '.' + os.sep + 'pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23'\n",
    "outpath = '.' + os.sep + 'dev_out'\n",
    "\n",
    "clean_folder(outpath)\n",
    "\n",
    "infocollection = dataset_path + os.sep + 'collection-info.json'\n",
    "problems = []\n",
    "language = []\n",
    "with open(infocollection, 'r') as f:\n",
    "    for attrib in json.load(f):\n",
    "        problems.append(attrib['problem-name'])\n",
    "        language.append(attrib['language'])\n",
    "print('Reading general data of the problems, done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reading problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc count to process:  819\n",
      "process_doc, done!\n",
      "word_set, ready!\n",
      "fit_transform_texts is done!\n",
      "doc count to process:  98\n",
      "Reading problem 1, done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "problem = problems[1]\n",
    "index = 1\n",
    "\n",
    "# used for n_gram extraction and word indexing, a threshold which prevent words appearing lower than this value to be counted in calculations\n",
    "tf = 5\n",
    "\n",
    "\n",
    "infoproblem = dataset_path + os.sep + problem + os.sep + 'problem-info.json'\n",
    "candidates = []\n",
    "with open(infoproblem, 'r') as f:\n",
    "    fj = json.load(f)\n",
    "    unk_folder = fj['unknown-folder']\n",
    "    for attrib in fj['candidate-authors']:\n",
    "        candidates.append(attrib['author-name'])\n",
    "\n",
    "candidates.sort()\n",
    "# Building training set\n",
    "train_docs = []\n",
    "for candidate in candidates:\n",
    "    train_docs.extend(read_files(dataset_path + os.sep + problem, candidate))\n",
    "train_texts = [text for i, (text, label) in enumerate(train_docs)]\n",
    "train_labels = [label for i, (text, label) in enumerate(train_docs)]\n",
    "initial_train_size = len(train_labels)\n",
    "\n",
    "train_texts, train_labels = shuffle_docs(train_texts, train_labels)\n",
    "validation_size = len(train_texts) - initial_train_size\n",
    "class_size = int(initial_train_size / len(set(train_labels)))\n",
    "\n",
    "# train_texts, train_labels, validation_start_index, class_size = shuffle_docs2(train_texts, train_labels)\n",
    "\n",
    "index_2_label_dict = {i: l for i, l in enumerate(set(train_labels))}\n",
    "label_2_index_dict = {l: i for i, l in enumerate(set(train_labels))}\n",
    "train_labels = [label_2_index_dict[v] for v in train_labels]\n",
    "w2d = Word2Dim(lang= language[index])\n",
    "train_tokenized_with_pos, train_tokenized_indexed = w2d.fit_transform_texts(train_texts, train_labels, tf= tf)\n",
    "\n",
    "maxlen = len(max(train_tokenized_indexed, key=len))  # We will cut the texts after # words\n",
    "embedding_dim = w2d.word_embedding.shape[1]\n",
    "\n",
    "# preparing test set\n",
    "ground_truth_file = dataset_path + os.sep + problem + os.sep + 'ground-truth.json'\n",
    "gt = {}\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    for attrib in json.load(f)['ground_truth']:\n",
    "        gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "test_docs = read_files(dataset_path + os.sep + problem, unk_folder, gt)\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs)]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs)]\n",
    "\n",
    "# Filter validation to known authors\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "\n",
    "test_labels = [label_2_index_dict[v] for v in test_labels]\n",
    "\n",
    "test_tokenized_with_pos, test_tokenized_indexed = w2d.transform(test_texts)\n",
    "print(\"Reading problem 1, done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Extraction for Neural Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from MyUtils import extract_n_grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "n = 3\n",
    "vocabulary = extract_n_grams(train_docs, n, tf)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(n, n), lowercase=False, vocabulary=vocabulary)\n",
    "n_gram_train_data = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "n_gram_train_data = n_gram_train_data.astype(float)\n",
    "\n",
    "for i, v in enumerate(train_texts):\n",
    "    n_gram_train_data[i] = n_gram_train_data[i] / len(train_texts[i])\n",
    "n_gram_test_data = vectorizer.transform(test_texts)\n",
    "n_gram_test_data = n_gram_test_data.astype(float)\n",
    "for i, v in enumerate(test_texts):\n",
    "    n_gram_test_data[i] = n_gram_test_data[i] / len(test_texts[i])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_ngrams = max_abs_scaler.fit_transform(n_gram_train_data)\n",
    "scaled_test_data_ngrams = max_abs_scaler.transform(n_gram_test_data)\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_words = max_abs_scaler.fit_transform(w2d.get_texts_vectorized_and_normalized(train_tokenized_indexed)[:, 1:])\n",
    "scaled_test_data_words = max_abs_scaler.transform(w2d.get_texts_vectorized_and_normalized(test_tokenized_indexed)[:, 1:])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(819, 7568)\n",
      "(98, 7568)\n",
      "7568\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train_data_words.shape)\n",
    "print(scaled_test_data_words.shape)\n",
    "print(len(w2d.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 9)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import layers, Input, callbacks\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "callbacks_list_neu = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_neu_ngrams = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu_ngrams.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_neu_words = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu_words.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_convnet = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=35,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_convnet.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=10,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_stacked = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_stacked.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "train_data = pad_sequences(train_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "test_data = pad_sequences(test_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "# X_scaled_train_data_words, X_scaled_val_data_words, _, _ = train_test_split(scaled_train_data_words, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "# X_scaled_train_data_ngrams, X_scaled_val_data_ngrams, _, _ = train_test_split(scaled_train_data_ngrams, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "\n",
    "# train_val_split_index = validation_start_index\n",
    "train_val_split_index = initial_train_size\n",
    "y_train, y_val = train_labels[:train_val_split_index], train_labels[train_val_split_index:]\n",
    "X_train, X_val = train_data[:train_val_split_index], train_data[train_val_split_index:]\n",
    "X_scaled_train_data_words, X_scaled_val_data_words = scaled_train_data_words[:train_val_split_index], scaled_train_data_words[train_val_split_index:]\n",
    "X_scaled_train_data_ngrams, X_scaled_val_data_ngrams = scaled_train_data_ngrams[:train_val_split_index], scaled_train_data_ngrams[train_val_split_index:]\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "# y_test = to_categorical(test_labels)\n",
    "# print(X_train.shape)\n",
    "\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                133664    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 140,521\n",
      "Trainable params: 140,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 5.5567 - acc: 0.1270 - val_loss: 5.5323 - val_acc: 0.1098\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.5214 - acc: 0.0476 - val_loss: 5.4818 - val_acc: 0.1257\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.4492 - acc: 0.1587 - val_loss: 5.4326 - val_acc: 0.1442\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.3976 - acc: 0.1905 - val_loss: 5.3875 - val_acc: 0.1746\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.3531 - acc: 0.1905 - val_loss: 5.3438 - val_acc: 0.1772\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.3246 - acc: 0.2063 - val_loss: 5.2988 - val_acc: 0.2116\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.3139 - acc: 0.1587 - val_loss: 5.2545 - val_acc: 0.2632\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.2213 - acc: 0.1905 - val_loss: 5.2106 - val_acc: 0.2857\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.1800 - acc: 0.2540 - val_loss: 5.1671 - val_acc: 0.3161\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.1266 - acc: 0.1587 - val_loss: 5.1213 - val_acc: 0.2791\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.1223 - acc: 0.2381 - val_loss: 5.0754 - val_acc: 0.2130\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.0776 - acc: 0.1746 - val_loss: 5.0281 - val_acc: 0.2183\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.0159 - acc: 0.2063 - val_loss: 4.9827 - val_acc: 0.3095\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.9633 - acc: 0.2222 - val_loss: 4.9344 - val_acc: 0.3466\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.9367 - acc: 0.2540 - val_loss: 4.8874 - val_acc: 0.4378\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.9107 - acc: 0.2698 - val_loss: 4.8445 - val_acc: 0.4854\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.8293 - acc: 0.2698 - val_loss: 4.7989 - val_acc: 0.4220\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.8324 - acc: 0.2222 - val_loss: 4.7492 - val_acc: 0.4246\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.7518 - acc: 0.3016 - val_loss: 4.7049 - val_acc: 0.5159\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.7497 - acc: 0.3492 - val_loss: 4.6604 - val_acc: 0.4709\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6596 - acc: 0.3175 - val_loss: 4.6181 - val_acc: 0.4021\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6842 - acc: 0.2857 - val_loss: 4.5712 - val_acc: 0.5146\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6174 - acc: 0.3016 - val_loss: 4.5237 - val_acc: 0.5847\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.5715 - acc: 0.3651 - val_loss: 4.4770 - val_acc: 0.6058\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.5285 - acc: 0.3016 - val_loss: 4.4373 - val_acc: 0.6839\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.5351 - acc: 0.3175 - val_loss: 4.3943 - val_acc: 0.6772\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.5149 - acc: 0.3016 - val_loss: 4.3611 - val_acc: 0.6706\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4149 - acc: 0.3651 - val_loss: 4.3164 - val_acc: 0.6865\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4404 - acc: 0.3175 - val_loss: 4.2760 - val_acc: 0.7103\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4001 - acc: 0.3651 - val_loss: 4.2342 - val_acc: 0.7341\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.3201 - acc: 0.4286 - val_loss: 4.1913 - val_acc: 0.7328\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2946 - acc: 0.4127 - val_loss: 4.1441 - val_acc: 0.7540\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2041 - acc: 0.4286 - val_loss: 4.1002 - val_acc: 0.7765\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.1771 - acc: 0.5238 - val_loss: 4.0530 - val_acc: 0.7989\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.1944 - acc: 0.4286 - val_loss: 4.0060 - val_acc: 0.7976\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.1442 - acc: 0.4286 - val_loss: 3.9667 - val_acc: 0.8214\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0985 - acc: 0.4762 - val_loss: 3.9218 - val_acc: 0.8201\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0280 - acc: 0.5238 - val_loss: 3.8834 - val_acc: 0.8148\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0725 - acc: 0.4286 - val_loss: 3.8433 - val_acc: 0.8214\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9100 - acc: 0.5238 - val_loss: 3.8023 - val_acc: 0.8413\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9673 - acc: 0.5556 - val_loss: 3.7611 - val_acc: 0.8452\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9845 - acc: 0.4286 - val_loss: 3.7231 - val_acc: 0.8492\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8661 - acc: 0.5397 - val_loss: 3.6904 - val_acc: 0.8624\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8810 - acc: 0.4921 - val_loss: 3.6503 - val_acc: 0.8717\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8223 - acc: 0.5397 - val_loss: 3.6082 - val_acc: 0.8836\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.7247 - acc: 0.5238 - val_loss: 3.5729 - val_acc: 0.9008\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8020 - acc: 0.5079 - val_loss: 3.5342 - val_acc: 0.9087\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6917 - acc: 0.5873 - val_loss: 3.4894 - val_acc: 0.9246\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6916 - acc: 0.5714 - val_loss: 3.4468 - val_acc: 0.9312\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6544 - acc: 0.5714 - val_loss: 3.4114 - val_acc: 0.9365\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6099 - acc: 0.5556 - val_loss: 3.3774 - val_acc: 0.9656\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6387 - acc: 0.6349 - val_loss: 3.3450 - val_acc: 0.9802\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6277 - acc: 0.5238 - val_loss: 3.3118 - val_acc: 0.9802\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4554 - acc: 0.6667 - val_loss: 3.2746 - val_acc: 0.9802\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.5736 - acc: 0.5397 - val_loss: 3.2418 - val_acc: 0.9828\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4755 - acc: 0.5873 - val_loss: 3.2037 - val_acc: 0.9907\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4511 - acc: 0.6032 - val_loss: 3.1689 - val_acc: 0.9974\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4267 - acc: 0.6667 - val_loss: 3.1331 - val_acc: 0.9974\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4638 - acc: 0.6190 - val_loss: 3.1119 - val_acc: 0.9974\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4044 - acc: 0.6032 - val_loss: 3.0788 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3883 - acc: 0.5238 - val_loss: 3.0494 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2707 - acc: 0.6825 - val_loss: 3.0104 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3070 - acc: 0.7302 - val_loss: 2.9710 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2824 - acc: 0.6825 - val_loss: 2.9339 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2615 - acc: 0.6984 - val_loss: 2.9087 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1578 - acc: 0.6825 - val_loss: 2.8783 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0768 - acc: 0.7619 - val_loss: 2.8381 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1748 - acc: 0.6667 - val_loss: 2.8029 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1501 - acc: 0.6349 - val_loss: 2.7770 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1933 - acc: 0.6190 - val_loss: 2.7559 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0961 - acc: 0.6667 - val_loss: 2.7334 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1571 - acc: 0.6825 - val_loss: 2.7054 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9225 - acc: 0.8095 - val_loss: 2.6701 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0215 - acc: 0.7778 - val_loss: 2.6355 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9414 - acc: 0.6825 - val_loss: 2.6087 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1769 - acc: 0.6508 - val_loss: 2.5947 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0106 - acc: 0.7460 - val_loss: 2.5796 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0152 - acc: 0.7460 - val_loss: 2.5460 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9304 - acc: 0.7460 - val_loss: 2.5201 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9019 - acc: 0.7937 - val_loss: 2.4945 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9826 - acc: 0.7460 - val_loss: 2.4794 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7764 - acc: 0.7778 - val_loss: 2.4529 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9573 - acc: 0.6984 - val_loss: 2.4292 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9317 - acc: 0.6984 - val_loss: 2.4138 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8654 - acc: 0.7460 - val_loss: 2.3965 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7861 - acc: 0.8095 - val_loss: 2.3692 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6992 - acc: 0.8413 - val_loss: 2.3414 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6720 - acc: 0.8095 - val_loss: 2.3162 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7281 - acc: 0.8571 - val_loss: 2.2957 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6798 - acc: 0.7460 - val_loss: 2.2741 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7609 - acc: 0.7619 - val_loss: 2.2575 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5672 - acc: 0.8095 - val_loss: 2.2461 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6060 - acc: 0.8095 - val_loss: 2.2249 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6026 - acc: 0.8095 - val_loss: 2.2069 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7315 - acc: 0.7937 - val_loss: 2.1943 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6372 - acc: 0.7460 - val_loss: 2.1849 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5586 - acc: 0.8413 - val_loss: 2.1687 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5703 - acc: 0.8413 - val_loss: 2.1520 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4716 - acc: 0.8571 - val_loss: 2.1309 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5600 - acc: 0.7619 - val_loss: 2.1123 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5289 - acc: 0.8889 - val_loss: 2.0999 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6421 - acc: 0.7460 - val_loss: 2.0964 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6226 - acc: 0.8095 - val_loss: 2.0867 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4471 - acc: 0.8254 - val_loss: 2.0688 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4444 - acc: 0.8254 - val_loss: 2.0503 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5919 - acc: 0.7460 - val_loss: 2.0395 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4966 - acc: 0.8254 - val_loss: 2.0249 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4230 - acc: 0.8571 - val_loss: 2.0093 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3640 - acc: 0.8571 - val_loss: 1.9936 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3722 - acc: 0.8571 - val_loss: 1.9778 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4722 - acc: 0.8730 - val_loss: 1.9750 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3618 - acc: 0.8254 - val_loss: 1.9636 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3192 - acc: 0.8730 - val_loss: 1.9492 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2239 - acc: 0.9206 - val_loss: 1.9339 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4497 - acc: 0.8254 - val_loss: 1.9255 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2737 - acc: 0.8413 - val_loss: 1.9164 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2598 - acc: 0.8730 - val_loss: 1.9082 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3122 - acc: 0.8889 - val_loss: 1.8961 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1518 - acc: 0.8889 - val_loss: 1.8819 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3728 - acc: 0.8413 - val_loss: 1.8748 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2630 - acc: 0.8254 - val_loss: 1.8690 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2273 - acc: 0.9048 - val_loss: 1.8607 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2725 - acc: 0.7937 - val_loss: 1.8478 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2491 - acc: 0.8413 - val_loss: 1.8383 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2010 - acc: 0.9048 - val_loss: 1.8306 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2255 - acc: 0.8730 - val_loss: 1.8203 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2865 - acc: 0.8413 - val_loss: 1.8183 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1522 - acc: 0.9365 - val_loss: 1.8107 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1326 - acc: 0.8730 - val_loss: 1.7996 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1249 - acc: 0.9365 - val_loss: 1.7866 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1768 - acc: 0.8571 - val_loss: 1.7751 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1337 - acc: 0.8571 - val_loss: 1.7649 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2258 - acc: 0.8413 - val_loss: 1.7569 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9963 - acc: 0.9365 - val_loss: 1.7482 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1117 - acc: 0.8571 - val_loss: 1.7419 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0519 - acc: 0.9206 - val_loss: 1.7346 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0719 - acc: 0.9524 - val_loss: 1.7256 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0648 - acc: 0.8889 - val_loss: 1.7174 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0352 - acc: 0.9365 - val_loss: 1.7097 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0662 - acc: 0.9048 - val_loss: 1.7024 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1232 - acc: 0.8413 - val_loss: 1.6944 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0652 - acc: 0.8730 - val_loss: 1.6899 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9884 - acc: 0.8730 - val_loss: 1.6818 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9648 - acc: 0.8889 - val_loss: 1.6760 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0809 - acc: 0.8889 - val_loss: 1.6702 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9514 - acc: 0.9524 - val_loss: 1.6640 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9776 - acc: 0.8571 - val_loss: 1.6573 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9961 - acc: 0.8889 - val_loss: 1.6484 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9835 - acc: 0.9206 - val_loss: 1.6416 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9330 - acc: 0.8889 - val_loss: 1.6382 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0830 - acc: 0.8889 - val_loss: 1.6314 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8745 - acc: 0.9524 - val_loss: 1.6226 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8560 - acc: 0.9841 - val_loss: 1.6104 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9042 - acc: 0.9048 - val_loss: 1.6023 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9592 - acc: 0.8730 - val_loss: 1.5987 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9248 - acc: 0.8730 - val_loss: 1.5960 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8461 - acc: 0.9365 - val_loss: 1.5893 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8792 - acc: 0.9524 - val_loss: 1.5809 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8878 - acc: 0.9206 - val_loss: 1.5748 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8807 - acc: 0.9206 - val_loss: 1.5688 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7775 - acc: 0.9365 - val_loss: 1.5618 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9007 - acc: 0.9206 - val_loss: 1.5577 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9133 - acc: 0.9206 - val_loss: 1.5538 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9762 - acc: 0.8730 - val_loss: 1.5501 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9050 - acc: 0.9048 - val_loss: 1.5463 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8885 - acc: 0.8730 - val_loss: 1.5391 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9623 - acc: 0.8413 - val_loss: 1.5350 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8277 - acc: 0.9048 - val_loss: 1.5324 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7584 - acc: 0.9841 - val_loss: 1.5251 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9073 - acc: 0.8730 - val_loss: 1.5197 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8113 - acc: 0.9206 - val_loss: 1.5133 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8050 - acc: 0.9524 - val_loss: 1.5089 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8939 - acc: 0.8413 - val_loss: 1.5077 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7171 - acc: 0.9841 - val_loss: 1.5011 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8133 - acc: 0.9683 - val_loss: 1.4918 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7793 - acc: 0.9206 - val_loss: 1.4837 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8760 - acc: 0.8889 - val_loss: 1.4800 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8184 - acc: 0.8889 - val_loss: 1.4801 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8168 - acc: 0.8889 - val_loss: 1.4778 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8031 - acc: 0.9048 - val_loss: 1.4723 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7724 - acc: 0.9683 - val_loss: 1.4656 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8932 - acc: 0.8571 - val_loss: 1.4642 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8462 - acc: 0.9365 - val_loss: 1.4605 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7888 - acc: 0.8730 - val_loss: 1.4560 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8164 - acc: 0.8571 - val_loss: 1.4503 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6860 - acc: 0.9683 - val_loss: 1.4458 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6726 - acc: 0.9365 - val_loss: 1.4397 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8242 - acc: 0.8730 - val_loss: 1.4368 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7367 - acc: 0.9206 - val_loss: 1.4331 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6662 - acc: 0.9206 - val_loss: 1.4278 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6400 - acc: 0.9524 - val_loss: 1.4226 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6561 - acc: 0.9524 - val_loss: 1.4155 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6941 - acc: 0.9524 - val_loss: 1.4087 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7407 - acc: 0.8889 - val_loss: 1.4041 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5907 - acc: 0.9524 - val_loss: 1.4002 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7410 - acc: 0.9048 - val_loss: 1.3964 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6784 - acc: 0.9048 - val_loss: 1.3932 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6033 - acc: 0.9365 - val_loss: 1.3873 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7770 - acc: 0.8571 - val_loss: 1.3841 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5822 - acc: 0.9524 - val_loss: 1.3783 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7480 - acc: 0.9365 - val_loss: 1.3758 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5630 - acc: 0.9524 - val_loss: 1.3720 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5325 - acc: 0.9524 - val_loss: 1.3668 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5603 - acc: 0.9841 - val_loss: 1.3607 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7276 - acc: 0.8889 - val_loss: 1.3565 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5455 - acc: 0.9841 - val_loss: 1.3539 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7476 - acc: 0.8571 - val_loss: 1.3530 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5688 - acc: 0.9524 - val_loss: 1.3487 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6028 - acc: 0.9524 - val_loss: 1.3441 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6377 - acc: 0.9206 - val_loss: 1.3407 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7042 - acc: 0.8413 - val_loss: 1.3382 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5241 - acc: 0.9841 - val_loss: 1.3352 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5882 - acc: 0.9683 - val_loss: 1.3289 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5966 - acc: 0.9365 - val_loss: 1.3251 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7152 - acc: 0.9206 - val_loss: 1.3236 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5312 - acc: 0.9524 - val_loss: 1.3225 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5902 - acc: 0.9206 - val_loss: 1.3183 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6706 - acc: 0.9048 - val_loss: 1.3151 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6205 - acc: 0.9206 - val_loss: 1.3134 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6251 - acc: 0.9524 - val_loss: 1.3116 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5175 - acc: 0.9524 - val_loss: 1.3070 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5057 - acc: 0.9683 - val_loss: 1.3011 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5292 - acc: 0.9524 - val_loss: 1.2963 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4842 - acc: 0.9841 - val_loss: 1.2913 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5732 - acc: 0.9365 - val_loss: 1.2879 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6059 - acc: 0.9048 - val_loss: 1.2871 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5001 - acc: 0.9524 - val_loss: 1.2889 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5107 - acc: 0.9683 - val_loss: 1.2864 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5415 - acc: 0.9365 - val_loss: 1.2821 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5726 - acc: 0.9365 - val_loss: 1.2800 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6139 - acc: 0.9206 - val_loss: 1.2779 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5974 - acc: 0.8889 - val_loss: 1.2759 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4998 - acc: 0.9524 - val_loss: 1.2718 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5410 - acc: 0.9365 - val_loss: 1.2671 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5004 - acc: 0.9683 - val_loss: 1.2618 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5241 - acc: 0.9524 - val_loss: 1.2570 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5116 - acc: 0.9841 - val_loss: 1.2526 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4458 - acc: 0.9524 - val_loss: 1.2485 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4835 - acc: 0.9206 - val_loss: 1.2478 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5080 - acc: 0.9048 - val_loss: 1.2476 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4958 - acc: 0.9206 - val_loss: 1.2456 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5232 - acc: 0.9683 - val_loss: 1.2432 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4540 - acc: 0.9683 - val_loss: 1.2421 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4679 - acc: 0.9683 - val_loss: 1.2392 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4940 - acc: 0.9683 - val_loss: 1.2347 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4487 - acc: 0.9683 - val_loss: 1.2309 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4562 - acc: 0.9524 - val_loss: 1.2270 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5207 - acc: 0.8571 - val_loss: 1.2238 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4739 - acc: 0.9683 - val_loss: 1.2221 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4889 - acc: 0.9206 - val_loss: 1.2204 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5106 - acc: 0.8730 - val_loss: 1.2188 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4648 - acc: 0.9206 - val_loss: 1.2156 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4141 - acc: 0.9683 - val_loss: 1.2110 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4006 - acc: 0.9841 - val_loss: 1.2074 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4611 - acc: 0.9524 - val_loss: 1.2044 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4206 - acc: 0.9683 - val_loss: 1.2025 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5007 - acc: 0.9365 - val_loss: 1.2006 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4077 - acc: 0.9365 - val_loss: 1.1980 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5129 - acc: 0.9048 - val_loss: 1.1965 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4293 - acc: 0.9365 - val_loss: 1.1931 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4448 - acc: 0.9365 - val_loss: 1.1899 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4224 - acc: 0.9365 - val_loss: 1.1897 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5022 - acc: 0.9048 - val_loss: 1.1900 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4300 - acc: 0.9524 - val_loss: 1.1879 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3783 - acc: 0.9683 - val_loss: 1.1847 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3060 - acc: 0.9841 - val_loss: 1.1796 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5302 - acc: 0.8889 - val_loss: 1.1780 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3896 - acc: 0.9524 - val_loss: 1.1771 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4300 - acc: 0.9365 - val_loss: 1.1749 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3401 - acc: 0.9841 - val_loss: 1.1717 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3419 - acc: 0.9683 - val_loss: 1.1667 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3583 - acc: 0.9524 - val_loss: 1.1621 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3486 - acc: 0.9365 - val_loss: 1.1590 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3063 - acc: 0.9841 - val_loss: 1.1570 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4084 - acc: 0.9206 - val_loss: 1.1555 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3977 - acc: 0.9365 - val_loss: 1.1543 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4358 - acc: 0.9524 - val_loss: 1.1521 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3963 - acc: 0.9206 - val_loss: 1.1556 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3935 - acc: 0.9524 - val_loss: 1.1564 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4068 - acc: 0.9524 - val_loss: 1.1542 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4028 - acc: 0.9524 - val_loss: 1.1524 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3758 - acc: 0.9841 - val_loss: 1.1487 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4115 - acc: 0.9365 - val_loss: 1.1463 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2952 - acc: 0.9841 - val_loss: 1.1437 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3093 - acc: 0.9683 - val_loss: 1.1379 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3656 - acc: 0.9683 - val_loss: 1.1375 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4307 - acc: 0.9206 - val_loss: 1.1400 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4247 - acc: 0.9365 - val_loss: 1.1392 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3178 - acc: 0.9524 - val_loss: 1.1355 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3393 - acc: 0.9365 - val_loss: 1.1325 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3453 - acc: 0.9524 - val_loss: 1.1308 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3821 - acc: 0.9841 - val_loss: 1.1280 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2677 - acc: 1.0000 - val_loss: 1.1245 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3337 - acc: 0.9524 - val_loss: 1.1216 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3493 - acc: 0.9841 - val_loss: 1.1202 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2708 - acc: 0.9841 - val_loss: 1.1187 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3015 - acc: 0.9524 - val_loss: 1.1149 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2866 - acc: 0.9524 - val_loss: 1.1120 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4021 - acc: 0.9524 - val_loss: 1.1112 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3437 - acc: 0.9206 - val_loss: 1.1109 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3560 - acc: 0.9206 - val_loss: 1.1095 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3293 - acc: 0.9683 - val_loss: 1.1099 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2776 - acc: 1.0000 - val_loss: 1.1079 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2834 - acc: 0.9683 - val_loss: 1.1033 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3370 - acc: 0.9524 - val_loss: 1.1004 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3539 - acc: 0.9206 - val_loss: 1.0987 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2854 - acc: 0.9683 - val_loss: 1.0979 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3817 - acc: 0.9206 - val_loss: 1.0982 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2521 - acc: 0.9841 - val_loss: 1.0970 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2260 - acc: 0.9841 - val_loss: 1.0913 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2691 - acc: 0.9683 - val_loss: 1.0861 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2364 - acc: 0.9841 - val_loss: 1.0842 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3148 - acc: 0.9365 - val_loss: 1.0829 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3131 - acc: 0.9365 - val_loss: 1.0852 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2713 - acc: 0.9683 - val_loss: 1.0851 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2531 - acc: 0.9683 - val_loss: 1.0815 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2943 - acc: 0.9841 - val_loss: 1.0786 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3413 - acc: 0.9365 - val_loss: 1.0799 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2346 - acc: 0.9841 - val_loss: 1.0789 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2727 - acc: 0.9683 - val_loss: 1.0759 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2874 - acc: 0.9683 - val_loss: 1.0743 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2838 - acc: 1.0000 - val_loss: 1.0721 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3036 - acc: 0.9841 - val_loss: 1.0711 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2824 - acc: 0.9365 - val_loss: 1.0726 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2522 - acc: 0.9683 - val_loss: 1.0719 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2484 - acc: 0.9683 - val_loss: 1.0680 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2781 - acc: 0.9683 - val_loss: 1.0651 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2640 - acc: 0.9524 - val_loss: 1.0636 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2533 - acc: 0.9524 - val_loss: 1.0643 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2725 - acc: 0.9524 - val_loss: 1.0617 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3041 - acc: 0.9365 - val_loss: 1.0609 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2538 - acc: 0.9841 - val_loss: 1.0619 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2104 - acc: 0.9841 - val_loss: 1.0601 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2969 - acc: 0.9524 - val_loss: 1.0574 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3271 - acc: 0.9048 - val_loss: 1.0561 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2322 - acc: 0.9841 - val_loss: 1.0546 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2265 - acc: 0.9841 - val_loss: 1.0511 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2420 - acc: 0.9524 - val_loss: 1.0487 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1846 - acc: 0.9683 - val_loss: 1.0462 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2143 - acc: 1.0000 - val_loss: 1.0442 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2261 - acc: 1.0000 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2022 - acc: 0.9683 - val_loss: 1.0396 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1770 - acc: 0.9683 - val_loss: 1.0379 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2724 - acc: 0.9524 - val_loss: 1.0375 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2218 - acc: 0.9524 - val_loss: 1.0377 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2120 - acc: 0.9683 - val_loss: 1.0370 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2138 - acc: 0.9524 - val_loss: 1.0384 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2650 - acc: 0.9524 - val_loss: 1.0353 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3250 - acc: 0.9206 - val_loss: 1.0380 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1987 - acc: 0.9524 - val_loss: 1.0367 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2382 - acc: 0.9841 - val_loss: 1.0342 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2290 - acc: 0.9683 - val_loss: 1.0338 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2219 - acc: 0.9365 - val_loss: 1.0315 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2258 - acc: 1.0000 - val_loss: 1.0283 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1863 - acc: 0.9683 - val_loss: 1.0248 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2076 - acc: 0.9524 - val_loss: 1.0248 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2320 - acc: 0.9365 - val_loss: 1.0233 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1440 - acc: 0.9683 - val_loss: 1.0227 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2256 - acc: 0.9524 - val_loss: 1.0200 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1568 - acc: 0.9683 - val_loss: 1.0158 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1757 - acc: 0.9841 - val_loss: 1.0138 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1506 - acc: 0.9524 - val_loss: 1.0108 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2032 - acc: 0.9841 - val_loss: 1.0098 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1886 - acc: 0.9365 - val_loss: 1.0125 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2023 - acc: 0.9683 - val_loss: 1.0157 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1616 - acc: 0.9683 - val_loss: 1.0139 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1909 - acc: 0.9683 - val_loss: 1.0114 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2224 - acc: 0.9365 - val_loss: 1.0101 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2053 - acc: 0.9365 - val_loss: 1.0103 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1915 - acc: 0.9206 - val_loss: 1.0084 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2071 - acc: 0.9524 - val_loss: 1.0055 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2138 - acc: 0.9365 - val_loss: 1.0055 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1885 - acc: 0.9365 - val_loss: 1.0057 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1316 - acc: 0.9841 - val_loss: 1.0038 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1365 - acc: 0.9841 - val_loss: 1.0004 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2534 - acc: 0.9365 - val_loss: 1.0024 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1910 - acc: 0.9683 - val_loss: 1.0044 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2746 - acc: 0.9365 - val_loss: 1.0031 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2193 - acc: 0.9365 - val_loss: 1.0031 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1643 - acc: 0.9683 - val_loss: 1.0017 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1594 - acc: 0.9841 - val_loss: 0.9992 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2126 - acc: 0.9841 - val_loss: 0.9963 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2257 - acc: 0.9365 - val_loss: 0.9976 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1721 - acc: 0.9683 - val_loss: 0.9956 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1664 - acc: 0.9841 - val_loss: 0.9928 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2043 - acc: 0.9524 - val_loss: 0.9910 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0921 - acc: 1.0000 - val_loss: 0.9895 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1028 - acc: 0.9841 - val_loss: 0.9856 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0760 - acc: 0.9841 - val_loss: 0.9819 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0781 - acc: 0.9683 - val_loss: 0.9774 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1539 - acc: 0.9841 - val_loss: 0.9737 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1959 - acc: 0.9524 - val_loss: 0.9780 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1529 - acc: 0.9683 - val_loss: 0.9776 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1471 - acc: 0.9524 - val_loss: 0.9769 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2000 - acc: 0.9683 - val_loss: 0.9793 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1442 - acc: 0.9683 - val_loss: 0.9789 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1516 - acc: 0.9683 - val_loss: 0.9782 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1717 - acc: 0.9524 - val_loss: 0.9781 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1802 - acc: 0.9683 - val_loss: 0.9759 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1804 - acc: 0.9524 - val_loss: 0.9738 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1326 - acc: 0.9841 - val_loss: 0.9760 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1824 - acc: 0.9365 - val_loss: 0.9777 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2122 - acc: 0.9524 - val_loss: 0.9783 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2178 - acc: 0.9683 - val_loss: 0.9760 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1291 - acc: 0.9841 - val_loss: 0.9719 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1263 - acc: 0.9683 - val_loss: 0.9673 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1078 - acc: 0.9683 - val_loss: 0.9639 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1381 - acc: 0.9524 - val_loss: 0.9632 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1444 - acc: 0.9841 - val_loss: 0.9659 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1004 - acc: 0.9841 - val_loss: 0.9653 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1940 - acc: 0.9683 - val_loss: 0.9629 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1492 - acc: 1.0000 - val_loss: 0.9616 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1207 - acc: 0.9841 - val_loss: 0.9594 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0704 - acc: 1.0000 - val_loss: 0.9573 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1877 - acc: 0.9683 - val_loss: 0.9567 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1701 - acc: 0.9365 - val_loss: 0.9584 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0620 - acc: 0.9841 - val_loss: 0.9577 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1236 - acc: 0.9683 - val_loss: 0.9560 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1164 - acc: 0.9841 - val_loss: 0.9535 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1381 - acc: 0.9365 - val_loss: 0.9531 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0969 - acc: 0.9683 - val_loss: 0.9530 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1636 - acc: 0.9524 - val_loss: 0.9520 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1552 - acc: 0.9683 - val_loss: 0.9504 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1315 - acc: 0.9365 - val_loss: 0.9512 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0753 - acc: 0.9841 - val_loss: 0.9513 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1158 - acc: 0.9841 - val_loss: 0.9485 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1059 - acc: 1.0000 - val_loss: 0.9460 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0468 - acc: 0.9841 - val_loss: 0.9437 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1102 - acc: 0.9841 - val_loss: 0.9418 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1041 - acc: 0.9841 - val_loss: 0.9415 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0778 - acc: 0.9841 - val_loss: 0.9398 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2896 - acc: 0.8730 - val_loss: 0.9417 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1328 - acc: 0.9524 - val_loss: 0.9467 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0630 - acc: 1.0000 - val_loss: 0.9452 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1046 - acc: 0.9841 - val_loss: 0.9407 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0800 - acc: 0.9683 - val_loss: 0.9381 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1154 - acc: 0.9841 - val_loss: 0.9378 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0976 - acc: 0.9683 - val_loss: 0.9375 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1183 - acc: 0.9683 - val_loss: 0.9367 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1105 - acc: 0.9365 - val_loss: 0.9368 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0825 - acc: 1.0000 - val_loss: 0.9351 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0607 - acc: 1.0000 - val_loss: 0.9326 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1182 - acc: 0.9683 - val_loss: 0.9322 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1149 - acc: 0.9841 - val_loss: 0.9311 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0905 - acc: 0.9841 - val_loss: 0.9313 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0892 - acc: 0.9841 - val_loss: 0.9307 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1080 - acc: 0.9524 - val_loss: 0.9287 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0633 - acc: 0.9841 - val_loss: 0.9272 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0709 - acc: 0.9841 - val_loss: 0.9256 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0933 - acc: 0.9841 - val_loss: 0.9242 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0167 - acc: 0.9841 - val_loss: 0.9220 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0856 - acc: 0.9841 - val_loss: 0.9221 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1166 - acc: 0.9365 - val_loss: 0.9213 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1297 - acc: 0.9841 - val_loss: 0.9258 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0729 - acc: 0.9524 - val_loss: 0.9264 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1013 - acc: 0.9683 - val_loss: 0.9241 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1281 - acc: 0.9206 - val_loss: 0.9226 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1750 - acc: 0.9683 - val_loss: 0.9227 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1023 - acc: 0.9524 - val_loss: 0.9237 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1081 - acc: 0.9683 - val_loss: 0.9232 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0519 - acc: 0.9683 - val_loss: 0.9190 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1012 - acc: 0.9841 - val_loss: 0.9153 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0303 - acc: 0.9683 - val_loss: 0.9145 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1322 - acc: 0.9524 - val_loss: 0.9156 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0722 - acc: 0.9683 - val_loss: 0.9149 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1084 - acc: 0.9524 - val_loss: 0.9138 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1196 - acc: 0.9841 - val_loss: 0.9139 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1023 - acc: 0.9365 - val_loss: 0.9155 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0401 - acc: 0.9841 - val_loss: 0.9139 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0781 - acc: 0.9841 - val_loss: 0.9120 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0592 - acc: 0.9683 - val_loss: 0.9138 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9972 - acc: 1.0000 - val_loss: 0.9114 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1057 - acc: 0.9841 - val_loss: 0.9086 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1212 - acc: 0.9524 - val_loss: 0.9088 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0176 - acc: 1.0000 - val_loss: 0.9080 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0113 - acc: 1.0000 - val_loss: 0.9065 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0584 - acc: 0.9841 - val_loss: 0.9048 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0786 - acc: 0.9524 - val_loss: 0.9055 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0722 - acc: 1.0000 - val_loss: 0.9048 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1387 - acc: 0.9524 - val_loss: 0.9068 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1006 - acc: 0.9683 - val_loss: 0.9086 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0013 - acc: 1.0000 - val_loss: 0.9064 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1276 - acc: 0.9365 - val_loss: 0.9042 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0165 - acc: 0.9683 - val_loss: 0.9030 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0646 - acc: 0.9683 - val_loss: 0.8998 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9942 - acc: 1.0000 - val_loss: 0.8996 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0323 - acc: 0.9841 - val_loss: 0.8978 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9973 - acc: 1.0000 - val_loss: 0.8952 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9932 - acc: 0.9841 - val_loss: 0.8926 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9954 - acc: 1.0000 - val_loss: 0.8899 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9940 - acc: 1.0000 - val_loss: 0.8872 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0625 - acc: 0.9524 - val_loss: 0.8870 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0144 - acc: 1.0000 - val_loss: 0.8866 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9799 - acc: 0.9841 - val_loss: 0.8854 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0593 - acc: 0.9841 - val_loss: 0.8866 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0150 - acc: 0.9683 - val_loss: 0.8868 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0366 - acc: 0.9683 - val_loss: 0.8881 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1594 - acc: 0.9365 - val_loss: 0.8921 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0458 - acc: 0.9841 - val_loss: 0.8954 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0368 - acc: 0.9841 - val_loss: 0.8938 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu_ngrams Test Accuracy: 0.541\n",
      "my_model_neu Test f-measure: 0.505\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "neu_ng = Sequential()\n",
    "neu_ng.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "neu_ng.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "neu_ng.summary()\n",
    "history = neu_ng.fit(X_scaled_train_data_ngrams, y_train,\n",
    "                    validation_data=(X_scaled_val_data_ngrams, y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu_ngrams,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu_ngrams.h5')\n",
    "yhat = l_model.predict(scaled_test_data_ngrams)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_ngrams Test Accuracy: %.3f' % acc)\n",
    "\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_neu_ngrams Test Accuracy: 0.825\n"
     ]
    }
   ],
   "source": [
    "l_model = load_model('my_model_neu_ngrams.h5')\n",
    "yhat = l_model.predict(scaled_test_data_ngrams)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_ngrams Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 32)                242208    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 249,065\n",
      "Trainable params: 249,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 6.3812 - acc: 0.1587 - val_loss: 6.2892 - val_acc: 0.1257\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 6.2300 - acc: 0.1270 - val_loss: 6.1532 - val_acc: 0.1481\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 6.0931 - acc: 0.1746 - val_loss: 6.0331 - val_acc: 0.1653\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.9557 - acc: 0.1905 - val_loss: 5.9204 - val_acc: 0.1892\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.8800 - acc: 0.1270 - val_loss: 5.8129 - val_acc: 0.2011\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.7525 - acc: 0.2222 - val_loss: 5.7100 - val_acc: 0.2235\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.6652 - acc: 0.2063 - val_loss: 5.6105 - val_acc: 0.2381\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.5497 - acc: 0.1587 - val_loss: 5.5140 - val_acc: 0.2434\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.4572 - acc: 0.2222 - val_loss: 5.4210 - val_acc: 0.2712\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.3541 - acc: 0.2222 - val_loss: 5.3303 - val_acc: 0.2870\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.2644 - acc: 0.3016 - val_loss: 5.2415 - val_acc: 0.3082\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.2040 - acc: 0.3175 - val_loss: 5.1567 - val_acc: 0.3148\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.1239 - acc: 0.2222 - val_loss: 5.0759 - val_acc: 0.3280\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.0088 - acc: 0.3175 - val_loss: 4.9972 - val_acc: 0.3426\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.9702 - acc: 0.2381 - val_loss: 4.9204 - val_acc: 0.3730\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.8821 - acc: 0.2698 - val_loss: 4.8460 - val_acc: 0.4074\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.7838 - acc: 0.3651 - val_loss: 4.7720 - val_acc: 0.4537\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.7377 - acc: 0.3968 - val_loss: 4.6984 - val_acc: 0.4868\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.6797 - acc: 0.3810 - val_loss: 4.6293 - val_acc: 0.5648\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.6098 - acc: 0.4286 - val_loss: 4.5600 - val_acc: 0.5847\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.5746 - acc: 0.3016 - val_loss: 4.4944 - val_acc: 0.5886\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.4812 - acc: 0.3810 - val_loss: 4.4339 - val_acc: 0.6217\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.4526 - acc: 0.3333 - val_loss: 4.3718 - val_acc: 0.6243\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.4186 - acc: 0.3651 - val_loss: 4.3133 - val_acc: 0.6561\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.3238 - acc: 0.4286 - val_loss: 4.2552 - val_acc: 0.6918\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.3522 - acc: 0.3333 - val_loss: 4.1965 - val_acc: 0.6984\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.2768 - acc: 0.4286 - val_loss: 4.1406 - val_acc: 0.6958\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.1404 - acc: 0.4286 - val_loss: 4.0856 - val_acc: 0.7169\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.1900 - acc: 0.3810 - val_loss: 4.0357 - val_acc: 0.7222\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.1490 - acc: 0.4127 - val_loss: 3.9876 - val_acc: 0.7540\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.0445 - acc: 0.4921 - val_loss: 3.9379 - val_acc: 0.7778\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.0405 - acc: 0.5079 - val_loss: 3.8849 - val_acc: 0.7950\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.9659 - acc: 0.4603 - val_loss: 3.8327 - val_acc: 0.8029\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.8771 - acc: 0.5556 - val_loss: 3.7828 - val_acc: 0.8029\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.8767 - acc: 0.5079 - val_loss: 3.7317 - val_acc: 0.8148\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.8350 - acc: 0.4762 - val_loss: 3.6828 - val_acc: 0.8267\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.8531 - acc: 0.4921 - val_loss: 3.6353 - val_acc: 0.8360\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6999 - acc: 0.6190 - val_loss: 3.5862 - val_acc: 0.8426\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.7178 - acc: 0.4762 - val_loss: 3.5362 - val_acc: 0.8558\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6036 - acc: 0.6032 - val_loss: 3.4861 - val_acc: 0.8743\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6451 - acc: 0.5397 - val_loss: 3.4371 - val_acc: 0.8915\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6209 - acc: 0.5238 - val_loss: 3.3920 - val_acc: 0.9087\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4954 - acc: 0.5714 - val_loss: 3.3487 - val_acc: 0.9167\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5217 - acc: 0.6508 - val_loss: 3.3060 - val_acc: 0.9418\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5173 - acc: 0.6508 - val_loss: 3.2615 - val_acc: 0.9418\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3727 - acc: 0.5873 - val_loss: 3.2171 - val_acc: 0.9550\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4255 - acc: 0.5714 - val_loss: 3.1739 - val_acc: 0.9577\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3109 - acc: 0.6984 - val_loss: 3.1280 - val_acc: 0.9590\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3556 - acc: 0.5873 - val_loss: 3.0837 - val_acc: 0.9709\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.2516 - acc: 0.6984 - val_loss: 3.0418 - val_acc: 0.9881\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.2145 - acc: 0.7302 - val_loss: 2.9998 - val_acc: 0.9894\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1864 - acc: 0.7302 - val_loss: 2.9594 - val_acc: 0.9921\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.2113 - acc: 0.6508 - val_loss: 2.9209 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0813 - acc: 0.7143 - val_loss: 2.8810 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0898 - acc: 0.6984 - val_loss: 2.8392 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0507 - acc: 0.7778 - val_loss: 2.7978 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0675 - acc: 0.6984 - val_loss: 2.7601 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9905 - acc: 0.7460 - val_loss: 2.7220 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0247 - acc: 0.6508 - val_loss: 2.6860 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9581 - acc: 0.7302 - val_loss: 2.6449 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9181 - acc: 0.7302 - val_loss: 2.6085 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9163 - acc: 0.7302 - val_loss: 2.5730 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.8984 - acc: 0.7460 - val_loss: 2.5417 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9316 - acc: 0.6349 - val_loss: 2.5125 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.8689 - acc: 0.7302 - val_loss: 2.4855 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7199 - acc: 0.7937 - val_loss: 2.4551 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6875 - acc: 0.8254 - val_loss: 2.4198 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7226 - acc: 0.7619 - val_loss: 2.3851 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6886 - acc: 0.7302 - val_loss: 2.3514 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7195 - acc: 0.7302 - val_loss: 2.3232 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6730 - acc: 0.7619 - val_loss: 2.2969 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6427 - acc: 0.8413 - val_loss: 2.2680 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4816 - acc: 0.8571 - val_loss: 2.2383 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6056 - acc: 0.7460 - val_loss: 2.2081 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6372 - acc: 0.7143 - val_loss: 2.1827 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7046 - acc: 0.6349 - val_loss: 2.1613 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5736 - acc: 0.7619 - val_loss: 2.1394 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5108 - acc: 0.7619 - val_loss: 2.1126 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4291 - acc: 0.8254 - val_loss: 2.0905 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4595 - acc: 0.8095 - val_loss: 2.0658 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4616 - acc: 0.8571 - val_loss: 2.0443 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3127 - acc: 0.8889 - val_loss: 2.0230 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4457 - acc: 0.8254 - val_loss: 2.0034 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4444 - acc: 0.7619 - val_loss: 1.9857 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4244 - acc: 0.7778 - val_loss: 1.9673 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3396 - acc: 0.8730 - val_loss: 1.9472 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2557 - acc: 0.8730 - val_loss: 1.9275 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2777 - acc: 0.8730 - val_loss: 1.9068 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3070 - acc: 0.8413 - val_loss: 1.8888 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2128 - acc: 0.7937 - val_loss: 1.8705 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2582 - acc: 0.8889 - val_loss: 1.8517 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1510 - acc: 0.8730 - val_loss: 1.8359 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2056 - acc: 0.8571 - val_loss: 1.8183 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1661 - acc: 0.8571 - val_loss: 1.8022 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3047 - acc: 0.7937 - val_loss: 1.7891 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1558 - acc: 0.8571 - val_loss: 1.7755 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1221 - acc: 0.8571 - val_loss: 1.7591 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1987 - acc: 0.8254 - val_loss: 1.7438 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1174 - acc: 0.9048 - val_loss: 1.7313 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1160 - acc: 0.8095 - val_loss: 1.7214 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1117 - acc: 0.8571 - val_loss: 1.7104 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2288 - acc: 0.8254 - val_loss: 1.7042 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1537 - acc: 0.9048 - val_loss: 1.6932 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0719 - acc: 0.8571 - val_loss: 1.6824 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9534 - acc: 0.8889 - val_loss: 1.6704 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1870 - acc: 0.8095 - val_loss: 1.6583 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1195 - acc: 0.8095 - val_loss: 1.6471 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0506 - acc: 0.8571 - val_loss: 1.6373 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9353 - acc: 0.9048 - val_loss: 1.6280 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0016 - acc: 0.9048 - val_loss: 1.6159 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9770 - acc: 0.8730 - val_loss: 1.6092 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0027 - acc: 0.9048 - val_loss: 1.6009 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9632 - acc: 0.8889 - val_loss: 1.5919 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7649 - acc: 0.9841 - val_loss: 1.5811 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0395 - acc: 0.8889 - val_loss: 1.5721 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8822 - acc: 0.9206 - val_loss: 1.5631 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8948 - acc: 0.9048 - val_loss: 1.5547 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9825 - acc: 0.8413 - val_loss: 1.5488 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7809 - acc: 0.9365 - val_loss: 1.5407 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9768 - acc: 0.8095 - val_loss: 1.5323 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8572 - acc: 0.9206 - val_loss: 1.5257 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8212 - acc: 0.9365 - val_loss: 1.5194 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9169 - acc: 0.8413 - val_loss: 1.5127 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8607 - acc: 0.9206 - val_loss: 1.5090 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7718 - acc: 0.9048 - val_loss: 1.5025 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8499 - acc: 0.9524 - val_loss: 1.4938 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8649 - acc: 0.9206 - val_loss: 1.4885 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8429 - acc: 0.9206 - val_loss: 1.4820 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7624 - acc: 0.9048 - val_loss: 1.4732 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7433 - acc: 0.9683 - val_loss: 1.4642 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7819 - acc: 0.9365 - val_loss: 1.4558 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8108 - acc: 0.8889 - val_loss: 1.4508 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9090 - acc: 0.8413 - val_loss: 1.4478 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6939 - acc: 0.9524 - val_loss: 1.4416 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7681 - acc: 0.9365 - val_loss: 1.4354 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7828 - acc: 0.8889 - val_loss: 1.4294 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7718 - acc: 0.9365 - val_loss: 1.4223 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7981 - acc: 0.9048 - val_loss: 1.4176 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7193 - acc: 0.9524 - val_loss: 1.4157 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7504 - acc: 0.8889 - val_loss: 1.4111 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7894 - acc: 0.9365 - val_loss: 1.4077 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7725 - acc: 0.9048 - val_loss: 1.4046 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6570 - acc: 0.9683 - val_loss: 1.3963 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5973 - acc: 0.9841 - val_loss: 1.3871 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6894 - acc: 0.9206 - val_loss: 1.3805 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6538 - acc: 0.9206 - val_loss: 1.3755 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6761 - acc: 0.8730 - val_loss: 1.3721 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6727 - acc: 0.9365 - val_loss: 1.3674 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7383 - acc: 0.9048 - val_loss: 1.3649 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6509 - acc: 0.9365 - val_loss: 1.3621 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7162 - acc: 0.8889 - val_loss: 1.3579 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6865 - acc: 0.9365 - val_loss: 1.3558 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6619 - acc: 0.9524 - val_loss: 1.3496 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5483 - acc: 0.9841 - val_loss: 1.3418 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6814 - acc: 0.9365 - val_loss: 1.3372 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6070 - acc: 0.9365 - val_loss: 1.3324 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6172 - acc: 0.9365 - val_loss: 1.3292 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6701 - acc: 0.9683 - val_loss: 1.3276 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6036 - acc: 0.9365 - val_loss: 1.3256 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5673 - acc: 0.9524 - val_loss: 1.3216 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5453 - acc: 0.9683 - val_loss: 1.3158 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6605 - acc: 0.9206 - val_loss: 1.3140 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6155 - acc: 0.9365 - val_loss: 1.3119 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6868 - acc: 0.8730 - val_loss: 1.3091 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6397 - acc: 0.9206 - val_loss: 1.3058 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6347 - acc: 0.9048 - val_loss: 1.3020 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7394 - acc: 0.9048 - val_loss: 1.3004 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5890 - acc: 0.9206 - val_loss: 1.2992 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4833 - acc: 0.9841 - val_loss: 1.2931 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6121 - acc: 0.9365 - val_loss: 1.2867 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5227 - acc: 0.9524 - val_loss: 1.2821 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5879 - acc: 0.9524 - val_loss: 1.2814 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6188 - acc: 0.9206 - val_loss: 1.2806 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5151 - acc: 0.9841 - val_loss: 1.2780 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6073 - acc: 0.9206 - val_loss: 1.2713 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5370 - acc: 0.9524 - val_loss: 1.2658 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6500 - acc: 0.8889 - val_loss: 1.2643 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5996 - acc: 0.9048 - val_loss: 1.2639 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5757 - acc: 0.9206 - val_loss: 1.2634 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4950 - acc: 0.9524 - val_loss: 1.2602 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5230 - acc: 0.9683 - val_loss: 1.2531 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6072 - acc: 0.9365 - val_loss: 1.2506 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6173 - acc: 0.9048 - val_loss: 1.2503 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5563 - acc: 0.9048 - val_loss: 1.2521 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6545 - acc: 0.8730 - val_loss: 1.2534 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5228 - acc: 0.9365 - val_loss: 1.2526 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4829 - acc: 0.9841 - val_loss: 1.2471 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5742 - acc: 0.8571 - val_loss: 1.2415 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5366 - acc: 0.9683 - val_loss: 1.2379 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4614 - acc: 0.9365 - val_loss: 1.2339 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4704 - acc: 0.9365 - val_loss: 1.2322 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4456 - acc: 0.9683 - val_loss: 1.2266 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4903 - acc: 0.9365 - val_loss: 1.2198 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5008 - acc: 0.9206 - val_loss: 1.2154 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4149 - acc: 0.9841 - val_loss: 1.2118 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5062 - acc: 0.9683 - val_loss: 1.2100 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5161 - acc: 0.8889 - val_loss: 1.2111 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3997 - acc: 0.9683 - val_loss: 1.2072 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5275 - acc: 0.9048 - val_loss: 1.2044 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3776 - acc: 0.9841 - val_loss: 1.2010 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5675 - acc: 0.9206 - val_loss: 1.1989 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3931 - acc: 0.9524 - val_loss: 1.1959 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3699 - acc: 0.9683 - val_loss: 1.1899 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3879 - acc: 0.9683 - val_loss: 1.1860 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5233 - acc: 0.9683 - val_loss: 1.1847 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3790 - acc: 0.9683 - val_loss: 1.1831 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5434 - acc: 0.9206 - val_loss: 1.1846 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4668 - acc: 0.9206 - val_loss: 1.1842 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4448 - acc: 0.9683 - val_loss: 1.1835 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4653 - acc: 0.9206 - val_loss: 1.1833 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4957 - acc: 0.8889 - val_loss: 1.1809 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4191 - acc: 0.9206 - val_loss: 1.1785 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4285 - acc: 0.9683 - val_loss: 1.1734 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4193 - acc: 0.9206 - val_loss: 1.1707 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5326 - acc: 0.9206 - val_loss: 1.1708 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3925 - acc: 0.9683 - val_loss: 1.1748 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4258 - acc: 0.9206 - val_loss: 1.1723 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4726 - acc: 0.9206 - val_loss: 1.1665 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5135 - acc: 0.9524 - val_loss: 1.1654 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5131 - acc: 0.9048 - val_loss: 1.1671 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4009 - acc: 0.9683 - val_loss: 1.1670 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3391 - acc: 0.9683 - val_loss: 1.1617 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3859 - acc: 0.9683 - val_loss: 1.1542 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3982 - acc: 0.9524 - val_loss: 1.1492 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4811 - acc: 0.8889 - val_loss: 1.1507 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4212 - acc: 0.9524 - val_loss: 1.1524 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3762 - acc: 0.9365 - val_loss: 1.1527 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3577 - acc: 0.9365 - val_loss: 1.1479 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3571 - acc: 0.9683 - val_loss: 1.1439 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4286 - acc: 0.9365 - val_loss: 1.1411 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4398 - acc: 0.9524 - val_loss: 1.1401 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4523 - acc: 0.8889 - val_loss: 1.1419 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3602 - acc: 0.9524 - val_loss: 1.1412 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4178 - acc: 0.9365 - val_loss: 1.1401 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3123 - acc: 1.0000 - val_loss: 1.1353 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3766 - acc: 0.9683 - val_loss: 1.1292 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3698 - acc: 0.9524 - val_loss: 1.1249 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3027 - acc: 0.9841 - val_loss: 1.1221 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3357 - acc: 0.9841 - val_loss: 1.1206 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2989 - acc: 1.0000 - val_loss: 1.1186 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4350 - acc: 0.9048 - val_loss: 1.1195 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3218 - acc: 0.9524 - val_loss: 1.1212 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3425 - acc: 0.9683 - val_loss: 1.1217 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3734 - acc: 0.9206 - val_loss: 1.1214 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3609 - acc: 0.9683 - val_loss: 1.1198 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3060 - acc: 0.9683 - val_loss: 1.1152 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2773 - acc: 1.0000 - val_loss: 1.1114 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3859 - acc: 0.9524 - val_loss: 1.1099 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3026 - acc: 0.9841 - val_loss: 1.1107 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3402 - acc: 0.9365 - val_loss: 1.1094 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3049 - acc: 0.9365 - val_loss: 1.1054 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2981 - acc: 1.0000 - val_loss: 1.1007 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2877 - acc: 0.9841 - val_loss: 1.0979 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2841 - acc: 1.0000 - val_loss: 1.0976 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3834 - acc: 0.9365 - val_loss: 1.0986 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2457 - acc: 0.9841 - val_loss: 1.0970 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3149 - acc: 0.9841 - val_loss: 1.0946 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3573 - acc: 0.9206 - val_loss: 1.0944 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3775 - acc: 0.9206 - val_loss: 1.0977 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3204 - acc: 0.9683 - val_loss: 1.0957 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3393 - acc: 0.9206 - val_loss: 1.0931 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2812 - acc: 0.9841 - val_loss: 1.0909 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3192 - acc: 0.9524 - val_loss: 1.0887 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3525 - acc: 0.9048 - val_loss: 1.0861 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3039 - acc: 0.9683 - val_loss: 1.0865 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1962 - acc: 1.0000 - val_loss: 1.0839 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3757 - acc: 0.9524 - val_loss: 1.0845 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3107 - acc: 0.9524 - val_loss: 1.0822 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2632 - acc: 0.9841 - val_loss: 1.0788 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2509 - acc: 0.9524 - val_loss: 1.0749 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2481 - acc: 0.9683 - val_loss: 1.0727 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2255 - acc: 0.9683 - val_loss: 1.0706 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2615 - acc: 0.9524 - val_loss: 1.0688 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2219 - acc: 0.9683 - val_loss: 1.0692 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2592 - acc: 0.9683 - val_loss: 1.0670 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2407 - acc: 0.9841 - val_loss: 1.0640 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2873 - acc: 0.9524 - val_loss: 1.0633 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1804 - acc: 0.9841 - val_loss: 1.0628 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2803 - acc: 0.9683 - val_loss: 1.0622 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2886 - acc: 0.9841 - val_loss: 1.0615 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2575 - acc: 0.9365 - val_loss: 1.0616 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3101 - acc: 0.9524 - val_loss: 1.0603 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3062 - acc: 0.9683 - val_loss: 1.0609 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3069 - acc: 0.9841 - val_loss: 1.0632 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2432 - acc: 0.9524 - val_loss: 1.0623 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2615 - acc: 1.0000 - val_loss: 1.0601 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3047 - acc: 0.9524 - val_loss: 1.0610 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3268 - acc: 0.9206 - val_loss: 1.0616 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2186 - acc: 0.9841 - val_loss: 1.0583 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2119 - acc: 0.9841 - val_loss: 1.0555 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2608 - acc: 0.9841 - val_loss: 1.0527 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2878 - acc: 0.9841 - val_loss: 1.0510 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2125 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2568 - acc: 0.9524 - val_loss: 1.0487 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2601 - acc: 0.9524 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2051 - acc: 0.9841 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2124 - acc: 0.9841 - val_loss: 1.0417 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2371 - acc: 0.9683 - val_loss: 1.0403 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3445 - acc: 0.8889 - val_loss: 1.0428 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2699 - acc: 0.9524 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3196 - acc: 0.9365 - val_loss: 1.0437 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2690 - acc: 0.9683 - val_loss: 1.0431 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2054 - acc: 0.9841 - val_loss: 1.0420 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2210 - acc: 0.9365 - val_loss: 1.0404 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2445 - acc: 0.9841 - val_loss: 1.0389 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2434 - acc: 0.9683 - val_loss: 1.0367 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2206 - acc: 0.9524 - val_loss: 1.0366 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2767 - acc: 0.9683 - val_loss: 1.0371 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2067 - acc: 0.9683 - val_loss: 1.0351 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1627 - acc: 1.0000 - val_loss: 1.0302 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1869 - acc: 0.9841 - val_loss: 1.0250 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1598 - acc: 0.9841 - val_loss: 1.0225 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2306 - acc: 0.9524 - val_loss: 1.0225 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2803 - acc: 0.9683 - val_loss: 1.0252 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2280 - acc: 0.9365 - val_loss: 1.0258 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1519 - acc: 0.9841 - val_loss: 1.0244 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2444 - acc: 0.9841 - val_loss: 1.0224 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3522 - acc: 0.9365 - val_loss: 1.0235 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1720 - acc: 1.0000 - val_loss: 1.0250 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1400 - acc: 0.9841 - val_loss: 1.0221 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2813 - acc: 0.9206 - val_loss: 1.0204 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2421 - acc: 0.9683 - val_loss: 1.0228 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2664 - acc: 0.9683 - val_loss: 1.0248 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1991 - acc: 0.9841 - val_loss: 1.0248 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1806 - acc: 0.9841 - val_loss: 1.0196 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1598 - acc: 0.9841 - val_loss: 1.0138 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2076 - acc: 0.9524 - val_loss: 1.0099 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1732 - acc: 0.9841 - val_loss: 1.0073 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2554 - acc: 0.9365 - val_loss: 1.0109 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1711 - acc: 0.9683 - val_loss: 1.0125 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2543 - acc: 0.9524 - val_loss: 1.0120 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1620 - acc: 0.9841 - val_loss: 1.0119 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1892 - acc: 0.9365 - val_loss: 1.0103 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1966 - acc: 0.9841 - val_loss: 1.0082 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3265 - acc: 0.9048 - val_loss: 1.0099 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2391 - acc: 0.9841 - val_loss: 1.0121 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1448 - acc: 1.0000 - val_loss: 1.0117 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1718 - acc: 1.0000 - val_loss: 1.0072 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1538 - acc: 0.9683 - val_loss: 1.0030 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2124 - acc: 0.9524 - val_loss: 1.0033 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1507 - acc: 1.0000 - val_loss: 1.0021 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1270 - acc: 0.9841 - val_loss: 0.9978 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1947 - acc: 0.9683 - val_loss: 0.9967 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2424 - acc: 0.9524 - val_loss: 1.0003 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1681 - acc: 0.9524 - val_loss: 1.0006 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1467 - acc: 0.9841 - val_loss: 0.9990 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1625 - acc: 0.9841 - val_loss: 0.9991 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1859 - acc: 0.9841 - val_loss: 0.9970 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2098 - acc: 0.9683 - val_loss: 0.9948 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1191 - acc: 0.9841 - val_loss: 0.9896 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1808 - acc: 0.9683 - val_loss: 0.9872 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1792 - acc: 0.9524 - val_loss: 0.9893 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1608 - acc: 0.9683 - val_loss: 0.9885 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1397 - acc: 0.9841 - val_loss: 0.9864 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1604 - acc: 0.9841 - val_loss: 0.9849 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1511 - acc: 0.9683 - val_loss: 0.9840 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1903 - acc: 0.9365 - val_loss: 0.9811 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1311 - acc: 0.9683 - val_loss: 0.9817 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2147 - acc: 0.9524 - val_loss: 0.9827 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1041 - acc: 0.9683 - val_loss: 0.9809 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1647 - acc: 0.9683 - val_loss: 0.9817 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0998 - acc: 0.9841 - val_loss: 0.9784 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1753 - acc: 0.9524 - val_loss: 0.9744 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1748 - acc: 0.9524 - val_loss: 0.9745 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1474 - acc: 0.9683 - val_loss: 0.9762 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0929 - acc: 1.0000 - val_loss: 0.9752 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1250 - acc: 0.9524 - val_loss: 0.9744 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1397 - acc: 0.9365 - val_loss: 0.9732 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1865 - acc: 0.9524 - val_loss: 0.9747 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1389 - acc: 1.0000 - val_loss: 0.9752 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1799 - acc: 0.9365 - val_loss: 0.9729 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1682 - acc: 0.9524 - val_loss: 0.9734 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1488 - acc: 0.9524 - val_loss: 0.9726 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0797 - acc: 0.9683 - val_loss: 0.9708 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1204 - acc: 1.0000 - val_loss: 0.9672 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2314 - acc: 0.9365 - val_loss: 0.9701 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1593 - acc: 0.9683 - val_loss: 0.9750 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2505 - acc: 0.9048 - val_loss: 0.9776 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1593 - acc: 0.9524 - val_loss: 0.9778 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1181 - acc: 0.9841 - val_loss: 0.9757 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1330 - acc: 1.0000 - val_loss: 0.9726 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1775 - acc: 1.0000 - val_loss: 0.9696 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1271 - acc: 0.9524 - val_loss: 0.9685 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1781 - acc: 0.9683 - val_loss: 0.9658 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1536 - acc: 0.9524 - val_loss: 0.9646 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1855 - acc: 0.9683 - val_loss: 0.9650 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0446 - acc: 1.0000 - val_loss: 0.9632 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0509 - acc: 1.0000 - val_loss: 0.9573 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0721 - acc: 0.9683 - val_loss: 0.9507 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0923 - acc: 0.9841 - val_loss: 0.9473 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1288 - acc: 0.9524 - val_loss: 0.9466 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1204 - acc: 0.9841 - val_loss: 0.9516 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1233 - acc: 0.9524 - val_loss: 0.9517 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1482 - acc: 0.9683 - val_loss: 0.9534 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1912 - acc: 0.9206 - val_loss: 0.9589 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1082 - acc: 1.0000 - val_loss: 0.9609 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1605 - acc: 0.9683 - val_loss: 0.9623 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1529 - acc: 0.9683 - val_loss: 0.9628 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1618 - acc: 0.9683 - val_loss: 0.9620 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1777 - acc: 0.9524 - val_loss: 0.9593 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0975 - acc: 0.9841 - val_loss: 0.9577 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1256 - acc: 0.9524 - val_loss: 0.9573 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1128 - acc: 0.9683 - val_loss: 0.9554 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1695 - acc: 0.9841 - val_loss: 0.9534 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0735 - acc: 0.9841 - val_loss: 0.9520 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0609 - acc: 0.9841 - val_loss: 0.9468 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0520 - acc: 1.0000 - val_loss: 0.9406 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0647 - acc: 0.9841 - val_loss: 0.9370 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0952 - acc: 0.9683 - val_loss: 0.9366 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0791 - acc: 0.9841 - val_loss: 0.9372 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1093 - acc: 0.9683 - val_loss: 0.9373 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1084 - acc: 1.0000 - val_loss: 0.9391 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0684 - acc: 0.9841 - val_loss: 0.9388 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0218 - acc: 0.9841 - val_loss: 0.9347 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1252 - acc: 0.9365 - val_loss: 0.9338 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1176 - acc: 0.9683 - val_loss: 0.9369 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0337 - acc: 0.9841 - val_loss: 0.9367 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1261 - acc: 0.9524 - val_loss: 0.9377 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1094 - acc: 1.0000 - val_loss: 0.9378 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1091 - acc: 0.9365 - val_loss: 0.9381 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0590 - acc: 0.9841 - val_loss: 0.9375 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1015 - acc: 0.9683 - val_loss: 0.9360 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1602 - acc: 0.9524 - val_loss: 0.9344 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0797 - acc: 0.9683 - val_loss: 0.9355 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0459 - acc: 0.9841 - val_loss: 0.9339 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0779 - acc: 1.0000 - val_loss: 0.9322 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1029 - acc: 0.9683 - val_loss: 0.9310 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0215 - acc: 1.0000 - val_loss: 0.9291 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1450 - acc: 0.9206 - val_loss: 0.9280 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1157 - acc: 0.9524 - val_loss: 0.9320 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0723 - acc: 0.9683 - val_loss: 0.9321 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1493 - acc: 0.9841 - val_loss: 0.9311 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1099 - acc: 1.0000 - val_loss: 0.9314 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0448 - acc: 1.0000 - val_loss: 0.9298 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0971 - acc: 1.0000 - val_loss: 0.9273 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0868 - acc: 0.9683 - val_loss: 0.9257 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0639 - acc: 0.9683 - val_loss: 0.9252 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0638 - acc: 0.9841 - val_loss: 0.9242 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0536 - acc: 0.9683 - val_loss: 0.9219 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0962 - acc: 0.9365 - val_loss: 0.9214 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0498 - acc: 0.9683 - val_loss: 0.9201 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0073 - acc: 1.0000 - val_loss: 0.9157 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0977 - acc: 0.9683 - val_loss: 0.9146 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0876 - acc: 0.9841 - val_loss: 0.9150 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1304 - acc: 0.9206 - val_loss: 0.9204 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0593 - acc: 0.9841 - val_loss: 0.9252 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0779 - acc: 0.9683 - val_loss: 0.9234 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0618 - acc: 0.9524 - val_loss: 0.9218 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0782 - acc: 0.9841 - val_loss: 0.9195 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1284 - acc: 0.9524 - val_loss: 0.9232 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0233 - acc: 1.0000 - val_loss: 0.9236 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1005 - acc: 0.9683 - val_loss: 0.9212 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0764 - acc: 0.9683 - val_loss: 0.9205 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1038 - acc: 0.9841 - val_loss: 0.9195 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0673 - acc: 0.9841 - val_loss: 0.9207 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0732 - acc: 0.9841 - val_loss: 0.9192 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0945 - acc: 0.9683 - val_loss: 0.9169 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1602 - acc: 0.9365 - val_loss: 0.9191 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1222 - acc: 0.9206 - val_loss: 0.9210 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0733 - acc: 1.0000 - val_loss: 0.9220 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0843 - acc: 0.9841 - val_loss: 0.9205 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0825 - acc: 0.9683 - val_loss: 0.9155 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9909 - acc: 1.0000 - val_loss: 0.9110 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0651 - acc: 0.9841 - val_loss: 0.9087 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0908 - acc: 0.9683 - val_loss: 0.9094 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0644 - acc: 0.9683 - val_loss: 0.9084 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0892 - acc: 0.9841 - val_loss: 0.9085 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0512 - acc: 0.9683 - val_loss: 0.9077 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9976 - acc: 1.0000 - val_loss: 0.9035 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0604 - acc: 1.0000 - val_loss: 0.9002 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0263 - acc: 0.9841 - val_loss: 0.8981 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0020 - acc: 0.9841 - val_loss: 0.8952 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0882 - acc: 0.9841 - val_loss: 0.8957 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0375 - acc: 0.9841 - val_loss: 0.8970 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0264 - acc: 0.9841 - val_loss: 0.8965 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9872 - acc: 0.9841 - val_loss: 0.8957 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0450 - acc: 0.9841 - val_loss: 0.8930 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0197 - acc: 0.9524 - val_loss: 0.8933 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0638 - acc: 0.9683 - val_loss: 0.8938 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0719 - acc: 0.9841 - val_loss: 0.8977 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0975 - acc: 0.9524 - val_loss: 0.9001 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0411 - acc: 0.9683 - val_loss: 0.9013 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1210 - acc: 0.9683 - val_loss: 0.9018 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9768 - acc: 1.0000 - val_loss: 0.8998 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0639 - acc: 0.9524 - val_loss: 0.8945 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9823 - acc: 1.0000 - val_loss: 0.8956 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0270 - acc: 0.9841 - val_loss: 0.8945 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0232 - acc: 0.9841 - val_loss: 0.8912 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0151 - acc: 0.9683 - val_loss: 0.8886 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0026 - acc: 1.0000 - val_loss: 0.8872 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0060 - acc: 0.9683 - val_loss: 0.8833 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0106 - acc: 0.9524 - val_loss: 0.8834 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0196 - acc: 0.9841 - val_loss: 0.8812 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9766 - acc: 1.0000 - val_loss: 0.8814 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0333 - acc: 0.9841 - val_loss: 0.8830 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0146 - acc: 0.9683 - val_loss: 0.8831 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0268 - acc: 0.9841 - val_loss: 0.8827 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1466 - acc: 0.9365 - val_loss: 0.8873 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0313 - acc: 0.9683 - val_loss: 0.8930 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0568 - acc: 0.9841 - val_loss: 0.8959 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu_words Test Accuracy: 0.633\n",
      "my_model_neu Test f-measure: 0.599\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "neu_wo = Sequential()\n",
    "neu_wo.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "\n",
    "neu_wo.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "neu_wo.summary()\n",
    "history = neu_wo.fit(X_scaled_train_data_words, y_train,\n",
    "                    validation_data=(X_scaled_val_data_words, y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu_words,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu_words.h5')\n",
    "yhat = l_model.predict( scaled_test_data_words)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_words Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "n_grams (InputLayer)            (None, 4152)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "words (InputLayer)              (None, 7623)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 64)           139168      n_grams[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 64)           250240      words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           sequential_4[1][0]               \n",
      "                                                                 sequential_5[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          16512       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 128)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 9)            1161        dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 407,081\n",
      "Trainable params: 407,081\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(63, 4152) (63, 7623) (63, 9)\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 11.1390 - acc: 0.0794 - val_loss: 10.9800 - val_acc: 0.1164\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.8978 - acc: 0.1746 - val_loss: 10.7778 - val_acc: 0.1177\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.6965 - acc: 0.1270 - val_loss: 10.5908 - val_acc: 0.1270\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.5163 - acc: 0.0794 - val_loss: 10.4118 - val_acc: 0.1349\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.3314 - acc: 0.0952 - val_loss: 10.2393 - val_acc: 0.1402\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.1639 - acc: 0.1587 - val_loss: 10.0738 - val_acc: 0.1481\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.9866 - acc: 0.1587 - val_loss: 9.9132 - val_acc: 0.1468\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.8838 - acc: 0.1270 - val_loss: 9.7562 - val_acc: 0.1561\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.6910 - acc: 0.1905 - val_loss: 9.6028 - val_acc: 0.1759\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.5307 - acc: 0.2222 - val_loss: 9.4529 - val_acc: 0.1878\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.3954 - acc: 0.1587 - val_loss: 9.3079 - val_acc: 0.2050\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.2633 - acc: 0.1587 - val_loss: 9.1660 - val_acc: 0.2302\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.1173 - acc: 0.2063 - val_loss: 9.0282 - val_acc: 0.2685\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.0175 - acc: 0.1587 - val_loss: 8.8941 - val_acc: 0.2778\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.8679 - acc: 0.2063 - val_loss: 8.7645 - val_acc: 0.2778\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.7598 - acc: 0.1429 - val_loss: 8.6373 - val_acc: 0.2765\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.5959 - acc: 0.2381 - val_loss: 8.5123 - val_acc: 0.3082\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.4877 - acc: 0.1905 - val_loss: 8.3888 - val_acc: 0.3452\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.3533 - acc: 0.2857 - val_loss: 8.2668 - val_acc: 0.3836\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.2367 - acc: 0.2222 - val_loss: 8.1467 - val_acc: 0.4193\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.1527 - acc: 0.2063 - val_loss: 8.0279 - val_acc: 0.4683\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.9649 - acc: 0.3175 - val_loss: 7.9094 - val_acc: 0.5714\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.9489 - acc: 0.2063 - val_loss: 7.7960 - val_acc: 0.6376\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.8427 - acc: 0.3016 - val_loss: 7.6879 - val_acc: 0.6733\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.6817 - acc: 0.3968 - val_loss: 7.5827 - val_acc: 0.6958\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.6255 - acc: 0.2857 - val_loss: 7.4784 - val_acc: 0.7262\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.4745 - acc: 0.3333 - val_loss: 7.3624 - val_acc: 0.7262\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.4031 - acc: 0.3492 - val_loss: 7.2574 - val_acc: 0.7421\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.3071 - acc: 0.4444 - val_loss: 7.1495 - val_acc: 0.7989\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.1936 - acc: 0.4444 - val_loss: 7.0463 - val_acc: 0.8135\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.1566 - acc: 0.3333 - val_loss: 6.9502 - val_acc: 0.8730\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.0687 - acc: 0.4762 - val_loss: 6.8611 - val_acc: 0.9193\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.0233 - acc: 0.5079 - val_loss: 6.7664 - val_acc: 0.9325\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.8847 - acc: 0.3968 - val_loss: 6.6716 - val_acc: 0.9339\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.7909 - acc: 0.5079 - val_loss: 6.5701 - val_acc: 0.9312\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.7306 - acc: 0.4603 - val_loss: 6.4753 - val_acc: 0.9577\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.6389 - acc: 0.5714 - val_loss: 6.3863 - val_acc: 0.9683\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.6084 - acc: 0.5556 - val_loss: 6.3004 - val_acc: 0.9788\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.4585 - acc: 0.5873 - val_loss: 6.2096 - val_acc: 0.9881\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.3246 - acc: 0.6508 - val_loss: 6.1092 - val_acc: 0.9947\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.2782 - acc: 0.5873 - val_loss: 6.0177 - val_acc: 0.9868\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.2055 - acc: 0.6190 - val_loss: 5.9297 - val_acc: 0.9987\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.1875 - acc: 0.5873 - val_loss: 5.8436 - val_acc: 1.0000\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.0736 - acc: 0.6032 - val_loss: 5.7622 - val_acc: 1.0000\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.9016 - acc: 0.7302 - val_loss: 5.6802 - val_acc: 1.0000\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.0003 - acc: 0.5873 - val_loss: 5.5969 - val_acc: 0.9987\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.8926 - acc: 0.6190 - val_loss: 5.5219 - val_acc: 0.9987\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.8001 - acc: 0.7460 - val_loss: 5.4391 - val_acc: 1.0000\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.7757 - acc: 0.6508 - val_loss: 5.3660 - val_acc: 1.0000\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.8112 - acc: 0.5238 - val_loss: 5.3107 - val_acc: 1.0000\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.6218 - acc: 0.7460 - val_loss: 5.2513 - val_acc: 1.0000\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.5569 - acc: 0.7460 - val_loss: 5.1733 - val_acc: 1.0000\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.3974 - acc: 0.7619 - val_loss: 5.0956 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.5195 - acc: 0.6508 - val_loss: 5.0340 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.4593 - acc: 0.7302 - val_loss: 4.9689 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.3652 - acc: 0.6984 - val_loss: 4.9132 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.2807 - acc: 0.7460 - val_loss: 4.8577 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1312 - acc: 0.8571 - val_loss: 4.7933 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1703 - acc: 0.7937 - val_loss: 4.7321 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0950 - acc: 0.7937 - val_loss: 4.6855 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0033 - acc: 0.8095 - val_loss: 4.6282 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1629 - acc: 0.6508 - val_loss: 4.5810 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0019 - acc: 0.7619 - val_loss: 4.5478 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8673 - acc: 0.8413 - val_loss: 4.4927 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8123 - acc: 0.8571 - val_loss: 4.4365 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8856 - acc: 0.7460 - val_loss: 4.3872 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.7933 - acc: 0.7460 - val_loss: 4.3418 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.7625 - acc: 0.7778 - val_loss: 4.2990 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.6020 - acc: 0.9048 - val_loss: 4.2545 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.6051 - acc: 0.8571 - val_loss: 4.2148 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.4777 - acc: 0.8889 - val_loss: 4.1783 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5933 - acc: 0.8095 - val_loss: 4.1423 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5499 - acc: 0.8254 - val_loss: 4.1137 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3968 - acc: 0.9048 - val_loss: 4.0779 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5440 - acc: 0.8095 - val_loss: 4.0432 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.4125 - acc: 0.9048 - val_loss: 4.0139 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3563 - acc: 0.9048 - val_loss: 3.9870 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3778 - acc: 0.8889 - val_loss: 3.9570 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2695 - acc: 0.9365 - val_loss: 3.9262 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3704 - acc: 0.8730 - val_loss: 3.8972 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1920 - acc: 0.9048 - val_loss: 3.8734 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1442 - acc: 0.9365 - val_loss: 3.8442 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1614 - acc: 0.9365 - val_loss: 3.8113 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2103 - acc: 0.8254 - val_loss: 3.7831 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2099 - acc: 0.8413 - val_loss: 3.7623 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0524 - acc: 0.9365 - val_loss: 3.7381 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1891 - acc: 0.8254 - val_loss: 3.7236 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0965 - acc: 0.8889 - val_loss: 3.7112 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0073 - acc: 0.8889 - val_loss: 3.6928 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.9747 - acc: 0.9206 - val_loss: 3.6685 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0255 - acc: 0.9206 - val_loss: 3.6418 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8926 - acc: 0.9524 - val_loss: 3.6135 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0181 - acc: 0.8889 - val_loss: 3.5909 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8826 - acc: 0.9365 - val_loss: 3.5710 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.9053 - acc: 0.9524 - val_loss: 3.5495 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8480 - acc: 0.9048 - val_loss: 3.5311 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7472 - acc: 0.9524 - val_loss: 3.5104 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8056 - acc: 0.9206 - val_loss: 3.4967 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8680 - acc: 0.8571 - val_loss: 3.4858 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8543 - acc: 0.9048 - val_loss: 3.4753 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8278 - acc: 0.9365 - val_loss: 3.4570 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7670 - acc: 0.9365 - val_loss: 3.4342 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6562 - acc: 0.9841 - val_loss: 3.4117 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7184 - acc: 0.9365 - val_loss: 3.3907 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7107 - acc: 0.9365 - val_loss: 3.3760 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6607 - acc: 0.9206 - val_loss: 3.3603 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6666 - acc: 0.9206 - val_loss: 3.3448 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7002 - acc: 0.9048 - val_loss: 3.3325 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5492 - acc: 0.9841 - val_loss: 3.3135 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6265 - acc: 0.9048 - val_loss: 3.2962 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5689 - acc: 0.9683 - val_loss: 3.2819 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6233 - acc: 0.8730 - val_loss: 3.2677 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5707 - acc: 0.8889 - val_loss: 3.2577 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5497 - acc: 0.9206 - val_loss: 3.2445 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5194 - acc: 0.9365 - val_loss: 3.2330 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4056 - acc: 0.9841 - val_loss: 3.2189 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5197 - acc: 0.9206 - val_loss: 3.2006 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4061 - acc: 0.9683 - val_loss: 3.1880 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3392 - acc: 0.9841 - val_loss: 3.1688 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4472 - acc: 0.9683 - val_loss: 3.1531 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4603 - acc: 0.9048 - val_loss: 3.1422 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4462 - acc: 0.9365 - val_loss: 3.1364 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3180 - acc: 0.9841 - val_loss: 3.1249 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3175 - acc: 0.9683 - val_loss: 3.1103 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3979 - acc: 0.9524 - val_loss: 3.1000 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3761 - acc: 0.9524 - val_loss: 3.0910 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4303 - acc: 0.9206 - val_loss: 3.0775 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3228 - acc: 0.9524 - val_loss: 3.0635 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3528 - acc: 0.9365 - val_loss: 3.0531 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2608 - acc: 0.9524 - val_loss: 3.0424 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2827 - acc: 0.9524 - val_loss: 3.0306 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2730 - acc: 0.9206 - val_loss: 3.0168 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2212 - acc: 0.9841 - val_loss: 3.0025 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3885 - acc: 0.9048 - val_loss: 2.9973 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2393 - acc: 0.9524 - val_loss: 2.9937 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2323 - acc: 0.9683 - val_loss: 2.9833 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2439 - acc: 0.9524 - val_loss: 2.9703 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1519 - acc: 0.9841 - val_loss: 2.9577 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1106 - acc: 1.0000 - val_loss: 2.9407 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1422 - acc: 0.9683 - val_loss: 2.9258 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1427 - acc: 0.9683 - val_loss: 2.9174 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3213 - acc: 0.8571 - val_loss: 2.9157 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1618 - acc: 0.9841 - val_loss: 2.9140 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2285 - acc: 0.9524 - val_loss: 2.9110 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1531 - acc: 0.9683 - val_loss: 2.8991 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1358 - acc: 0.9524 - val_loss: 2.8854 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1452 - acc: 0.9365 - val_loss: 2.8741 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0833 - acc: 1.0000 - val_loss: 2.8634 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1121 - acc: 0.9365 - val_loss: 2.8534 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0129 - acc: 0.9683 - val_loss: 2.8391 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0660 - acc: 0.9841 - val_loss: 2.8267 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0262 - acc: 0.9841 - val_loss: 2.8177 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0268 - acc: 0.9365 - val_loss: 2.8069 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0196 - acc: 0.9524 - val_loss: 2.7977 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0094 - acc: 0.9524 - val_loss: 2.7875 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9798 - acc: 0.9841 - val_loss: 2.7782 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9450 - acc: 0.9683 - val_loss: 2.7690 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0173 - acc: 0.9524 - val_loss: 2.7658 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9474 - acc: 0.9841 - val_loss: 2.7576 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9107 - acc: 0.9841 - val_loss: 2.7457 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9112 - acc: 0.9683 - val_loss: 2.7361 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9785 - acc: 0.9365 - val_loss: 2.7328 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9742 - acc: 0.9524 - val_loss: 2.7284 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9650 - acc: 0.9524 - val_loss: 2.7231 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9504 - acc: 0.9683 - val_loss: 2.7184 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9234 - acc: 0.9841 - val_loss: 2.7075 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8798 - acc: 0.9683 - val_loss: 2.6979 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8441 - acc: 1.0000 - val_loss: 2.6877 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8782 - acc: 0.9683 - val_loss: 2.6755 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8266 - acc: 0.9841 - val_loss: 2.6679 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7671 - acc: 1.0000 - val_loss: 2.6560 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8336 - acc: 0.9524 - val_loss: 2.6484 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8297 - acc: 1.0000 - val_loss: 2.6423 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8156 - acc: 0.9841 - val_loss: 2.6334 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8115 - acc: 0.9524 - val_loss: 2.6283 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8097 - acc: 0.9524 - val_loss: 2.6239 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7681 - acc: 0.9683 - val_loss: 2.6117 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7529 - acc: 1.0000 - val_loss: 2.6017 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7397 - acc: 1.0000 - val_loss: 2.5941 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7753 - acc: 0.9524 - val_loss: 2.5862 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7149 - acc: 0.9841 - val_loss: 2.5805 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7733 - acc: 0.9683 - val_loss: 2.5762 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7438 - acc: 1.0000 - val_loss: 2.5683 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7136 - acc: 0.9683 - val_loss: 2.5612 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6855 - acc: 0.9841 - val_loss: 2.5549 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7333 - acc: 0.9683 - val_loss: 2.5474 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7649 - acc: 0.9524 - val_loss: 2.5420 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7732 - acc: 0.9206 - val_loss: 2.5410 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7493 - acc: 0.9841 - val_loss: 2.5394 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6675 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8093 - acc: 0.9683 - val_loss: 2.5275 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6614 - acc: 0.9841 - val_loss: 2.5230 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7242 - acc: 0.9524 - val_loss: 2.5182 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6602 - acc: 0.9683 - val_loss: 2.5101 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6857 - acc: 0.9683 - val_loss: 2.5034 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6767 - acc: 0.9524 - val_loss: 2.4956 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6390 - acc: 1.0000 - val_loss: 2.4845 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5797 - acc: 1.0000 - val_loss: 2.4730 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6214 - acc: 0.9841 - val_loss: 2.4647 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6064 - acc: 0.9841 - val_loss: 2.4601 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6153 - acc: 1.0000 - val_loss: 2.4504 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6800 - acc: 0.9206 - val_loss: 2.4489 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6219 - acc: 0.9841 - val_loss: 2.4488 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6293 - acc: 0.9683 - val_loss: 2.4437 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6410 - acc: 0.9841 - val_loss: 2.4369 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6038 - acc: 0.9841 - val_loss: 2.4342 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6185 - acc: 0.9683 - val_loss: 2.4305 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5474 - acc: 0.9683 - val_loss: 2.4197 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5588 - acc: 0.9841 - val_loss: 2.4078 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5978 - acc: 0.9683 - val_loss: 2.4026 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5470 - acc: 0.9524 - val_loss: 2.3984 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4990 - acc: 0.9841 - val_loss: 2.3898 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4985 - acc: 1.0000 - val_loss: 2.3790 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5775 - acc: 0.9683 - val_loss: 2.3750 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6027 - acc: 0.9206 - val_loss: 2.3803 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5180 - acc: 1.0000 - val_loss: 2.3791 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5253 - acc: 0.9683 - val_loss: 2.3674 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4768 - acc: 0.9841 - val_loss: 2.3549 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4605 - acc: 1.0000 - val_loss: 2.3440 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5200 - acc: 0.9524 - val_loss: 2.3359 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5321 - acc: 0.9841 - val_loss: 2.3390 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4669 - acc: 1.0000 - val_loss: 2.3342 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4863 - acc: 1.0000 - val_loss: 2.3268 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5060 - acc: 0.9841 - val_loss: 2.3203 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4725 - acc: 0.9524 - val_loss: 2.3208 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4509 - acc: 0.9841 - val_loss: 2.3135 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4544 - acc: 0.9841 - val_loss: 2.3102 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4354 - acc: 0.9841 - val_loss: 2.3029 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5309 - acc: 0.9683 - val_loss: 2.3054 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4361 - acc: 1.0000 - val_loss: 2.3062 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4104 - acc: 1.0000 - val_loss: 2.2980 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4147 - acc: 1.0000 - val_loss: 2.2870 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3494 - acc: 1.0000 - val_loss: 2.2734 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3726 - acc: 1.0000 - val_loss: 2.2645 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3849 - acc: 0.9683 - val_loss: 2.2529 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4672 - acc: 0.9365 - val_loss: 2.2551 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3911 - acc: 0.9841 - val_loss: 2.2554 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3609 - acc: 1.0000 - val_loss: 2.2470 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3423 - acc: 1.0000 - val_loss: 2.2346 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3485 - acc: 1.0000 - val_loss: 2.2253 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3577 - acc: 0.9841 - val_loss: 2.2183 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3332 - acc: 1.0000 - val_loss: 2.2181 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3526 - acc: 1.0000 - val_loss: 2.2189 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3576 - acc: 0.9841 - val_loss: 2.2220 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3430 - acc: 0.9683 - val_loss: 2.2187 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3751 - acc: 0.9683 - val_loss: 2.2125 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4137 - acc: 0.9841 - val_loss: 2.2119 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3431 - acc: 0.9841 - val_loss: 2.2093 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3299 - acc: 0.9841 - val_loss: 2.2060 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3058 - acc: 1.0000 - val_loss: 2.1953 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3804 - acc: 0.9841 - val_loss: 2.1867 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3195 - acc: 0.9841 - val_loss: 2.1875 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2840 - acc: 0.9841 - val_loss: 2.1812 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2819 - acc: 0.9841 - val_loss: 2.1736 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3006 - acc: 0.9841 - val_loss: 2.1668 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2780 - acc: 0.9841 - val_loss: 2.1644 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3036 - acc: 0.9683 - val_loss: 2.1603 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3089 - acc: 0.9841 - val_loss: 2.1578 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2611 - acc: 1.0000 - val_loss: 2.1540 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2778 - acc: 1.0000 - val_loss: 2.1449 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2831 - acc: 0.9841 - val_loss: 2.1403 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2971 - acc: 0.9841 - val_loss: 2.1405 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2458 - acc: 1.0000 - val_loss: 2.1373 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3268 - acc: 0.9365 - val_loss: 2.1367 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2831 - acc: 0.9683 - val_loss: 2.1335 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2524 - acc: 0.9841 - val_loss: 2.1270 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2359 - acc: 1.0000 - val_loss: 2.1224 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3023 - acc: 0.9683 - val_loss: 2.1221 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2306 - acc: 0.9841 - val_loss: 2.1218 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2681 - acc: 0.9841 - val_loss: 2.1217 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2154 - acc: 0.9841 - val_loss: 2.1157 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2607 - acc: 1.0000 - val_loss: 2.1070 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1875 - acc: 0.9683 - val_loss: 2.1033 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2687 - acc: 0.9683 - val_loss: 2.0974 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1892 - acc: 1.0000 - val_loss: 2.0941 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2184 - acc: 1.0000 - val_loss: 2.0867 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2280 - acc: 1.0000 - val_loss: 2.0858 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2326 - acc: 1.0000 - val_loss: 2.0845 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1809 - acc: 0.9841 - val_loss: 2.0816 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1955 - acc: 1.0000 - val_loss: 2.0758 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2455 - acc: 0.9365 - val_loss: 2.0743 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1880 - acc: 1.0000 - val_loss: 2.0658 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2074 - acc: 0.9841 - val_loss: 2.0578 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1391 - acc: 0.9841 - val_loss: 2.0483 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1238 - acc: 0.9841 - val_loss: 2.0355 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1899 - acc: 0.9683 - val_loss: 2.0367 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2469 - acc: 0.9524 - val_loss: 2.0437 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2299 - acc: 0.9524 - val_loss: 2.0490 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1585 - acc: 1.0000 - val_loss: 2.0473 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1207 - acc: 0.9841 - val_loss: 2.0382 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1479 - acc: 1.0000 - val_loss: 2.0265 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1412 - acc: 0.9841 - val_loss: 2.0144 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2079 - acc: 0.9524 - val_loss: 2.0155 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1833 - acc: 0.9683 - val_loss: 2.0181 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1918 - acc: 0.9683 - val_loss: 2.0168 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1151 - acc: 1.0000 - val_loss: 2.0164 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1217 - acc: 1.0000 - val_loss: 2.0092 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0691 - acc: 1.0000 - val_loss: 1.9992 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0969 - acc: 0.9841 - val_loss: 1.9867 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1294 - acc: 0.9683 - val_loss: 1.9837 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0860 - acc: 1.0000 - val_loss: 1.9891 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1806 - acc: 0.9841 - val_loss: 1.9929 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1073 - acc: 1.0000 - val_loss: 1.9940 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1073 - acc: 0.9841 - val_loss: 1.9878 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0736 - acc: 0.9841 - val_loss: 1.9772 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0359 - acc: 1.0000 - val_loss: 1.9666 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0740 - acc: 0.9841 - val_loss: 1.9636 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0953 - acc: 0.9683 - val_loss: 1.9609 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1138 - acc: 0.9683 - val_loss: 1.9603 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1141 - acc: 0.9683 - val_loss: 1.9617 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0662 - acc: 1.0000 - val_loss: 1.9557 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0516 - acc: 1.0000 - val_loss: 1.9483 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0701 - acc: 0.9841 - val_loss: 1.9394 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.1153 - acc: 0.9524 - val_loss: 1.9409 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 2.0723 - acc: 0.9841 - val_loss: 1.9387 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0352 - acc: 1.0000 - val_loss: 1.9386 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0573 - acc: 0.9841 - val_loss: 1.9368 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0583 - acc: 0.9683 - val_loss: 1.9360 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0257 - acc: 0.9841 - val_loss: 1.9323 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0611 - acc: 0.9841 - val_loss: 1.9251 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0186 - acc: 0.9841 - val_loss: 1.9174 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0298 - acc: 0.9841 - val_loss: 1.9139 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0360 - acc: 0.9841 - val_loss: 1.9092 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0087 - acc: 1.0000 - val_loss: 1.9064 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0174 - acc: 1.0000 - val_loss: 1.9013 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0031 - acc: 1.0000 - val_loss: 1.8982 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0310 - acc: 0.9841 - val_loss: 1.8939 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0278 - acc: 0.9841 - val_loss: 1.8916 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9821 - acc: 0.9841 - val_loss: 1.8913 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0161 - acc: 0.9841 - val_loss: 1.8890 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0731 - acc: 0.9524 - val_loss: 1.8916 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0354 - acc: 0.9841 - val_loss: 1.8979 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0580 - acc: 0.9683 - val_loss: 1.8956 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0806 - acc: 0.9524 - val_loss: 1.8975 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9748 - acc: 1.0000 - val_loss: 1.8943 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9551 - acc: 1.0000 - val_loss: 1.8818 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0633 - acc: 0.9683 - val_loss: 1.8761 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9737 - acc: 0.9841 - val_loss: 1.8762 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9667 - acc: 1.0000 - val_loss: 1.8674 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0154 - acc: 0.9524 - val_loss: 1.8615 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9636 - acc: 0.9841 - val_loss: 1.8634 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9677 - acc: 1.0000 - val_loss: 1.8588 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9638 - acc: 0.9841 - val_loss: 1.8534 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9304 - acc: 1.0000 - val_loss: 1.8450 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9842 - acc: 0.9841 - val_loss: 1.8434 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9892 - acc: 0.9841 - val_loss: 1.8463 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9518 - acc: 1.0000 - val_loss: 1.8442 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9563 - acc: 0.9841 - val_loss: 1.8392 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0060 - acc: 0.9524 - val_loss: 1.8353 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9587 - acc: 0.9841 - val_loss: 1.8385 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9751 - acc: 0.9841 - val_loss: 1.8353 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8960 - acc: 1.0000 - val_loss: 1.8291 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9600 - acc: 0.9524 - val_loss: 1.8211 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9373 - acc: 0.9841 - val_loss: 1.8167 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9317 - acc: 0.9841 - val_loss: 1.8117 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9248 - acc: 0.9841 - val_loss: 1.8124 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9327 - acc: 0.9524 - val_loss: 1.8111 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9583 - acc: 0.9841 - val_loss: 1.8108 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8907 - acc: 1.0000 - val_loss: 1.8085 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8895 - acc: 1.0000 - val_loss: 1.7977 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9315 - acc: 0.9524 - val_loss: 1.7918 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9214 - acc: 0.9841 - val_loss: 1.7926 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8863 - acc: 1.0000 - val_loss: 1.7868 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8984 - acc: 1.0000 - val_loss: 1.7814 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8785 - acc: 0.9841 - val_loss: 1.7824 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8856 - acc: 1.0000 - val_loss: 1.7788 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9085 - acc: 0.9841 - val_loss: 1.7759 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9396 - acc: 0.9524 - val_loss: 1.7740 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8911 - acc: 1.0000 - val_loss: 1.7723 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8644 - acc: 0.9683 - val_loss: 1.7711 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8619 - acc: 0.9841 - val_loss: 1.7632 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8857 - acc: 1.0000 - val_loss: 1.7617 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8731 - acc: 1.0000 - val_loss: 1.7592 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8627 - acc: 0.9841 - val_loss: 1.7565 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8732 - acc: 0.9524 - val_loss: 1.7509 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8178 - acc: 1.0000 - val_loss: 1.7471 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8227 - acc: 1.0000 - val_loss: 1.7371 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8178 - acc: 1.0000 - val_loss: 1.7289 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7948 - acc: 1.0000 - val_loss: 1.7217 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8451 - acc: 1.0000 - val_loss: 1.7197 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8047 - acc: 1.0000 - val_loss: 1.7186 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8032 - acc: 0.9841 - val_loss: 1.7124 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7959 - acc: 1.0000 - val_loss: 1.7091 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8231 - acc: 0.9841 - val_loss: 1.7129 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8015 - acc: 1.0000 - val_loss: 1.7071 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7898 - acc: 1.0000 - val_loss: 1.7028 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7904 - acc: 0.9841 - val_loss: 1.7022 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7605 - acc: 1.0000 - val_loss: 1.6989 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8183 - acc: 0.9683 - val_loss: 1.6957 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7928 - acc: 0.9841 - val_loss: 1.6987 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7832 - acc: 0.9841 - val_loss: 1.6977 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7649 - acc: 1.0000 - val_loss: 1.6928 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7480 - acc: 1.0000 - val_loss: 1.6836 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7770 - acc: 1.0000 - val_loss: 1.6771 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8034 - acc: 0.9683 - val_loss: 1.6753 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7450 - acc: 1.0000 - val_loss: 1.6771 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7658 - acc: 1.0000 - val_loss: 1.6725 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7855 - acc: 1.0000 - val_loss: 1.6678 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7444 - acc: 1.0000 - val_loss: 1.6622 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7464 - acc: 0.9841 - val_loss: 1.6576 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7971 - acc: 0.9841 - val_loss: 1.6603 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7967 - acc: 1.0000 - val_loss: 1.6630 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7802 - acc: 0.9841 - val_loss: 1.6608 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7514 - acc: 0.9841 - val_loss: 1.6612 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7517 - acc: 0.9841 - val_loss: 1.6539 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7958 - acc: 0.9683 - val_loss: 1.6495 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7433 - acc: 1.0000 - val_loss: 1.6565 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7994 - acc: 0.9841 - val_loss: 1.6591 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7787 - acc: 1.0000 - val_loss: 1.6588 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7449 - acc: 0.9841 - val_loss: 1.6578 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7433 - acc: 0.9841 - val_loss: 1.6509 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7295 - acc: 1.0000 - val_loss: 1.6442 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7081 - acc: 0.9841 - val_loss: 1.6348 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7563 - acc: 0.9683 - val_loss: 1.6339 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7729 - acc: 0.9683 - val_loss: 1.6354 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8153 - acc: 0.9683 - val_loss: 1.6489 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7731 - acc: 0.9841 - val_loss: 1.6599 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7565 - acc: 1.0000 - val_loss: 1.6556 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7485 - acc: 0.9841 - val_loss: 1.6476 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7308 - acc: 1.0000 - val_loss: 1.6441 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7268 - acc: 1.0000 - val_loss: 1.6335 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7921 - acc: 0.9524 - val_loss: 1.6295 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7172 - acc: 1.0000 - val_loss: 1.6308 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6992 - acc: 0.9841 - val_loss: 1.6246 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7569 - acc: 0.9683 - val_loss: 1.6220 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7067 - acc: 0.9841 - val_loss: 1.6201 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6834 - acc: 1.0000 - val_loss: 1.6128 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7013 - acc: 1.0000 - val_loss: 1.6067 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6979 - acc: 1.0000 - val_loss: 1.6010 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6508 - acc: 1.0000 - val_loss: 1.5934 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7307 - acc: 1.0000 - val_loss: 1.5930 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6976 - acc: 0.9841 - val_loss: 1.5957 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7052 - acc: 0.9841 - val_loss: 1.5961 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7140 - acc: 0.9841 - val_loss: 1.5929 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6978 - acc: 0.9841 - val_loss: 1.5924 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6729 - acc: 0.9841 - val_loss: 1.5897 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6425 - acc: 1.0000 - val_loss: 1.5813 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7417 - acc: 0.9524 - val_loss: 1.5868 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6895 - acc: 0.9841 - val_loss: 1.5880 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7011 - acc: 0.9841 - val_loss: 1.5809 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7235 - acc: 0.9683 - val_loss: 1.5779 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6401 - acc: 1.0000 - val_loss: 1.5725 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6484 - acc: 0.9683 - val_loss: 1.5607 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6510 - acc: 1.0000 - val_loss: 1.5576 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6485 - acc: 1.0000 - val_loss: 1.5569 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6334 - acc: 1.0000 - val_loss: 1.5519 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7070 - acc: 0.9683 - val_loss: 1.5565 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6413 - acc: 1.0000 - val_loss: 1.5570 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6677 - acc: 0.9841 - val_loss: 1.5532 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6577 - acc: 0.9841 - val_loss: 1.5559 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6478 - acc: 0.9841 - val_loss: 1.5576 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6298 - acc: 0.9683 - val_loss: 1.5532 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6422 - acc: 1.0000 - val_loss: 1.5455 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6136 - acc: 1.0000 - val_loss: 1.5356 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6729 - acc: 0.9683 - val_loss: 1.5386 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6146 - acc: 1.0000 - val_loss: 1.5363 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6570 - acc: 0.9841 - val_loss: 1.5390 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6371 - acc: 1.0000 - val_loss: 1.5360 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5979 - acc: 1.0000 - val_loss: 1.5271 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6252 - acc: 0.9683 - val_loss: 1.5246 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6407 - acc: 0.9841 - val_loss: 1.5220 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5840 - acc: 1.0000 - val_loss: 1.5132 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6182 - acc: 1.0000 - val_loss: 1.5100 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6238 - acc: 0.9683 - val_loss: 1.5123 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5906 - acc: 1.0000 - val_loss: 1.5157 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6366 - acc: 0.9841 - val_loss: 1.5149 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5966 - acc: 0.9841 - val_loss: 1.5158 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5989 - acc: 0.9841 - val_loss: 1.5105 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5543 - acc: 1.0000 - val_loss: 1.5024 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6128 - acc: 0.9841 - val_loss: 1.4967 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6123 - acc: 0.9841 - val_loss: 1.5008 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6094 - acc: 0.9683 - val_loss: 1.5033 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6227 - acc: 0.9841 - val_loss: 1.5006 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6180 - acc: 0.9841 - val_loss: 1.5003 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5876 - acc: 1.0000 - val_loss: 1.4981 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6300 - acc: 0.9841 - val_loss: 1.4975 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5670 - acc: 0.9841 - val_loss: 1.4932 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5867 - acc: 1.0000 - val_loss: 1.4915 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5671 - acc: 0.9683 - val_loss: 1.4854 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6690 - acc: 0.9683 - val_loss: 1.4931 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5771 - acc: 1.0000 - val_loss: 1.4964 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5757 - acc: 0.9841 - val_loss: 1.4951 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6089 - acc: 0.9683 - val_loss: 1.4903 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6145 - acc: 0.9841 - val_loss: 1.4847 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5966 - acc: 0.9683 - val_loss: 1.4797 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5708 - acc: 1.0000 - val_loss: 1.4763 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5571 - acc: 1.0000 - val_loss: 1.4686 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5599 - acc: 1.0000 - val_loss: 1.4637 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5666 - acc: 0.9841 - val_loss: 1.4599 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5914 - acc: 0.9683 - val_loss: 1.4651 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5666 - acc: 0.9841 - val_loss: 1.4689 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6172 - acc: 0.9841 - val_loss: 1.4694 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5577 - acc: 0.9841 - val_loss: 1.4639 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5710 - acc: 0.9841 - val_loss: 1.4558 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5981 - acc: 0.9841 - val_loss: 1.4532 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5455 - acc: 1.0000 - val_loss: 1.4526 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5022 - acc: 1.0000 - val_loss: 1.4437 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5159 - acc: 1.0000 - val_loss: 1.4370 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5721 - acc: 1.0000 - val_loss: 1.4387 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5480 - acc: 0.9841 - val_loss: 1.4406 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xt8VNW5//HPQ0AC4WrAo4IQby8VMEBMFQ8oKNSjVK1aq2LwbhHsxWp7KrXWqi2/2koVsYpVK/UUKvVo7QW1HqtYtO1BuSuiB1tBEdQQBeWiGHh+f+ydMISZyWRmMpnZ832/XvNiZu81e689Cc+sPGvttczdERGRwteurSsgIiLZoYAuIhIRCugiIhGhgC4iEhEK6CIiEaGALiISEQro0sjMSsxss5n1y2bZtmRmh5hZ1sfmmtkYM1sd8/p1MzsulbJpnOt+M7su3fcnOe6PzOxX2T6utJ32bV0BSZ+ZbY552Rn4FNgRvr7C3We35HjuvgPoku2yxcDdD8vGcczscmC8u4+KOfbl2Ti2RJ8CegFz98aAGrYAL3f3vyQqb2bt3b0+F3UTkdxTyiXCwj+pf2tmD5nZx8B4MzvWzP7XzDaa2Xozm25mHcLy7c3MzawifD0r3P+kmX1sZv8wswNbWjbcf4qZ/Z+ZbTKzO83sb2Z2cYJ6p1LHK8zsDTP70Mymx7y3xMxuN7M6M/sncHKSz+d6M5vTZNtdZnZb+PxyM1sZXs8/w9ZzomOtNbNR4fPOZvbrsG4rgKPinPdf4XFXmNnp4fYjgZ8Dx4XprA0xn+2NMe+fGF57nZn93sz2S+WzaY6ZnRHWZ6OZPWtmh8Xsu87M1pnZR2b2Wsy1DjOzxeH298zs1lTPJ63A3fWIwANYDYxpsu1HwHbgNIIv707A54BjCP46Owj4P+BrYfn2gAMV4etZwAagGugA/BaYlUbZfYCPgS+G+64BPgMuTnAtqdTxD0B3oAL4oOHaga8BK4C+QDkwP/g1j3ueg4DNQFnMsd8HqsPXp4VlDDgR2AZUhvvGAKtjjrUWGBU+nwo8B/QE+gOvNil7DrBf+DM5P6zDv4X7Lgeea1LPWcCN4fOTwjoOAUqBu4FnU/ls4lz/j4Bfhc+PCOtxYvgzui783DsAA4E1wL5h2QOBg8LnLwHjwuddgWPa+v9CMT/UQo++F9z9T+6+0923uftL7r7A3evd/V/AvcDIJO9/xN0XuvtnwGyCQNLSsqcCS939D+G+2wmCf1wp1vHH7r7J3VcTBM+Gc50D3O7ua929DrglyXn+BbxC8EUD8Hlgo7svDPf/yd3/5YFngWeAuB2fTZwD/MjdP3T3NQSt7tjzPuzu68OfyW8IvoyrUzguQA1wv7svdfdPgMnASDPrG1Mm0WeTzHnAH9392fBndAvQjeCLtZ7gy2NgmLZ7M/zsIPhiPtTMyt39Y3dfkOJ1SCtQQI++t2NfmNnhZva4mb1rZh8BNwO9krz/3ZjnW0neEZqo7P6x9XB3J2jRxpViHVM6F0HLMpnfAOPC5+cTfBE11ONUM1tgZh+Y2UaC1nGyz6rBfsnqYGYXm9myMLWxETg8xeNCcH2Nx3P3j4APgT4xZVryM0t03J0EP6M+7v468C2Cn8P7YQpv37DoJcAA4HUze9HMxqZ4HdIKFNCjr+mQvV8QtEoPcfduwA0EKYXWtJ4gBQKAmRm7B6CmMqnjeuCAmNfNDav8LTAmbOF+kSDAY2adgEeAHxOkQ3oA/5NiPd5NVAczOwiYAUwCysPjvhZz3OaGWK4jSOM0HK8rQWrnnRTq1ZLjtiP4mb0D4O6z3H04QbqlhOBzwd1fd/fzCNJqPwMeNbPSDOsiaVJALz5dgU3AFjM7ArgiB+ecC1SZ2Wlm1h64CujdSnV8GPimmfUxs3Lg2mSF3f094AVgJvC6u68Kd3UE9gJqgR1mdiowugV1uM7MelgwTv9rMfu6EATtWoLvtssJWugN3gP6NnQCx/EQcJmZVZpZR4LA+ry7J/yLpwV1Pt3MRoXn/k+Cfo8FZnaEmZ0Qnm9b+NhBcAEXmFmvsEW/Kby2nRnWRdKkgF58vgVcRPCf9RcELdRWFQbNc4HbgDrgYGAJwbj5bNdxBkGu+2WCDrtHUnjPbwg6OX8TU+eNwNXAYwQdi2cTfDGl4gcEfymsBp4E/ivmuMuB6cCLYZnDgdi889PAKuA9M4tNnTS8/88EqY/Hwvf3I8irZ8TdVxB85jMIvmxOBk4P8+kdgZ8S9Hu8S/AXwfXhW8cCKy0YRTUVONfdt2daH0mPBelMkdwxsxKCP/HPdvfn27o+IlGhFrrkhJmdbGbdwz/bv08wcuLFNq6WSKQooEuujAD+RfBn+8nAGe6eKOUiImlQykVEJCLUQhcRiYicTs7Vq1cvr6ioyOUpRUQK3qJFiza4e7KhvkCOA3pFRQULFy7M5SlFRAqemTV3xzOglIuISGQooIuIRIQCuohIRGjFIpEI++yzz1i7di2ffPJJW1dFUlBaWkrfvn3p0CHRVD7JKaCLRNjatWvp2rUrFRUVBJNcSr5yd+rq6li7di0HHnhg82+II+9TLrNnQ0UFtGsX/Du7RcseixS3Tz75hPLycgXzAmBmlJeXZ/TXVF630GfPhgkTYOvW4PWaNcFrgJqM55cTKQ4K5oUj059VXrfQv/e9XcG8wdatwXYREdldXgf0t95q2XYRyS91dXUMGTKEIUOGsO+++9KnT5/G19u3pzZt+iWXXMLrr7+etMxdd93F7CzlY0eMGMHSpUuzcqxcy+uUS79+QZol3nYRyb7Zs4O/gN96K/h/NmVKZunN8vLyxuB444030qVLF7797W/vVqZxxfp28duXM2fObPY8X/3qV9OvZITkdQt9yhTo3Hn3bWYwVsvQimRdQ5/VmjXgvqvPqjUGIrzxxhsMGjSIiRMnUlVVxfr165kwYQLV1dUMHDiQm2++ubFsQ4u5vr6eHj16MHnyZAYPHsyxxx7L+++/D8D111/PtGnTGstPnjyZo48+msMOO4y///3vAGzZsoUvfelLDB48mHHjxlFdXd1sS3zWrFkceeSRDBo0iOuuuw6A+vp6Lrjggsbt06dPB+D2229nwIABDB48mPHjx2f9M0tFXgf0mhq46KLdt7nDgw9qtItItuW6z+rVV1/lsssuY8mSJfTp04dbbrmFhQsXsmzZMp5++mleffXVPd6zadMmRo4cybJlyzj22GN54IEH4h7b3XnxxRe59dZbG78c7rzzTvbdd1+WLVvG5MmTWbJkSdL6rV27luuvv5558+axZMkS/va3vzF37lwWLVrEhg0bePnll3nllVe48MILAfjpT3/K0qVLWbZsGT//+c8z/HTSk9cBHeCJJ/bcpo5RkezLdZ/VwQcfzOc+97nG1w899BBVVVVUVVWxcuXKuAG9U6dOnHLKKQAcddRRrF69Ou6xzzrrrD3KvPDCC5x33nkADB48mIEDByat34IFCzjxxBPp1asXHTp04Pzzz2f+/PkccsghvP7661x11VU89dRTdO/eHYCBAwcyfvx4Zs+enfaNQZnK+4CujlGR3EjUN9VafVZlZWWNz1etWsUdd9zBs88+y/Llyzn55JPjjsfea6+9Gp+XlJRQX18f99gdO3bco0xLF/NJVL68vJzly5czYsQIpk+fzhVXXAHAU089xcSJE3nxxReprq5mx44dLTpfNuR9QM/1L5lIsYrXZ9W5c7C9tX300Ud07dqVbt26sX79ep566qmsn2PEiBE8/PDDALz88stx/wKINWzYMObNm0ddXR319fXMmTOHkSNHUltbi7vz5S9/mZtuuonFixezY8cO1q5dy4knnsitt95KbW0tW5vmr3Igr0e5QNABOmNG/O0ikj0No1myOcolVVVVVQwYMIBBgwZx0EEHMXz48Kyf4+tf/zoXXnghlZWVVFVVMWjQoMZ0STx9+/bl5ptvZtSoUbg7p512Gl/4whdYvHgxl112Ge6OmfGTn/yE+vp6zj//fD7++GN27tzJtddeS9euXbN+Dc3J6Zqi1dXV3tIFLioq4g9d7N8fEqTPRCS0cuVKjjjiiLauRl6or6+nvr6e0tJSVq1axUknncSqVato3z6/2rXxfmZmtsjdq5t7b35dSRzKoYtINmzevJnRo0dTX1+Pu/OLX/wi74J5pvL+ahLdXLT33rmvi4gUrh49erBo0aK2rkaryvtO0SlTIN4IoI8/1lh0EZFYeR/Qa2qgW7c9t2/frrHoIiKx8j6gA3zwQfztyqOLiOxSEAFdY9FFRJpXEAE90ZhzjUUXyW+jRo3a4yahadOmceWVVyZ9X5cuXQBYt24dZ599dsJjNzcMetq0abvd4DN27Fg2btyYStWTuvHGG5k6dWrGx8m2ggjo8eZzSbZdRPLDuHHjmDNnzm7b5syZw7hx41J6//77788jjzyS9vmbBvQnnniCHj16pH28fFcQAT1RrjzecEYRyR9nn302c+fO5dNPPwVg9erVrFu3jhEjRjSOC6+qquLII4/kD3/4wx7vX716NYMGDQJg27ZtnHfeeVRWVnLuueeybdu2xnKTJk1qnHr3Bz/4AQDTp09n3bp1nHDCCZxwwgkAVFRUsGHDBgBuu+02Bg0axKBBgxqn3l29ejVHHHEEX/nKVxg4cCAnnXTSbueJZ+nSpQwbNozKykrOPPNMPvzww8bzDxgwgMrKysZJwf761782LvAxdOhQPv7447Q/23jyfhw6JB6LbhYMXdT6oiLN++Y3IdsL8QwZAmEsjKu8vJyjjz6aP//5z3zxi19kzpw5nHvuuZgZpaWlPPbYY3Tr1o0NGzYwbNgwTj/99ITras6YMYPOnTuzfPlyli9fTlVVVeO+KVOmsPfee7Njxw5Gjx7N8uXL+cY3vsFtt93GvHnz6NWr127HWrRoETNnzmTBggW4O8cccwwjR46kZ8+erFq1ioceeoj77ruPc845h0cffTTp/OYXXnghd955JyNHjuSGG27gpptuYtq0adxyyy28+eabdOzYsTHNM3XqVO666y6GDx/O5s2bKS0tbcGn3byCaKFPmRIE76bcNXRRJN/Fpl1i0y3uznXXXUdlZSVjxozhnXfe4b333kt4nPnz5zcG1srKSiorKxv3Pfzww1RVVTF06FBWrFjR7MRbL7zwAmeeeSZlZWV06dKFs846i+effx6AAw88kCFDhgDJp+iFYH72jRs3MnLkSAAuuugi5s+f31jHmpoaZs2a1XhH6vDhw7nmmmuYPn06GzduzPqdqs0ezcweAE4F3nf3QeG2vYHfAhXAauAcd/8wqzWLUVMDib4gNXRRJDXJWtKt6YwzzuCaa65h8eLFbNu2rbFlPXv2bGpra1m0aBEdOnSgoqIi7pS5seK13t98802mTp3KSy+9RM+ePbn44oubPU6yOawapt6FYPrd5lIuiTz++OPMnz+fP/7xj/zwhz9kxYoVTJ48mS984Qs88cQTDBs2jL/85S8cfvjhaR0/nlRa6L8CTm6ybTLwjLsfCjwTvm5V/fvH364pAETyW5cuXRg1ahSXXnrpbp2hmzZtYp999qFDhw7MmzePNc10ih1//PGNC0G/8sorLF++HAim3i0rK6N79+689957PPnkk43v6dq1a9w89fHHH8/vf/97tm7dypYtW3jsscc47rjjWnxt3bt3p2fPno2t+1//+teMHDmSnTt38vbbb3PCCSfw05/+lI0bN7J582b++c9/cuSRR3LttddSXV3Na6+91uJzJtNsC93d55tZRZPNXwRGhc8fBJ4Drs1ivfYwZQpccgl89tnu2xumAFAeXSR/jRs3jrPOOmu3ES81NTWcdtppVFdXM2TIkGZbqpMmTeKSSy6hsrKSIUOGcPTRRwPB6kNDhw5l4MCBe0y9O2HCBE455RT2228/5s2b17i9qqqKiy++uPEYl19+OUOHDk2aXknkwQcfZOLEiWzdupWDDjqImTNnsmPHDsaPH8+mTZtwd66++mp69OjB97//febNm0dJSQkDBgxoXH0pW1KaPjcM6HNjUi4b3b1HzP4P3b1ngvdOACYA9OvX76jmvoWT6dUL6ur23K6pdEXi0/S5hSeT6XNbvVPU3e9192p3r+7du3dGx4oXzEHDF0VEIP2A/p6Z7QcQ/vt+9qqUWElJy7aLiBSTdAP6H4GLwucXAXveEdAKEq252gZrsYoUjFyuSiaZyfRn1WxAN7OHgH8Ah5nZWjO7DLgF+LyZrQI+H75udYlGupSX5+LsIoWntLSUuro6BfUC4O7U1dVldLNRKqNcEk26MDrts6ZJI11EWqZv376sXbuW2tratq6KpKC0tJS+ffum/f68XyS6KY10EZFikzejXLIt0WIXGukiIsWu4AJ6okUtGibqEhEpVgUX0DVRl4hIfAUX0GtqguAdjybqEpFiVnABHRIPU9REXSJSzAoyoCfSzIyZIiKRVpABPdFIly1b1DEqIsWrIAN6opEuoI5RESleBRnQp0xJvE8doyJSrAoyoNfUQFlZ/H3qGBWRYlWQAR0g0fw16hgVkWJVsAFdHaMiIrsr2ICujlERkd0VbEBXx6iIyO4KNqDX1OiOURGRWAUb0AHuuAPax1mio2HBCxGRYlLQAb2mBrp333P79u3Ko4tI8SnogA7xVy8CLXghIsWn4AN6SUnLtouIRFXBB/QdO1q2XUQkqgo+oPfvH3+7lqQTkWJT8AFdS9KJiAQKPqAnW5JOHaMiUkwKPqCDOkZFRCAiAV0doyIiEQno6hgVEYlIQE/WMXrVVbmvj4hIW4hEQE/WMVpXp1a6iBSHSAR0SJx2AQ1fFJHiEJmAnmx+dA1fFJFiEJmAXlMD7RJcjYYvikgxyCigm9nVZrbCzF4xs4fMLMHSzbmxc2f87Rq+KCLFIO2AbmZ9gG8A1e4+CCgBzstWxdKRKI9eVpbbeoiItIVMUy7tgU5m1h7oDKzLvErpmzIlftplyxa48src10dEJJfSDuju/g4wFXgLWA9scvf/aVrOzCaY2UIzW1hbW5t+TVNQU5N43z33tOqpRUTaXCYpl57AF4EDgf2BMjMb37Scu9/r7tXuXt27d+/0a5qiRHl0d41HF5FoyyTlMgZ4091r3f0z4HfAv2enWulLNqJF49FFJMoyCehvAcPMrLOZGTAaWJmdaqVvwoTE+956K3f1EBHJtUxy6AuAR4DFwMvhse7NUr3Sdvfd0KVL/H17753buoiI5FJGo1zc/Qfufri7D3L3C9z902xVLBP33APt2++5feNG5dFFJLoic6dorJoa6Nhxz+07dmj2RRGJrkgGdAjGnsdTV5fbeoiI5EpkA7qISLGJbEAvL0+8T3eNikgURTag33FH4n333KPOURGJnsgG9GTTALjrJiMRiZ7IBnRIvoqRFr0QkaiJdEBPtoqRFr0QkaiJdEBPlnbRohciEjWRDuiQPO2i0S4iEiWRD+jJ0i6aI11EoiTyAb250S5qpYtIVEQ+oEPyDlCNSReRqCiKgJ5sjnSNSReRqCiKgJ5sjnTQwhciEg1FEdAheQeoFr4QkSgomoBeUwOdO8ff98knua2LiEhrKJqADrBtW/ztW7aoY1RECl9RBfR+/RLv00pGIlLoiiqgJ7vJSCsZiUihK6qAnuwmI9BNRiJS2IoqoEPylYx0k5GIFLKiC+jJVjLSTUYiUsiKLqDX1CQfd66bjESkUBVdQAeYPj3xPt1kJCKFqigDek0NdOoUf59uMhKRQlWUAR10k5GIRE/RBvRkKxnpJiMRKURFG9Cbu8lIrXQRKTRFG9Cbu8no0ksV1EWksBRtQIfkNxlt364x6SJSWIo6oCe7yQhgzZrc1ENEJBsyCuhm1sPMHjGz18xspZkdm62K5UJzNxmZ5a4uIiKZyrSFfgfwZ3c/HBgMrMy8SrmV7CYjd+XRRaRwpB3QzawbcDzwSwB33+7uG7NVsVypqUm+3qiGMIpIocikhX4QUAvMNLMlZna/mZU1LWRmE8xsoZktrK2tzeB0ree22xLv0xBGESkUmQT09kAVMMPdhwJbgMlNC7n7ve5e7e7VvXv3zuB0recrX0m+/8ILFdRFJP9lEtDXAmvdfUH4+hGCAB85O3dqXLqI5L+0A7q7vwu8bWaHhZtGA69mpVZt4IADku/XuHQRyXeZjnL5OjDbzJYDQ4D/l3mV2saPf9z8MEXNlS4i+ax9Jm9296VAdZbq0qZqauCVV+CWWxKX0VzpIpLPivpO0aZ+9KPkrfSNG5VHF5H8pYAeo6QETjkl8f4dO+CKK3JXHxGRllBAbyJZygWCBTCuvDI3dRERaQkF9CaOPBJGjEhe5p57lHoRkfyjgB7HV7+afL+7pgQQkfyjgB7HWWdB9+7Jy2hKABHJNwrocey1F3zjG82X041GIpJPFNATSGU0ixbAEJF8ooCeQJ8+MGxY8+UqKpR6EZH8oICexP33N19mzRqYMEFBXUTangJ6EgMHwvDh0L6ZCRK2blU+XUTangJ6My69FOrrmy+nfLqItDUF9GZ8+cvQtWtqZZV2EZG2pIDejK5d4etfT62s0i4i0pYU0FNw9dXQsWPz5dasUStdRNqOAnoKevUKbvU3a34RDI14EZG2ooCeom99Czp1gn//9+Tltm7VPC8i0jYU0FO0zz4waRL84x/Nl9U8LyLSFhTQW+Db3w7meSkra76sOkhFJNcU0Ftg331h4kTYtq35shqXLiK5poDeQt/9LvToAfvt13zZTp2UehGR3FFAb6F99gmWqVu/Hk46KXnZTz6B8eO1ZJ2I5IYCehouvRQGD4aVK+Huu5svryXrRCQXFNDTUFIC994LGzbAE08Er5PRknUikgsK6Gk6+mj4wQ9g7lwYO7b58nV1wTQCaqmLSGtRQM/AVVdB//7w9tupld+8OUjXKKiLSGtQQM9AaWnQQbp0Key9d2rv2b5dY9RFpHUooGfo3HPhxBNhy5YgwKdizZpg0QyNfhGRbFJAz5AZzJwZBPMDDoB2KX6iO3bAjBkwZkzr1k9EiocCehb06wc//zmsWgVf+hJ06JD6e595Rjl1EckOBfQsqamBc86BRx+Fr30NystTf6+GNIpINiigZ4kZPPAAHHVUcCPRn/4Es2al9t66uuD9vXqptS4i6cs4oJtZiZktMbO52ahQISsrC8al778/nHpqENxboq5OwxpFJH3ZaKFfBazMwnEiYZ994KmnglEs//EfwUReLaFhjSKSrowCupn1Bb4A3J+d6kTDwQfDk0/CBx8Eo19a0kkKwbDGdu2gokKtdRFJXaYt9GnAd4CdiQqY2QQzW2hmC2trazM8XeGoqoI//zkYn96zJ/Tp07L3uweBXbM1ikiq0g7oZnYq8L67L0pWzt3vdfdqd6/u3bt3uqcrSMOHw9NPw6efBi3uqVOhc+eWH2fGDAV1EWleJi304cDpZrYamAOcaGYpjusoHsccA/PmBascTZ0KN9yQ2hJ2TSmoi0hz0g7o7v5dd+/r7hXAecCz7j4+azWLkKFD4a9/DYYm3norPP54sOB0S82YEUzVq8AuIvFoHHqODBgAzz8fjHoZNSoIzHfdlfpUAQ127gwCu8ati0hT7bNxEHd/DnguG8eKsoMPhmXL4Lrr4M47g/nRJ06E++8Phiu2VF0dXHRR8LymJrt1FZHCoxZ6jpWVwR13wKuvwqBBwRJ2V1+dXmcpBJN8jR+v1rqIKKC3mcMPD/LqEyfCT34SpGTaZ/D3ku4yFREF9DbUvn3QQv/Zz4JUTMeOLZvUq6nt24PWemlp0GLXzUkixUUBvY2ZwTXXBAG9qipoaR9yCOy1V/rH/PTT4DgNNyddcokCvEgxUEDPE0ccAc89B/fdFwTe7dszC+qxPvts9wA/YYKCukgUKaDnkXbt4PLL4bXX4De/CTpKu3QJRrJ06pS982zdGszBXlGhVrtIlCig5yEzGDcuSMN87nPw4INBYM8kv95UXV3QWm9otV9wgW5YEil0Cuh5rF+/YIm6J58MntfVwaGHZjewN3APFuZQS12kcCmg5zkzOPlkeOkleOyxYARLXV3rnMs9aKkrqIsUJgX0AmEGZ5wBS5fC734Hgwe3znncg6GPY8a0zvFFpPUooBeYdu3gzDODwL5qFRx3XOuc55lngqkJ1FoXKRwK6AXskENg/vyg83To0N33ZWPI4+bNQWvdTJOBiRQCBfQIqKyExYvhnXfgllvgsMOCcewdO2Y2nUBTdXW7B3i14EXyiwJ6hOy/P1x7LaxcCX//O1x44a7x69kM7A0aWvAa7iiSHxTQI8gMjj0W7r0X3n03GMc+bFjLF6tOleZnF8kPCugR17lz0FJ//vmgRf2f/xkMfWzQpUswh0w2cu6xKZlE6ZjZs3WHqkhrMXfP2cmqq6t94cKFOTufJLZiBfz3f0NtbZB//9//bZt6dO4c/CWhBTpEEjOzRe5e3Vy5VsisSiEYODB4NFi5Er7znWC90xx+x7N1a/AXBCioi2RKKRcBgtke//SnYHTM44/D2We3TkdqPDt37prHvUsXDZMUSZcCuuymfXsYOzZIx9TVwUMPwQknBItat7ZPP4UtW3a9bsjJt2u3K8BrXneRxBTQJaFu3eC88+DZZ4M51VesgN/+Fr70pSCo5kpDCqiuTvO6iySjgC4pMQvWPT3nHHjkkWBx6gsuaNs6bd26552sasFLMVNAl7T913/BrFnQv3/wulu31hvrnoqmLXgtvSfFRgFdMlJTA6tXB0F006agU7VhbvV99mnbujVdei92EQ+Nh5co0jh0yZm774Yf/jC4ezVf9e8PU6ZoCKXkl1THoauFLjlz5ZWwfn3QYnYP0jXZXCs1G9as2T0vH/vo0kUpHMlvCujSZmpq4L77glaxWfDvAw/AkiUwaVL+BfstW3ZP4cQOqayoCL6wGtI46qCVtqCUi+S9CRPgl78MbkAqZF26BF8K/foprSMto5SLRMa99wbDJBvSNK2xSHYubN68e+t+zBh1zkp2KaBLQampgQ0bduXhYx+zZkFZWVvXMHXPPBME9jVrdg/0paW78vYlJbtSOgr20hwFdImMmppdreBJk4JAWIg+/XTX84Y0U9PO2tgA39DKj9eRqy+C4pIWleehAAAHAElEQVR2QDezA8xsnpmtNLMVZnZVNismkom77w6CYdMWfGwH7KRJhZu+iQ3wDa38ZOUaJjpTiifa0u4UNbP9gP3cfbGZdQUWAWe4+6uJ3qNOUclXV165K1dfjMrKglTPBx+o0zYftXqnqLuvd/fF4fOPgZVAn3SPJ9KW7r4b6uvj5+ZjW/dRFW9Iptnu+XxNZ5z/spJDN7MKYCiwIM6+CWa20MwW1tbWZuN0IjkXO8VBvNE2ZWWF1SGbqth8fuwSgw0dtlogPL9kHNDNrAvwKPBNd/+o6X53v9fdq929unfv3pmeTiQvNB1ts3nz7h2yDfPHN9xhCrmZUz6Xdu7ctUB47KNdu2BN2XRvsFKeP30Z3VhkZh2AucBT7n5bc+WVQxcJAtT3vhekNkpKijdvH89eewUTvDVVWgqffLLr8yq2OXdaPYduZgb8EliZSjAXkUBs+qYhb990BM6sWXvm7gt1GGZLxAvmEARz2PXlF2/OnXbt9sz5x07HUBStfXdP6wGMABxYDiwNH2OTveeoo45yEcmeWbPc+/d3Nwv+HT3avaQkUdeuHuDesaN7Wdmu1+XlweeYzued6vsyBSz0FOKy5nIRibiGFM9bbwVDEseOhQcfDFZ8kuwoL4c77giex37W2UoLpZpyUUAXKULxgvzDDwcjWSS7SkuD0UKZBHgFdBFpFVdeGYxukZbr0AFmzmx5UNdsiyLSKu6+e88O26gNyWwtn30GV7XiJCkK6CKSlnijdRLdXRsv4CcbtWMGo0dH82at1kxrKaCLSKtoLuA3nTyt6b6//GXXzVpRn3ohWxTQRSTvxZt6oenMmQ2vy8uDG5TyVWvO8Nm+9Q4tItI6amoyHw6Y6h27JSXBSJUtWzI7X4OG4Y2tQS10ESlKiVJCTVv/Dz64K/WT6mPSpPjnnDSpdacr0LBFEZFW0HSsfyY3GaU6bFEpFxGRVpCNtFBLKeUiIhIRCugiIhGhgC4iEhEK6CIiEaGALiISEQroIiIRkdNx6GZWC6xJ4629gA1Zrk6+0zUXB11zccj0mvu7e+/mCuU0oKfLzBamMqg+SnTNxUHXXBxydc1KuYiIRIQCuohIRBRKQL+3rSvQBnTNxUHXXBxycs0FkUMXEZHmFUoLXUREmqGALiISEXkf0M3sZDN73czeMLPJbV2fbDGzB8zsfTN7JWbb3mb2tJmtCv/tGW43M5sefgbLzayq7WqePjM7wMzmmdlKM1thZleF2yN73WZWamYvmtmy8JpvCrcfaGYLwmv+rZntFW7vGL5+I9xf0Zb1T5eZlZjZEjObG76O9PUCmNlqM3vZzJaa2cJwW05/t/M6oJtZCXAXcAowABhnZgPatlZZ8yvg5CbbJgPPuPuhwDPhawiu/9DwMQGYkaM6Zls98C13PwIYBnw1/HlG+bo/BU5098HAEOBkMxsG/AS4PbzmD4HLwvKXAR+6+yHA7WG5QnQVsDLmddSvt8EJ7j4kZsx5bn+33T1vH8CxwFMxr78LfLet65XF66sAXol5/TqwX/h8P+D18PkvgHHxyhXyA/gD8PliuW6gM7AYOIbgrsH24fbG33PgKeDY8Hn7sJy1dd1beJ19CYLXicBcwKJ8vTHXvRro1WRbTn+387qFDvQB3o55vTbcFlX/5u7rAcJ/9wm3R+5zCP+0HgosIOLXHaYflgLvA08D/wQ2unt9WCT2uhqvOdy/CWjFdeJbxTTgO8DO8HU50b7eBg78j5ktMrMJ4bac/m7n+xJ0FmdbMY6zjNTnYGZdgEeBb7r7R2bxLi8oGmdbwV23u+8AhphZD+Ax4Ih4xcJ/C/qazexU4H13X2Rmoxo2xykaiettYri7rzOzfYCnzey1JGVb5brzvYW+Fjgg5nVfYF0b1SUX3jOz/QDCf98Pt0fmczCzDgTBfLa7/y7cHPnrBnD3jcBzBP0HPcysoUEVe12N1xzu7w58kNuaZmQ4cLqZrQbmEKRdphHd623k7uvCf98n+OI+mhz/bud7QH8JODTsId8LOA/4YxvXqTX9EbgofH4RQY65YfuFYc/4MGBTw59xhcSCpvgvgZXuflvMrshet5n1DlvmmFknYAxBZ+E84OywWNNrbvgszgae9TDJWgjc/bvu3tfdKwj+vz7r7jVE9HobmFmZmXVteA6cBLxCrn+327ojIYWOhrHA/xHkHb/X1vXJ4nU9BKwHPiP4tr6MIHf4DLAq/HfvsKwRjPb5J/AyUN3W9U/zmkcQ/Fm5HFgaPsZG+bqBSmBJeM2vADeE2w8CXgTeAP4b6BhuLw1fvxHuP6itryGDax8FzC2G6w2vb1n4WNEQq3L9u61b/0VEIiLfUy4iIpIiBXQRkYhQQBcRiQgFdBGRiFBAFxGJCAV0EZGIUEAXEYmI/w+4nNBUykR1ugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu Test Accuracy: 0.878\n",
      "my_model_neu Test f-measure: 0.768\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_input_tensor = Input(shape=(scaled_train_data_words.shape[1],) , name='words')\n",
    "ngram_input_tensor = Input(shape=(scaled_train_data_ngrams.shape[1],) , name='n_grams')\n",
    "\n",
    "neu0 = Sequential()\n",
    "neu0.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "\n",
    "neu1 = Sequential()\n",
    "neu1.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "\n",
    "output_tensor_0 = neu0(ngram_input_tensor)\n",
    "output_tensor_1 = neu1(word_input_tensor)\n",
    "\n",
    "\n",
    "# conv_input_tensor = Input(shape=(maxlen,) , name='convnets')\n",
    "\n",
    "# conv_1d_s3_model = Sequential()\n",
    "# conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_s3_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_0 = conv_1d_s3_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "# conv_1d_s1_model = Sequential()\n",
    "# conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_s1_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_1 = conv_1d_s1_model(conv_input_tensor)\n",
    "\n",
    "# conv_1d_complex_model = Sequential()\n",
    "# conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_complex_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_2 = conv_1d_complex_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "\n",
    "concatenated = layers.concatenate([output_tensor_0,\n",
    "                                   output_tensor_1,\n",
    "#                                    conv_output_tensor_0,\n",
    "#                                    conv_output_tensor_1,\n",
    "#                                    conv_output_tensor_2,\n",
    "                                  ], axis=-1)\n",
    "\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "concatenated = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model([ngram_input_tensor, word_input_tensor,\n",
    "#                conv_input_tensor\n",
    "              ], concatenated)\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "print(X_scaled_train_data_ngrams.shape, X_scaled_train_data_words.shape, y_train.shape) \n",
    "history = model.fit([X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
    "#                      X_train\n",
    "                    ], y_train,\n",
    "                    validation_data=([X_scaled_val_data_ngrams, X_scaled_val_data_words,\n",
    "#                                       X_val\n",
    "                                     ], y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu.h5')\n",
    "yhat = l_model.predict([scaled_test_data_ngrams, scaled_test_data_words])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_neu Test Accuracy: 0.853\n",
      "my_model_neu Test f-measure: 0.668\n"
     ]
    }
   ],
   "source": [
    "l_model = load_model('my_model_neu.h5')\n",
    "yhat = l_model.predict([scaled_test_data_ngrams, scaled_test_data_words])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "\n",
    "print('my_model_neu Test Accuracy: %.3f' % acc)\n",
    "\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "convnet_words (InputLayer)      (None, 1053)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 64)           72451       convnet_words[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 64)           69521       convnet_words[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 64)           72378       convnet_words[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 192)          0           sequential_11[1][0]              \n",
      "                                                                 sequential_12[1][0]              \n",
      "                                                                 sequential_13[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 128)          24704       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 128)          0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 9)            1161        dropout_30[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 240,215\n",
      "Trainable params: 239,959\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/300\n",
      " - 9s - loss: 4.1205 - acc: 0.0635 - val_loss: 4.0365 - val_acc: 0.1336\n",
      "Epoch 2/300\n",
      " - 2s - loss: 4.1029 - acc: 0.2063 - val_loss: 4.0073 - val_acc: 0.1601\n",
      "Epoch 3/300\n",
      " - 2s - loss: 4.0155 - acc: 0.1429 - val_loss: 3.9801 - val_acc: 0.1786\n",
      "Epoch 4/300\n",
      " - 2s - loss: 3.9800 - acc: 0.1746 - val_loss: 3.9549 - val_acc: 0.2288\n",
      "Epoch 5/300\n",
      " - 2s - loss: 3.9583 - acc: 0.1270 - val_loss: 3.9311 - val_acc: 0.2222\n",
      "Epoch 6/300\n",
      " - 2s - loss: 3.9670 - acc: 0.1270 - val_loss: 3.9076 - val_acc: 0.2288\n",
      "Epoch 7/300\n",
      " - 2s - loss: 3.9452 - acc: 0.1111 - val_loss: 3.8843 - val_acc: 0.2368\n",
      "Epoch 8/300\n",
      " - 2s - loss: 3.8963 - acc: 0.1270 - val_loss: 3.8629 - val_acc: 0.2183\n",
      "Epoch 9/300\n",
      " - 2s - loss: 3.8710 - acc: 0.1429 - val_loss: 3.8405 - val_acc: 0.2262\n",
      "Epoch 10/300\n",
      " - 2s - loss: 3.8835 - acc: 0.1270 - val_loss: 3.8189 - val_acc: 0.1799\n",
      "Epoch 11/300\n",
      " - 2s - loss: 3.8812 - acc: 0.1111 - val_loss: 3.7974 - val_acc: 0.1971\n",
      "Epoch 12/300\n",
      " - 2s - loss: 3.8129 - acc: 0.1111 - val_loss: 3.7762 - val_acc: 0.2407\n",
      "Epoch 13/300\n",
      " - 2s - loss: 3.8052 - acc: 0.0952 - val_loss: 3.7554 - val_acc: 0.2474\n",
      "Epoch 14/300\n",
      " - 2s - loss: 3.7991 - acc: 0.1746 - val_loss: 3.7347 - val_acc: 0.2725\n",
      "Epoch 15/300\n",
      " - 2s - loss: 3.7535 - acc: 0.1270 - val_loss: 3.7141 - val_acc: 0.3452\n",
      "Epoch 16/300\n",
      " - 2s - loss: 3.6919 - acc: 0.2540 - val_loss: 3.6938 - val_acc: 0.3426\n",
      "Epoch 17/300\n",
      " - 2s - loss: 3.7283 - acc: 0.1270 - val_loss: 3.6730 - val_acc: 0.3730\n",
      "Epoch 18/300\n",
      " - 2s - loss: 3.6677 - acc: 0.2063 - val_loss: 3.6528 - val_acc: 0.4087\n",
      "Epoch 19/300\n",
      " - 2s - loss: 3.6985 - acc: 0.1270 - val_loss: 3.6331 - val_acc: 0.4206\n",
      "Epoch 20/300\n",
      " - 2s - loss: 3.6776 - acc: 0.1429 - val_loss: 3.6138 - val_acc: 0.3968\n",
      "Epoch 21/300\n",
      " - 2s - loss: 3.6416 - acc: 0.1746 - val_loss: 3.5943 - val_acc: 0.3783\n",
      "Epoch 22/300\n",
      " - 2s - loss: 3.6359 - acc: 0.1270 - val_loss: 3.5742 - val_acc: 0.3836\n",
      "Epoch 23/300\n",
      " - 2s - loss: 3.5657 - acc: 0.2063 - val_loss: 3.5548 - val_acc: 0.3677\n",
      "Epoch 24/300\n",
      " - 2s - loss: 3.5779 - acc: 0.1429 - val_loss: 3.5354 - val_acc: 0.3320\n",
      "Epoch 25/300\n",
      " - 2s - loss: 3.5522 - acc: 0.1746 - val_loss: 3.5159 - val_acc: 0.3267\n",
      "Epoch 26/300\n",
      " - 2s - loss: 3.5494 - acc: 0.1587 - val_loss: 3.4966 - val_acc: 0.3122\n",
      "Epoch 27/300\n",
      " - 2s - loss: 3.4822 - acc: 0.2381 - val_loss: 3.4772 - val_acc: 0.4061\n",
      "Epoch 28/300\n",
      " - 2s - loss: 3.4766 - acc: 0.1905 - val_loss: 3.4582 - val_acc: 0.4511\n",
      "Epoch 29/300\n",
      " - 2s - loss: 3.5149 - acc: 0.1905 - val_loss: 3.4391 - val_acc: 0.5741\n",
      "Epoch 30/300\n",
      " - 2s - loss: 3.4264 - acc: 0.3492 - val_loss: 3.4204 - val_acc: 0.5384\n",
      "Epoch 31/300\n",
      " - 2s - loss: 3.4306 - acc: 0.3016 - val_loss: 3.4010 - val_acc: 0.5767\n",
      "Epoch 32/300\n",
      " - 2s - loss: 3.4131 - acc: 0.2381 - val_loss: 3.3816 - val_acc: 0.5767\n",
      "Epoch 33/300\n",
      " - 2s - loss: 3.3522 - acc: 0.2857 - val_loss: 3.3622 - val_acc: 0.6892\n",
      "Epoch 34/300\n",
      " - 2s - loss: 3.3915 - acc: 0.2063 - val_loss: 3.3431 - val_acc: 0.7156\n",
      "Epoch 35/300\n",
      " - 2s - loss: 3.3430 - acc: 0.3333 - val_loss: 3.3235 - val_acc: 0.7341\n",
      "Epoch 36/300\n",
      " - 2s - loss: 3.3229 - acc: 0.2381 - val_loss: 3.3033 - val_acc: 0.7526\n",
      "Epoch 37/300\n",
      " - 2s - loss: 3.2901 - acc: 0.3333 - val_loss: 3.2834 - val_acc: 0.7870\n",
      "Epoch 38/300\n",
      " - 2s - loss: 3.3276 - acc: 0.1746 - val_loss: 3.2637 - val_acc: 0.7474\n",
      "Epoch 39/300\n",
      " - 2s - loss: 3.2573 - acc: 0.2857 - val_loss: 3.2430 - val_acc: 0.7725\n",
      "Epoch 40/300\n",
      " - 2s - loss: 3.2597 - acc: 0.3651 - val_loss: 3.2206 - val_acc: 0.7368\n",
      "Epoch 41/300\n",
      " - 2s - loss: 3.2465 - acc: 0.3333 - val_loss: 3.1996 - val_acc: 0.7315\n",
      "Epoch 42/300\n",
      " - 2s - loss: 3.2136 - acc: 0.2698 - val_loss: 3.1807 - val_acc: 0.7381\n",
      "Epoch 43/300\n",
      " - 2s - loss: 3.2113 - acc: 0.2698 - val_loss: 3.1606 - val_acc: 0.7474\n",
      "Epoch 44/300\n",
      " - 2s - loss: 3.1392 - acc: 0.3333 - val_loss: 3.1402 - val_acc: 0.8042\n",
      "Epoch 45/300\n",
      " - 2s - loss: 3.1301 - acc: 0.3651 - val_loss: 3.1217 - val_acc: 0.7870\n",
      "Epoch 46/300\n",
      " - 2s - loss: 3.1416 - acc: 0.3810 - val_loss: 3.1020 - val_acc: 0.7937\n",
      "Epoch 47/300\n",
      " - 2s - loss: 3.1042 - acc: 0.3492 - val_loss: 3.0810 - val_acc: 0.8320\n",
      "Epoch 48/300\n",
      " - 2s - loss: 3.0356 - acc: 0.4603 - val_loss: 3.0603 - val_acc: 0.8320\n",
      "Epoch 49/300\n",
      " - 2s - loss: 3.0806 - acc: 0.3492 - val_loss: 3.0395 - val_acc: 0.8466\n",
      "Epoch 50/300\n",
      " - 2s - loss: 3.0114 - acc: 0.4603 - val_loss: 3.0186 - val_acc: 0.8717\n",
      "Epoch 51/300\n",
      " - 2s - loss: 3.0082 - acc: 0.4286 - val_loss: 2.9984 - val_acc: 0.8611\n",
      "Epoch 52/300\n",
      " - 2s - loss: 2.9767 - acc: 0.4921 - val_loss: 2.9783 - val_acc: 0.8452\n",
      "Epoch 53/300\n",
      " - 2s - loss: 2.9840 - acc: 0.4286 - val_loss: 2.9576 - val_acc: 0.8399\n",
      "Epoch 54/300\n",
      " - 2s - loss: 2.9477 - acc: 0.4603 - val_loss: 2.9371 - val_acc: 0.8585\n",
      "Epoch 55/300\n",
      " - 2s - loss: 2.9961 - acc: 0.3492 - val_loss: 2.9158 - val_acc: 0.8770\n",
      "Epoch 56/300\n",
      " - 2s - loss: 2.9515 - acc: 0.3968 - val_loss: 2.8949 - val_acc: 0.8664\n",
      "Epoch 57/300\n",
      " - 2s - loss: 2.8405 - acc: 0.5714 - val_loss: 2.8733 - val_acc: 0.8704\n",
      "Epoch 58/300\n",
      " - 2s - loss: 2.8373 - acc: 0.5714 - val_loss: 2.8512 - val_acc: 0.8929\n",
      "Epoch 59/300\n",
      " - 2s - loss: 2.8370 - acc: 0.5079 - val_loss: 2.8300 - val_acc: 0.9008\n",
      "Epoch 60/300\n",
      " - 2s - loss: 2.8805 - acc: 0.4127 - val_loss: 2.8092 - val_acc: 0.8862\n",
      "Epoch 61/300\n",
      " - 2s - loss: 2.7935 - acc: 0.5556 - val_loss: 2.7887 - val_acc: 0.9074\n",
      "Epoch 62/300\n",
      " - 2s - loss: 2.7629 - acc: 0.5873 - val_loss: 2.7687 - val_acc: 0.9061\n",
      "Epoch 63/300\n",
      " - 2s - loss: 2.7574 - acc: 0.4762 - val_loss: 2.7471 - val_acc: 0.9233\n",
      "Epoch 64/300\n",
      " - 2s - loss: 2.7559 - acc: 0.5238 - val_loss: 2.7273 - val_acc: 0.9272\n",
      "Epoch 65/300\n",
      " - 2s - loss: 2.7348 - acc: 0.5873 - val_loss: 2.7072 - val_acc: 0.9312\n",
      "Epoch 66/300\n",
      " - 2s - loss: 2.7185 - acc: 0.5556 - val_loss: 2.6872 - val_acc: 0.9325\n",
      "Epoch 67/300\n",
      " - 2s - loss: 2.7061 - acc: 0.5714 - val_loss: 2.6663 - val_acc: 0.9365\n",
      "Epoch 68/300\n",
      " - 2s - loss: 2.7114 - acc: 0.5238 - val_loss: 2.6458 - val_acc: 0.9484\n",
      "Epoch 69/300\n",
      " - 2s - loss: 2.6525 - acc: 0.5397 - val_loss: 2.6252 - val_acc: 0.9550\n",
      "Epoch 70/300\n",
      " - 2s - loss: 2.5928 - acc: 0.6349 - val_loss: 2.6031 - val_acc: 0.9392\n",
      "Epoch 71/300\n",
      " - 2s - loss: 2.6134 - acc: 0.5238 - val_loss: 2.5808 - val_acc: 0.9378\n",
      "Epoch 72/300\n",
      " - 2s - loss: 2.5655 - acc: 0.6032 - val_loss: 2.5583 - val_acc: 0.9418\n",
      "Epoch 73/300\n",
      " - 2s - loss: 2.5300 - acc: 0.6825 - val_loss: 2.5367 - val_acc: 0.9153\n",
      "Epoch 74/300\n",
      " - 2s - loss: 2.5214 - acc: 0.5873 - val_loss: 2.5158 - val_acc: 0.8955\n",
      "Epoch 75/300\n",
      " - 2s - loss: 2.4761 - acc: 0.6349 - val_loss: 2.4940 - val_acc: 0.9153\n",
      "Epoch 76/300\n",
      " - 2s - loss: 2.5709 - acc: 0.4762 - val_loss: 2.4731 - val_acc: 0.9074\n",
      "Epoch 77/300\n",
      " - 2s - loss: 2.4120 - acc: 0.6667 - val_loss: 2.4517 - val_acc: 0.9127\n",
      "Epoch 78/300\n",
      " - 2s - loss: 2.4451 - acc: 0.6825 - val_loss: 2.4283 - val_acc: 0.9299\n",
      "Epoch 79/300\n",
      " - 2s - loss: 2.4067 - acc: 0.5873 - val_loss: 2.4053 - val_acc: 0.9365\n",
      "Epoch 80/300\n",
      " - 2s - loss: 2.4006 - acc: 0.5873 - val_loss: 2.3814 - val_acc: 0.9550\n",
      "Epoch 81/300\n",
      " - 2s - loss: 2.4278 - acc: 0.5556 - val_loss: 2.3595 - val_acc: 0.9511\n",
      "Epoch 82/300\n",
      " - 2s - loss: 2.2646 - acc: 0.6667 - val_loss: 2.3371 - val_acc: 0.9431\n",
      "Epoch 83/300\n",
      " - 2s - loss: 2.3614 - acc: 0.6032 - val_loss: 2.3147 - val_acc: 0.9656\n",
      "Epoch 84/300\n",
      " - 2s - loss: 2.2629 - acc: 0.6825 - val_loss: 2.2930 - val_acc: 0.9418\n",
      "Epoch 85/300\n",
      " - 2s - loss: 2.2531 - acc: 0.6667 - val_loss: 2.2705 - val_acc: 0.9431\n",
      "Epoch 86/300\n",
      " - 2s - loss: 2.2927 - acc: 0.6349 - val_loss: 2.2492 - val_acc: 0.9405\n",
      "Epoch 87/300\n",
      " - 2s - loss: 2.2838 - acc: 0.6667 - val_loss: 2.2274 - val_acc: 0.9669\n",
      "Epoch 88/300\n",
      " - 2s - loss: 2.2179 - acc: 0.6190 - val_loss: 2.2037 - val_acc: 0.9696\n",
      "Epoch 89/300\n",
      " - 2s - loss: 2.2338 - acc: 0.6190 - val_loss: 2.1820 - val_acc: 0.9722\n",
      "Epoch 90/300\n",
      " - 2s - loss: 2.2138 - acc: 0.6349 - val_loss: 2.1602 - val_acc: 0.9775\n",
      "Epoch 91/300\n",
      " - 2s - loss: 2.1869 - acc: 0.6032 - val_loss: 2.1387 - val_acc: 0.9762\n",
      "Epoch 92/300\n",
      " - 2s - loss: 2.1992 - acc: 0.6667 - val_loss: 2.1177 - val_acc: 0.9762\n",
      "Epoch 93/300\n",
      " - 2s - loss: 2.1433 - acc: 0.6508 - val_loss: 2.0975 - val_acc: 0.9775\n",
      "Epoch 94/300\n",
      " - 2s - loss: 2.0975 - acc: 0.6667 - val_loss: 2.0763 - val_acc: 0.9775\n",
      "Epoch 95/300\n",
      " - 2s - loss: 2.0868 - acc: 0.6667 - val_loss: 2.0558 - val_acc: 0.9722\n",
      "Epoch 96/300\n",
      " - 2s - loss: 2.0357 - acc: 0.6825 - val_loss: 2.0333 - val_acc: 0.9735\n",
      "Epoch 97/300\n",
      " - 2s - loss: 2.0485 - acc: 0.7460 - val_loss: 2.0107 - val_acc: 0.9775\n",
      "Epoch 98/300\n",
      " - 2s - loss: 2.0077 - acc: 0.7460 - val_loss: 1.9890 - val_acc: 0.9775\n",
      "Epoch 99/300\n",
      " - 2s - loss: 1.9689 - acc: 0.7460 - val_loss: 1.9668 - val_acc: 0.9802\n",
      "Epoch 100/300\n",
      " - 2s - loss: 1.9060 - acc: 0.7937 - val_loss: 1.9465 - val_acc: 0.9749\n",
      "Epoch 101/300\n",
      " - 2s - loss: 2.0229 - acc: 0.6032 - val_loss: 1.9279 - val_acc: 0.9762\n",
      "Epoch 102/300\n",
      " - 2s - loss: 1.9285 - acc: 0.7619 - val_loss: 1.9079 - val_acc: 0.9775\n",
      "Epoch 103/300\n",
      " - 2s - loss: 1.9770 - acc: 0.7143 - val_loss: 1.8876 - val_acc: 0.9762\n",
      "Epoch 104/300\n",
      " - 2s - loss: 1.8281 - acc: 0.8095 - val_loss: 1.8674 - val_acc: 0.9775\n",
      "Epoch 105/300\n",
      " - 2s - loss: 1.8679 - acc: 0.7460 - val_loss: 1.8461 - val_acc: 0.9775\n",
      "Epoch 106/300\n",
      " - 2s - loss: 1.8119 - acc: 0.7937 - val_loss: 1.8260 - val_acc: 0.9788\n",
      "Epoch 107/300\n",
      " - 2s - loss: 1.8084 - acc: 0.7778 - val_loss: 1.8057 - val_acc: 0.9775\n",
      "Epoch 108/300\n",
      " - 2s - loss: 1.8069 - acc: 0.7619 - val_loss: 1.7860 - val_acc: 0.9788\n",
      "Epoch 109/300\n",
      " - 2s - loss: 1.8103 - acc: 0.8095 - val_loss: 1.7667 - val_acc: 0.9788\n",
      "Epoch 110/300\n",
      " - 2s - loss: 1.7774 - acc: 0.8095 - val_loss: 1.7453 - val_acc: 0.9788\n",
      "Epoch 111/300\n",
      " - 2s - loss: 1.7277 - acc: 0.8413 - val_loss: 1.7250 - val_acc: 0.9775\n",
      "Epoch 112/300\n",
      " - 2s - loss: 1.7601 - acc: 0.7460 - val_loss: 1.7045 - val_acc: 0.9775\n",
      "Epoch 113/300\n",
      " - 2s - loss: 1.7053 - acc: 0.7937 - val_loss: 1.6852 - val_acc: 0.9802\n",
      "Epoch 114/300\n",
      " - 2s - loss: 1.7243 - acc: 0.7937 - val_loss: 1.6662 - val_acc: 0.9788\n",
      "Epoch 115/300\n",
      " - 2s - loss: 1.6349 - acc: 0.7937 - val_loss: 1.6486 - val_acc: 0.9775\n",
      "Epoch 116/300\n",
      " - 2s - loss: 1.5785 - acc: 0.8730 - val_loss: 1.6276 - val_acc: 0.9775\n",
      "Epoch 117/300\n",
      " - 2s - loss: 1.7051 - acc: 0.7619 - val_loss: 1.6090 - val_acc: 0.9788\n",
      "Epoch 118/300\n",
      " - 2s - loss: 1.6545 - acc: 0.7937 - val_loss: 1.5889 - val_acc: 0.9802\n",
      "Epoch 119/300\n",
      " - 2s - loss: 1.6155 - acc: 0.7937 - val_loss: 1.5712 - val_acc: 0.9802\n",
      "Epoch 120/300\n",
      " - 2s - loss: 1.5136 - acc: 0.8889 - val_loss: 1.5525 - val_acc: 0.9828\n",
      "Epoch 121/300\n",
      " - 2s - loss: 1.6337 - acc: 0.7619 - val_loss: 1.5349 - val_acc: 0.9841\n",
      "Epoch 122/300\n",
      " - 2s - loss: 1.5268 - acc: 0.8571 - val_loss: 1.5179 - val_acc: 0.9815\n",
      "Epoch 123/300\n",
      " - 2s - loss: 1.5883 - acc: 0.7460 - val_loss: 1.5001 - val_acc: 0.9788\n",
      "Epoch 124/300\n",
      " - 2s - loss: 1.4930 - acc: 0.8413 - val_loss: 1.4817 - val_acc: 0.9788\n",
      "Epoch 125/300\n",
      " - 2s - loss: 1.4514 - acc: 0.9048 - val_loss: 1.4626 - val_acc: 0.9788\n",
      "Epoch 126/300\n",
      " - 2s - loss: 1.4111 - acc: 0.9048 - val_loss: 1.4456 - val_acc: 0.9788\n",
      "Epoch 127/300\n",
      " - 2s - loss: 1.5102 - acc: 0.8413 - val_loss: 1.4269 - val_acc: 0.9788\n",
      "Epoch 128/300\n",
      " - 2s - loss: 1.4391 - acc: 0.8730 - val_loss: 1.4092 - val_acc: 0.9802\n",
      "Epoch 129/300\n",
      " - 2s - loss: 1.4123 - acc: 0.8730 - val_loss: 1.3918 - val_acc: 0.9802\n",
      "Epoch 130/300\n",
      " - 2s - loss: 1.3981 - acc: 0.8889 - val_loss: 1.3722 - val_acc: 0.9854\n",
      "Epoch 131/300\n",
      " - 2s - loss: 1.4613 - acc: 0.8889 - val_loss: 1.3555 - val_acc: 0.9854\n",
      "Epoch 132/300\n",
      " - 2s - loss: 1.4383 - acc: 0.8730 - val_loss: 1.3399 - val_acc: 0.9841\n",
      "Epoch 133/300\n",
      " - 2s - loss: 1.3349 - acc: 0.9048 - val_loss: 1.3231 - val_acc: 0.9841\n",
      "Epoch 134/300\n",
      " - 2s - loss: 1.3437 - acc: 0.9206 - val_loss: 1.3071 - val_acc: 0.9802\n",
      "Epoch 135/300\n",
      " - 2s - loss: 1.3083 - acc: 0.9048 - val_loss: 1.2895 - val_acc: 0.9828\n",
      "Epoch 136/300\n",
      " - 2s - loss: 1.2925 - acc: 0.8889 - val_loss: 1.2733 - val_acc: 0.9841\n",
      "Epoch 137/300\n",
      " - 2s - loss: 1.2997 - acc: 0.9048 - val_loss: 1.2556 - val_acc: 0.9841\n",
      "Epoch 138/300\n",
      " - 2s - loss: 1.2695 - acc: 0.9048 - val_loss: 1.2398 - val_acc: 0.9815\n",
      "Epoch 139/300\n",
      " - 2s - loss: 1.2105 - acc: 0.9365 - val_loss: 1.2221 - val_acc: 0.9815\n",
      "Epoch 140/300\n",
      " - 2s - loss: 1.2811 - acc: 0.8571 - val_loss: 1.2071 - val_acc: 0.9802\n",
      "Epoch 141/300\n",
      " - 2s - loss: 1.3297 - acc: 0.8571 - val_loss: 1.1949 - val_acc: 0.9802\n",
      "Epoch 142/300\n",
      " - 2s - loss: 1.2446 - acc: 0.9206 - val_loss: 1.1796 - val_acc: 0.9788\n",
      "Epoch 143/300\n",
      " - 2s - loss: 1.1722 - acc: 0.9365 - val_loss: 1.1655 - val_acc: 0.9802\n",
      "Epoch 144/300\n",
      " - 2s - loss: 1.1569 - acc: 0.9206 - val_loss: 1.1514 - val_acc: 0.9802\n",
      "Epoch 145/300\n",
      " - 2s - loss: 1.1809 - acc: 0.8730 - val_loss: 1.1382 - val_acc: 0.9802\n",
      "Epoch 146/300\n",
      " - 2s - loss: 1.1466 - acc: 0.9048 - val_loss: 1.1245 - val_acc: 0.9802\n",
      "Epoch 147/300\n",
      " - 2s - loss: 1.0565 - acc: 0.9206 - val_loss: 1.1071 - val_acc: 0.9802\n",
      "Epoch 148/300\n",
      " - 2s - loss: 1.1304 - acc: 0.9048 - val_loss: 1.0932 - val_acc: 0.9854\n",
      "Epoch 149/300\n",
      " - 2s - loss: 1.1517 - acc: 0.9365 - val_loss: 1.0802 - val_acc: 0.9868\n",
      "Epoch 150/300\n",
      " - 2s - loss: 1.1256 - acc: 0.9841 - val_loss: 1.0680 - val_acc: 0.9868\n",
      "Epoch 151/300\n",
      " - 2s - loss: 1.0491 - acc: 0.9048 - val_loss: 1.0553 - val_acc: 0.9868\n",
      "Epoch 152/300\n",
      " - 2s - loss: 1.0947 - acc: 0.9841 - val_loss: 1.0406 - val_acc: 0.9881\n",
      "Epoch 153/300\n",
      " - 2s - loss: 1.0297 - acc: 0.9683 - val_loss: 1.0248 - val_acc: 0.9907\n",
      "Epoch 154/300\n",
      " - 2s - loss: 1.0532 - acc: 0.9365 - val_loss: 1.0127 - val_acc: 0.9881\n",
      "Epoch 155/300\n",
      " - 2s - loss: 1.0699 - acc: 0.9048 - val_loss: 1.0008 - val_acc: 0.9854\n",
      "Epoch 156/300\n",
      " - 2s - loss: 1.0380 - acc: 0.9206 - val_loss: 0.9901 - val_acc: 0.9854\n",
      "Epoch 157/300\n",
      " - 2s - loss: 0.9909 - acc: 0.9524 - val_loss: 0.9815 - val_acc: 0.9881\n",
      "Epoch 158/300\n",
      " - 2s - loss: 1.0023 - acc: 0.9206 - val_loss: 0.9721 - val_acc: 0.9868\n",
      "Epoch 159/300\n",
      " - 2s - loss: 1.0584 - acc: 0.9365 - val_loss: 0.9593 - val_acc: 0.9828\n",
      "Epoch 160/300\n",
      " - 2s - loss: 1.0113 - acc: 0.9048 - val_loss: 0.9457 - val_acc: 0.9868\n",
      "Epoch 161/300\n",
      " - 2s - loss: 0.9375 - acc: 0.9524 - val_loss: 0.9352 - val_acc: 0.9868\n",
      "Epoch 162/300\n",
      " - 2s - loss: 0.9359 - acc: 0.9524 - val_loss: 0.9216 - val_acc: 0.9894\n",
      "Epoch 163/300\n",
      " - 2s - loss: 0.9640 - acc: 0.9365 - val_loss: 0.9109 - val_acc: 0.9894\n",
      "Epoch 164/300\n",
      " - 2s - loss: 0.9773 - acc: 0.9365 - val_loss: 0.9015 - val_acc: 0.9894\n",
      "Epoch 165/300\n",
      " - 2s - loss: 0.8962 - acc: 0.9365 - val_loss: 0.8926 - val_acc: 0.9894\n",
      "Epoch 166/300\n",
      " - 2s - loss: 0.9275 - acc: 0.9206 - val_loss: 0.8833 - val_acc: 0.9894\n",
      "Epoch 167/300\n",
      " - 2s - loss: 0.9533 - acc: 0.9048 - val_loss: 0.8738 - val_acc: 0.9894\n",
      "Epoch 168/300\n",
      " - 2s - loss: 0.9299 - acc: 0.9365 - val_loss: 0.8649 - val_acc: 0.9881\n",
      "Epoch 169/300\n",
      " - 2s - loss: 1.0244 - acc: 0.8730 - val_loss: 0.8524 - val_acc: 0.9894\n",
      "Epoch 170/300\n",
      " - 2s - loss: 0.8564 - acc: 0.9841 - val_loss: 0.8423 - val_acc: 0.9907\n",
      "Epoch 171/300\n",
      " - 2s - loss: 0.8628 - acc: 0.9206 - val_loss: 0.8316 - val_acc: 0.9947\n",
      "Epoch 172/300\n",
      " - 2s - loss: 0.8945 - acc: 0.9206 - val_loss: 0.8220 - val_acc: 0.9974\n",
      "Epoch 173/300\n",
      " - 2s - loss: 0.8422 - acc: 1.0000 - val_loss: 0.8143 - val_acc: 0.9947\n",
      "Epoch 174/300\n",
      " - 2s - loss: 0.9214 - acc: 0.9524 - val_loss: 0.8081 - val_acc: 0.9921\n",
      "Epoch 175/300\n",
      " - 2s - loss: 0.8797 - acc: 0.9841 - val_loss: 0.7995 - val_acc: 0.9934\n",
      "Epoch 176/300\n",
      " - 2s - loss: 0.7471 - acc: 0.9841 - val_loss: 0.7904 - val_acc: 0.9947\n",
      "Epoch 177/300\n",
      " - 2s - loss: 0.8440 - acc: 0.9524 - val_loss: 0.7822 - val_acc: 0.9974\n",
      "Epoch 178/300\n",
      " - 2s - loss: 0.8825 - acc: 0.9683 - val_loss: 0.7746 - val_acc: 0.9947\n",
      "Epoch 179/300\n",
      " - 2s - loss: 0.7684 - acc: 0.9683 - val_loss: 0.7673 - val_acc: 0.9934\n",
      "Epoch 180/300\n",
      " - 2s - loss: 0.8426 - acc: 0.9524 - val_loss: 0.7624 - val_acc: 0.9907\n",
      "Epoch 181/300\n",
      " - 2s - loss: 0.7604 - acc: 0.9683 - val_loss: 0.7532 - val_acc: 0.9947\n",
      "Epoch 182/300\n",
      " - 2s - loss: 0.8233 - acc: 0.9365 - val_loss: 0.7463 - val_acc: 0.9947\n",
      "Epoch 183/300\n",
      " - 2s - loss: 0.8046 - acc: 0.9206 - val_loss: 0.7393 - val_acc: 0.9974\n",
      "Epoch 184/300\n",
      " - 2s - loss: 0.7464 - acc: 0.9683 - val_loss: 0.7331 - val_acc: 0.9974\n",
      "Epoch 185/300\n",
      " - 2s - loss: 0.8083 - acc: 0.9683 - val_loss: 0.7261 - val_acc: 0.9974\n",
      "Epoch 186/300\n",
      " - 2s - loss: 0.7470 - acc: 0.9683 - val_loss: 0.7192 - val_acc: 0.9974\n",
      "Epoch 187/300\n",
      " - 2s - loss: 0.7129 - acc: 1.0000 - val_loss: 0.7118 - val_acc: 0.9974\n",
      "Epoch 188/300\n",
      " - 2s - loss: 0.7636 - acc: 0.9841 - val_loss: 0.7046 - val_acc: 0.9987\n",
      "Epoch 189/300\n",
      " - 2s - loss: 0.7272 - acc: 0.9683 - val_loss: 0.6986 - val_acc: 1.0000\n",
      "Epoch 190/300\n",
      " - 2s - loss: 0.7393 - acc: 0.9841 - val_loss: 0.6928 - val_acc: 1.0000\n",
      "Epoch 191/300\n",
      " - 2s - loss: 0.7358 - acc: 0.9683 - val_loss: 0.6873 - val_acc: 1.0000\n",
      "Epoch 192/300\n",
      " - 2s - loss: 0.7933 - acc: 0.9524 - val_loss: 0.6823 - val_acc: 0.9987\n",
      "Epoch 193/300\n",
      " - 2s - loss: 0.7243 - acc: 0.9524 - val_loss: 0.6760 - val_acc: 0.9974\n",
      "Epoch 194/300\n",
      " - 2s - loss: 0.6872 - acc: 1.0000 - val_loss: 0.6699 - val_acc: 0.9974\n",
      "Epoch 195/300\n",
      " - 2s - loss: 0.7150 - acc: 0.9683 - val_loss: 0.6634 - val_acc: 0.9974\n",
      "Epoch 196/300\n",
      " - 2s - loss: 0.6638 - acc: 1.0000 - val_loss: 0.6572 - val_acc: 0.9987\n",
      "Epoch 197/300\n",
      " - 2s - loss: 0.6310 - acc: 1.0000 - val_loss: 0.6521 - val_acc: 0.9987\n",
      "Epoch 198/300\n",
      " - 2s - loss: 0.6380 - acc: 0.9841 - val_loss: 0.6485 - val_acc: 0.9974\n",
      "Epoch 199/300\n",
      " - 2s - loss: 0.6646 - acc: 1.0000 - val_loss: 0.6431 - val_acc: 0.9974\n",
      "Epoch 200/300\n",
      " - 2s - loss: 0.7114 - acc: 0.9841 - val_loss: 0.6358 - val_acc: 0.9987\n",
      "Epoch 201/300\n",
      " - 2s - loss: 0.6035 - acc: 1.0000 - val_loss: 0.6296 - val_acc: 0.9987\n",
      "Epoch 202/300\n",
      " - 2s - loss: 0.6284 - acc: 0.9841 - val_loss: 0.6248 - val_acc: 0.9987\n",
      "Epoch 203/300\n",
      " - 2s - loss: 0.6298 - acc: 1.0000 - val_loss: 0.6206 - val_acc: 0.9987\n",
      "Epoch 204/300\n",
      " - 2s - loss: 0.6937 - acc: 1.0000 - val_loss: 0.6162 - val_acc: 0.9987\n",
      "Epoch 205/300\n",
      " - 2s - loss: 0.5964 - acc: 0.9841 - val_loss: 0.6109 - val_acc: 0.9987\n",
      "Epoch 206/300\n",
      " - 2s - loss: 0.6533 - acc: 1.0000 - val_loss: 0.6065 - val_acc: 1.0000\n",
      "Epoch 207/300\n",
      " - 2s - loss: 0.6406 - acc: 1.0000 - val_loss: 0.6027 - val_acc: 0.9987\n",
      "Epoch 208/300\n",
      " - 2s - loss: 0.6222 - acc: 1.0000 - val_loss: 0.5979 - val_acc: 1.0000\n",
      "Epoch 209/300\n",
      " - 2s - loss: 0.5858 - acc: 1.0000 - val_loss: 0.5939 - val_acc: 1.0000\n",
      "Epoch 210/300\n",
      " - 2s - loss: 0.5992 - acc: 0.9841 - val_loss: 0.5900 - val_acc: 1.0000\n",
      "Epoch 211/300\n",
      " - 2s - loss: 0.6640 - acc: 0.9206 - val_loss: 0.5850 - val_acc: 1.0000\n",
      "Epoch 212/300\n",
      " - 2s - loss: 0.5878 - acc: 1.0000 - val_loss: 0.5816 - val_acc: 1.0000\n",
      "Epoch 213/300\n",
      " - 2s - loss: 0.5813 - acc: 1.0000 - val_loss: 0.5786 - val_acc: 1.0000\n",
      "Epoch 214/300\n",
      " - 2s - loss: 0.6277 - acc: 0.9683 - val_loss: 0.5764 - val_acc: 0.9987\n",
      "Epoch 215/300\n",
      " - 2s - loss: 0.6850 - acc: 0.9683 - val_loss: 0.5746 - val_acc: 1.0000\n",
      "Epoch 216/300\n",
      " - 2s - loss: 0.5964 - acc: 1.0000 - val_loss: 0.5715 - val_acc: 1.0000\n",
      "Epoch 217/300\n",
      " - 2s - loss: 0.5983 - acc: 0.9841 - val_loss: 0.5675 - val_acc: 1.0000\n",
      "Epoch 218/300\n",
      " - 2s - loss: 0.6240 - acc: 0.9683 - val_loss: 0.5635 - val_acc: 1.0000\n",
      "Epoch 219/300\n",
      " - 2s - loss: 0.6041 - acc: 1.0000 - val_loss: 0.5599 - val_acc: 1.0000\n",
      "Epoch 220/300\n",
      " - 2s - loss: 0.5646 - acc: 1.0000 - val_loss: 0.5563 - val_acc: 1.0000\n",
      "Epoch 221/300\n",
      " - 2s - loss: 0.5608 - acc: 1.0000 - val_loss: 0.5527 - val_acc: 1.0000\n",
      "Epoch 222/300\n",
      " - 2s - loss: 0.5646 - acc: 1.0000 - val_loss: 0.5498 - val_acc: 1.0000\n",
      "Epoch 223/300\n",
      " - 2s - loss: 0.6014 - acc: 0.9683 - val_loss: 0.5484 - val_acc: 0.9987\n",
      "Epoch 224/300\n",
      " - 2s - loss: 0.5635 - acc: 0.9841 - val_loss: 0.5498 - val_acc: 0.9987\n",
      "Epoch 225/300\n",
      " - 2s - loss: 0.5469 - acc: 1.0000 - val_loss: 0.5490 - val_acc: 0.9987\n",
      "Epoch 226/300\n",
      " - 2s - loss: 0.5599 - acc: 0.9841 - val_loss: 0.5434 - val_acc: 1.0000\n",
      "Epoch 227/300\n",
      " - 2s - loss: 0.5582 - acc: 0.9841 - val_loss: 0.5406 - val_acc: 1.0000\n",
      "Epoch 228/300\n",
      " - 2s - loss: 0.5339 - acc: 1.0000 - val_loss: 0.5378 - val_acc: 1.0000\n",
      "Epoch 229/300\n",
      " - 2s - loss: 0.5432 - acc: 1.0000 - val_loss: 0.5350 - val_acc: 1.0000\n",
      "Epoch 230/300\n",
      " - 2s - loss: 0.5917 - acc: 1.0000 - val_loss: 0.5328 - val_acc: 1.0000\n",
      "Epoch 231/300\n",
      " - 2s - loss: 0.5593 - acc: 1.0000 - val_loss: 0.5292 - val_acc: 1.0000\n",
      "Epoch 232/300\n",
      " - 2s - loss: 0.5700 - acc: 1.0000 - val_loss: 0.5256 - val_acc: 1.0000\n",
      "Epoch 233/300\n",
      " - 2s - loss: 0.5470 - acc: 1.0000 - val_loss: 0.5229 - val_acc: 1.0000\n",
      "Epoch 234/300\n",
      " - 2s - loss: 0.5429 - acc: 1.0000 - val_loss: 0.5208 - val_acc: 1.0000\n",
      "Epoch 235/300\n",
      " - 2s - loss: 0.5624 - acc: 0.9841 - val_loss: 0.5187 - val_acc: 1.0000\n",
      "Epoch 236/300\n",
      " - 2s - loss: 0.5390 - acc: 1.0000 - val_loss: 0.5161 - val_acc: 1.0000\n",
      "Epoch 237/300\n",
      " - 2s - loss: 0.5406 - acc: 1.0000 - val_loss: 0.5144 - val_acc: 1.0000\n",
      "Epoch 238/300\n",
      " - 2s - loss: 0.5551 - acc: 0.9841 - val_loss: 0.5134 - val_acc: 1.0000\n",
      "Epoch 239/300\n",
      " - 2s - loss: 0.5318 - acc: 1.0000 - val_loss: 0.5121 - val_acc: 1.0000\n",
      "Epoch 240/300\n",
      " - 2s - loss: 0.5275 - acc: 1.0000 - val_loss: 0.5094 - val_acc: 1.0000\n",
      "Epoch 241/300\n",
      " - 2s - loss: 0.5213 - acc: 1.0000 - val_loss: 0.5071 - val_acc: 1.0000\n",
      "Epoch 242/300\n",
      " - 2s - loss: 0.5268 - acc: 0.9841 - val_loss: 0.5049 - val_acc: 1.0000\n",
      "Epoch 243/300\n",
      " - 2s - loss: 0.5141 - acc: 1.0000 - val_loss: 0.5030 - val_acc: 1.0000\n",
      "Epoch 244/300\n",
      " - 2s - loss: 0.5214 - acc: 1.0000 - val_loss: 0.5009 - val_acc: 1.0000\n",
      "Epoch 245/300\n",
      " - 2s - loss: 0.5451 - acc: 0.9841 - val_loss: 0.4982 - val_acc: 1.0000\n",
      "Epoch 246/300\n",
      " - 2s - loss: 0.4930 - acc: 1.0000 - val_loss: 0.4960 - val_acc: 1.0000\n",
      "Epoch 247/300\n",
      " - 2s - loss: 0.5150 - acc: 0.9841 - val_loss: 0.4944 - val_acc: 1.0000\n",
      "Epoch 248/300\n",
      " - 2s - loss: 0.5258 - acc: 1.0000 - val_loss: 0.4949 - val_acc: 1.0000\n",
      "Epoch 249/300\n",
      " - 2s - loss: 0.5403 - acc: 1.0000 - val_loss: 0.4950 - val_acc: 1.0000\n",
      "Epoch 250/300\n",
      " - 2s - loss: 0.5038 - acc: 1.0000 - val_loss: 0.4947 - val_acc: 1.0000\n",
      "Epoch 251/300\n",
      " - 2s - loss: 0.5270 - acc: 0.9683 - val_loss: 0.4916 - val_acc: 1.0000\n",
      "Epoch 252/300\n",
      " - 2s - loss: 0.5209 - acc: 1.0000 - val_loss: 0.4882 - val_acc: 1.0000\n",
      "Epoch 253/300\n",
      " - 2s - loss: 0.4889 - acc: 1.0000 - val_loss: 0.4860 - val_acc: 1.0000\n",
      "Epoch 254/300\n",
      " - 2s - loss: 0.5036 - acc: 1.0000 - val_loss: 0.4845 - val_acc: 1.0000\n",
      "Epoch 255/300\n",
      " - 2s - loss: 0.5195 - acc: 0.9841 - val_loss: 0.4849 - val_acc: 1.0000\n",
      "Epoch 256/300\n",
      " - 2s - loss: 0.5019 - acc: 1.0000 - val_loss: 0.4842 - val_acc: 1.0000\n",
      "Epoch 257/300\n",
      " - 2s - loss: 0.5091 - acc: 0.9841 - val_loss: 0.4836 - val_acc: 1.0000\n",
      "Epoch 258/300\n",
      " - 2s - loss: 0.4871 - acc: 1.0000 - val_loss: 0.4835 - val_acc: 1.0000\n",
      "Epoch 259/300\n",
      " - 2s - loss: 0.4939 - acc: 1.0000 - val_loss: 0.4817 - val_acc: 1.0000\n",
      "Epoch 260/300\n",
      " - 2s - loss: 0.4755 - acc: 1.0000 - val_loss: 0.4792 - val_acc: 1.0000\n",
      "Epoch 261/300\n",
      " - 2s - loss: 0.5035 - acc: 1.0000 - val_loss: 0.4769 - val_acc: 1.0000\n",
      "Epoch 262/300\n",
      " - 2s - loss: 0.5070 - acc: 1.0000 - val_loss: 0.4745 - val_acc: 1.0000\n",
      "Epoch 263/300\n",
      " - 2s - loss: 0.5146 - acc: 1.0000 - val_loss: 0.4736 - val_acc: 1.0000\n",
      "Epoch 264/300\n",
      " - 2s - loss: 0.4812 - acc: 1.0000 - val_loss: 0.4727 - val_acc: 1.0000\n",
      "Epoch 265/300\n",
      " - 2s - loss: 0.4699 - acc: 1.0000 - val_loss: 0.4714 - val_acc: 1.0000\n",
      "Epoch 266/300\n",
      " - 2s - loss: 0.4864 - acc: 0.9841 - val_loss: 0.4694 - val_acc: 1.0000\n",
      "Epoch 267/300\n",
      " - 2s - loss: 0.4607 - acc: 1.0000 - val_loss: 0.4684 - val_acc: 1.0000\n",
      "Epoch 268/300\n",
      " - 2s - loss: 0.4623 - acc: 1.0000 - val_loss: 0.4678 - val_acc: 1.0000\n",
      "Epoch 269/300\n",
      " - 2s - loss: 0.5048 - acc: 0.9841 - val_loss: 0.4667 - val_acc: 1.0000\n",
      "Epoch 270/300\n",
      " - 2s - loss: 0.4769 - acc: 0.9841 - val_loss: 0.4662 - val_acc: 1.0000\n",
      "Epoch 271/300\n",
      " - 2s - loss: 0.4698 - acc: 1.0000 - val_loss: 0.4661 - val_acc: 1.0000\n",
      "Epoch 272/300\n",
      " - 2s - loss: 0.4746 - acc: 1.0000 - val_loss: 0.4653 - val_acc: 1.0000\n",
      "Epoch 273/300\n",
      " - 2s - loss: 0.4784 - acc: 1.0000 - val_loss: 0.4636 - val_acc: 1.0000\n",
      "Epoch 274/300\n",
      " - 2s - loss: 0.4708 - acc: 1.0000 - val_loss: 0.4618 - val_acc: 1.0000\n",
      "Epoch 275/300\n",
      " - 2s - loss: 0.4976 - acc: 1.0000 - val_loss: 0.4608 - val_acc: 1.0000\n",
      "Epoch 276/300\n",
      " - 2s - loss: 0.4619 - acc: 1.0000 - val_loss: 0.4611 - val_acc: 1.0000\n",
      "Epoch 277/300\n",
      " - 2s - loss: 0.4893 - acc: 1.0000 - val_loss: 0.4607 - val_acc: 1.0000\n",
      "Epoch 278/300\n",
      " - 2s - loss: 0.4500 - acc: 1.0000 - val_loss: 0.4596 - val_acc: 1.0000\n",
      "Epoch 279/300\n",
      " - 2s - loss: 0.4589 - acc: 1.0000 - val_loss: 0.4582 - val_acc: 1.0000\n",
      "Epoch 280/300\n",
      " - 2s - loss: 0.4591 - acc: 1.0000 - val_loss: 0.4572 - val_acc: 1.0000\n",
      "Epoch 281/300\n",
      " - 2s - loss: 0.4830 - acc: 1.0000 - val_loss: 0.4562 - val_acc: 1.0000\n",
      "Epoch 282/300\n",
      " - 2s - loss: 0.4694 - acc: 1.0000 - val_loss: 0.4547 - val_acc: 1.0000\n",
      "Epoch 283/300\n",
      " - 2s - loss: 0.4605 - acc: 1.0000 - val_loss: 0.4544 - val_acc: 1.0000\n",
      "Epoch 284/300\n",
      " - 2s - loss: 0.4616 - acc: 0.9841 - val_loss: 0.4539 - val_acc: 1.0000\n",
      "Epoch 285/300\n",
      " - 2s - loss: 0.4776 - acc: 1.0000 - val_loss: 0.4528 - val_acc: 1.0000\n",
      "Epoch 286/300\n",
      " - 2s - loss: 0.4427 - acc: 1.0000 - val_loss: 0.4520 - val_acc: 1.0000\n",
      "Epoch 287/300\n",
      " - 2s - loss: 0.4611 - acc: 1.0000 - val_loss: 0.4508 - val_acc: 1.0000\n",
      "Epoch 288/300\n",
      " - 2s - loss: 0.4708 - acc: 1.0000 - val_loss: 0.4501 - val_acc: 1.0000\n",
      "Epoch 289/300\n",
      " - 2s - loss: 0.4780 - acc: 1.0000 - val_loss: 0.4502 - val_acc: 1.0000\n",
      "Epoch 290/300\n",
      " - 2s - loss: 0.4654 - acc: 1.0000 - val_loss: 0.4496 - val_acc: 1.0000\n",
      "Epoch 291/300\n",
      " - 2s - loss: 0.4763 - acc: 0.9841 - val_loss: 0.4491 - val_acc: 1.0000\n",
      "Epoch 292/300\n",
      " - 2s - loss: 0.4389 - acc: 1.0000 - val_loss: 0.4479 - val_acc: 1.0000\n",
      "Epoch 293/300\n",
      " - 2s - loss: 0.4557 - acc: 1.0000 - val_loss: 0.4465 - val_acc: 1.0000\n",
      "Epoch 294/300\n",
      " - 2s - loss: 0.4637 - acc: 1.0000 - val_loss: 0.4455 - val_acc: 1.0000\n",
      "Epoch 295/300\n",
      " - 2s - loss: 0.4491 - acc: 1.0000 - val_loss: 0.4445 - val_acc: 1.0000\n",
      "Epoch 296/300\n",
      " - 2s - loss: 0.4504 - acc: 1.0000 - val_loss: 0.4435 - val_acc: 1.0000\n",
      "Epoch 297/300\n",
      " - 2s - loss: 0.4449 - acc: 1.0000 - val_loss: 0.4427 - val_acc: 1.0000\n",
      "Epoch 298/300\n",
      " - 2s - loss: 0.4557 - acc: 0.9841 - val_loss: 0.4418 - val_acc: 1.0000\n",
      "Epoch 299/300\n",
      " - 2s - loss: 0.4464 - acc: 1.0000 - val_loss: 0.4416 - val_acc: 1.0000\n",
      "Epoch 300/300\n",
      " - 2s - loss: 0.4414 - acc: 1.0000 - val_loss: 0.4411 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_convnet Test Accuracy: 0.372\n",
      "my_model_neu Test f-measure: 0.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "convnet_input_tensor = Input(shape=(maxlen,) , name='convnet_words')\n",
    "\n",
    "conv_1d_s3_model = Sequential()\n",
    "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s3_model.add(layers.Dropout(0.3))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "conv_1d_s3_model.add(layers.MaxPooling1D(2))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s3_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s3_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s3_model.layers[0].trainable = False\n",
    "conv_output_tensor_0 = conv_1d_s3_model(convnet_input_tensor)\n",
    "\n",
    "\n",
    "conv_1d_s1_model = Sequential()\n",
    "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s1_model.add(layers.Dropout(0.3))\n",
    "conv_1d_s1_model.add(layers.SeparableConv1D(64, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s1_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s1_model.layers[0].trainable = False\n",
    "conv_output_tensor_1 = conv_1d_s1_model(convnet_input_tensor)\n",
    "\n",
    "conv_1d_complex_model = Sequential()\n",
    "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_complex_model.add(layers.Dropout(0.3))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(64, 2, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_complex_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_complex_model.layers[0].trainable = False\n",
    "conv_output_tensor_2 = conv_1d_complex_model(convnet_input_tensor)\n",
    "\n",
    "# x = layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen)(word_input_tensor)\n",
    "# x = layers.Conv1D(128, 5, activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# y = layers.Conv1D(128, 10, activation='relu', padding='same')(x)\n",
    "# added = layers.add([y, x])\n",
    "# added = layers.GlobalMaxPooling1D()(added)\n",
    "\n",
    "concatenated = layers.concatenate([conv_output_tensor_0,\n",
    "                                   conv_output_tensor_1,\n",
    "                                   conv_output_tensor_2,\n",
    "#                                    ,added\n",
    "                                  ], axis=-1)\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "# concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "# concatenated = layers.Dropout(0.3)(concatenated)\n",
    "answer = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model(convnet_input_tensor, answer)\n",
    "model.summary()\n",
    "\n",
    "# model.layers[0].set_weights([w2d.word_embedding])\n",
    "# model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=300,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_convnet,\n",
    "                    verbose= 2\n",
    "                   )\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "l_model = load_model('my_model_convnet.h5')\n",
    "yhat = l_model.predict(test_data)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_convnet Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_convnet Test Accuracy: 0.434\n"
     ]
    }
   ],
   "source": [
    "l_model = load_model('my_model_convnet.h5')\n",
    "yhat = l_model.predict(test_data)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_convnet Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(model_names_list):\n",
    "    all_models = list()\n",
    "    for model_name in model_names_list:\n",
    "        # define filename for this ensemble\n",
    "#         filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "        filename = model_name + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "\n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer.name = 'ensemble_' + str(i + 1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "#     print(ensemble_visible)\n",
    "#     ensemble_visible = [[ngram_input_tensor, word_input_tensor], convnet_input_tensor]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "#     ensemble_outputs = [concatenated, answer]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = layers.Dense(128, activation='relu')(merge)\n",
    "    hidden = layers.Dropout(0.3)(hidden)\n",
    "    output = layers.Dense(len(set(train_labels)), activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    # plot graph of ensemble\n",
    "#     plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=3e-4), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy, valX, valy):\n",
    "    # prepare input data\n",
    "#     X = [inputX for _ in range(len(model.input))]\n",
    "    # encode output data\n",
    "#     inputy_enc = to_categorical(inputy)\n",
    "    # fit model\n",
    "    model.fit(inputX, inputy, validation_data=(valX, valy), batch_size=class_size,\n",
    "              callbacks=callbacks_list_stacked, epochs=300, verbose=1)\n",
    "    \n",
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "#     X = [inputX for _ in range(len(model.input))]\n",
    "    # make prediction\n",
    "    return model.predict(inputX, verbose=0)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded my_model_neu_ngrams.h5\n",
      ">loaded my_model_neu_words.h5\n",
      "Loaded 2 models\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/300\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 3.9746 - acc: 0.0952 - val_loss: 3.9717 - val_acc: 0.1111\n",
      "Epoch 2/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.9469 - acc: 0.1270 - val_loss: 3.9344 - val_acc: 0.1111\n",
      "Epoch 3/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.9121 - acc: 0.1111 - val_loss: 3.8974 - val_acc: 0.1111\n",
      "Epoch 4/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8945 - acc: 0.1111 - val_loss: 3.8600 - val_acc: 0.1111\n",
      "Epoch 5/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8515 - acc: 0.2540 - val_loss: 3.8233 - val_acc: 0.2222\n",
      "Epoch 6/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8476 - acc: 0.2063 - val_loss: 3.7860 - val_acc: 0.2222\n",
      "Epoch 7/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7840 - acc: 0.2540 - val_loss: 3.7481 - val_acc: 0.2222\n",
      "Epoch 8/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7556 - acc: 0.3968 - val_loss: 3.7116 - val_acc: 0.2222\n",
      "Epoch 9/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7337 - acc: 0.4286 - val_loss: 3.6751 - val_acc: 0.5556\n",
      "Epoch 10/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7128 - acc: 0.4127 - val_loss: 3.6378 - val_acc: 0.7778\n",
      "Epoch 11/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6748 - acc: 0.6508 - val_loss: 3.5999 - val_acc: 0.7778\n",
      "Epoch 12/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6446 - acc: 0.6825 - val_loss: 3.5634 - val_acc: 1.0000\n",
      "Epoch 13/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6041 - acc: 0.7460 - val_loss: 3.5265 - val_acc: 1.0000\n",
      "Epoch 14/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5906 - acc: 0.7619 - val_loss: 3.4893 - val_acc: 1.0000\n",
      "Epoch 15/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5663 - acc: 0.6984 - val_loss: 3.4529 - val_acc: 1.0000\n",
      "Epoch 16/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5034 - acc: 0.8730 - val_loss: 3.4164 - val_acc: 1.0000\n",
      "Epoch 17/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4916 - acc: 0.9048 - val_loss: 3.3803 - val_acc: 1.0000\n",
      "Epoch 18/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4521 - acc: 0.8254 - val_loss: 3.3440 - val_acc: 1.0000\n",
      "Epoch 19/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3716 - acc: 0.9683 - val_loss: 3.3064 - val_acc: 1.0000\n",
      "Epoch 20/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4081 - acc: 0.8889 - val_loss: 3.2684 - val_acc: 1.0000\n",
      "Epoch 21/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3314 - acc: 1.0000 - val_loss: 3.2300 - val_acc: 1.0000\n",
      "Epoch 22/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3120 - acc: 0.9841 - val_loss: 3.1920 - val_acc: 1.0000\n",
      "Epoch 23/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2772 - acc: 1.0000 - val_loss: 3.1538 - val_acc: 1.0000\n",
      "Epoch 24/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2048 - acc: 0.9683 - val_loss: 3.1151 - val_acc: 1.0000\n",
      "Epoch 25/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2272 - acc: 1.0000 - val_loss: 3.0762 - val_acc: 1.0000\n",
      "Epoch 26/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1550 - acc: 1.0000 - val_loss: 3.0367 - val_acc: 1.0000\n",
      "Epoch 27/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0993 - acc: 1.0000 - val_loss: 2.9970 - val_acc: 1.0000\n",
      "Epoch 28/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0701 - acc: 1.0000 - val_loss: 2.9576 - val_acc: 1.0000\n",
      "Epoch 29/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0563 - acc: 1.0000 - val_loss: 2.9192 - val_acc: 1.0000\n",
      "Epoch 30/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9905 - acc: 1.0000 - val_loss: 2.8804 - val_acc: 1.0000\n",
      "Epoch 31/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9838 - acc: 1.0000 - val_loss: 2.8417 - val_acc: 1.0000\n",
      "Epoch 32/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9396 - acc: 1.0000 - val_loss: 2.8035 - val_acc: 1.0000\n",
      "Epoch 33/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8707 - acc: 0.9841 - val_loss: 2.7656 - val_acc: 1.0000\n",
      "Epoch 34/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8903 - acc: 1.0000 - val_loss: 2.7279 - val_acc: 1.0000\n",
      "Epoch 35/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8781 - acc: 1.0000 - val_loss: 2.6911 - val_acc: 1.0000\n",
      "Epoch 36/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7856 - acc: 1.0000 - val_loss: 2.6546 - val_acc: 1.0000\n",
      "Epoch 37/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8286 - acc: 0.9841 - val_loss: 2.6190 - val_acc: 1.0000\n",
      "Epoch 38/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7151 - acc: 0.9841 - val_loss: 2.5842 - val_acc: 1.0000\n",
      "Epoch 39/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7050 - acc: 1.0000 - val_loss: 2.5506 - val_acc: 1.0000\n",
      "Epoch 40/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6587 - acc: 1.0000 - val_loss: 2.5173 - val_acc: 1.0000\n",
      "Epoch 41/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6429 - acc: 1.0000 - val_loss: 2.4849 - val_acc: 1.0000\n",
      "Epoch 42/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6502 - acc: 1.0000 - val_loss: 2.4529 - val_acc: 1.0000\n",
      "Epoch 43/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5858 - acc: 1.0000 - val_loss: 2.4220 - val_acc: 1.0000\n",
      "Epoch 44/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5268 - acc: 1.0000 - val_loss: 2.3917 - val_acc: 1.0000\n",
      "Epoch 45/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5021 - acc: 1.0000 - val_loss: 2.3625 - val_acc: 1.0000\n",
      "Epoch 46/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4847 - acc: 1.0000 - val_loss: 2.3343 - val_acc: 1.0000\n",
      "Epoch 47/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4291 - acc: 1.0000 - val_loss: 2.3070 - val_acc: 1.0000\n",
      "Epoch 48/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4447 - acc: 0.9841 - val_loss: 2.2807 - val_acc: 1.0000\n",
      "Epoch 49/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4278 - acc: 0.9841 - val_loss: 2.2554 - val_acc: 1.0000\n",
      "Epoch 50/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4194 - acc: 1.0000 - val_loss: 2.2312 - val_acc: 1.0000\n",
      "Epoch 51/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3440 - acc: 1.0000 - val_loss: 2.2082 - val_acc: 1.0000\n",
      "Epoch 52/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3652 - acc: 1.0000 - val_loss: 2.1859 - val_acc: 1.0000\n",
      "Epoch 53/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3227 - acc: 1.0000 - val_loss: 2.1645 - val_acc: 1.0000\n",
      "Epoch 54/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2543 - acc: 1.0000 - val_loss: 2.1443 - val_acc: 1.0000\n",
      "Epoch 55/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2522 - acc: 1.0000 - val_loss: 2.1250 - val_acc: 1.0000\n",
      "Epoch 56/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2431 - acc: 1.0000 - val_loss: 2.1068 - val_acc: 1.0000\n",
      "Epoch 57/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2156 - acc: 1.0000 - val_loss: 2.0893 - val_acc: 1.0000\n",
      "Epoch 58/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2192 - acc: 1.0000 - val_loss: 2.0728 - val_acc: 1.0000\n",
      "Epoch 59/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2160 - acc: 1.0000 - val_loss: 2.0570 - val_acc: 1.0000\n",
      "Epoch 60/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1904 - acc: 1.0000 - val_loss: 2.0421 - val_acc: 1.0000\n",
      "Epoch 61/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1889 - acc: 1.0000 - val_loss: 2.0277 - val_acc: 1.0000\n",
      "Epoch 62/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0938 - acc: 1.0000 - val_loss: 2.0143 - val_acc: 1.0000\n",
      "Epoch 63/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1423 - acc: 1.0000 - val_loss: 2.0016 - val_acc: 1.0000\n",
      "Epoch 64/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1108 - acc: 1.0000 - val_loss: 1.9897 - val_acc: 1.0000\n",
      "Epoch 65/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1004 - acc: 1.0000 - val_loss: 1.9784 - val_acc: 1.0000\n",
      "Epoch 66/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1198 - acc: 1.0000 - val_loss: 1.9678 - val_acc: 1.0000\n",
      "Epoch 67/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0875 - acc: 0.9841 - val_loss: 1.9579 - val_acc: 1.0000\n",
      "Epoch 68/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0958 - acc: 1.0000 - val_loss: 1.9483 - val_acc: 1.0000\n",
      "Epoch 69/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0516 - acc: 1.0000 - val_loss: 1.9392 - val_acc: 1.0000\n",
      "Epoch 70/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0537 - acc: 1.0000 - val_loss: 1.9305 - val_acc: 1.0000\n",
      "Epoch 71/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0422 - acc: 1.0000 - val_loss: 1.9222 - val_acc: 1.0000\n",
      "Epoch 72/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0594 - acc: 0.9841 - val_loss: 1.9143 - val_acc: 1.0000\n",
      "Epoch 73/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0217 - acc: 1.0000 - val_loss: 1.9070 - val_acc: 1.0000\n",
      "Epoch 74/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0172 - acc: 1.0000 - val_loss: 1.9002 - val_acc: 1.0000\n",
      "Epoch 75/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0079 - acc: 1.0000 - val_loss: 1.8938 - val_acc: 1.0000\n",
      "Epoch 76/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9734 - acc: 1.0000 - val_loss: 1.8878 - val_acc: 1.0000\n",
      "Epoch 77/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9848 - acc: 1.0000 - val_loss: 1.8819 - val_acc: 1.0000\n",
      "Epoch 78/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9598 - acc: 1.0000 - val_loss: 1.8765 - val_acc: 1.0000\n",
      "Epoch 79/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9905 - acc: 1.0000 - val_loss: 1.8711 - val_acc: 1.0000\n",
      "Epoch 80/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9857 - acc: 1.0000 - val_loss: 1.8662 - val_acc: 1.0000\n",
      "Epoch 81/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9581 - acc: 1.0000 - val_loss: 1.8613 - val_acc: 1.0000\n",
      "Epoch 82/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9420 - acc: 1.0000 - val_loss: 1.8569 - val_acc: 1.0000\n",
      "Epoch 83/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9743 - acc: 1.0000 - val_loss: 1.8526 - val_acc: 1.0000\n",
      "Epoch 84/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9683 - acc: 1.0000 - val_loss: 1.8484 - val_acc: 1.0000\n",
      "Epoch 85/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9322 - acc: 1.0000 - val_loss: 1.8445 - val_acc: 1.0000\n",
      "Epoch 86/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9503 - acc: 1.0000 - val_loss: 1.8408 - val_acc: 1.0000\n",
      "Epoch 87/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9437 - acc: 1.0000 - val_loss: 1.8372 - val_acc: 1.0000\n",
      "Epoch 88/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9078 - acc: 1.0000 - val_loss: 1.8339 - val_acc: 1.0000\n",
      "Epoch 89/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9134 - acc: 1.0000 - val_loss: 1.8307 - val_acc: 1.0000\n",
      "Epoch 90/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9168 - acc: 1.0000 - val_loss: 1.8277 - val_acc: 1.0000\n",
      "Epoch 91/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9284 - acc: 1.0000 - val_loss: 1.8248 - val_acc: 1.0000\n",
      "Epoch 92/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9205 - acc: 1.0000 - val_loss: 1.8220 - val_acc: 1.0000\n",
      "Epoch 93/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9064 - acc: 1.0000 - val_loss: 1.8194 - val_acc: 1.0000\n",
      "Epoch 94/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9234 - acc: 0.9841 - val_loss: 1.8169 - val_acc: 1.0000\n",
      "Epoch 95/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8894 - acc: 1.0000 - val_loss: 1.8145 - val_acc: 1.0000\n",
      "Epoch 96/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8659 - acc: 1.0000 - val_loss: 1.8123 - val_acc: 1.0000\n",
      "Epoch 97/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8834 - acc: 1.0000 - val_loss: 1.8101 - val_acc: 1.0000\n",
      "Epoch 98/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8860 - acc: 1.0000 - val_loss: 1.8080 - val_acc: 1.0000\n",
      "Epoch 99/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9332 - acc: 0.9841 - val_loss: 1.8059 - val_acc: 1.0000\n",
      "Epoch 100/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8653 - acc: 1.0000 - val_loss: 1.8039 - val_acc: 1.0000\n",
      "Epoch 101/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8652 - acc: 0.9841 - val_loss: 1.8022 - val_acc: 1.0000\n",
      "Epoch 102/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8752 - acc: 1.0000 - val_loss: 1.8004 - val_acc: 1.0000\n",
      "Epoch 103/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8642 - acc: 1.0000 - val_loss: 1.7987 - val_acc: 1.0000\n",
      "Epoch 104/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8756 - acc: 1.0000 - val_loss: 1.7971 - val_acc: 1.0000\n",
      "Epoch 105/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8547 - acc: 1.0000 - val_loss: 1.7955 - val_acc: 1.0000\n",
      "Epoch 106/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8730 - acc: 1.0000 - val_loss: 1.7940 - val_acc: 1.0000\n",
      "Epoch 107/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8503 - acc: 1.0000 - val_loss: 1.7925 - val_acc: 1.0000\n",
      "Epoch 108/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8855 - acc: 0.9841 - val_loss: 1.7912 - val_acc: 1.0000\n",
      "Epoch 109/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8601 - acc: 1.0000 - val_loss: 1.7899 - val_acc: 1.0000\n",
      "Epoch 110/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8437 - acc: 1.0000 - val_loss: 1.7887 - val_acc: 1.0000\n",
      "Epoch 111/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8340 - acc: 1.0000 - val_loss: 1.7875 - val_acc: 1.0000\n",
      "Epoch 112/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8603 - acc: 1.0000 - val_loss: 1.7864 - val_acc: 1.0000\n",
      "Epoch 113/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8222 - acc: 1.0000 - val_loss: 1.7853 - val_acc: 1.0000\n",
      "Epoch 114/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8473 - acc: 1.0000 - val_loss: 1.7842 - val_acc: 1.0000\n",
      "Epoch 115/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8484 - acc: 1.0000 - val_loss: 1.7832 - val_acc: 1.0000\n",
      "Epoch 116/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8488 - acc: 1.0000 - val_loss: 1.7822 - val_acc: 1.0000\n",
      "Epoch 117/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8524 - acc: 1.0000 - val_loss: 1.7812 - val_acc: 1.0000\n",
      "Epoch 118/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8242 - acc: 1.0000 - val_loss: 1.7803 - val_acc: 1.0000\n",
      "Epoch 119/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8227 - acc: 1.0000 - val_loss: 1.7794 - val_acc: 1.0000\n",
      "Epoch 120/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8460 - acc: 0.9841 - val_loss: 1.7786 - val_acc: 1.0000\n",
      "Epoch 121/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8638 - acc: 1.0000 - val_loss: 1.7778 - val_acc: 1.0000\n",
      "Epoch 122/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8297 - acc: 1.0000 - val_loss: 1.7769 - val_acc: 1.0000\n",
      "Epoch 123/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8387 - acc: 0.9841 - val_loss: 1.7761 - val_acc: 1.0000\n",
      "Epoch 124/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8423 - acc: 1.0000 - val_loss: 1.7754 - val_acc: 1.0000\n",
      "Epoch 125/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8414 - acc: 1.0000 - val_loss: 1.7746 - val_acc: 1.0000\n",
      "Epoch 126/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8672 - acc: 1.0000 - val_loss: 1.7738 - val_acc: 1.0000\n",
      "Epoch 127/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8004 - acc: 1.0000 - val_loss: 1.7731 - val_acc: 1.0000\n",
      "Epoch 128/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8297 - acc: 1.0000 - val_loss: 1.7725 - val_acc: 1.0000\n",
      "Epoch 129/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8218 - acc: 1.0000 - val_loss: 1.7718 - val_acc: 1.0000\n",
      "Epoch 130/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8118 - acc: 1.0000 - val_loss: 1.7713 - val_acc: 1.0000\n",
      "Epoch 131/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8126 - acc: 1.0000 - val_loss: 1.7708 - val_acc: 1.0000\n",
      "Epoch 132/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8160 - acc: 1.0000 - val_loss: 1.7702 - val_acc: 1.0000\n",
      "Epoch 133/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8156 - acc: 1.0000 - val_loss: 1.7697 - val_acc: 1.0000\n",
      "Epoch 134/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8203 - acc: 1.0000 - val_loss: 1.7691 - val_acc: 1.0000\n",
      "Epoch 135/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8067 - acc: 1.0000 - val_loss: 1.7686 - val_acc: 1.0000\n",
      "Epoch 136/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8370 - acc: 1.0000 - val_loss: 1.7680 - val_acc: 1.0000\n",
      "Epoch 137/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7901 - acc: 1.0000 - val_loss: 1.7675 - val_acc: 1.0000\n",
      "Epoch 138/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8078 - acc: 1.0000 - val_loss: 1.7671 - val_acc: 1.0000\n",
      "Epoch 139/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8182 - acc: 1.0000 - val_loss: 1.7666 - val_acc: 1.0000\n",
      "Epoch 140/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7956 - acc: 1.0000 - val_loss: 1.7661 - val_acc: 1.0000\n",
      "Epoch 141/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8069 - acc: 1.0000 - val_loss: 1.7657 - val_acc: 1.0000\n",
      "Epoch 142/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8078 - acc: 1.0000 - val_loss: 1.7653 - val_acc: 1.0000\n",
      "Epoch 143/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8399 - acc: 1.0000 - val_loss: 1.7648 - val_acc: 1.0000\n",
      "Epoch 144/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8184 - acc: 1.0000 - val_loss: 1.7644 - val_acc: 1.0000\n",
      "Epoch 145/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8195 - acc: 1.0000 - val_loss: 1.7640 - val_acc: 1.0000\n",
      "Epoch 146/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8009 - acc: 1.0000 - val_loss: 1.7636 - val_acc: 1.0000\n",
      "Epoch 147/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8146 - acc: 0.9841 - val_loss: 1.7633 - val_acc: 1.0000\n",
      "Epoch 148/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8347 - acc: 0.9841 - val_loss: 1.7629 - val_acc: 1.0000\n",
      "Epoch 149/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7867 - acc: 1.0000 - val_loss: 1.7626 - val_acc: 1.0000\n",
      "Epoch 150/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8002 - acc: 1.0000 - val_loss: 1.7623 - val_acc: 1.0000\n",
      "Epoch 151/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7871 - acc: 1.0000 - val_loss: 1.7620 - val_acc: 1.0000\n",
      "Epoch 152/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8040 - acc: 1.0000 - val_loss: 1.7617 - val_acc: 1.0000\n",
      "Epoch 153/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7828 - acc: 1.0000 - val_loss: 1.7615 - val_acc: 1.0000\n",
      "Epoch 154/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8010 - acc: 1.0000 - val_loss: 1.7612 - val_acc: 1.0000\n",
      "Epoch 155/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8044 - acc: 1.0000 - val_loss: 1.7609 - val_acc: 1.0000\n",
      "Epoch 156/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7974 - acc: 1.0000 - val_loss: 1.7606 - val_acc: 1.0000\n",
      "Epoch 157/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7937 - acc: 1.0000 - val_loss: 1.7603 - val_acc: 1.0000\n",
      "Epoch 158/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8018 - acc: 1.0000 - val_loss: 1.7600 - val_acc: 1.0000\n",
      "Epoch 159/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8008 - acc: 1.0000 - val_loss: 1.7597 - val_acc: 1.0000\n",
      "Epoch 160/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7839 - acc: 1.0000 - val_loss: 1.7595 - val_acc: 1.0000\n",
      "Epoch 161/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7938 - acc: 1.0000 - val_loss: 1.7592 - val_acc: 1.0000\n",
      "Epoch 162/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7998 - acc: 1.0000 - val_loss: 1.7590 - val_acc: 1.0000\n",
      "Epoch 163/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7949 - acc: 1.0000 - val_loss: 1.7587 - val_acc: 1.0000\n",
      "Epoch 164/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7922 - acc: 1.0000 - val_loss: 1.7585 - val_acc: 1.0000\n",
      "Epoch 165/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7981 - acc: 1.0000 - val_loss: 1.7582 - val_acc: 1.0000\n",
      "Epoch 166/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7788 - acc: 1.0000 - val_loss: 1.7580 - val_acc: 1.0000\n",
      "Epoch 167/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8087 - acc: 0.9841 - val_loss: 1.7578 - val_acc: 1.0000\n",
      "Epoch 168/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7783 - acc: 1.0000 - val_loss: 1.7576 - val_acc: 1.0000\n",
      "Epoch 169/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7822 - acc: 1.0000 - val_loss: 1.7574 - val_acc: 1.0000\n",
      "Epoch 170/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7824 - acc: 1.0000 - val_loss: 1.7572 - val_acc: 1.0000\n",
      "Epoch 171/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7934 - acc: 1.0000 - val_loss: 1.7571 - val_acc: 1.0000\n",
      "Epoch 172/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7837 - acc: 1.0000 - val_loss: 1.7569 - val_acc: 1.0000\n",
      "Epoch 173/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8169 - acc: 1.0000 - val_loss: 1.7567 - val_acc: 1.0000\n",
      "Epoch 174/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7749 - acc: 1.0000 - val_loss: 1.7565 - val_acc: 1.0000\n",
      "Epoch 175/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7867 - acc: 1.0000 - val_loss: 1.7563 - val_acc: 1.0000\n",
      "Epoch 176/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7832 - acc: 1.0000 - val_loss: 1.7562 - val_acc: 1.0000\n",
      "Epoch 177/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7726 - acc: 1.0000 - val_loss: 1.7560 - val_acc: 1.0000\n",
      "Epoch 178/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7826 - acc: 1.0000 - val_loss: 1.7558 - val_acc: 1.0000\n",
      "Epoch 179/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7810 - acc: 1.0000 - val_loss: 1.7557 - val_acc: 1.0000\n",
      "Epoch 180/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7864 - acc: 1.0000 - val_loss: 1.7555 - val_acc: 1.0000\n",
      "Epoch 181/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7842 - acc: 1.0000 - val_loss: 1.7554 - val_acc: 1.0000\n",
      "Epoch 182/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7760 - acc: 1.0000 - val_loss: 1.7552 - val_acc: 1.0000\n",
      "Epoch 183/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8129 - acc: 0.9841 - val_loss: 1.7551 - val_acc: 1.0000\n",
      "Epoch 184/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7941 - acc: 1.0000 - val_loss: 1.7549 - val_acc: 1.0000\n",
      "Epoch 185/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7810 - acc: 1.0000 - val_loss: 1.7548 - val_acc: 1.0000\n",
      "Epoch 186/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7752 - acc: 1.0000 - val_loss: 1.7547 - val_acc: 1.0000\n",
      "Epoch 187/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7869 - acc: 1.0000 - val_loss: 1.7545 - val_acc: 1.0000\n",
      "Epoch 188/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7714 - acc: 1.0000 - val_loss: 1.7544 - val_acc: 1.0000\n",
      "Epoch 189/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7918 - acc: 1.0000 - val_loss: 1.7543 - val_acc: 1.0000\n",
      "Epoch 190/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7778 - acc: 1.0000 - val_loss: 1.7542 - val_acc: 1.0000\n",
      "Epoch 191/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8047 - acc: 1.0000 - val_loss: 1.7540 - val_acc: 1.0000\n",
      "Epoch 192/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8081 - acc: 0.9841 - val_loss: 1.7539 - val_acc: 1.0000\n",
      "Epoch 193/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7772 - acc: 1.0000 - val_loss: 1.7538 - val_acc: 1.0000\n",
      "Epoch 194/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7896 - acc: 0.9841 - val_loss: 1.7537 - val_acc: 1.0000\n",
      "Epoch 195/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7994 - acc: 1.0000 - val_loss: 1.7536 - val_acc: 1.0000\n",
      "Epoch 196/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7864 - acc: 0.9841 - val_loss: 1.7534 - val_acc: 1.0000\n",
      "Epoch 197/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7782 - acc: 1.0000 - val_loss: 1.7533 - val_acc: 1.0000\n",
      "Epoch 198/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7711 - acc: 1.0000 - val_loss: 1.7532 - val_acc: 1.0000\n",
      "Epoch 199/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7747 - acc: 1.0000 - val_loss: 1.7531 - val_acc: 1.0000\n",
      "Epoch 200/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7773 - acc: 1.0000 - val_loss: 1.7530 - val_acc: 1.0000\n",
      "Epoch 201/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7734 - acc: 1.0000 - val_loss: 1.7530 - val_acc: 1.0000\n",
      "Epoch 202/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7784 - acc: 1.0000 - val_loss: 1.7529 - val_acc: 1.0000\n",
      "Epoch 203/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7683 - acc: 1.0000 - val_loss: 1.7528 - val_acc: 1.0000\n",
      "Epoch 204/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7845 - acc: 1.0000 - val_loss: 1.7527 - val_acc: 1.0000\n",
      "Epoch 205/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7776 - acc: 1.0000 - val_loss: 1.7526 - val_acc: 1.0000\n",
      "Epoch 206/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7913 - acc: 0.9841 - val_loss: 1.7525 - val_acc: 1.0000\n",
      "Epoch 207/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7789 - acc: 1.0000 - val_loss: 1.7524 - val_acc: 1.0000\n",
      "Epoch 208/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7959 - acc: 1.0000 - val_loss: 1.7523 - val_acc: 1.0000\n",
      "Epoch 209/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7877 - acc: 1.0000 - val_loss: 1.7522 - val_acc: 1.0000\n",
      "Epoch 210/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7606 - acc: 1.0000 - val_loss: 1.7522 - val_acc: 1.0000\n",
      "Epoch 211/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7628 - acc: 1.0000 - val_loss: 1.7521 - val_acc: 1.0000\n",
      "Epoch 212/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7667 - acc: 1.0000 - val_loss: 1.7520 - val_acc: 1.0000\n",
      "Epoch 213/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7955 - acc: 1.0000 - val_loss: 1.7519 - val_acc: 1.0000\n",
      "Epoch 214/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7875 - acc: 1.0000 - val_loss: 1.7518 - val_acc: 1.0000\n",
      "Epoch 215/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7761 - acc: 1.0000 - val_loss: 1.7518 - val_acc: 1.0000\n",
      "Epoch 216/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7698 - acc: 1.0000 - val_loss: 1.7517 - val_acc: 1.0000\n",
      "Epoch 217/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7618 - acc: 1.0000 - val_loss: 1.7516 - val_acc: 1.0000\n",
      "Epoch 218/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7641 - acc: 1.0000 - val_loss: 1.7516 - val_acc: 1.0000\n",
      "Epoch 219/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7698 - acc: 1.0000 - val_loss: 1.7515 - val_acc: 1.0000\n",
      "Epoch 220/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7807 - acc: 1.0000 - val_loss: 1.7514 - val_acc: 1.0000\n",
      "Epoch 221/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7835 - acc: 1.0000 - val_loss: 1.7514 - val_acc: 1.0000\n",
      "Epoch 222/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7822 - acc: 1.0000 - val_loss: 1.7513 - val_acc: 1.0000\n",
      "Epoch 223/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7659 - acc: 1.0000 - val_loss: 1.7512 - val_acc: 1.0000\n",
      "Epoch 224/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7664 - acc: 1.0000 - val_loss: 1.7512 - val_acc: 1.0000\n",
      "Epoch 225/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7788 - acc: 1.0000 - val_loss: 1.7511 - val_acc: 1.0000\n",
      "Epoch 226/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7614 - acc: 1.0000 - val_loss: 1.7510 - val_acc: 1.0000\n",
      "Epoch 227/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7658 - acc: 1.0000 - val_loss: 1.7510 - val_acc: 1.0000\n",
      "Epoch 228/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7803 - acc: 1.0000 - val_loss: 1.7509 - val_acc: 1.0000\n",
      "Epoch 229/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7702 - acc: 1.0000 - val_loss: 1.7509 - val_acc: 1.0000\n",
      "Epoch 230/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8030 - acc: 0.9841 - val_loss: 1.7508 - val_acc: 1.0000\n",
      "Epoch 231/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7711 - acc: 1.0000 - val_loss: 1.7508 - val_acc: 1.0000\n",
      "Epoch 232/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7885 - acc: 1.0000 - val_loss: 1.7507 - val_acc: 1.0000\n",
      "Epoch 233/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7894 - acc: 1.0000 - val_loss: 1.7507 - val_acc: 1.0000\n",
      "Epoch 234/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8035 - acc: 0.9841 - val_loss: 1.7506 - val_acc: 1.0000\n",
      "Epoch 235/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7822 - acc: 0.9841 - val_loss: 1.7506 - val_acc: 1.0000\n",
      "Epoch 236/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7996 - acc: 1.0000 - val_loss: 1.7505 - val_acc: 1.0000\n",
      "Epoch 237/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7703 - acc: 1.0000 - val_loss: 1.7505 - val_acc: 1.0000\n",
      "Epoch 238/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7659 - acc: 1.0000 - val_loss: 1.7504 - val_acc: 1.0000\n",
      "Epoch 239/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7757 - acc: 1.0000 - val_loss: 1.7503 - val_acc: 1.0000\n",
      "Epoch 240/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7686 - acc: 1.0000 - val_loss: 1.7503 - val_acc: 1.0000\n",
      "Epoch 241/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7689 - acc: 1.0000 - val_loss: 1.7502 - val_acc: 1.0000\n",
      "Epoch 242/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7589 - acc: 1.0000 - val_loss: 1.7502 - val_acc: 1.0000\n",
      "Epoch 243/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7824 - acc: 1.0000 - val_loss: 1.7502 - val_acc: 1.0000\n",
      "Epoch 244/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7746 - acc: 0.9841 - val_loss: 1.7501 - val_acc: 1.0000\n",
      "Epoch 245/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7611 - acc: 1.0000 - val_loss: 1.7501 - val_acc: 1.0000\n",
      "Epoch 246/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7921 - acc: 0.9841 - val_loss: 1.7500 - val_acc: 1.0000\n",
      "Epoch 247/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7979 - acc: 0.9841 - val_loss: 1.7500 - val_acc: 1.0000\n",
      "Epoch 248/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7878 - acc: 1.0000 - val_loss: 1.7499 - val_acc: 1.0000\n",
      "Epoch 249/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7612 - acc: 1.0000 - val_loss: 1.7499 - val_acc: 1.0000\n",
      "Epoch 250/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7857 - acc: 1.0000 - val_loss: 1.7499 - val_acc: 1.0000\n",
      "Epoch 251/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7591 - acc: 1.0000 - val_loss: 1.7498 - val_acc: 1.0000\n",
      "Epoch 252/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7764 - acc: 0.9841 - val_loss: 1.7498 - val_acc: 1.0000\n",
      "Epoch 253/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7544 - acc: 1.0000 - val_loss: 1.7498 - val_acc: 1.0000\n",
      "Epoch 254/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7774 - acc: 1.0000 - val_loss: 1.7497 - val_acc: 1.0000\n",
      "Epoch 255/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7815 - acc: 1.0000 - val_loss: 1.7497 - val_acc: 1.0000\n",
      "Epoch 256/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7642 - acc: 1.0000 - val_loss: 1.7496 - val_acc: 1.0000\n",
      "Epoch 257/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7556 - acc: 1.0000 - val_loss: 1.7496 - val_acc: 1.0000\n",
      "Epoch 258/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7644 - acc: 1.0000 - val_loss: 1.7496 - val_acc: 1.0000\n",
      "Epoch 259/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7620 - acc: 1.0000 - val_loss: 1.7496 - val_acc: 1.0000\n",
      "Epoch 260/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7568 - acc: 1.0000 - val_loss: 1.7495 - val_acc: 1.0000\n",
      "Epoch 261/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7620 - acc: 1.0000 - val_loss: 1.7495 - val_acc: 1.0000\n",
      "Epoch 262/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7577 - acc: 1.0000 - val_loss: 1.7495 - val_acc: 1.0000\n",
      "Epoch 263/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7629 - acc: 1.0000 - val_loss: 1.7494 - val_acc: 1.0000\n",
      "Epoch 264/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7682 - acc: 1.0000 - val_loss: 1.7494 - val_acc: 1.0000\n",
      "Epoch 265/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7582 - acc: 1.0000 - val_loss: 1.7494 - val_acc: 1.0000\n",
      "Epoch 266/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7725 - acc: 1.0000 - val_loss: 1.7494 - val_acc: 1.0000\n",
      "Epoch 267/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7747 - acc: 1.0000 - val_loss: 1.7493 - val_acc: 1.0000\n",
      "Epoch 268/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7636 - acc: 1.0000 - val_loss: 1.7493 - val_acc: 1.0000\n",
      "Epoch 269/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7833 - acc: 1.0000 - val_loss: 1.7493 - val_acc: 1.0000\n",
      "Epoch 270/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7575 - acc: 1.0000 - val_loss: 1.7492 - val_acc: 1.0000\n",
      "Epoch 271/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7645 - acc: 1.0000 - val_loss: 1.7492 - val_acc: 1.0000\n",
      "Epoch 272/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7715 - acc: 1.0000 - val_loss: 1.7492 - val_acc: 1.0000\n",
      "Epoch 273/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7604 - acc: 1.0000 - val_loss: 1.7492 - val_acc: 1.0000\n",
      "Epoch 274/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7669 - acc: 1.0000 - val_loss: 1.7491 - val_acc: 1.0000\n",
      "Epoch 275/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7630 - acc: 1.0000 - val_loss: 1.7491 - val_acc: 1.0000\n",
      "Epoch 276/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7637 - acc: 1.0000 - val_loss: 1.7491 - val_acc: 1.0000\n",
      "Epoch 277/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7679 - acc: 1.0000 - val_loss: 1.7491 - val_acc: 1.0000\n",
      "Epoch 278/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7783 - acc: 0.9841 - val_loss: 1.7490 - val_acc: 1.0000\n",
      "Epoch 279/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7627 - acc: 1.0000 - val_loss: 1.7490 - val_acc: 1.0000\n",
      "Epoch 280/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7631 - acc: 1.0000 - val_loss: 1.7490 - val_acc: 1.0000\n",
      "Epoch 281/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7612 - acc: 1.0000 - val_loss: 1.7490 - val_acc: 1.0000\n",
      "Epoch 282/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7556 - acc: 1.0000 - val_loss: 1.7489 - val_acc: 1.0000\n",
      "Epoch 283/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7642 - acc: 1.0000 - val_loss: 1.7489 - val_acc: 1.0000\n",
      "Epoch 284/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7584 - acc: 1.0000 - val_loss: 1.7489 - val_acc: 1.0000\n",
      "Epoch 285/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7580 - acc: 1.0000 - val_loss: 1.7489 - val_acc: 1.0000\n",
      "Epoch 286/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7589 - acc: 1.0000 - val_loss: 1.7489 - val_acc: 1.0000\n",
      "Epoch 287/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7588 - acc: 1.0000 - val_loss: 1.7489 - val_acc: 1.0000\n",
      "Epoch 288/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7904 - acc: 1.0000 - val_loss: 1.7488 - val_acc: 1.0000\n",
      "Epoch 289/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7570 - acc: 1.0000 - val_loss: 1.7488 - val_acc: 1.0000\n",
      "Epoch 290/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7801 - acc: 0.9841 - val_loss: 1.7488 - val_acc: 1.0000\n",
      "Epoch 291/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7598 - acc: 1.0000 - val_loss: 1.7488 - val_acc: 1.0000\n",
      "Epoch 292/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7808 - acc: 1.0000 - val_loss: 1.7487 - val_acc: 1.0000\n",
      "Epoch 293/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7650 - acc: 1.0000 - val_loss: 1.7487 - val_acc: 1.0000\n",
      "Epoch 294/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7580 - acc: 1.0000 - val_loss: 1.7487 - val_acc: 1.0000\n",
      "Epoch 295/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7704 - acc: 1.0000 - val_loss: 1.7487 - val_acc: 1.0000\n",
      "Epoch 296/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7759 - acc: 1.0000 - val_loss: 1.7487 - val_acc: 1.0000\n",
      "Epoch 297/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7919 - acc: 0.9841 - val_loss: 1.7487 - val_acc: 1.0000\n",
      "Epoch 298/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7537 - acc: 1.0000 - val_loss: 1.7486 - val_acc: 1.0000\n",
      "Epoch 299/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7686 - acc: 1.0000 - val_loss: 1.7486 - val_acc: 1.0000\n",
      "Epoch 300/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7614 - acc: 1.0000 - val_loss: 1.7486 - val_acc: 1.0000\n",
      "Stacked Test Accuracy: 0.653\n",
      "my_model_neu Test f-measure: 0.627\n"
     ]
    }
   ],
   "source": [
    "\n",
    "members = load_all_models(['my_model_neu_ngrams', 'my_model_neu_words'\n",
    "#                            , 'my_model_convnet'\n",
    "                          ])\n",
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, [X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
    "#                                   X_train\n",
    "                                 ], y_train, [X_scaled_val_data_ngrams, X_scaled_val_data_words, \n",
    "#                                               X_val\n",
    "                                             ], y_val)\n",
    "final_model = load_model('my_model_stacked.h5')\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, [scaled_test_data_ngrams, scaled_test_data_words\n",
    "#                                              , test_data\n",
    "                                            ])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded my_model_neu_ngrams.h5\n",
      ">loaded my_model_neu_words.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'my_model_convnet.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-586ad4dbdf2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m members = load_all_models(['my_model_neu_ngrams', 'my_model_neu_words'\n\u001b[0;32m----> 3\u001b[0;31m                            \u001b[0;34m,\u001b[0m \u001b[0;34m'my_model_convnet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                           ])\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded %d models'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-db38a9b2e742>\u001b[0m in \u001b[0;36mload_all_models\u001b[0;34m(model_names_list)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# load model from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# add to list of members\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mall_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'my_model_convnet.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "members = load_all_models(['my_model_neu_ngrams', 'my_model_neu_words'\n",
    "                           , 'my_model_convnet'\n",
    "                          ])\n",
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, [X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
    "                                  X_train\n",
    "                                 ], y_train, [X_scaled_val_data_ngrams, X_scaled_val_data_words, \n",
    "                                              X_val\n",
    "                                             ], y_val)\n",
    "final_model = load_model('my_model_stacked.h5')\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, [scaled_test_data_ngrams, scaled_test_data_words\n",
    "                                             , test_data\n",
    "                                            ])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_neu Test f-measure: 0.668\n"
     ]
    }
   ],
   "source": [
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
