{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Reading general data of the problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading general data of the problems, done!\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import division\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "# multiprocessing.set_start_method('spawn')\n",
    "from MyUtils import clean_folder, read_files, shuffle_docs, shuffle_docs2\n",
    "from Word2Dim import Word2Dim\n",
    "\n",
    "dataset_path = '.' + os.sep + 'pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23'\n",
    "outpath = '.' + os.sep + 'dev_out'\n",
    "\n",
    "clean_folder(outpath)\n",
    "\n",
    "infocollection = dataset_path + os.sep + 'collection-info.json'\n",
    "problems = []\n",
    "language = []\n",
    "with open(infocollection, 'r') as f:\n",
    "    for attrib in json.load(f):\n",
    "        problems.append(attrib['problem-name'])\n",
    "        language.append(attrib['language'])\n",
    "print('Reading general data of the problems, done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reading problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc count to process:  819\n",
      "process_doc, done!\n",
      "word_set, ready!\n",
      "fit_transform_texts is done!\n",
      "doc count to process:  468\n",
      "Reading problem 1, done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "problem = problems[0]\n",
    "index = 0\n",
    "\n",
    "# used for n_gram extraction and word indexing, a threshold which prevent words appearing lower than this value to be counted in calculations\n",
    "tf = 5\n",
    "\n",
    "\n",
    "infoproblem = dataset_path + os.sep + problem + os.sep + 'problem-info.json'\n",
    "candidates = []\n",
    "with open(infoproblem, 'r') as f:\n",
    "    fj = json.load(f)\n",
    "    unk_folder = fj['unknown-folder']\n",
    "    for attrib in fj['candidate-authors']:\n",
    "        candidates.append(attrib['author-name'])\n",
    "\n",
    "candidates.sort()\n",
    "# Building training set\n",
    "train_docs = []\n",
    "for candidate in candidates:\n",
    "    train_docs.extend(read_files(dataset_path + os.sep + problem, candidate))\n",
    "train_texts = [text for i, (text, label) in enumerate(train_docs)]\n",
    "train_labels = [label for i, (text, label) in enumerate(train_docs)]\n",
    "initial_train_size = len(train_labels)\n",
    "\n",
    "train_texts, train_labels = shuffle_docs(train_texts, train_labels)\n",
    "validation_size = len(train_texts) - initial_train_size\n",
    "class_size = int(initial_train_size / len(set(train_labels)))\n",
    "\n",
    "# train_texts, train_labels, validation_start_index, class_size = shuffle_docs2(train_texts, train_labels)\n",
    "\n",
    "index_2_label_dict = {i: l for i, l in enumerate(set(train_labels))}\n",
    "label_2_index_dict = {l: i for i, l in enumerate(set(train_labels))}\n",
    "train_labels = [label_2_index_dict[v] for v in train_labels]\n",
    "w2d = Word2Dim(lang= language[index])\n",
    "train_tokenized_with_pos, train_tokenized_indexed = w2d.fit_transform_texts(train_texts, train_labels, tf= tf)\n",
    "\n",
    "maxlen = len(max(train_tokenized_indexed, key=len))  # We will cut the texts after # words\n",
    "embedding_dim = w2d.word_embedding.shape[1]\n",
    "\n",
    "# preparing test set\n",
    "ground_truth_file = dataset_path + os.sep + problem + os.sep + 'ground-truth.json'\n",
    "gt = {}\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    for attrib in json.load(f)['ground_truth']:\n",
    "        gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "test_docs = read_files(dataset_path + os.sep + problem, unk_folder, gt)\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs)]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs)]\n",
    "\n",
    "# Filter validation to known authors\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "\n",
    "test_labels = [label_2_index_dict[v] for v in test_labels]\n",
    "\n",
    "test_tokenized_with_pos, test_tokenized_indexed = w2d.transform(test_texts)\n",
    "print(\"Reading problem 1, done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Extraction for Neural Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from MyUtils import extract_n_grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "n = 3\n",
    "vocabulary = extract_n_grams(train_docs, n, tf)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(n, n), lowercase=False, vocabulary=vocabulary)\n",
    "n_gram_train_data = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "n_gram_train_data = n_gram_train_data.astype(float)\n",
    "\n",
    "for i, v in enumerate(train_texts):\n",
    "    n_gram_train_data[i] = n_gram_train_data[i] / len(train_texts[i])\n",
    "n_gram_test_data = vectorizer.transform(test_texts)\n",
    "n_gram_test_data = n_gram_test_data.astype(float)\n",
    "for i, v in enumerate(test_texts):\n",
    "    n_gram_test_data[i] = n_gram_test_data[i] / len(test_texts[i])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_ngrams = max_abs_scaler.fit_transform(n_gram_train_data)\n",
    "scaled_test_data_ngrams = max_abs_scaler.transform(n_gram_test_data)\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_words = max_abs_scaler.fit_transform(w2d.get_texts_vectorized_and_normalized(train_tokenized_indexed)[:, 1:])\n",
    "scaled_test_data_words = max_abs_scaler.transform(w2d.get_texts_vectorized_and_normalized(test_tokenized_indexed)[:, 1:])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(819, 7623)\n",
      "(468, 7623)\n",
      "7623\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train_data_words.shape)\n",
    "print(scaled_test_data_words.shape)\n",
    "print(len(w2d.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 9)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import layers, Input, callbacks\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "callbacks_list_neu = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_neu_ngrams = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu_ngrams.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_neu_words = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu_words.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_convnet = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=35,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_convnet.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=10,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_stacked = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_stacked.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "train_data = pad_sequences(train_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "test_data = pad_sequences(test_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "# X_scaled_train_data_words, X_scaled_val_data_words, _, _ = train_test_split(scaled_train_data_words, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "# X_scaled_train_data_ngrams, X_scaled_val_data_ngrams, _, _ = train_test_split(scaled_train_data_ngrams, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "\n",
    "# train_val_split_index = validation_start_index\n",
    "train_val_split_index = initial_train_size\n",
    "y_train, y_val = train_labels[:train_val_split_index], train_labels[train_val_split_index:]\n",
    "X_train, X_val = train_data[:train_val_split_index], train_data[train_val_split_index:]\n",
    "X_scaled_train_data_words, X_scaled_val_data_words = scaled_train_data_words[:train_val_split_index], scaled_train_data_words[train_val_split_index:]\n",
    "X_scaled_train_data_ngrams, X_scaled_val_data_ngrams = scaled_train_data_ngrams[:train_val_split_index], scaled_train_data_ngrams[train_val_split_index:]\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "# y_test = to_categorical(test_labels)\n",
    "# print(X_train.shape)\n",
    "\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 32)                132896    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 139,753\n",
      "Trainable params: 139,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 5.5519 - acc: 0.0635 - val_loss: 5.5135 - val_acc: 0.1296\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.4895 - acc: 0.1429 - val_loss: 5.4589 - val_acc: 0.1389\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.4465 - acc: 0.1111 - val_loss: 5.4066 - val_acc: 0.1601\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.3581 - acc: 0.1746 - val_loss: 5.3571 - val_acc: 0.1786\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.3226 - acc: 0.1746 - val_loss: 5.3079 - val_acc: 0.1944\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.2531 - acc: 0.2222 - val_loss: 5.2597 - val_acc: 0.2090\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.2199 - acc: 0.2698 - val_loss: 5.2087 - val_acc: 0.2394\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.2099 - acc: 0.2222 - val_loss: 5.1614 - val_acc: 0.2579\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.1441 - acc: 0.1746 - val_loss: 5.1180 - val_acc: 0.2646\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.1061 - acc: 0.1905 - val_loss: 5.0715 - val_acc: 0.2672\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.0853 - acc: 0.2222 - val_loss: 5.0282 - val_acc: 0.2712\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.0270 - acc: 0.2222 - val_loss: 4.9838 - val_acc: 0.2778\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.9652 - acc: 0.1905 - val_loss: 4.9354 - val_acc: 0.2989\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.9486 - acc: 0.2222 - val_loss: 4.8880 - val_acc: 0.3479\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.8955 - acc: 0.2222 - val_loss: 4.8425 - val_acc: 0.3849\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.8643 - acc: 0.2381 - val_loss: 4.7951 - val_acc: 0.4101\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.8069 - acc: 0.2381 - val_loss: 4.7475 - val_acc: 0.3981\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.7650 - acc: 0.2222 - val_loss: 4.7014 - val_acc: 0.4788\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6828 - acc: 0.3333 - val_loss: 4.6560 - val_acc: 0.5331\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.7334 - acc: 0.2222 - val_loss: 4.6078 - val_acc: 0.5304\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6329 - acc: 0.2698 - val_loss: 4.5625 - val_acc: 0.5569\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6062 - acc: 0.3175 - val_loss: 4.5162 - val_acc: 0.5278\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4953 - acc: 0.3492 - val_loss: 4.4698 - val_acc: 0.5926\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.5183 - acc: 0.2857 - val_loss: 4.4176 - val_acc: 0.6429\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4999 - acc: 0.3175 - val_loss: 4.3705 - val_acc: 0.6958\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.3851 - acc: 0.4444 - val_loss: 4.3234 - val_acc: 0.7381\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.3856 - acc: 0.3968 - val_loss: 4.2745 - val_acc: 0.7209\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.3501 - acc: 0.3651 - val_loss: 4.2307 - val_acc: 0.6905\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2958 - acc: 0.3810 - val_loss: 4.1902 - val_acc: 0.6958\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.3063 - acc: 0.3968 - val_loss: 4.1415 - val_acc: 0.7619\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2429 - acc: 0.3968 - val_loss: 4.0987 - val_acc: 0.8135\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.1788 - acc: 0.4603 - val_loss: 4.0504 - val_acc: 0.8571\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2360 - acc: 0.3492 - val_loss: 3.9995 - val_acc: 0.8902\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.1790 - acc: 0.5079 - val_loss: 3.9621 - val_acc: 0.8571\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0980 - acc: 0.4286 - val_loss: 3.9199 - val_acc: 0.8571\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0386 - acc: 0.4762 - val_loss: 3.8701 - val_acc: 0.8955\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0040 - acc: 0.4603 - val_loss: 3.8184 - val_acc: 0.9233\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9944 - acc: 0.4286 - val_loss: 3.7748 - val_acc: 0.9497\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9631 - acc: 0.4921 - val_loss: 3.7309 - val_acc: 0.9511\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9072 - acc: 0.4921 - val_loss: 3.6892 - val_acc: 0.9630\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8829 - acc: 0.4603 - val_loss: 3.6484 - val_acc: 0.9749\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8331 - acc: 0.5079 - val_loss: 3.6071 - val_acc: 0.9775\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.7583 - acc: 0.5397 - val_loss: 3.5705 - val_acc: 0.9841\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.7908 - acc: 0.5238 - val_loss: 3.5279 - val_acc: 0.9960\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.7940 - acc: 0.5556 - val_loss: 3.4949 - val_acc: 0.9987\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6927 - acc: 0.5873 - val_loss: 3.4486 - val_acc: 0.9987\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6573 - acc: 0.5556 - val_loss: 3.4022 - val_acc: 0.9987\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6852 - acc: 0.5397 - val_loss: 3.3625 - val_acc: 0.9987\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.5115 - acc: 0.6825 - val_loss: 3.3182 - val_acc: 1.0000\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.5156 - acc: 0.6190 - val_loss: 3.2767 - val_acc: 1.0000\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.5223 - acc: 0.6032 - val_loss: 3.2298 - val_acc: 1.0000\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.5331 - acc: 0.6508 - val_loss: 3.1849 - val_acc: 1.0000\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4939 - acc: 0.5714 - val_loss: 3.1429 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4604 - acc: 0.6508 - val_loss: 3.1093 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3815 - acc: 0.7143 - val_loss: 3.0775 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2954 - acc: 0.7460 - val_loss: 3.0417 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2677 - acc: 0.6825 - val_loss: 2.9987 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3933 - acc: 0.6825 - val_loss: 2.9615 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3027 - acc: 0.6825 - val_loss: 2.9314 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3055 - acc: 0.7302 - val_loss: 2.8929 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2710 - acc: 0.7143 - val_loss: 2.8688 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2272 - acc: 0.6825 - val_loss: 2.8342 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1284 - acc: 0.7778 - val_loss: 2.7909 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0219 - acc: 0.7778 - val_loss: 2.7491 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0822 - acc: 0.7619 - val_loss: 2.7139 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0725 - acc: 0.7460 - val_loss: 2.6796 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1141 - acc: 0.6984 - val_loss: 2.6559 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0243 - acc: 0.7619 - val_loss: 2.6342 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9467 - acc: 0.8095 - val_loss: 2.6086 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9646 - acc: 0.7778 - val_loss: 2.5773 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9719 - acc: 0.8095 - val_loss: 2.5445 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9102 - acc: 0.7619 - val_loss: 2.5167 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8131 - acc: 0.8254 - val_loss: 2.4900 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8697 - acc: 0.8254 - val_loss: 2.4619 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7051 - acc: 0.8889 - val_loss: 2.4383 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6732 - acc: 0.8413 - val_loss: 2.4109 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7766 - acc: 0.8571 - val_loss: 2.3873 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8265 - acc: 0.7460 - val_loss: 2.3683 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8454 - acc: 0.7619 - val_loss: 2.3598 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7841 - acc: 0.8413 - val_loss: 2.3427 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7638 - acc: 0.8254 - val_loss: 2.3219 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8325 - acc: 0.7302 - val_loss: 2.3027 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7415 - acc: 0.7619 - val_loss: 2.2865 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6312 - acc: 0.8095 - val_loss: 2.2693 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7457 - acc: 0.8095 - val_loss: 2.2435 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7214 - acc: 0.7778 - val_loss: 2.2233 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6489 - acc: 0.7937 - val_loss: 2.2081 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4982 - acc: 0.9206 - val_loss: 2.1898 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6323 - acc: 0.8095 - val_loss: 2.1706 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5329 - acc: 0.9206 - val_loss: 2.1537 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5314 - acc: 0.9048 - val_loss: 2.1363 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6255 - acc: 0.8254 - val_loss: 2.1246 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6043 - acc: 0.8413 - val_loss: 2.1136 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4453 - acc: 0.8413 - val_loss: 2.0998 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4931 - acc: 0.8413 - val_loss: 2.0847 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5858 - acc: 0.8413 - val_loss: 2.0725 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5251 - acc: 0.8095 - val_loss: 2.0605 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5151 - acc: 0.7937 - val_loss: 2.0480 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3935 - acc: 0.9048 - val_loss: 2.0374 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4270 - acc: 0.9048 - val_loss: 2.0260 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3660 - acc: 0.9048 - val_loss: 2.0105 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4819 - acc: 0.8413 - val_loss: 1.9974 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4560 - acc: 0.8730 - val_loss: 1.9853 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4696 - acc: 0.7937 - val_loss: 1.9784 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2855 - acc: 0.8889 - val_loss: 1.9652 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3629 - acc: 0.8730 - val_loss: 1.9505 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3910 - acc: 0.8254 - val_loss: 1.9407 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2453 - acc: 0.9206 - val_loss: 1.9312 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3149 - acc: 0.8889 - val_loss: 1.9170 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2724 - acc: 0.9206 - val_loss: 1.9062 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3488 - acc: 0.8254 - val_loss: 1.8988 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3190 - acc: 0.8889 - val_loss: 1.8923 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2843 - acc: 0.8889 - val_loss: 1.8832 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1177 - acc: 0.9206 - val_loss: 1.8747 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1327 - acc: 0.9365 - val_loss: 1.8632 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2696 - acc: 0.8730 - val_loss: 1.8517 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2834 - acc: 0.8254 - val_loss: 1.8456 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1295 - acc: 0.9365 - val_loss: 1.8388 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1860 - acc: 0.9206 - val_loss: 1.8280 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1301 - acc: 0.9524 - val_loss: 1.8174 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1156 - acc: 0.9365 - val_loss: 1.8083 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1197 - acc: 0.9048 - val_loss: 1.7999 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1831 - acc: 0.8889 - val_loss: 1.7920 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1029 - acc: 0.9206 - val_loss: 1.7860 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1289 - acc: 0.8413 - val_loss: 1.7795 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1368 - acc: 0.9206 - val_loss: 1.7717 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1327 - acc: 0.8889 - val_loss: 1.7631 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0682 - acc: 0.9206 - val_loss: 1.7533 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0940 - acc: 0.9206 - val_loss: 1.7435 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0941 - acc: 0.9206 - val_loss: 1.7362 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0959 - acc: 0.8571 - val_loss: 1.7290 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0984 - acc: 0.8889 - val_loss: 1.7238 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0319 - acc: 0.9524 - val_loss: 1.7150 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9650 - acc: 0.9365 - val_loss: 1.7049 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9994 - acc: 0.9206 - val_loss: 1.6990 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0420 - acc: 0.9048 - val_loss: 1.6952 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0801 - acc: 0.8571 - val_loss: 1.6884 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0456 - acc: 0.8730 - val_loss: 1.6815 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0131 - acc: 0.9206 - val_loss: 1.6734 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0099 - acc: 0.9048 - val_loss: 1.6650 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9758 - acc: 0.9524 - val_loss: 1.6582 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9109 - acc: 0.9048 - val_loss: 1.6518 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8689 - acc: 0.9841 - val_loss: 1.6449 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9777 - acc: 0.9048 - val_loss: 1.6399 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9834 - acc: 0.9048 - val_loss: 1.6350 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9959 - acc: 0.9206 - val_loss: 1.6305 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8752 - acc: 0.9524 - val_loss: 1.6260 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0496 - acc: 0.8571 - val_loss: 1.6176 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9255 - acc: 0.9206 - val_loss: 1.6110 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8905 - acc: 0.9524 - val_loss: 1.6050 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8368 - acc: 0.9524 - val_loss: 1.5981 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9354 - acc: 0.9365 - val_loss: 1.5919 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8073 - acc: 0.9683 - val_loss: 1.5850 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8802 - acc: 0.8730 - val_loss: 1.5780 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8473 - acc: 0.9683 - val_loss: 1.5723 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9188 - acc: 0.9048 - val_loss: 1.5665 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7981 - acc: 0.9365 - val_loss: 1.5612 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7675 - acc: 0.9683 - val_loss: 1.5546 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7803 - acc: 0.9365 - val_loss: 1.5475 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8050 - acc: 0.9365 - val_loss: 1.5418 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9202 - acc: 0.9365 - val_loss: 1.5365 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7475 - acc: 0.9524 - val_loss: 1.5319 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7230 - acc: 0.9365 - val_loss: 1.5262 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8128 - acc: 0.9365 - val_loss: 1.5194 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7543 - acc: 0.9524 - val_loss: 1.5139 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8857 - acc: 0.8730 - val_loss: 1.5098 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7817 - acc: 0.9365 - val_loss: 1.5061 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7720 - acc: 0.9206 - val_loss: 1.5009 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6904 - acc: 0.9524 - val_loss: 1.4942 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7676 - acc: 0.9524 - val_loss: 1.4873 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6561 - acc: 0.9683 - val_loss: 1.4814 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7826 - acc: 0.8889 - val_loss: 1.4764 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8065 - acc: 0.9048 - val_loss: 1.4740 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7955 - acc: 0.9206 - val_loss: 1.4710 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7646 - acc: 0.9524 - val_loss: 1.4659 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6690 - acc: 0.9524 - val_loss: 1.4602 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7135 - acc: 0.9683 - val_loss: 1.4545 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6615 - acc: 1.0000 - val_loss: 1.4487 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7745 - acc: 0.9048 - val_loss: 1.4448 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7912 - acc: 0.9048 - val_loss: 1.4427 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7092 - acc: 0.9524 - val_loss: 1.4387 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7706 - acc: 0.8889 - val_loss: 1.4336 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6935 - acc: 0.9524 - val_loss: 1.4280 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7771 - acc: 0.9206 - val_loss: 1.4244 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6108 - acc: 0.9524 - val_loss: 1.4196 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6577 - acc: 0.9365 - val_loss: 1.4141 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6004 - acc: 0.9524 - val_loss: 1.4087 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5909 - acc: 0.9524 - val_loss: 1.4029 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6606 - acc: 0.9206 - val_loss: 1.3991 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6118 - acc: 0.9524 - val_loss: 1.3953 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7284 - acc: 0.9048 - val_loss: 1.3930 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6008 - acc: 0.9524 - val_loss: 1.3901 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6089 - acc: 0.9841 - val_loss: 1.3858 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5958 - acc: 0.9683 - val_loss: 1.3807 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5959 - acc: 0.9841 - val_loss: 1.3744 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6258 - acc: 0.9524 - val_loss: 1.3695 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6075 - acc: 0.9206 - val_loss: 1.3674 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5495 - acc: 0.9841 - val_loss: 1.3633 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5946 - acc: 0.9524 - val_loss: 1.3586 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6193 - acc: 0.9365 - val_loss: 1.3536 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6309 - acc: 0.9524 - val_loss: 1.3487 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6707 - acc: 0.9206 - val_loss: 1.3445 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5828 - acc: 0.9524 - val_loss: 1.3435 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5823 - acc: 0.9365 - val_loss: 1.3415 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5870 - acc: 0.9365 - val_loss: 1.3375 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4399 - acc: 0.9841 - val_loss: 1.3321 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6075 - acc: 0.9365 - val_loss: 1.3271 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5591 - acc: 0.9524 - val_loss: 1.3236 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5188 - acc: 0.9524 - val_loss: 1.3212 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5323 - acc: 0.9841 - val_loss: 1.3170 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4781 - acc: 0.9841 - val_loss: 1.3109 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5494 - acc: 0.9048 - val_loss: 1.3073 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4825 - acc: 1.0000 - val_loss: 1.3034 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4986 - acc: 0.9365 - val_loss: 1.2984 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5642 - acc: 0.9206 - val_loss: 1.2962 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5018 - acc: 0.9524 - val_loss: 1.2950 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4998 - acc: 0.9524 - val_loss: 1.2902 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5857 - acc: 0.9365 - val_loss: 1.2872 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5706 - acc: 0.9365 - val_loss: 1.2860 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4735 - acc: 1.0000 - val_loss: 1.2809 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5251 - acc: 0.9206 - val_loss: 1.2765 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4939 - acc: 0.9524 - val_loss: 1.2738 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4785 - acc: 0.9524 - val_loss: 1.2695 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5292 - acc: 0.9206 - val_loss: 1.2650 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5319 - acc: 0.9524 - val_loss: 1.2640 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4776 - acc: 0.9683 - val_loss: 1.2627 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4657 - acc: 0.9365 - val_loss: 1.2581 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4621 - acc: 0.9841 - val_loss: 1.2535 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5066 - acc: 0.9365 - val_loss: 1.2513 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5082 - acc: 0.9524 - val_loss: 1.2502 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4100 - acc: 0.9683 - val_loss: 1.2457 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4508 - acc: 0.9365 - val_loss: 1.2401 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4651 - acc: 0.9206 - val_loss: 1.2368 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3857 - acc: 0.9841 - val_loss: 1.2337 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4160 - acc: 0.9683 - val_loss: 1.2297 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4677 - acc: 0.9524 - val_loss: 1.2257 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4853 - acc: 0.9524 - val_loss: 1.2235 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3943 - acc: 0.9683 - val_loss: 1.2206 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4580 - acc: 0.9365 - val_loss: 1.2190 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4133 - acc: 0.9683 - val_loss: 1.2179 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4592 - acc: 0.9365 - val_loss: 1.2160 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5082 - acc: 0.9365 - val_loss: 1.2147 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4154 - acc: 0.9683 - val_loss: 1.2126 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4947 - acc: 0.9206 - val_loss: 1.2106 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4392 - acc: 0.9524 - val_loss: 1.2075 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4406 - acc: 0.9524 - val_loss: 1.2048 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3870 - acc: 0.9841 - val_loss: 1.2017 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3555 - acc: 0.9524 - val_loss: 1.1977 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4611 - acc: 0.9365 - val_loss: 1.1943 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4876 - acc: 0.9524 - val_loss: 1.1977 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4608 - acc: 0.9365 - val_loss: 1.1992 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3674 - acc: 0.9841 - val_loss: 1.1962 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3009 - acc: 1.0000 - val_loss: 1.1904 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3465 - acc: 0.9683 - val_loss: 1.1852 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3363 - acc: 0.9683 - val_loss: 1.1799 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4599 - acc: 0.9206 - val_loss: 1.1765 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4393 - acc: 0.9841 - val_loss: 1.1753 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3426 - acc: 0.9683 - val_loss: 1.1715 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4399 - acc: 0.9206 - val_loss: 1.1712 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4331 - acc: 0.9206 - val_loss: 1.1722 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3324 - acc: 0.9524 - val_loss: 1.1692 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4099 - acc: 0.9683 - val_loss: 1.1668 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3696 - acc: 0.9683 - val_loss: 1.1648 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3565 - acc: 0.9683 - val_loss: 1.1611 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3625 - acc: 0.9365 - val_loss: 1.1593 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3000 - acc: 0.9683 - val_loss: 1.1557 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3640 - acc: 0.9841 - val_loss: 1.1518 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3956 - acc: 0.9524 - val_loss: 1.1489 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3065 - acc: 0.9683 - val_loss: 1.1466 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3517 - acc: 0.9365 - val_loss: 1.1417 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3385 - acc: 0.9524 - val_loss: 1.1396 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3654 - acc: 0.9841 - val_loss: 1.1382 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3451 - acc: 0.9524 - val_loss: 1.1377 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2807 - acc: 0.9683 - val_loss: 1.1362 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3353 - acc: 0.9365 - val_loss: 1.1348 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3133 - acc: 0.9841 - val_loss: 1.1325 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2889 - acc: 0.9841 - val_loss: 1.1279 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2546 - acc: 0.9683 - val_loss: 1.1239 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3177 - acc: 0.9524 - val_loss: 1.1217 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3226 - acc: 0.9524 - val_loss: 1.1215 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2811 - acc: 0.9841 - val_loss: 1.1207 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3104 - acc: 0.9524 - val_loss: 1.1187 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3207 - acc: 1.0000 - val_loss: 1.1150 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2491 - acc: 0.9683 - val_loss: 1.1130 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3692 - acc: 0.9206 - val_loss: 1.1112 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2595 - acc: 0.9841 - val_loss: 1.1087 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3141 - acc: 0.9841 - val_loss: 1.1059 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3597 - acc: 0.9365 - val_loss: 1.1043 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2771 - acc: 0.9841 - val_loss: 1.1050 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3352 - acc: 0.9365 - val_loss: 1.1043 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2848 - acc: 0.9524 - val_loss: 1.1018 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3264 - acc: 0.9683 - val_loss: 1.1006 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2750 - acc: 0.9683 - val_loss: 1.0992 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3456 - acc: 0.9365 - val_loss: 1.0964 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2596 - acc: 0.9683 - val_loss: 1.0931 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2767 - acc: 0.9683 - val_loss: 1.0900 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3253 - acc: 0.9365 - val_loss: 1.0891 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2068 - acc: 0.9841 - val_loss: 1.0888 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2999 - acc: 0.9206 - val_loss: 1.0866 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2741 - acc: 0.9683 - val_loss: 1.0872 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2559 - acc: 0.9841 - val_loss: 1.0832 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2720 - acc: 0.9683 - val_loss: 1.0816 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3253 - acc: 0.9365 - val_loss: 1.0798 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2546 - acc: 0.9683 - val_loss: 1.0791 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2022 - acc: 0.9841 - val_loss: 1.0767 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2911 - acc: 0.9683 - val_loss: 1.0753 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3052 - acc: 0.9365 - val_loss: 1.0749 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2395 - acc: 0.9683 - val_loss: 1.0720 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1986 - acc: 0.9524 - val_loss: 1.0677 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2854 - acc: 0.9524 - val_loss: 1.0676 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2813 - acc: 0.9365 - val_loss: 1.0681 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1788 - acc: 0.9841 - val_loss: 1.0671 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2664 - acc: 0.9206 - val_loss: 1.0663 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3488 - acc: 0.8889 - val_loss: 1.0642 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2454 - acc: 0.9683 - val_loss: 1.0618 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2750 - acc: 0.9683 - val_loss: 1.0592 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2761 - acc: 0.9365 - val_loss: 1.0596 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2132 - acc: 0.9841 - val_loss: 1.0587 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1795 - acc: 0.9683 - val_loss: 1.0580 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3044 - acc: 0.9365 - val_loss: 1.0559 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2287 - acc: 0.9841 - val_loss: 1.0562 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2494 - acc: 0.9683 - val_loss: 1.0540 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2458 - acc: 0.9683 - val_loss: 1.0513 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2677 - acc: 0.9365 - val_loss: 1.0507 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2041 - acc: 0.9683 - val_loss: 1.0478 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2581 - acc: 0.9683 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2520 - acc: 0.9524 - val_loss: 1.0450 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2218 - acc: 0.9841 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2566 - acc: 0.9683 - val_loss: 1.0424 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1689 - acc: 0.9683 - val_loss: 1.0398 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1670 - acc: 0.9683 - val_loss: 1.0389 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2325 - acc: 0.9524 - val_loss: 1.0364 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2580 - acc: 0.9365 - val_loss: 1.0373 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2908 - acc: 0.9524 - val_loss: 1.0410 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2086 - acc: 0.9841 - val_loss: 1.0418 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2185 - acc: 0.9365 - val_loss: 1.0400 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1534 - acc: 1.0000 - val_loss: 1.0347 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2259 - acc: 1.0000 - val_loss: 1.0307 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2268 - acc: 0.9524 - val_loss: 1.0280 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1217 - acc: 0.9841 - val_loss: 1.0252 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1431 - acc: 1.0000 - val_loss: 1.0203 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1756 - acc: 0.9683 - val_loss: 1.0174 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1978 - acc: 0.9365 - val_loss: 1.0168 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2635 - acc: 0.9683 - val_loss: 1.0182 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2103 - acc: 0.9524 - val_loss: 1.0197 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1885 - acc: 0.9841 - val_loss: 1.0188 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1036 - acc: 1.0000 - val_loss: 1.0153 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1652 - acc: 1.0000 - val_loss: 1.0126 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1508 - acc: 0.9841 - val_loss: 1.0121 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3493 - acc: 0.9048 - val_loss: 1.0153 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1995 - acc: 0.9683 - val_loss: 1.0188 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1819 - acc: 0.9683 - val_loss: 1.0157 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2441 - acc: 0.9365 - val_loss: 1.0119 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2161 - acc: 0.9683 - val_loss: 1.0083 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1968 - acc: 0.9365 - val_loss: 1.0082 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1566 - acc: 0.9683 - val_loss: 1.0062 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2102 - acc: 0.9365 - val_loss: 1.0023 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1545 - acc: 0.9841 - val_loss: 1.0028 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1198 - acc: 0.9841 - val_loss: 1.0009 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1255 - acc: 0.9841 - val_loss: 0.9978 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1282 - acc: 1.0000 - val_loss: 0.9939 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1265 - acc: 0.9841 - val_loss: 0.9916 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2129 - acc: 0.9524 - val_loss: 0.9922 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1221 - acc: 0.9683 - val_loss: 0.9922 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2033 - acc: 0.9524 - val_loss: 0.9933 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1649 - acc: 0.9841 - val_loss: 0.9930 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1671 - acc: 0.9683 - val_loss: 0.9921 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1424 - acc: 0.9524 - val_loss: 0.9916 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1137 - acc: 0.9841 - val_loss: 0.9884 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1546 - acc: 0.9841 - val_loss: 0.9857 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1565 - acc: 0.9841 - val_loss: 0.9842 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0746 - acc: 1.0000 - val_loss: 0.9814 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1546 - acc: 0.9841 - val_loss: 0.9809 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0802 - acc: 1.0000 - val_loss: 0.9790 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1471 - acc: 0.9524 - val_loss: 0.9761 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1958 - acc: 0.9524 - val_loss: 0.9793 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2471 - acc: 0.9524 - val_loss: 0.9847 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2161 - acc: 0.9365 - val_loss: 0.9847 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2011 - acc: 0.9524 - val_loss: 0.9841 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 1.1530 - acc: 1.0000 - val_loss: 0.9836 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.1223 - acc: 0.9841 - val_loss: 0.9815 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.1343 - acc: 0.9841 - val_loss: 0.9779 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.2142 - acc: 0.9365 - val_loss: 0.9771 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0971 - acc: 1.0000 - val_loss: 0.9773 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1305 - acc: 0.9683 - val_loss: 0.9758 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0804 - acc: 1.0000 - val_loss: 0.9751 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2082 - acc: 0.9524 - val_loss: 0.9746 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0786 - acc: 1.0000 - val_loss: 0.9731 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1615 - acc: 0.9524 - val_loss: 0.9716 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0854 - acc: 0.9683 - val_loss: 0.9683 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1566 - acc: 0.9365 - val_loss: 0.9667 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1906 - acc: 0.9683 - val_loss: 0.9678 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1513 - acc: 0.9524 - val_loss: 0.9690 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0854 - acc: 0.9841 - val_loss: 0.9692 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1117 - acc: 0.9683 - val_loss: 0.9660 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1910 - acc: 0.9365 - val_loss: 0.9642 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1143 - acc: 0.9524 - val_loss: 0.9614 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0963 - acc: 0.9683 - val_loss: 0.9587 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1502 - acc: 0.9683 - val_loss: 0.9566 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0777 - acc: 0.9841 - val_loss: 0.9550 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1332 - acc: 0.9683 - val_loss: 0.9552 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0770 - acc: 0.9841 - val_loss: 0.9551 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1589 - acc: 0.9524 - val_loss: 0.9528 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1197 - acc: 0.9683 - val_loss: 0.9530 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1137 - acc: 0.9683 - val_loss: 0.9525 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0609 - acc: 0.9841 - val_loss: 0.9500 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1130 - acc: 0.9683 - val_loss: 0.9500 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0846 - acc: 0.9841 - val_loss: 0.9530 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1593 - acc: 0.9524 - val_loss: 0.9534 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1878 - acc: 0.9524 - val_loss: 0.9518 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1217 - acc: 0.9683 - val_loss: 0.9497 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2578 - acc: 0.8571 - val_loss: 0.9493 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0655 - acc: 0.9841 - val_loss: 0.9489 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0884 - acc: 0.9683 - val_loss: 0.9457 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1195 - acc: 0.9365 - val_loss: 0.9451 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1133 - acc: 0.9206 - val_loss: 0.9447 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1110 - acc: 0.9683 - val_loss: 0.9435 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0698 - acc: 0.9841 - val_loss: 0.9428 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0477 - acc: 1.0000 - val_loss: 0.9397 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0723 - acc: 0.9841 - val_loss: 0.9380 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1180 - acc: 0.9683 - val_loss: 0.9405 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0402 - acc: 1.0000 - val_loss: 0.9383 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1628 - acc: 0.9206 - val_loss: 0.9403 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0918 - acc: 0.9683 - val_loss: 0.9396 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0808 - acc: 1.0000 - val_loss: 0.9373 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0429 - acc: 0.9683 - val_loss: 0.9344 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0726 - acc: 1.0000 - val_loss: 0.9315 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1003 - acc: 0.9683 - val_loss: 0.9298 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0788 - acc: 0.9841 - val_loss: 0.9294 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0376 - acc: 0.9841 - val_loss: 0.9278 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0631 - acc: 0.9683 - val_loss: 0.9242 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9902 - acc: 1.0000 - val_loss: 0.9221 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1121 - acc: 0.9206 - val_loss: 0.9217 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0321 - acc: 1.0000 - val_loss: 0.9216 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0852 - acc: 0.9841 - val_loss: 0.9196 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9819 - acc: 1.0000 - val_loss: 0.9172 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0376 - acc: 1.0000 - val_loss: 0.9144 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0654 - acc: 0.9683 - val_loss: 0.9151 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1053 - acc: 0.9683 - val_loss: 0.9152 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0877 - acc: 0.9365 - val_loss: 0.9178 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0619 - acc: 0.9524 - val_loss: 0.9173 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0107 - acc: 1.0000 - val_loss: 0.9147 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0851 - acc: 0.9683 - val_loss: 0.9133 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0561 - acc: 0.9841 - val_loss: 0.9112 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0448 - acc: 0.9683 - val_loss: 0.9124 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0699 - acc: 0.9524 - val_loss: 0.9117 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1092 - acc: 0.9683 - val_loss: 0.9127 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0969 - acc: 0.9683 - val_loss: 0.9134 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0644 - acc: 0.9683 - val_loss: 0.9121 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0253 - acc: 0.9841 - val_loss: 0.9092 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0583 - acc: 0.9683 - val_loss: 0.9077 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0779 - acc: 0.9365 - val_loss: 0.9063 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0380 - acc: 0.9683 - val_loss: 0.9047 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0262 - acc: 0.9841 - val_loss: 0.9031 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0211 - acc: 0.9841 - val_loss: 0.9014 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0857 - acc: 0.9683 - val_loss: 0.9002 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0809 - acc: 0.9683 - val_loss: 0.9027 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1080 - acc: 0.9841 - val_loss: 0.9054 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0232 - acc: 1.0000 - val_loss: 0.9056 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0636 - acc: 0.9683 - val_loss: 0.9026 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0990 - acc: 0.9524 - val_loss: 0.9008 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0222 - acc: 1.0000 - val_loss: 0.8996 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0410 - acc: 0.9683 - val_loss: 0.8998 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0243 - acc: 1.0000 - val_loss: 0.8999 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9726 - acc: 1.0000 - val_loss: 0.8974 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9998 - acc: 1.0000 - val_loss: 0.8951 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1063 - acc: 0.9841 - val_loss: 0.8942 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0523 - acc: 0.9524 - val_loss: 0.8991 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0469 - acc: 0.9683 - val_loss: 0.8990 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0317 - acc: 0.9683 - val_loss: 0.8966 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0282 - acc: 0.9683 - val_loss: 0.8966 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9902 - acc: 0.9841 - val_loss: 0.8935 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0561 - acc: 0.9683 - val_loss: 0.8926 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9836 - acc: 1.0000 - val_loss: 0.8924 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9650 - acc: 1.0000 - val_loss: 0.8891 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9998 - acc: 0.9683 - val_loss: 0.8861 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0060 - acc: 1.0000 - val_loss: 0.8874 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9961 - acc: 1.0000 - val_loss: 0.8865 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0855 - acc: 0.9206 - val_loss: 0.8894 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0079 - acc: 0.9841 - val_loss: 0.8921 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1289 - acc: 0.9683 - val_loss: 0.8929 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0601 - acc: 0.9841 - val_loss: 0.8918 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0402 - acc: 0.9841 - val_loss: 0.8907 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9947 - acc: 1.0000 - val_loss: 0.8892 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0463 - acc: 0.9365 - val_loss: 0.8877 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9893 - acc: 1.0000 - val_loss: 0.8863 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1020 - acc: 0.9524 - val_loss: 0.8850 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0095 - acc: 0.9841 - val_loss: 0.8844 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0238 - acc: 0.9683 - val_loss: 0.8833 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0291 - acc: 0.9524 - val_loss: 0.8821 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9640 - acc: 1.0000 - val_loss: 0.8783 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9825 - acc: 0.9683 - val_loss: 0.8756 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0160 - acc: 0.9683 - val_loss: 0.8740 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9815 - acc: 0.9841 - val_loss: 0.8739 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0297 - acc: 0.9683 - val_loss: 0.8767 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0390 - acc: 0.9524 - val_loss: 0.8773 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0130 - acc: 0.9683 - val_loss: 0.8766 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0448 - acc: 0.9683 - val_loss: 0.8779 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0048 - acc: 0.9683 - val_loss: 0.8780 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0087 - acc: 0.9524 - val_loss: 0.8773 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJztnXmcFMX1wL+Pa5dzgV0uOQQVlUMOJXiwKkZDwAvPKGq8JR5o/MVEjTFqjCYe8YhHjMYjHiiiRgUDGoMkeAsEFxDkUFdZbhAWluVaqN8f1UX3zPbMzu7O7uzMvO/nM5/urqquftXT8+b1q1dVYoxBURRFySwapVoARVEUJfmoclcURclAVLkriqJkIKrcFUVRMhBV7oqiKBmIKndFUZQMRJV7BiMijUWkTER6JLNsKhGR/UQk6fG7InKciBQHjheJyJGJlK3BtZ4UkZtqer6iJEKTVAug+IhIWeCwBbAd2OUd/8wYM7469RljdgGtkl02GzDGHJCMekTkUuA8Y8zwQN2XJqNuRYmHKvcGhDFmj3L1LMNLjTH/jlVeRJoYYyrqQzZFqQp9HhsW6pZJI0TkDhF5WUReEpHNwHkicriIfCIiG0VkpYg8JCJNvfJNRMSISE/v+AUvf6qIbBaRj0WkV3XLevmjRGSxiJSKyMMi8qGIXBhD7kRk/JmILBWRDSLyUODcxiLygIisF5GvgJFx7s/NIjIhKu1REbnf279URBZ67fnKs6pj1VUiIsO9/RYi8rwn2xfAISHX/dqr9wsROdlLPwh4BDjSc3mtC9zb2wLnX+61fb2IvCEiXRK5N9W5z04eEfm3iHwvIqtE5PrAdX7r3ZNNIjJLRPYKc4GJyAfue/bu5wzvOt8DN4tIbxGZ7rVlnXff8gLn7+21ca2X/2cRyfVk7hMo10VEykUkP1Z7lSowxuinAX6AYuC4qLQ7gB3ASdg/5ubAD4BDsW9h+wCLgXFe+SaAAXp6xy8A64AhQFPgZeCFGpTtCGwGRnt5vwB2AhfGaEsiMr4J5AE9ge9d24FxwBdANyAfmGEf29Dr7AOUAS0Dda8BhnjHJ3llBPghsBUY4OUdBxQH6ioBhnv7fwL+A7QD9gYWRJX9CdDF+07O8WTo5OVdCvwnSs4XgNu8/RGejIOAXOAvwHuJ3Jtq3uc8YDXwcyAHaAMM9fJ+DRQBvb02DALaA/tF32vgA/c9e22rAK4AGmOfx/2BY4Fm3nPyIfCnQHvme/ezpVd+mJf3BHBn4DrXAa+n+neYzp+UC6CfGF9MbOX+XhXn/RJ4xdsPU9h/DZQ9GZhfg7IXA+8H8gRYSQzlnqCMhwXy/wH80tufgXVPubzjoxVOVN2fAOd4+6OAxXHKvgVc5e3HU+7fBb8L4Mpg2ZB65wMnePtVKfdngT8E8tpg+1m6VXVvqnmffwrMilHuKydvVHoiyv3rKmQ4A5jp7R8JrAIah5QbBnwDiHf8OXBasn9X2fRRt0z6sSx4ICIHisg/vdfsTcDtQEGc81cF9suJ34kaq+xeQTmM/TWWxKokQRkTuhbwbRx5AV4Exnj75wB7OqFF5EQR+dRzS2zEWs3x7pWjSzwZRORCESnyXAsbgQMTrBds+/bUZ4zZBGwAugbKJPSdVXGfuwNLY8jQHavga0L089hZRCaKyHJPhr9HyVBsbOd9BMaYD7FvAYUi0h/oAfyzhjIpqM89HYkOA3wcaynuZ4xpA9yCtaTrkpVYyxIAEREilVE0tZFxJVYpOKoK1XwZOE5EumHdRi96MjYHXgX+iHWZtAX+laAcq2LJICL7AI9hXRP5Xr1fBuqtKmxzBdbV4+prjXX/LE9Armji3edlwL4xzouVt8WTqUUgrXNUmej23Y2N8jrIk+HCKBn2FpHGMeR4DjgP+5Yx0RizPUY5JQFUuac/rYFSYIvXIfWzerjmW8DBInKSiDTB+nE71JGME4FrRaSr17l2Q7zCxpjVWNfBM8AiY8wSLysH6wdeC+wSkROxvuFEZbhJRNqKHQcwLpDXCqvg1mL/5y7FWu6O1UC3YMdmFC8Bl4jIABHJwf75vG+MifkmFId493kS0ENExolIMxFpIyJDvbwngTtEZF+xDBKR9tg/tVXYjvvGIjKWwB9RHBm2AKUi0h3rGnJ8DKwH/iC2k7q5iAwL5D+PdeOcg1X0Si1Q5Z7+XAdcgO3gfBxrudYpngI9C7gf+2PdF5iDtdiSLeNjwDRgHjATa31XxYtYH/qLAZk3Av8HvI7tlDwD+yeVCLdi3yCKgakEFI8xZi7wEPCZV+ZA4NPAue8CS4DVIhJ0r7jz38a6T173zu8BnJugXNHEvM/GmFLgR8Dp2A7cxcDRXva9wBvY+7wJ27mZ67nbLgNuwnau7xfVtjBuBYZi/2QmAa8FZKgATgT6YK3477Dfg8svxn7PO4wxH1Wz7UoUrvNCUWqM95q9AjjDGPN+quVR0hcReQ7bSXtbqmVJd3QQk1IjRGQk9jV7GzaUrgJrvSpKjfD6L0YDB6ValkxA3TJKTSkEvsa+ro8ETtEOMKWmiMgfsbH2fzDGfJdqeTIBdcsoiqJkIGq5K4qiZCAp87kXFBSYnj17puryiqIoacns2bPXGWPihR4DKVTuPXv2ZNasWam6vKIoSloiIlWN0gbULaMoipKRqHJXFEXJQFS5K4qiZCCq3BVFUTIQVe6KoigZSJXKXUSeFpE1IjI/Rr54y2wtFZG5InJw8sVUFEVRqkMilvvfibNuJXa1m97eZyx2Fj9FURQlhVQZ526MmSHeoskxGA08500P+ok353UXY8zKJMmYMrZsgaIiqKiAjz+2x0o12bwZli+HAw+sumyiGAPz5sKBfaBZM5v2zTfQpjXkJ7oAUhUs99bK6BpvDZIE2LIFvv0W+vatvUw1pXQjFM21MqxaBfvsAy1aWNlmz4b99oW9YrTz66/tfS2IGjNjDBR9Ds1yYK8u0LYd7NoFc+fCwIGwbBmUl9uyu3ZB//61b8emTfZ7yc+HbdusDOXlkJsLvXrBhg22fTt2wPbtto39+0NZmZWnT5/49RsD8+fBPvvC4sX2eNMmOOAA6NLFL7fgC/tcSyNo0Rz6e/OcFRfD1q3QvDn07AkbvofvN8C+lddBOekk+MEPan9L4pHQ3DKecn/LGFPpGxKRt4C7jDEfeMfTgBuMMZVGKHmT/Y8F6NGjxyHffptQLH5KKC2FQw+FRYv8NKnr9Y0yEbPbbkVI3gJRxv7wwP7AIq6TpG6kZNVXJ+2voQxBpBGh9zHyxDj5gbzo+kQi8yA57Q9rR/D6oe0MyFKlDKay3MH6Y5VxiiH6fsR5hv7yF7j88jiixEFEZhtjhlRVLhm/hLC7FXqHjDFPGGOGGGOGdOhQ5ejZlPLPf1rF/thj8M479o9/9279VPtDY/vZXJ68Ov/5tq1zxKjK10lG/dt3Jq++nvvaeuYtSN13cNU1VoZGTe326B/a9J+Mscd57dm9y1Q+75vvYt+HRx7z83Jb2rT7HrTHl1/l57nP/4pq347hx1au13127gpPX76K3R272P0vl8Svf/yEyue3bGPPd2XWfl+5zJZt7F6xOjIt+Exu2VrpWjVV7NUhGcq9hMj1JbthF25Ia2bMgLw8uOwyGDECunWr+py0Z9s22LmzbureHmc2YPf6u2mT3d+8uXJ+WZnd//57WL/e7u/a5eeHsWVLZJnoeqOv4fK//DIyL9Z5mzZVLrN9u3VlGGPdA0625cth6VJ7j3fsqFzX5s32V+/qcceO7dsjz3P3KigD2Pu0a1fked9/b/ddfQWe66qoyG5LS631EmT7dmvVBK/nZP/+e/jwQz9vb2/lvcbe0qgffFC5fc6nuX27rcdRWmpdKzt3Rj4HwfsCNm/u3Mr1Oj6LsZTA6tWQk2P3Fy/203fu9GVaudK6dMLqHzEC1qyx9UTX4VizBv7978i0jRv9/e++i2xzfWGMqfID9ATmx8g7Abv0mACHAZ8lUuchhxxiGioVFcb07GnMCSekWpJ6BowZNCj5dYIxy5fHLvPCC365k0+2288+8/PvuMOmrV1rTKNGftmjj7b5W7f6aY5du+zx5Zfb4/feM6ZZM2OKi8NluPtuW37NGmNeesmv78UXK8tjjDEffmjTp0415ssv7f6zzxpz+ul2f8IEYwYO9Ovp3Nnf79kzsq5XX7XpP/6x304w5qab/DItWxrTqZPdX7PG5o8YYbfTp9v0igp7fMUV/nkdO/rXdZ8RI4zZts3ey2OOsWlTpkTKdO65ked8/LG93xdfbEy7dpF5hYX2nNtvr3wt9/nnP22ZHj2Mad/e7q9eHSnTnXf6x1OmGPP113b/8ceNWbYsdt3xPm+/bUzv3v7xe+/Za596qj0uLrbb1q2NGTmy8vl/+YvdvvuuMfPm1UwGMGbvvcOfuxoAzDIJ6NhEQiFfwq64c4CIlIjIJSJyuYi4F4sp2EUblgJ/A65M7t9P/fP887Zv5IILUi1JCvj887qpN57lErQQJ02y2xkz/LTnvCVLFy6MtGYrKuzWWfVBnMX317/a7ddfW6vz0xhLgD77rN2WlMC6dZXlcVvHW97yqx995Ft8r79urTSwPfDOMgbb0ecoLo6sa/Jku3X3YeJEu/3zn/0yW7b41uOaNXb7r3/Z7WveMqVr19rtY4GANVc2yKpVtq7du2HQIJsW/QZQXGw7I//yF3u8YgV88QW88Ya1ci++GObMsZ2F7l4HrdVoXN5331nL35jIZ+1f//KvBfbNwLXnz3/273GbNrGvEWTaNL+tubl++rvv2u3rr9ut+x43b4a3365czxDPtb1iRexnJxG+/Tb2G2YdUaVyN8aMMcZ0McY0NcZ0M8Y8ZYz5qzHmr16+McZcZYzZ1xhzkAnpSE0n1q2Dn/0MDjsMTj011dKkOUFFHE+55+dXTgtT2NEd8E65h4UxlZZGHrsyQYUbxHWKrV8fea5zNSxdGlneKdoWLXw5RHxFF+s6YbjzHc8/b7exlEG0Inb3xclUFatX+2XdtNtbt0aWKS2F3r3hlFPs8fLl9t44F8/IkfaP4YgjfMVdWgp77RV+zejvo6SkshskGLHQooV/vHChfz9PPz2hJjJ0qN2uXg1NAkGBa9ZEPpfuXsfigAP8epYsiV/2+OPj56+oX2+1rqEaxfvvWwPvT3+KfCaymj//GaZPh44d7b/eunVw/fXhZe+7z/4wr7jCWniOeD736B8+hCvsaH9nLMt9/XoYPtw/Pu00/w9k7lwr++LFNhxqxw7Yf3+/7OrVkRbo+PF2+/XXVsGddZY953//s+m//rWvAJ01CFUr9wsvhK++svvRPupPPrHb8nIYNQqeeMLPu+IK/23EMXkyHHssvPeen3bEEfDUU+HXXr3ath185X7RRfD009YybtoU5s+3sXou8OGaayLr6NTJbtu29b+/jRvtcZgSu/JKePNN//j11+FXv4osU1Li7//nP/Doo3bf+dv33huOOgqeeSa8XUFatbK+9htu8PsYwN6TOXP8408/tffAvU0NGhT5RtG6tbX8r78e2rWLf82QkMcIjjjCPoejR8Ott1bdhtqSiO+mLj4N1ed+7bXG5OZal2RWsXt3Zb+1I8yHGItg/kcf+ccffRT7nOOPr1z/lVf6+QccYNN+8pPIMq5/4JNPIq97yy2V6+vUyW47dAhvT79+dvunPxlz2WXWLxz07x9wgDFPP52Yf7VFi5r7ZsM+Z55ZvfL9+9vtDTdUzisoiDyeOzd2PddeG/v7X7TI5t18szEito/j2GONOfzwxGTs0yf8Ox03Lrz84Ydbn//69cY0buyn/+xnVs5PPrH+/OBzcMopkXX07Wu3IpHXPekkY267zZif/tSY++/30//2N1uPew4OPtiYq64y5t57jXnlFfucuLKPPGLMjTeGy37aaZHHHTvG/SlWBcnyuWcbM2ZY49R1sGcNYREc1SVoqUOk9RrPLbN6tbVQR4/209zrf5DgoAPwI3uirfygfzt4DfD9uNE4t4Sz3Dt0sG8hjs2brfXYooVvzcbiqKPi51eXsAiNINFurY8+spZ1sC/DEb36WbzV0Nq2jZ0XtNyNsW9PpaWR5zwWZ7D6woVW7gkT/L6Fn/wEHn4Yhg2rXH7pUlt3+/aRbyR//Ss88IB9E/nb3yLPufNOf//GG/03hdxcePll3+XSubO1pJ97zv74HZdearfOjfPww/DII/DLX8IZZ8All/hlr7wy0rfvePRRv08E4Ic/jIy+qUNUuQcoLbVv20cfnWpJaslHH1k3QiymT/dHYDqCD2BZmX3VLy6OVHBBNm+2P5CNG/0OwT/+0c/fsgVuv90/nj/fdmZNmWLPe+UVeOEFePVV28nWqZP9kTkWLrSDDV5+2Q/ti3Z1bN5sf2zBMLQHHqhZp7C7XzNn2va0bWtjYR2lpfb6/ftDy5bx6zryyOpf3+FGQh50kJ9WlYvHjdJ1tGoFAwaE3wcXtuho3Tp2vcH2Ozp1spaP69h0ZR580Pr+8/L8foqqRvcOHGj96u6P3/1huG2QtWv9P47mzcPri04P1pOXZ68XxLlrgs9d8L5HE50XvJ5I+CjH6Hv405/abXX6ZGpKIuZ9XXwaolvmpJPsW9N//5tqSWpJ8NU0jLw8Y8aO9Y9XrIh8bXRhYqNGxX6t/tWvIo+DdbRpY0MBwYaYJfKafv314e6UZH46dLAhhYmUPe44Y/7xj8qv0xdcYEMZwZj8/PBzn3/euozi1Z+XF55+9NF2Gx2KGO/TpYvd5uYas9de9jv95S8jy7RpY7dvvOGn9eoV+bxEf555xuaff76fdtNNxgwf7j87H3wQ6Sa5+mpjcnLs/qxZ4fW68Ezn9pkzxx7PnGmPgyGRwc8119j8SZPCn/GZMyPTXTgsGPPkk9bXCsY8/LDNd/f6kUcq/z6CbsGrrjKmbdvKv6PFiyOv97vfRd6H/HzbNmOs26dpU2PWrbP5Dz0U/ttMANQtUz2MsYblqFHJf6uuV4KRALHyS0sjrbroQTozZ9qtC+sLIziIBSLD+1q08DvVpk6NL4+jbdtwiy2a2oRqdu9uXUeXXVY5b8gQGDvWP163rrLVtWZNZDTIhAn+q/lNN9lOTbBW7axZthPZGLj66srXC77+B3FWZE0Wj58yxX8ji7ZSDz/cynLMMX5avLc78K36Z5/1Veydd9o3P8ewYZEd2gcd5FvueXn+eY7iYvtWEZRx0CBbxoUd3nST/zYYxFnuLVqEyxud3iig3g46yL5xGAPjxkWWi37uNm70O3PBvh1GuxzjyfGb38BDD9lnyIWa3nqrdX3m59vnKOyZSDKq3D02bLBvhyNGpFqSGlJRYR/cYFxz2MhKN5nTvHm+uyM6tM5Fi0SPWgzi/gAcLlSwe3frv161yroLghMuxaNt28jX41hUFZEQhvOFtmplI0HCwqB69IicWKq4ONznHFQEnTr5P/o+fSJ9ro0b++6SMF+sU3DRuGs2beqnJXJfomWLrt9FFsVzw0QTHOkaj2D7Bg7072+Y+yrofov+AwoS5hKKdstEu0FiuWsg/sRl8foW4hHvevGop6lXVLl7JGsSwJTRtKkN04uYvW5B5XKu83HrVl8hR4ciOkssWukHiZ6m4Pzz7bZXL1v36tU2dDLRH0Benq+cevSIXa4mPyhnrTpl4zrSgrRqFRkS2a9fuILp3NkPIywosO0Fq9wHD7b77dtHnhP2Z3JwjGUPXPo++/hpJ54YXhbsW4J7CwgqjehZOPv1s1unEBOZBa+gGjNsuk7Qfv1syB9EKnf3w8rNtQZATk78WRrDIhrc9+Gegei+hnjPRpiV7azqjh1jnxeP6Ou5ZyH43aUQjeT2cCG2aancXXTIK69EpofFGwdfoefOtYou3sjCa66xcdfBuTsefdT+mRxwgB1V+PLLfl7PnjbkaOVKq6zDrFaw8cVOSUKkW6Z/f7jnHjj7bHv89NP2Wnvt5b/yO9q1s26Dv/8d/vEPP/3JJ209W7fakZVTp/rtvPpqq1gqKuDxx+3o09xc65N7+22rRHr3Dpe7UycbrXHVVfaP9I474IQT4JBDrCV69NFQWBh5jlPuN9xgB7ps22ZdOEOG2LetLVv8P4bLLrP38Ec/svktWtjr5OfD3XfbMg8/bPO3b7cKvXVr664KKvdmzew97tXLdmY7hQv2rS3MWn31VTuJkjH2re+HPwy/B2FMmmSjmVq2tCNs582LHE06e7b/PF53nR0bEOvZcPJHE+2Wif4DCFPg33xTeYCW4+67bYRWrLeoqoiW/7zz7PPxox/VrL4ko8rdw1nuaTlBWKwJlcJCAoPKvagIzjwzfBCRo3t3a8EFlfvFF0c+2NHKHeyPav/9Y/+A3QhCR16e/7resqUNNXNcdFFs+TZssJNjL1gQqdwHDbIKF/w/BDdoqFEj3//25Ze+cheBH//YryNsErXOna18LiImN9d/M2jSBI47rvI5Trk3bRrZobPffpXLiviyBS3bk0/2lXu/fpXfPqL/UMC/x0E/O8R2UQweXHOrs31769cH+2cT/DMBq/Tcn3d+fvio5CBBxd2+vQ2NrYnlHq/vIien8r2pDtFvP8HvrgGgyt1j+XL73STqIk4qX31lwwZ37YK77qr6H6a83I5UbNzYdui5sKpWrSKV93vvWat71y5rvaxbF+mzLiqyIWbBeN1oOnWK/JPIy4tU2NHWkrMei4utAkx0wEDbtlb+Fi3sNtpCr4po6yzoGnGWWVjsvPMrh/mHg35vRyKdvtE4JRTsWKwurVr5+zX19VZFVSGe9Unwuenb147idcECTqlG34ew7yuLUeXuUVJiXW8peT7OPdeflGjlSn/So1i8+aY/mdYzz1hFD77y2Hdfa42/+mrlc92EXHvvbS3+eANNwCrJoNsmWrkFf2Bjx/rHO3dal0CjkG4dF1P/+ON2Ih/wrbLf/MZfoua++8KtW8ett/oW9DXX2D+Un/0MXnwxMp47Lw9uvjncqh471rptoofCO+64w8rz7LNW+dWk8+3KK+0bwnXXxS4zcaI/lXEYwaHvdaXcg38gqSZolT/7LPz+9/533bWr/b6D0U2OW26pX+v5j3/03xAbGonES9bFp6HFuY8aZUzKRDr0UD8+tnPnqstPnuyXb9688vDmqVP94fSxPmPG2K2bTjfW5z//sVOuuuOjjoqU5auv/Lx16/wpcsHGIxsTWV+3bpHnu/TNmxO/X2ExzpmOi9EGG1+dTFy9u3Ylt97a4KY1Bjs1hrIHNM69eixfnsLO1KDvLtpPvnSptXCD05EGrZq8vMrn5OSEh88Fw+CcT3RWFZN4tmoV6YaJttyDbplWrSKtyrBQt1hRGg3JJdAQCbop6spyD3vLShXB9ur6ljWiAX2bqaWkpIFGyvz613ZNrhNOiFzNxpGXV3meipyc8MYE/e2usy06Xh0i/wSilbsLH3MEFU2zZpHH3b0FuuItDu0mza/OD7hbtwYTbpYSkq3cXVRSQyIsWkapFqrcsX1x33/fQCz3aNxApOASbMFJuMKUe7Nm4RERQYXoYsmXL7dW8+7d9iV4w4bIiapatvSV+3HH2T+bINHza0Qfg52w5+OPw9v3zDNVj6qNZtkyP/IlG0m2cn/ppep/B3WNKvdak/UdqosW+YEWsdYZqHOilXtFhR8+F5yt0VnuQeXepImNkAmu8p6TEx676wZZgI1qadzYRou0b+/L0LZtZOSJmxfbyRkta3QPtPsjCIagxXITuTqV6hEvPrymNLTvoSG5iNKUrL+DBx7ojx+pzoC8avHOO/bHE72ajyP6hxVUrkEXTGmpLfvb3/ppH31kt8HRlTk54bPbBYe2N27sR6hEj8QM+jtbtvR/aGEKIDrNhTBGD+Rwvnk3UlKpPi5OVxWfkgBZ/ZRETwNe1biKGvPCC3YbPdmWI1pBuvlfwCp3pzDdXOTRy82BP5Uo2FfavfaC++/3095/3w91c6/1TqlHh/cFlUfTpv4bQSJx2occYgcTPfxwZHrHjrZTeMKEqutQwpk9G/7731RLoaQJWa3co9cOrjPL3blYotfKdMSz3Hfs8AfkBBduBn8+jzPPjHR7OMvbLf8GdgSjU+4uMsUp9ZpOnBSLU08NH7z04x+Hz9eiJEaXLmk+ZalSn6hyD1BnlntVyj2aaLeME+y88yLLOUXZqlXkABSnWKM73pxSd2VjuWUURUl7srpDNajcRZJvwO7BKfewuUrcxYNEu2VihfEELfFg+KKLNIieGsCVj7bgqwoTOvpou8j0XXeF5991V82G5SuKUmdktXIPRhC2bl396UwSxlUcNr86JO6WiSY4T3nQ+o5lubtjp9Td4Kd482q7+oLL8EVzww3xz1cUpd5Rt4xHbeZ0qhI3OdW774YvFF1VtEwsf5Hr+Iye8yQ4C2EQd21nubuO2ZpOeaooSoMlq5W7Cz6B2GsnJAUXljN9Otx2W+X8eG6ZHTti+4vcn0a05R4rZtmFS55zjt260aFhi1c0bhwZF68o9U3z5uruqwVZ7ZbZuNGOkP/ggzpe+So4Da+bOD5IVZZ7rBGJQeWeSIfB/vtbWZxb5p577FTDYVNhlpc3vIEtSnaxcaM+g7Ugq5V7aak1eOOt6pYUgso9bIWf6LUqy8v9eRR37ow9J7qLvmnRIvGJt4LlGjWKvcivDv9WUo0+g7Uiq90yGzfWYYRMkOBoqYULrTUSXD0pOormkkus4nV/BLGUuzuvWTO1cBRFiSCrlbuz3OucsjL/9WDyZLt94gk/Pzh/jGPkSPj6a7vfrFn4yERnuYctwOz44AO7hqaiKFlFViv3erXcCwutknbDYJcs8fPDlPu55/r7OTnhIxOdOyeech82TOdzUZQsJKuVe71Y7rNm2elp3eyKmzbZ9OC0urEWYnZU5XOPp9wVRclKslYrGFNPlvv48XZ73HF2IJBbpDm4WHPQcr/4YuuSCYaARXcsjR1rp7M89ljr8ikstOl3323nY1cUJevJWuVeXm69GnVuuZeV2RkazzwTrr3WHy0VnHkxqNyfespugyOsoi33hx7y04IzTV5/ffLkVhQlrUnILSMiI0VkkYgsFZEbQ/J7iMh0EZkjInNF5Pjki5pcNm602zqz3JcutR2iziU/cooZAAAf7ElEQVQDldeF/OYbmDYtfNRqcFRqtOWuIWKKolRBlZa7iDQGHgV+BJQAM0VkkjFmQaDYzcBEY8xjItIXmAL0rAN5k0Zpqd3WieVuTGQ8uxv+GlTKu3bB8OHw3Xd+WnDyr+BEN9GWu4Y9KopSBYlY7kOBpcaYr40xO4AJwOioMgZo4+3nASuSJ2LdUKeWe3DQEvgDh4JKetu2SMU+dqw/kZfDjUyN1aGqKIoSg0SUe1dgWeC4xEsLchtwnoiUYK32q8MqEpGxIjJLRGatDU7skgLq1HJ3lTvC3DLR4Y/t21ceLXrggXary6opilJNEtEaYT6A6DkUxwB/N8Z0A44HnheRSnUbY54wxgwxxgzpUKeTuVRNnVrurnKHs9zj+crD5nc54gi7DU4kpiiKkgCJRMuUAN0Dx92o7Ha5BBgJYIz5WERygQIgaq2jhkO9Wu5hbploevasnHbvvXbGxlGj7PHcuVBSkhQRFUXJbBKx3GcCvUWkl4g0A84GJkWV+Q44FkBE+gC5QGr9LlVQr5a76wCNZ7mHzanevDlcfbXfuXrQQb6iVxRFiUOVyt0YUwGMA94BFmKjYr4QkdtF5GSv2HXAZSJSBLwEXGhMnS5/UWtKS62udYsZ1ZoPP4Qrr7QjT6MtdzeSNJ7lrlMEKIqSRBIaxGSMmYLtKA2m3RLYXwAMS65odcvGjdYlk7SowkcegQkTbKfoPvtE5sVT7ueeG3/OdkVRlBqQtWEYpaVJdsm4gUhz5/qW+7PP2q2b4CvMLfOrX8ErryRREEVRlCxW7s5yTxpB5b5xo1XkLgQy2nJv08Y/r16mpVQUJdvIWuWe9BkhnXJfvdpO55uX569Z6qbrdcrdTfsLqtwVRakTsnbisLKyJK+bum2bnXq3osJ2rrZuDf372/lj9t7blnHT/Q4e7C/EEbTiFUVRkkTWWu5btiS+7GhCbN8Ohx5q99es8Svv2dPvtV20yG6HDvXP03liFEWpA1S515SRI+Evf/GPt22DLl2gWzd77PztQZzvPajcFUVR6oCsU+5ugGetlLsx8M47cNVVftq2bTZovrs3mDes8okT4a9/9csoiqLUEVnlc//0UzjsMBuhWFYWblwnRPQIVPCVu+ulDat8v/3sZ/nyGl5YURQlMbJKubsZdp9/3hrfNbbcV6/290tK4L33rM89N9ePfon3z+Hi3bUzVVGUOiKrlLvDrU1dY+UenHf9nHPg/fftftByj1d5Xp4Nh3z44RoKoCiKEp+sUu5u4Kiz4GttuTdrZp33jpwcf5Kvqiz3FM9nryhKZpNVHaqxZuKtNk65t2oFPXr46bm5/lJ5TbLqf1NRlAZGVmmgpCn3Nd409Tk5kask5eZaZz74YY+KoigpIKss9+gglxpHy2zebLfl5ZH/GLm5vltGlbuiKClELfea4BbALiuL/MfIyYHdu+2+KndFUVKIKvea4JT7rl2RYZFByz3ewhyKoih1TNYp99697aSNAO3b17CiYIRMcEBS06Zw1lmwdCnceGON5VQURaktWedz328/u7/33tC5cw0rcpY7+B2oYH3xzZrBH/6gA5QURUkpWaXcS0shPx+KiuCLL6pxojHw3HN2FCpEWu7RF1AURWkAZJ1yz8uDAQOq6W+fOhUuuABuuskel5XZhTiaN7fx7H/8o00fNSrpMiuKotSErFHuxtRi9SXnhvn2W//4iCNsKOTOnda/bgz065c0eRVFUWpD1ij38nIb3FIj5e4m+tqxw263bKlFkLyiKErdkzXK3YWj12jJUhfW6JR7reYLVhRFqXuyRrm7vs4aWe5unpjt262C37kzyWv0KYqiJBdV7omwc6fdbt/uR8qo5a4oSgNGlXsiOOW+cKFdOxXUclcUpUGTNcq9Vj53p9w3boTPPrP7qtwVRWnAZI1yr5Xl7jpSgzRvXit5FEVR6hJV7ongLPcgLVrUSh5FUZS6JKuUe+PGNdTJYcpdLXdFURowWaPcN260/naRGpysyl1RlDQja5R7jaceAHXLKIqSdqhyTwTtUFUUJc1Q5Z4I6pZRFCXNSEi5i8hIEVkkIktFJHSJIRH5iYgsEJEvROTF5IpZe5zPvUaoclcUJc2ocpk9EWkMPAr8CCgBZorIJGPMgkCZ3sCvgWHGmA0i0rGuBK4p6nNXFCWbSMRyHwosNcZ8bYzZAUwARkeVuQx41BizAcAYsya5YtaeWiv3Zs0i10Vt2jQpcimKotQFiSj3rsCywHGJlxZkf2B/EflQRD4RkZFhFYnIWBGZJSKz1q5dWzOJa8Du3bBpUy2Ve9Om/opLUMOYSkVRlPohEeUepsVM1HEToDcwHBgDPCkilTzcxpgnjDFDjDFDOnToUF1Za8yWLXahpBqtWf3xx7Bmjb9gh6IoShpQpc8da6l3Dxx3A1aElPnEGLMT+EZEFmGV/cykSFlL3Cp51Z6ld9s2u5weQMcG142gKIoSk0Qs95lAbxHpJSLNgLOBSVFl3gCOARCRAqyb5utkClobajwF++rV/r762BVFSSOqVO7GmApgHPAOsBCYaIz5QkRuF5GTvWLvAOtFZAEwHfiVMWZ9XQldXZzlXu1ZelW5K4qSpiTilsEYMwWYEpV2S2DfAL/wPg0OZ7lXW7mvWuXvq3JXFCWNSEi5pzs19rmHWe7z59vwG0VRlAZMVij3GlvuYcq9X7+kyKQoilKXZPzcMl98AaefbverbbkH3TLbtydNJkVRlLom45X7c8/5+7Wy3L/9NinyKIqi1AcZr9x37fL3a6TcO3e2+1u3Jk0mRVGUuibjlXvQm1Ktub6WLoWiIhg2LOkyKYqi1DUZ36G6cqW/37hxNU7s3dtu99rLbg86KGkyKYqi1DUZrdzfeANee62WlXTqZF0y1fpnUBRFSS0ZrdxnzKjhiUH/em6u/SiKoqQRGa3cKyqsn33SJFi2rOryewhGyahiVxQlDclo5b5pE3ToAMceW80TXXz7qafC2LFJl0tRFKWuyehomU2boHXrGpzoLPff/EbnlFEUJS3JaOW+eXMCC3Rs2AC//a314Ticcu/Uqc5kUxRFqUsyWrknZLn/6ldwxx02tMaxbp3dFhTUmWyKoih1SUYr94QsdzfKqbzcTysthZwc7UxVFCVtyfgO1SqVu/Op79xpp/KdMQM+/xzaVloCVlEUJW3IeOVepVumiXcLKirg/ffhmGPs8f7716lsiqIodUnGumV277aLdFTLcl+71k9Xy11RlDQmY5X7li1gTDWVu1uyCaB58zqTTVEUpa7JWLdMwkvrObfML34BJ57op+/YUSdyKYqi1AcZa7m74JcqDfDgIKW33vL3t21LukyKoij1RcYqdzf3V7WUexBV7oqipDEZr9zjLtCxcCGMHx+ep2umKoqSxmSszz0hy71v38ppzZvDPvvAvffWiVyKoij1QXYr9zB27oT585Muj6IoSn2S8W6ZSsq9uDj+icEJxBRFUdKUjFXuodEyH3wAvXrBc8+lRCZFUZT6ImOVe6jlvnix3U6fXu/yKIqi1CcZr9wjomWcpg+ukaooipKBZLxyj7DcnaYPTu+rKIqSgWRXtEyjRpGZQaZMsfO49+5d57IpiqLUNRmt3Js18/U54M8XE2a5jxpVL3IpiqLUBxnrlikvDwmDjKfcFUVRMoiElLuIjBSRRSKyVERujFPuDBExIjIkeSJWnzlz4KGHrJclgp077ba8XGd9VBQlo6lSuYtIY+BRYBTQFxgjIpXG7YtIa+Aa4NNkC1ldXnghRkbQct+ypd7kURRFqW8SsdyHAkuNMV8bY3YAE4DRIeV+D9wDpHw6xZhTDjjlvnUrrFhRb/IoiqLUN4ko967AssBxiZe2BxEZDHQ3xrxFHERkrIjMEpFZa4NL2iWZZZ60w4dHZTjlvmULnH9+nV1fURQl1SSi3CUkzezJFGkEPABcV1VFxpgnjDFDjDFDOnTokLiU1WTZMhg2LGQgqlPu27bZ0aqx5nJXFEVJcxJR7iVA98BxNyDo02gN9Af+IyLFwGHApFR2qn73HXTvHpIR7EQtK4Nrr603mRRFUeqTRJT7TKC3iPQSkWbA2cAkl2mMKTXGFBhjehpjegKfACcbY2bVicQJsHIl7LVXSIaLlnH84Af1Io+iKEp9U6VyN8ZUAOOAd4CFwERjzBcicruInFzXAlaX8nL7KSgIyQxa7o0aweDB9SaXoihKfZLQCFVjzBRgSlTaLTHKDq+9WDVn/Xq7rVK59+4N7drVi0yKoij1TcaNUF23zm5jKnc3H8HAgZCTU29yKYqi1CcZN7dMlcq9Qwfo2hWOP95OPqMoipKBZJ9yz8mB2bPtsTEhhRRFUdKf7HHLbN0K27dHxrZLWAi/oihK+pOxlntEX2l5ObRsaff79Kl8ks7hrihKhpFxyn3DBsjLgybBlq1a5e9H+9nnz4cuXepFNkVRlPoi45R7aalV7hEE57GJVu79+tW5TIqiKPVNxvncI5T7xx/DZZfZIasOjZBRFCULyGzL/dhjbUdq0M+uyl1RlCwgsy13x/Ll/r4qd0VRsoDMVu4u1PG77/wCOs2voihZQEYq97ZtoxJLSvz9mMs0KYqiZA4ZpdyNieGWWbPG3+/UqV5lUhRFSQUZpdy3boWKClXuiqIoGaXcS0vttpJyLyvz91W5K4qSBWSUcncGeuikYQ5V7oqiZAEZpdy/+spu99nHSwib9bFjx3qTR1EUJVVklHJfutRu9903TqHWretFFkVRlFSSUSNUv/oK8vND4twBrrjCWvIHHpgS2RRFUeqTjFLu8+ZFWe1Bt8ywYXDuufUuk6IoSipIf7fMokVQXMycOXaesNNOi1Gu0sgmRVGUzCX9LfcLLmBdu96cX/I8BQV2Esg9BN0yleIjFUVRMpf0V+6rV3P10t+zdAtMngzt2wfygm4ZtdwVRcki0t4ts/L7HF5ZfwzjxsFxx8UpqJa7oihZRHor9927mb15f3bRhFNPDckPumXUclcUJYtIb+VeVsaXZn8gRoSjc8s0agStWtWfXIqiKCkmvZV7aSlfciAdWU371jsr5zvlnpcXacUriqJkOOmt3DduZBEHcACL/FnDwK6ZevbZsG2bPVaXjKIoWUZ6K/fSUkroxt58G6ncJ02Cl1/2j7UzVVGULCOtlbvZsJGVdKEzq2DjRj+jqCiyoFruiqJkGWkd5166spzt5FrlvmYNvPkm9OgBTz0VWVAtd0VRsoy0ttxXLbOdqF1YCU8/DaecAgcfDDt2wDnn+AV7906RhIqiKKkhrS33lSvttjOrYN4KP6NxY3j2WXjsMWjSBJo2TY2AiqIoKSIhy11ERorIIhFZKiI3huT/QkQWiMhcEZkmInsnX9TKrFptwxs7swoWL/YzDjnEKvU2baBFC1XuiqJkHVUqdxFpDDwKjAL6AmNEpG9UsTnAEGPMAOBV4J5kCxrG16taANCdZZHzyLRpUx+XVxRFabAkYrkPBZYaY742xuwAJgCjgwWMMdONMeXe4SdAt+SKGc6cVZ3Zt+m3tG6T1l0HiqIoSScRrdgVWBY4LvHSYnEJMDUsQ0TGisgsEZm1du3axKWMwefrezC41VI/GqZJE3ehWtetKIqSziTSoRqmKUNWngYROQ8YAhwdlm+MeQJ4AmDIkCGhdSRKaSl8tXUvLur6L2jeFpYtg65d4dtva1OtomQ8O3fupKSkhG1uBLfSIMnNzaVbt240rWGfYSLKvQToHjjuBqyILiQixwG/AY42xmyvkTTVYO5cux3ceSVs8v5/zjsPpk+He++t68srStpSUlJC69at6dmzJ6JvuQ0SYwzr16+npKSEXr161aiORNwyM4HeItJLRJoBZwOTggVEZDDwOHCyMWZNjSSpJnPm2O3gvb+H776zB0ccAR9+CAMH1ocIipKWbNu2jfz8fFXsDRgRIT8/v1ZvV1Uqd2NMBTAOeAdYCEw0xnwhIreLyMlesXuBVsArIvK5iEyKUV3SKPpsOx1YQ+cu4k89MGBAXV9WUTICVewNn9p+RwkNYjLGTAGmRKXdEtiPtwZSnbBo2jL6sBzp1hVOOsmusdc1Xj+voihK9pC2MYRLNhTQW76Cq66CiRNh3TqNklGUNGD9+vUMGjSIQYMG0blzZ7p27brneMeOHQnVcdFFF7Fo0aK4ZR599FHGjx+fDJHTkrScfqC0FNZsb8v+Pcpt+GOTJpCbm2qxFEVJgPz8fD7//HMAbrvtNlq1asUvf/nLiDLGGIwxNGoUbn8+88wzVV7nqquuqr2waUxaKvclS+y2d17tY+UVJau59lrwFG3SGDQIHnyw2qctXbqUU045hcLCQj799FPeeustfve73/G///2PrVu3ctZZZ3HLLdYbXFhYyCOPPEL//v0pKCjg8ssvZ+rUqbRo0YI333yTjh07cvPNN1NQUMC1115LYWEhhYWFvPfee5SWlvLMM89wxBFHsGXLFs4//3yWLl1K3759WbJkCU8++SSDBg2KkO3WW29lypQpbN26lcLCQh577DFEhMWLF3P55Zezfv16GjduzD/+8Q969uzJH/7wB1566SUaNWrEiSeeyJ133pmUW1sd0tIts0e5F2xIrSCKoiSVBQsWcMkllzBnzhy6du3KXXfdxaxZsygqKuLdd99lwYIFlc4pLS3l6KOPpqioiMMPP5ynn346tG5jDJ999hn33nsvt99+OwAPP/wwnTt3pqioiBtvvJE5Lgwvip///OfMnDmTefPmUVpayttvvw3AmDFj+L//+z+Kior46KOP6NixI5MnT2bq1Kl89tlnFBUVcd111yXp7lSPtLTcFy8GYTf7dtycalEUJb2pgYVdl+y777784Ac/2HP80ksv8dRTT1FRUcGKFStYsGABfftGTm3VvHlzRo0aBcAhhxzC+++/H1r3aaedtqdMcXExAB988AE33HADAAMHDqRfv36h506bNo17772Xbdu2sW7dOg455BAOO+ww1q1bx0knnQTYQUcA//73v7n44otp3rw5AO3bt6/Jrag1aancl0xfRnd20zyvWapFURQlibRs2XLP/pIlS/jzn//MZ599Rtu2bTnvvPNC476bNfP1QOPGjamoqAitOycnp1IZY6oeKF9eXs64ceP43//+R9euXbn55pv3yBEWrmiMaRChpmnplln835X0Zgm0apVqURRFqSM2bdpE69atadOmDStXruSdd95J+jUKCwuZOHEiAPPmzQt1+2zdupVGjRpRUFDA5s2bee211wBo164dBQUFTJ48GbCDw8rLyxkxYgRPPfUUW7duBeD7779PutyJkHbKfeNGmMNghjALAv/yiqJkFgcffDB9+/alf//+XHbZZQwbNizp17j66qtZvnw5AwYM4L777qN///7kRS3LmZ+fzwUXXED//v059dRTOfTQQ/fkjR8/nvvuu48BAwZQWFjI2rVrOfHEExk5ciRDhgxh0KBBPPDAA0mXOxEkkdeSumDIkCFm1qxZ1T7vxRfh3HPhYw7jsLtPg+uvrwPpFCVzWbhwIX369Em1GA2CiooKKioqyM3NZcmSJYwYMYIlS5bQpEnD8FiHfVciMtsYM6SqcxtGC6pBTg78iH8xlM+g5U9TLY6iKGlMWVkZxx57LBUVFRhjePzxxxuMYq8tadeK00/Zxen8ONViKIqSAbRt25bZs2enWow6Ie187nidFACUlaVODkVRlAZMeiv3GEOTFUVRsp30045OuffoAePGpVYWRVGUBkr6Kfdybx3uu+4CbwSYoiiKEkn6KXdnuatiV5S0ZPjw4ZUGJD344INceeWVcc9r5Q1aXLFiBWeccUbMuqsKsX7wwQcpd0YicPzxx7PRLfiTQahyVxSlXhkzZgwTJkyISJswYQJjxoxJ6Py99tqLV199tcbXj1buU6ZMoW3btjWur6GSdqGQe9wyqtwVpdakYsbfM844g5tvvpnt27eTk5NDcXExK1asoLCwkLKyMkaPHs2GDRvYuXMnd9xxB6NHj444v7i4mBNPPJH58+ezdetWLrroIhYsWECfPn32DPkHuOKKK5g5cyZbt27ljDPO4He/+x0PPfQQK1as4JhjjqGgoIDp06fTs2dPZs2aRUFBAffff/+eWSUvvfRSrr32WoqLixk1ahSFhYV89NFHdO3alTfffHPPxGCOyZMnc8cdd7Bjxw7y8/MZP348nTp1oqysjKuvvppZs2YhItx6662cfvrpvP3229x0003s2rWLgoICpk2blrwvgXRU7u7La9EitXIoilIj8vPzGTp0KG+//TajR49mwoQJnHXWWYgIubm5vP7667Rp04Z169Zx2GGHcfLJJ8eciOuxxx6jRYsWzJ07l7lz53LwwQfvybvzzjtp3749u3bt4thjj2Xu3Llcc8013H///UyfPp2CgoKIumbPns0zzzzDp59+ijGGQw89lKOPPpp27dqxZMkSXnrpJf72t7/xk5/8hNdee43zzjsv4vzCwkI++eQTRIQnn3ySe+65h/vuu4/f//735OXlMW/ePAA2bNjA2rVrueyyy5gxYwa9evWqk/ln0le5q+WuKLUmVTP+OteMU+7OWjbGcNNNNzFjxgwaNWrE8uXLWb16NZ07dw6tZ8aMGVxzzTUADBgwgAEDBuzJmzhxIk888QQVFRWsXLmSBQsWRORH88EHH3DqqafumZnytNNO4/333+fkk0+mV69eexbwCE4ZHKSkpISzzjqLlStXsmPHDnr16gXYKYCDbqh27doxefJkjjrqqD1l6mJa4PTzuatbRlHSnlNOOYVp06btWWXJWdzjx49n7dq1zJ49m88//5xOnTqFTvMbJMyq/+abb/jTn/7EtGnTmDt3LieccEKV9cSbZ8tNFwyxpxW++uqrGTduHPPmzePxxx/fc72wKYDrY1rg9FPu6pZRlLSnVatWDB8+nIsvvjiiI7W0tJSOHTvStGlTpk+fzrfffhu3nqOOOmrPItjz589n7ty5gJ0uuGXLluTl5bF69WqmTp2655zWrVuzeXPlhX6OOuoo3njjDcrLy9myZQuvv/46Rx55ZMJtKi0tpWvXrgA8++yze9JHjBjBI488sud4w4YNHH744fz3v//lm2++AepmWuD0Ve5quStKWjNmzBiKioo4++yz96Sde+65zJo1iyFDhjB+/HgOPPDAuHVcccUVlJWVMWDAAO655x6GDh0K2FWVBg8eTL9+/bj44osjpgseO3Yso0aN4phjjomo6+CDD+bCCy9k6NChHHrooVx66aUMHjw44fbcdtttnHnmmRx55JER/vybb76ZDRs20L9/fwYOHMj06dPp0KEDTzzxBKeddhoDBw7krLPOSvg6iZJ2U/7y5pvw/PPw0kvQtGnyBVOUDEen/E0fsmrKX0aPth9FURQlJunnllEURVGqRJW7omQhqXLHKolT2+9IlbuiZBm5ubmsX79eFXwDxhjD+vXryc3NrXEd6edzVxSlVnTr1o2SkhLWrl2balGUOOTm5tKtW7can6/KXVGyjKZNm+4ZGalkLuqWURRFyUBUuSuKomQgqtwVRVEykJSNUBWRtUD8iSNiUwCsS6I46YC2OTvQNmcHtWnz3saYDlUVSplyrw0iMiuR4beZhLY5O9A2Zwf10WZ1yyiKomQgqtwVRVEykHRV7k+kWoAUoG3ODrTN2UGdtzktfe6KoihKfNLVclcURVHioMpdURQlA0kr5S4iI0VkkYgsFZEbUy1PshCRp0VkjYjMD6S1F5F3RWSJt23npYuIPOTdg7kicnDqJK85ItJdRKaLyEIR+UJEfu6lZ2y7RSRXRD4TkSKvzb/z0nuJyKdem18WkWZeeo53vNTL75lK+WuDiDQWkTki8pZ3nNFtFpFiEZknIp+LyCwvrV6f7bRR7iLSGHgUGAX0BcaISN/USpU0/g6MjEq7EZhmjOkNTPOOwba/t/cZCzxWTzImmwrgOmNMH+Aw4Crv+8zkdm8HfmiMGQgMAkaKyGHA3cADXps3AJd45S8BNhhj9gMe8MqlKz8HFgaOs6HNxxhjBgXi2ev32TbGpMUHOBx4J3D8a+DXqZYrie3rCcwPHC8Cunj7XYBF3v7jwJiwcun8Ad4EfpQt7QZaAP8DDsWOVGzipe95zoF3gMO9/SZeOUm17DVoazesMvsh8BYgWdDmYqAgKq1en+20sdyBrsCywHGJl5apdDLGrATwth299Iy7D96r92DgUzK83Z574nNgDfAu8BWw0RhT4RUJtmtPm738UiC/fiVOCg8C1wO7veN8Mr/NBviXiMwWkbFeWr0+2+k0n7uEpGVjHGdG3QcRaQW8BlxrjNkkEtY8WzQkLe3abYzZBQwSkbbA60CfsGLeNu3bLCInAmuMMbNFZLhLDimaMW32GGaMWSEiHYF3ReTLOGXrpM3pZLmXAN0Dx92AFSmSpT5YLSJdALztGi89Y+6DiDTFKvbxxph/eMkZ324AY8xG4D/Y/oa2IuIMrWC79rTZy88Dvq9fSWvNMOBkESkGJmBdMw+S2W3GGLPC267B/okPpZ6f7XRS7jOB3l4vezPgbGBSimWqSyYBF3j7F2B90i79fK+H/TCg1L3qpRNiTfSngIXGmPsDWRnbbhHp4FnsiEhz4DhsJ+N04AyvWHSb3b04A3jPeE7ZdMEY82tjTDdjTE/sb/Y9Y8y5ZHCbRaSliLR2+8AIYD71/WynuuOhmp0UxwOLsX7K36RaniS26yVgJbAT+y9+CdbPOA1Y4m3be2UFGzX0FTAPGJJq+WvY5kLsq+dc4HPvc3wmtxsYAMzx2jwfuMVL3wf4DFgKvALkeOm53vFSL3+fVLehlu0fDryV6W322lbkfb5wuqq+n22dfkBRFCUDSSe3jKIoipIgqtwVRVEyEFXuiqIoGYgqd0VRlAxElbuiKEoGospdURQlA1HlriiKkoH8P/O26ZCbDd7FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu_ngrams Test Accuracy: 0.880\n",
      "my_model_neu Test f-measure: 0.749\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "neu_ng = Sequential()\n",
    "neu_ng.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "neu_ng.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "neu_ng.summary()\n",
    "history = neu_ng.fit(X_scaled_train_data_ngrams, y_train,\n",
    "                    validation_data=(X_scaled_val_data_ngrams, y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu_ngrams,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu_ngrams.h5')\n",
    "yhat = l_model.predict(scaled_test_data_ngrams)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_ngrams Test Accuracy: %.3f' % acc)\n",
    "\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_neu_ngrams Test Accuracy: 0.825\n"
     ]
    }
   ],
   "source": [
    "l_model = load_model('my_model_neu_ngrams.h5')\n",
    "yhat = l_model.predict(scaled_test_data_ngrams)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_ngrams Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 32)                243968    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 250,825\n",
      "Trainable params: 250,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 6.3804 - acc: 0.1429 - val_loss: 6.2966 - val_acc: 0.1481\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 6.2143 - acc: 0.2063 - val_loss: 6.1567 - val_acc: 0.1653\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 6.0832 - acc: 0.1429 - val_loss: 6.0302 - val_acc: 0.1799\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.9577 - acc: 0.1429 - val_loss: 5.9120 - val_acc: 0.1839\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.8265 - acc: 0.1905 - val_loss: 5.7996 - val_acc: 0.1944\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.7164 - acc: 0.2540 - val_loss: 5.6919 - val_acc: 0.2024\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.6155 - acc: 0.2222 - val_loss: 5.5878 - val_acc: 0.2143\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.5307 - acc: 0.1587 - val_loss: 5.4871 - val_acc: 0.2302\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.4490 - acc: 0.2063 - val_loss: 5.3918 - val_acc: 0.2474\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.3632 - acc: 0.2698 - val_loss: 5.2996 - val_acc: 0.2606\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.2817 - acc: 0.2063 - val_loss: 5.2102 - val_acc: 0.2738\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.1992 - acc: 0.2222 - val_loss: 5.1239 - val_acc: 0.2791\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.0983 - acc: 0.2857 - val_loss: 5.0404 - val_acc: 0.2910\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.0326 - acc: 0.1587 - val_loss: 4.9600 - val_acc: 0.3082\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.9299 - acc: 0.3492 - val_loss: 4.8816 - val_acc: 0.3294\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.8815 - acc: 0.2222 - val_loss: 4.8042 - val_acc: 0.3413\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.8022 - acc: 0.3016 - val_loss: 4.7277 - val_acc: 0.3505\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.7126 - acc: 0.2540 - val_loss: 4.6540 - val_acc: 0.3836\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.6332 - acc: 0.2381 - val_loss: 4.5824 - val_acc: 0.4034\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.6046 - acc: 0.3492 - val_loss: 4.5131 - val_acc: 0.4140\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.5355 - acc: 0.3810 - val_loss: 4.4469 - val_acc: 0.4511\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.4716 - acc: 0.3016 - val_loss: 4.3802 - val_acc: 0.4841\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.3433 - acc: 0.4603 - val_loss: 4.3144 - val_acc: 0.5185\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.3053 - acc: 0.3968 - val_loss: 4.2476 - val_acc: 0.5569\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.3197 - acc: 0.4127 - val_loss: 4.1839 - val_acc: 0.5913\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.1748 - acc: 0.4444 - val_loss: 4.1228 - val_acc: 0.6243\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.1729 - acc: 0.3810 - val_loss: 4.0634 - val_acc: 0.6614\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.0858 - acc: 0.4286 - val_loss: 4.0046 - val_acc: 0.6865\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.1275 - acc: 0.3810 - val_loss: 3.9514 - val_acc: 0.7169\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.0862 - acc: 0.4127 - val_loss: 3.8995 - val_acc: 0.7394\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.9754 - acc: 0.4603 - val_loss: 3.8476 - val_acc: 0.7606\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.9342 - acc: 0.5079 - val_loss: 3.7938 - val_acc: 0.7791\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.9103 - acc: 0.4444 - val_loss: 3.7429 - val_acc: 0.7937\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.9183 - acc: 0.4444 - val_loss: 3.6936 - val_acc: 0.8003\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.8865 - acc: 0.4603 - val_loss: 3.6437 - val_acc: 0.8214\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.7288 - acc: 0.5556 - val_loss: 3.5925 - val_acc: 0.8439\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6568 - acc: 0.5238 - val_loss: 3.5392 - val_acc: 0.8320\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.7552 - acc: 0.3810 - val_loss: 3.4897 - val_acc: 0.8598\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6620 - acc: 0.4603 - val_loss: 3.4433 - val_acc: 0.8770\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5583 - acc: 0.5714 - val_loss: 3.3989 - val_acc: 0.9180\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5837 - acc: 0.4921 - val_loss: 3.3525 - val_acc: 0.9365\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4365 - acc: 0.6508 - val_loss: 3.3028 - val_acc: 0.9524\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4009 - acc: 0.5873 - val_loss: 3.2557 - val_acc: 0.9669\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4643 - acc: 0.5556 - val_loss: 3.2101 - val_acc: 0.9788\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4674 - acc: 0.5556 - val_loss: 3.1696 - val_acc: 0.9815\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3722 - acc: 0.6825 - val_loss: 3.1273 - val_acc: 0.9854\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3599 - acc: 0.5873 - val_loss: 3.0843 - val_acc: 0.9868\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3245 - acc: 0.5714 - val_loss: 3.0423 - val_acc: 0.9854\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1796 - acc: 0.6825 - val_loss: 2.9976 - val_acc: 0.9854\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1884 - acc: 0.6667 - val_loss: 2.9537 - val_acc: 0.9868\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.2026 - acc: 0.6190 - val_loss: 2.9142 - val_acc: 0.9868\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1343 - acc: 0.6825 - val_loss: 2.8727 - val_acc: 0.9987\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1233 - acc: 0.6667 - val_loss: 2.8323 - val_acc: 0.9987\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1343 - acc: 0.6825 - val_loss: 2.7921 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9927 - acc: 0.7143 - val_loss: 2.7542 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0237 - acc: 0.6825 - val_loss: 2.7179 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9891 - acc: 0.7460 - val_loss: 2.6822 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0067 - acc: 0.6508 - val_loss: 2.6466 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.8986 - acc: 0.6984 - val_loss: 2.6087 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0249 - acc: 0.6508 - val_loss: 2.5716 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9362 - acc: 0.7302 - val_loss: 2.5387 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.8531 - acc: 0.7460 - val_loss: 2.5049 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7884 - acc: 0.7302 - val_loss: 2.4701 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7532 - acc: 0.7302 - val_loss: 2.4366 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7391 - acc: 0.7778 - val_loss: 2.4042 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6697 - acc: 0.7460 - val_loss: 2.3705 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7214 - acc: 0.7619 - val_loss: 2.3428 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6045 - acc: 0.7460 - val_loss: 2.3138 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5928 - acc: 0.7937 - val_loss: 2.2836 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5960 - acc: 0.7302 - val_loss: 2.2541 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6069 - acc: 0.7937 - val_loss: 2.2283 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5648 - acc: 0.6825 - val_loss: 2.2054 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4730 - acc: 0.7460 - val_loss: 2.1828 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5207 - acc: 0.7302 - val_loss: 2.1574 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4513 - acc: 0.7937 - val_loss: 2.1347 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3844 - acc: 0.7619 - val_loss: 2.1109 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4684 - acc: 0.7460 - val_loss: 2.0879 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4477 - acc: 0.8571 - val_loss: 2.0643 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4482 - acc: 0.7937 - val_loss: 2.0468 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4217 - acc: 0.8730 - val_loss: 2.0265 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3903 - acc: 0.8730 - val_loss: 2.0056 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3947 - acc: 0.7302 - val_loss: 1.9844 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3590 - acc: 0.7937 - val_loss: 1.9645 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3039 - acc: 0.7937 - val_loss: 1.9474 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4207 - acc: 0.7460 - val_loss: 1.9306 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3115 - acc: 0.8254 - val_loss: 1.9135 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2906 - acc: 0.7937 - val_loss: 1.8994 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1968 - acc: 0.8254 - val_loss: 1.8842 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2492 - acc: 0.8095 - val_loss: 1.8675 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2371 - acc: 0.7619 - val_loss: 1.8517 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1694 - acc: 0.8889 - val_loss: 1.8342 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2072 - acc: 0.8254 - val_loss: 1.8186 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2123 - acc: 0.7937 - val_loss: 1.8059 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1464 - acc: 0.9048 - val_loss: 1.7920 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1578 - acc: 0.8413 - val_loss: 1.7776 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2703 - acc: 0.7778 - val_loss: 1.7652 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1850 - acc: 0.9206 - val_loss: 1.7532 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2048 - acc: 0.8095 - val_loss: 1.7441 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0095 - acc: 0.8889 - val_loss: 1.7301 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0793 - acc: 0.9048 - val_loss: 1.7146 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0713 - acc: 0.8254 - val_loss: 1.7002 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1998 - acc: 0.7778 - val_loss: 1.6908 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1859 - acc: 0.7937 - val_loss: 1.6821 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1679 - acc: 0.7937 - val_loss: 1.6783 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9693 - acc: 0.9048 - val_loss: 1.6680 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0694 - acc: 0.8413 - val_loss: 1.6558 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1006 - acc: 0.7778 - val_loss: 1.6470 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9320 - acc: 0.8730 - val_loss: 1.6404 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0036 - acc: 0.8730 - val_loss: 1.6294 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9785 - acc: 0.8889 - val_loss: 1.6196 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0620 - acc: 0.8254 - val_loss: 1.6103 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1451 - acc: 0.7302 - val_loss: 1.6037 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0084 - acc: 0.8095 - val_loss: 1.5954 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8637 - acc: 0.8571 - val_loss: 1.5890 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8151 - acc: 0.9206 - val_loss: 1.5799 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9617 - acc: 0.8254 - val_loss: 1.5676 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9265 - acc: 0.8095 - val_loss: 1.5577 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8547 - acc: 0.8889 - val_loss: 1.5486 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9309 - acc: 0.8889 - val_loss: 1.5403 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8821 - acc: 0.8730 - val_loss: 1.5344 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8102 - acc: 0.9048 - val_loss: 1.5287 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8582 - acc: 0.8571 - val_loss: 1.5208 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8357 - acc: 0.8889 - val_loss: 1.5137 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8438 - acc: 0.8571 - val_loss: 1.5077 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8401 - acc: 0.8571 - val_loss: 1.4998 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8148 - acc: 0.8889 - val_loss: 1.4935 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8105 - acc: 0.9048 - val_loss: 1.4864 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7804 - acc: 0.8889 - val_loss: 1.4763 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8608 - acc: 0.8254 - val_loss: 1.4682 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8556 - acc: 0.8889 - val_loss: 1.4621 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8373 - acc: 0.9206 - val_loss: 1.4575 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8481 - acc: 0.8571 - val_loss: 1.4562 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7532 - acc: 0.8889 - val_loss: 1.4498 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7057 - acc: 0.9365 - val_loss: 1.4422 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8090 - acc: 0.8254 - val_loss: 1.4391 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8103 - acc: 0.8889 - val_loss: 1.4383 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8367 - acc: 0.9206 - val_loss: 1.4348 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7706 - acc: 0.9524 - val_loss: 1.4301 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7563 - acc: 0.9206 - val_loss: 1.4238 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7676 - acc: 0.9048 - val_loss: 1.4150 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7043 - acc: 0.9365 - val_loss: 1.4070 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7405 - acc: 0.8889 - val_loss: 1.3988 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6870 - acc: 0.9365 - val_loss: 1.3940 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7213 - acc: 0.8730 - val_loss: 1.3901 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7427 - acc: 0.9365 - val_loss: 1.3855 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7428 - acc: 0.8571 - val_loss: 1.3810 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6694 - acc: 0.9048 - val_loss: 1.3744 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7877 - acc: 0.8254 - val_loss: 1.3708 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7014 - acc: 0.8889 - val_loss: 1.3687 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6617 - acc: 0.9048 - val_loss: 1.3654 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6625 - acc: 0.8889 - val_loss: 1.3596 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6847 - acc: 0.8889 - val_loss: 1.3556 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6453 - acc: 0.9524 - val_loss: 1.3518 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6736 - acc: 0.8889 - val_loss: 1.3487 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6296 - acc: 0.9206 - val_loss: 1.3450 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8174 - acc: 0.7937 - val_loss: 1.3418 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5593 - acc: 0.9206 - val_loss: 1.3360 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5865 - acc: 0.9206 - val_loss: 1.3281 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5418 - acc: 0.9524 - val_loss: 1.3218 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6475 - acc: 0.9048 - val_loss: 1.3158 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6915 - acc: 0.9524 - val_loss: 1.3139 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6022 - acc: 0.9048 - val_loss: 1.3126 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5419 - acc: 0.9524 - val_loss: 1.3096 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6509 - acc: 0.9365 - val_loss: 1.3051 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5870 - acc: 0.9206 - val_loss: 1.3033 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6297 - acc: 0.9206 - val_loss: 1.3011 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5964 - acc: 0.9365 - val_loss: 1.2960 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6215 - acc: 0.9206 - val_loss: 1.2937 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5559 - acc: 0.9841 - val_loss: 1.2928 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6047 - acc: 0.9048 - val_loss: 1.2871 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4736 - acc: 0.9683 - val_loss: 1.2803 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6192 - acc: 0.9206 - val_loss: 1.2754 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6810 - acc: 0.8571 - val_loss: 1.2761 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5729 - acc: 0.9206 - val_loss: 1.2749 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6015 - acc: 0.9206 - val_loss: 1.2719 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4779 - acc: 0.9841 - val_loss: 1.2682 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5540 - acc: 0.9206 - val_loss: 1.2627 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5796 - acc: 0.9365 - val_loss: 1.2574 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5821 - acc: 0.9206 - val_loss: 1.2545 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6096 - acc: 0.9048 - val_loss: 1.2525 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5419 - acc: 0.9206 - val_loss: 1.2492 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6261 - acc: 0.8889 - val_loss: 1.2477 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5584 - acc: 0.9206 - val_loss: 1.2474 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6808 - acc: 0.8413 - val_loss: 1.2456 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3985 - acc: 0.9841 - val_loss: 1.2397 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5017 - acc: 0.9206 - val_loss: 1.2341 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4420 - acc: 0.9524 - val_loss: 1.2290 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4653 - acc: 0.9524 - val_loss: 1.2257 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4975 - acc: 0.9841 - val_loss: 1.2251 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4968 - acc: 0.9206 - val_loss: 1.2233 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5244 - acc: 0.9524 - val_loss: 1.2226 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4966 - acc: 0.9206 - val_loss: 1.2212 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4597 - acc: 1.0000 - val_loss: 1.2169 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5005 - acc: 0.9365 - val_loss: 1.2129 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5557 - acc: 0.8889 - val_loss: 1.2091 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4514 - acc: 0.9524 - val_loss: 1.2066 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4322 - acc: 0.9524 - val_loss: 1.2016 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4035 - acc: 0.9365 - val_loss: 1.1967 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4780 - acc: 0.9048 - val_loss: 1.1940 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5049 - acc: 0.9206 - val_loss: 1.1927 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5229 - acc: 0.9365 - val_loss: 1.1927 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5016 - acc: 0.9365 - val_loss: 1.1920 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5038 - acc: 0.9206 - val_loss: 1.1946 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4906 - acc: 0.9683 - val_loss: 1.1950 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5185 - acc: 0.9048 - val_loss: 1.1933 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3515 - acc: 0.9683 - val_loss: 1.1892 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4282 - acc: 0.9524 - val_loss: 1.1829 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4367 - acc: 0.9524 - val_loss: 1.1773 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3902 - acc: 0.9524 - val_loss: 1.1740 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4391 - acc: 0.9365 - val_loss: 1.1729 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4232 - acc: 0.9365 - val_loss: 1.1709 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3863 - acc: 0.9683 - val_loss: 1.1681 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3704 - acc: 0.9841 - val_loss: 1.1634 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3702 - acc: 0.9048 - val_loss: 1.1576 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4627 - acc: 0.8730 - val_loss: 1.1550 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4001 - acc: 0.9683 - val_loss: 1.1567 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3654 - acc: 0.9365 - val_loss: 1.1546 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4810 - acc: 0.9048 - val_loss: 1.1534 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4159 - acc: 0.9365 - val_loss: 1.1533 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3963 - acc: 0.9365 - val_loss: 1.1487 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3849 - acc: 0.9365 - val_loss: 1.1452 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3858 - acc: 0.9841 - val_loss: 1.1435 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4177 - acc: 0.9206 - val_loss: 1.1410 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4479 - acc: 0.8730 - val_loss: 1.1406 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4489 - acc: 0.9365 - val_loss: 1.1421 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4023 - acc: 0.9365 - val_loss: 1.1414 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3994 - acc: 0.9206 - val_loss: 1.1386 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3382 - acc: 0.9683 - val_loss: 1.1332 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4416 - acc: 0.8730 - val_loss: 1.1329 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4015 - acc: 0.9524 - val_loss: 1.1329 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3379 - acc: 0.9524 - val_loss: 1.1296 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3754 - acc: 0.9365 - val_loss: 1.1251 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4481 - acc: 0.8730 - val_loss: 1.1224 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3144 - acc: 0.9524 - val_loss: 1.1215 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3303 - acc: 0.9524 - val_loss: 1.1198 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3780 - acc: 0.9683 - val_loss: 1.1165 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4346 - acc: 0.9365 - val_loss: 1.1143 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3451 - acc: 0.9365 - val_loss: 1.1124 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3765 - acc: 0.9365 - val_loss: 1.1121 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3219 - acc: 0.9841 - val_loss: 1.1144 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3785 - acc: 0.9365 - val_loss: 1.1143 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4333 - acc: 0.9206 - val_loss: 1.1162 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4393 - acc: 0.9048 - val_loss: 1.1182 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3509 - acc: 0.9206 - val_loss: 1.1178 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4100 - acc: 0.9524 - val_loss: 1.1135 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3368 - acc: 0.9841 - val_loss: 1.1094 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3141 - acc: 0.9524 - val_loss: 1.1064 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2650 - acc: 0.9683 - val_loss: 1.1000 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3689 - acc: 0.9683 - val_loss: 1.0961 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3357 - acc: 0.9683 - val_loss: 1.0957 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3479 - acc: 0.9683 - val_loss: 1.0960 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2952 - acc: 0.9841 - val_loss: 1.0940 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2193 - acc: 0.9683 - val_loss: 1.0877 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2798 - acc: 0.9524 - val_loss: 1.0844 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3225 - acc: 0.9365 - val_loss: 1.0831 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3928 - acc: 0.9524 - val_loss: 1.0837 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3243 - acc: 0.9683 - val_loss: 1.0864 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3043 - acc: 0.9365 - val_loss: 1.0844 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3026 - acc: 0.9365 - val_loss: 1.0829 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3529 - acc: 0.9841 - val_loss: 1.0814 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2533 - acc: 0.9683 - val_loss: 1.0772 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3552 - acc: 0.9206 - val_loss: 1.0751 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2731 - acc: 0.9841 - val_loss: 1.0751 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3596 - acc: 0.9206 - val_loss: 1.0751 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2700 - acc: 0.9841 - val_loss: 1.0770 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2261 - acc: 0.9841 - val_loss: 1.0735 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3449 - acc: 0.9524 - val_loss: 1.0688 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3217 - acc: 0.9206 - val_loss: 1.0672 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2048 - acc: 0.9841 - val_loss: 1.0646 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2891 - acc: 0.9683 - val_loss: 1.0619 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2786 - acc: 0.9841 - val_loss: 1.0590 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3100 - acc: 0.9683 - val_loss: 1.0606 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2840 - acc: 0.9841 - val_loss: 1.0616 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2301 - acc: 0.9683 - val_loss: 1.0612 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2212 - acc: 0.9841 - val_loss: 1.0585 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2921 - acc: 0.9524 - val_loss: 1.0572 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2529 - acc: 0.9365 - val_loss: 1.0557 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1864 - acc: 0.9841 - val_loss: 1.0533 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2667 - acc: 0.9206 - val_loss: 1.0502 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3165 - acc: 0.9365 - val_loss: 1.0514 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2554 - acc: 0.9524 - val_loss: 1.0516 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3090 - acc: 0.9206 - val_loss: 1.0525 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2677 - acc: 0.9683 - val_loss: 1.0528 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1885 - acc: 0.9683 - val_loss: 1.0504 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4090 - acc: 0.9048 - val_loss: 1.0496 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2286 - acc: 0.9524 - val_loss: 1.0484 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3005 - acc: 0.9524 - val_loss: 1.0468 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2736 - acc: 0.9683 - val_loss: 1.0439 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2435 - acc: 0.9683 - val_loss: 1.0407 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2291 - acc: 0.9683 - val_loss: 1.0371 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2522 - acc: 0.9683 - val_loss: 1.0360 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3024 - acc: 0.9206 - val_loss: 1.0363 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2631 - acc: 0.9365 - val_loss: 1.0363 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2690 - acc: 0.9683 - val_loss: 1.0337 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1904 - acc: 0.9841 - val_loss: 1.0319 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2468 - acc: 0.9524 - val_loss: 1.0312 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3054 - acc: 0.9206 - val_loss: 1.0335 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1392 - acc: 1.0000 - val_loss: 1.0324 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2243 - acc: 0.9524 - val_loss: 1.0271 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2734 - acc: 0.9524 - val_loss: 1.0269 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2497 - acc: 0.9683 - val_loss: 1.0260 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2758 - acc: 0.9048 - val_loss: 1.0281 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2652 - acc: 0.9524 - val_loss: 1.0285 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1831 - acc: 0.9841 - val_loss: 1.0267 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1363 - acc: 1.0000 - val_loss: 1.0201 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2623 - acc: 0.9683 - val_loss: 1.0153 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2867 - acc: 0.9365 - val_loss: 1.0133 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1785 - acc: 0.9841 - val_loss: 1.0143 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1676 - acc: 0.9524 - val_loss: 1.0112 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1892 - acc: 0.9524 - val_loss: 1.0077 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2541 - acc: 0.9206 - val_loss: 1.0087 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1484 - acc: 1.0000 - val_loss: 1.0090 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2814 - acc: 0.8889 - val_loss: 1.0146 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2734 - acc: 0.9365 - val_loss: 1.0166 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1997 - acc: 0.9365 - val_loss: 1.0132 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2389 - acc: 0.9524 - val_loss: 1.0107 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2073 - acc: 0.9841 - val_loss: 1.0097 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1977 - acc: 0.9683 - val_loss: 1.0073 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1455 - acc: 0.9683 - val_loss: 1.0037 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2447 - acc: 0.9524 - val_loss: 1.0009 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2282 - acc: 0.9683 - val_loss: 1.0030 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2655 - acc: 0.9524 - val_loss: 1.0068 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2620 - acc: 0.9206 - val_loss: 1.0083 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2259 - acc: 0.9524 - val_loss: 1.0069 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2106 - acc: 0.9524 - val_loss: 1.0048 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1739 - acc: 0.9841 - val_loss: 1.0015 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2279 - acc: 0.9524 - val_loss: 1.0003 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1955 - acc: 0.9841 - val_loss: 0.9997 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1855 - acc: 0.9683 - val_loss: 1.0002 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1629 - acc: 0.9683 - val_loss: 0.9981 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1426 - acc: 1.0000 - val_loss: 0.9958 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2162 - acc: 0.9524 - val_loss: 0.9932 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1749 - acc: 0.9841 - val_loss: 0.9911 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2076 - acc: 0.9524 - val_loss: 0.9911 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1857 - acc: 0.9683 - val_loss: 0.9900 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1633 - acc: 0.9683 - val_loss: 0.9885 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2085 - acc: 0.9524 - val_loss: 0.9892 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2396 - acc: 0.9365 - val_loss: 0.9884 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2195 - acc: 0.9524 - val_loss: 0.9886 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1098 - acc: 1.0000 - val_loss: 0.9859 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1231 - acc: 1.0000 - val_loss: 0.9818 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1375 - acc: 0.9683 - val_loss: 0.9784 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1533 - acc: 0.9841 - val_loss: 0.9764 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2027 - acc: 0.9683 - val_loss: 0.9765 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1300 - acc: 0.9524 - val_loss: 0.9769 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1508 - acc: 1.0000 - val_loss: 0.9755 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1093 - acc: 1.0000 - val_loss: 0.9757 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2130 - acc: 0.9365 - val_loss: 0.9794 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1497 - acc: 0.9841 - val_loss: 0.9816 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2206 - acc: 0.9524 - val_loss: 0.9800 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1967 - acc: 0.9524 - val_loss: 0.9775 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1145 - acc: 0.9683 - val_loss: 0.9745 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2492 - acc: 0.9048 - val_loss: 0.9744 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2001 - acc: 0.9365 - val_loss: 0.9775 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1551 - acc: 0.9683 - val_loss: 0.9782 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1406 - acc: 0.9841 - val_loss: 0.9767 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1619 - acc: 0.9683 - val_loss: 0.9752 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1858 - acc: 0.9524 - val_loss: 0.9763 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1387 - acc: 0.9683 - val_loss: 0.9740 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1412 - acc: 0.9524 - val_loss: 0.9721 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1341 - acc: 0.9683 - val_loss: 0.9665 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1330 - acc: 0.9683 - val_loss: 0.9627 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1914 - acc: 0.9365 - val_loss: 0.9653 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1397 - acc: 0.9524 - val_loss: 0.9659 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2061 - acc: 0.9206 - val_loss: 0.9678 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1718 - acc: 0.9683 - val_loss: 0.9666 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1168 - acc: 0.9683 - val_loss: 0.9634 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1136 - acc: 0.9841 - val_loss: 0.9601 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0922 - acc: 1.0000 - val_loss: 0.9545 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1873 - acc: 0.9365 - val_loss: 0.9524 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1609 - acc: 0.9524 - val_loss: 0.9580 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0804 - acc: 0.9841 - val_loss: 0.9592 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1770 - acc: 0.9524 - val_loss: 0.9590 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1072 - acc: 0.9841 - val_loss: 0.9572 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1491 - acc: 0.9524 - val_loss: 0.9537 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1865 - acc: 0.9206 - val_loss: 0.9551 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1744 - acc: 0.9365 - val_loss: 0.9577 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2025 - acc: 0.9683 - val_loss: 0.9568 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1781 - acc: 0.9841 - val_loss: 0.9562 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1599 - acc: 0.9206 - val_loss: 0.9559 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0875 - acc: 0.9683 - val_loss: 0.9547 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1576 - acc: 0.9841 - val_loss: 0.9523 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1851 - acc: 0.9524 - val_loss: 0.9508 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0684 - acc: 0.9841 - val_loss: 0.9490 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1333 - acc: 0.9524 - val_loss: 0.9469 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0977 - acc: 0.9524 - val_loss: 0.9453 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1315 - acc: 0.9524 - val_loss: 0.9432 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1596 - acc: 0.9206 - val_loss: 0.9418 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1260 - acc: 0.9683 - val_loss: 0.9430 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0375 - acc: 0.9683 - val_loss: 0.9406 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0783 - acc: 0.9841 - val_loss: 0.9375 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0926 - acc: 0.9683 - val_loss: 0.9353 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1721 - acc: 0.9365 - val_loss: 0.9324 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0463 - acc: 0.9841 - val_loss: 0.9316 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0925 - acc: 0.9683 - val_loss: 0.9321 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2217 - acc: 0.9048 - val_loss: 0.9347 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0910 - acc: 0.9841 - val_loss: 0.9348 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0672 - acc: 0.9841 - val_loss: 0.9328 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2183 - acc: 0.9524 - val_loss: 0.9342 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0715 - acc: 0.9841 - val_loss: 0.9354 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2063 - acc: 0.9206 - val_loss: 0.9338 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0735 - acc: 0.9683 - val_loss: 0.9348 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1423 - acc: 0.9683 - val_loss: 0.9321 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1220 - acc: 0.9841 - val_loss: 0.9321 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1052 - acc: 0.9683 - val_loss: 0.9334 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0934 - acc: 0.9841 - val_loss: 0.9313 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0663 - acc: 0.9841 - val_loss: 0.9296 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0242 - acc: 0.9841 - val_loss: 0.9259 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0950 - acc: 0.9683 - val_loss: 0.9248 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1617 - acc: 0.9206 - val_loss: 0.9270 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1034 - acc: 0.9683 - val_loss: 0.9296 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2011 - acc: 0.9683 - val_loss: 0.9283 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0703 - acc: 0.9524 - val_loss: 0.9276 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1151 - acc: 0.9365 - val_loss: 0.9276 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1678 - acc: 0.9365 - val_loss: 0.9293 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0860 - acc: 0.9206 - val_loss: 0.9269 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1932 - acc: 0.9048 - val_loss: 0.9271 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0527 - acc: 0.9841 - val_loss: 0.9306 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0486 - acc: 1.0000 - val_loss: 0.9289 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1144 - acc: 0.9524 - val_loss: 0.9277 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0960 - acc: 0.9683 - val_loss: 0.9268 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0574 - acc: 0.9683 - val_loss: 0.9227 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1098 - acc: 0.9524 - val_loss: 0.9221 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1105 - acc: 0.9524 - val_loss: 0.9197 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1149 - acc: 0.9524 - val_loss: 0.9208 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0722 - acc: 0.9683 - val_loss: 0.9163 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0614 - acc: 0.9841 - val_loss: 0.9129 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1201 - acc: 0.9365 - val_loss: 0.9114 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0409 - acc: 1.0000 - val_loss: 0.9093 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0620 - acc: 0.9841 - val_loss: 0.9076 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0550 - acc: 0.9683 - val_loss: 0.9040 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0004 - acc: 0.9841 - val_loss: 0.9014 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1234 - acc: 0.9365 - val_loss: 0.9008 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0265 - acc: 0.9841 - val_loss: 0.9013 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0540 - acc: 0.9683 - val_loss: 0.8996 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9731 - acc: 1.0000 - val_loss: 0.8975 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0374 - acc: 0.9841 - val_loss: 0.8960 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0438 - acc: 1.0000 - val_loss: 0.8967 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0804 - acc: 0.9206 - val_loss: 0.8986 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0603 - acc: 0.9841 - val_loss: 0.9020 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0507 - acc: 0.9683 - val_loss: 0.9007 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0230 - acc: 0.9524 - val_loss: 0.8981 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0434 - acc: 0.9841 - val_loss: 0.8965 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0623 - acc: 0.9841 - val_loss: 0.8952 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0246 - acc: 0.9683 - val_loss: 0.8958 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0742 - acc: 0.9365 - val_loss: 0.8951 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0484 - acc: 0.9841 - val_loss: 0.8940 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0443 - acc: 0.9683 - val_loss: 0.8935 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1024 - acc: 0.9365 - val_loss: 0.8970 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0032 - acc: 0.9841 - val_loss: 0.8962 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0453 - acc: 1.0000 - val_loss: 0.8941 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0908 - acc: 0.9683 - val_loss: 0.8948 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0375 - acc: 0.9524 - val_loss: 0.8946 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0194 - acc: 0.9683 - val_loss: 0.8940 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0607 - acc: 0.9524 - val_loss: 0.8934 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1039 - acc: 0.9524 - val_loss: 0.8935 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0928 - acc: 0.9524 - val_loss: 0.8945 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1165 - acc: 0.9206 - val_loss: 0.8960 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1311 - acc: 0.9206 - val_loss: 0.8976 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0922 - acc: 0.9524 - val_loss: 0.8953 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0159 - acc: 0.9841 - val_loss: 0.8931 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0110 - acc: 0.9841 - val_loss: 0.8891 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0816 - acc: 0.9841 - val_loss: 0.8884 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0213 - acc: 1.0000 - val_loss: 0.8890 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0092 - acc: 0.9841 - val_loss: 0.8847 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0034 - acc: 0.9683 - val_loss: 0.8826 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0625 - acc: 0.9841 - val_loss: 0.8803 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1235 - acc: 0.9206 - val_loss: 0.8842 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0248 - acc: 1.0000 - val_loss: 0.8849 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0617 - acc: 0.9683 - val_loss: 0.8835 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9914 - acc: 0.9683 - val_loss: 0.8819 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9494 - acc: 1.0000 - val_loss: 0.8763 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0643 - acc: 0.9841 - val_loss: 0.8752 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0353 - acc: 0.9683 - val_loss: 0.8757 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9901 - acc: 0.9841 - val_loss: 0.8757 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9873 - acc: 1.0000 - val_loss: 0.8749 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9998 - acc: 1.0000 - val_loss: 0.8724 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9844 - acc: 0.9841 - val_loss: 0.8722 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0371 - acc: 0.9683 - val_loss: 0.8730 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0362 - acc: 0.9683 - val_loss: 0.8786 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0507 - acc: 0.9683 - val_loss: 0.8811 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0537 - acc: 0.9841 - val_loss: 0.8809 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0073 - acc: 0.9841 - val_loss: 0.8784 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9865 - acc: 0.9683 - val_loss: 0.8731 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9893 - acc: 0.9841 - val_loss: 0.8699 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9935 - acc: 0.9841 - val_loss: 0.8680 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0784 - acc: 0.9524 - val_loss: 0.8712 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0150 - acc: 0.9683 - val_loss: 0.8732 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0110 - acc: 0.9841 - val_loss: 0.8732 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0493 - acc: 0.9365 - val_loss: 0.8731 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9676 - acc: 1.0000 - val_loss: 0.8705 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9801 - acc: 0.9683 - val_loss: 0.8674 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0451 - acc: 0.9683 - val_loss: 0.8665 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0148 - acc: 0.9524 - val_loss: 0.8669 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0062 - acc: 0.9841 - val_loss: 0.8654 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9970 - acc: 0.9841 - val_loss: 0.8625 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0284 - acc: 0.9365 - val_loss: 0.8609 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0174 - acc: 0.9683 - val_loss: 0.8618 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0425 - acc: 0.9524 - val_loss: 0.8658 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9731 - acc: 0.9841 - val_loss: 0.8671 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu_words Test Accuracy: 0.915\n",
      "my_model_neu Test f-measure: 0.796\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "neu_wo = Sequential()\n",
    "neu_wo.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "\n",
    "neu_wo.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "neu_wo.summary()\n",
    "history = neu_wo.fit(X_scaled_train_data_words, y_train,\n",
    "                    validation_data=(X_scaled_val_data_words, y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu_words,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu_words.h5')\n",
    "yhat = l_model.predict( scaled_test_data_words)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_words Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "n_grams (InputLayer)            (None, 4152)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "words (InputLayer)              (None, 7623)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 64)           139168      n_grams[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 64)           250240      words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           sequential_4[1][0]               \n",
      "                                                                 sequential_5[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          16512       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 128)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 9)            1161        dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 407,081\n",
      "Trainable params: 407,081\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(63, 4152) (63, 7623) (63, 9)\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 11.1390 - acc: 0.0794 - val_loss: 10.9800 - val_acc: 0.1164\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.8978 - acc: 0.1746 - val_loss: 10.7778 - val_acc: 0.1177\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.6965 - acc: 0.1270 - val_loss: 10.5908 - val_acc: 0.1270\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.5163 - acc: 0.0794 - val_loss: 10.4118 - val_acc: 0.1349\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.3314 - acc: 0.0952 - val_loss: 10.2393 - val_acc: 0.1402\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.1639 - acc: 0.1587 - val_loss: 10.0738 - val_acc: 0.1481\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.9866 - acc: 0.1587 - val_loss: 9.9132 - val_acc: 0.1468\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.8838 - acc: 0.1270 - val_loss: 9.7562 - val_acc: 0.1561\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.6910 - acc: 0.1905 - val_loss: 9.6028 - val_acc: 0.1759\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.5307 - acc: 0.2222 - val_loss: 9.4529 - val_acc: 0.1878\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.3954 - acc: 0.1587 - val_loss: 9.3079 - val_acc: 0.2050\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.2633 - acc: 0.1587 - val_loss: 9.1660 - val_acc: 0.2302\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.1173 - acc: 0.2063 - val_loss: 9.0282 - val_acc: 0.2685\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.0175 - acc: 0.1587 - val_loss: 8.8941 - val_acc: 0.2778\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.8679 - acc: 0.2063 - val_loss: 8.7645 - val_acc: 0.2778\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.7598 - acc: 0.1429 - val_loss: 8.6373 - val_acc: 0.2765\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.5959 - acc: 0.2381 - val_loss: 8.5123 - val_acc: 0.3082\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.4877 - acc: 0.1905 - val_loss: 8.3888 - val_acc: 0.3452\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.3533 - acc: 0.2857 - val_loss: 8.2668 - val_acc: 0.3836\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.2367 - acc: 0.2222 - val_loss: 8.1467 - val_acc: 0.4193\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.1527 - acc: 0.2063 - val_loss: 8.0279 - val_acc: 0.4683\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.9649 - acc: 0.3175 - val_loss: 7.9094 - val_acc: 0.5714\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.9489 - acc: 0.2063 - val_loss: 7.7960 - val_acc: 0.6376\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.8427 - acc: 0.3016 - val_loss: 7.6879 - val_acc: 0.6733\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.6817 - acc: 0.3968 - val_loss: 7.5827 - val_acc: 0.6958\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.6255 - acc: 0.2857 - val_loss: 7.4784 - val_acc: 0.7262\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.4745 - acc: 0.3333 - val_loss: 7.3624 - val_acc: 0.7262\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.4031 - acc: 0.3492 - val_loss: 7.2574 - val_acc: 0.7421\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.3071 - acc: 0.4444 - val_loss: 7.1495 - val_acc: 0.7989\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.1936 - acc: 0.4444 - val_loss: 7.0463 - val_acc: 0.8135\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.1566 - acc: 0.3333 - val_loss: 6.9502 - val_acc: 0.8730\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.0687 - acc: 0.4762 - val_loss: 6.8611 - val_acc: 0.9193\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.0233 - acc: 0.5079 - val_loss: 6.7664 - val_acc: 0.9325\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.8847 - acc: 0.3968 - val_loss: 6.6716 - val_acc: 0.9339\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.7909 - acc: 0.5079 - val_loss: 6.5701 - val_acc: 0.9312\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.7306 - acc: 0.4603 - val_loss: 6.4753 - val_acc: 0.9577\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.6389 - acc: 0.5714 - val_loss: 6.3863 - val_acc: 0.9683\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.6084 - acc: 0.5556 - val_loss: 6.3004 - val_acc: 0.9788\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.4585 - acc: 0.5873 - val_loss: 6.2096 - val_acc: 0.9881\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.3246 - acc: 0.6508 - val_loss: 6.1092 - val_acc: 0.9947\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.2782 - acc: 0.5873 - val_loss: 6.0177 - val_acc: 0.9868\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.2055 - acc: 0.6190 - val_loss: 5.9297 - val_acc: 0.9987\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.1875 - acc: 0.5873 - val_loss: 5.8436 - val_acc: 1.0000\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.0736 - acc: 0.6032 - val_loss: 5.7622 - val_acc: 1.0000\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.9016 - acc: 0.7302 - val_loss: 5.6802 - val_acc: 1.0000\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.0003 - acc: 0.5873 - val_loss: 5.5969 - val_acc: 0.9987\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.8926 - acc: 0.6190 - val_loss: 5.5219 - val_acc: 0.9987\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.8001 - acc: 0.7460 - val_loss: 5.4391 - val_acc: 1.0000\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.7757 - acc: 0.6508 - val_loss: 5.3660 - val_acc: 1.0000\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.8112 - acc: 0.5238 - val_loss: 5.3107 - val_acc: 1.0000\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.6218 - acc: 0.7460 - val_loss: 5.2513 - val_acc: 1.0000\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.5569 - acc: 0.7460 - val_loss: 5.1733 - val_acc: 1.0000\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.3974 - acc: 0.7619 - val_loss: 5.0956 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.5195 - acc: 0.6508 - val_loss: 5.0340 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.4593 - acc: 0.7302 - val_loss: 4.9689 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.3652 - acc: 0.6984 - val_loss: 4.9132 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.2807 - acc: 0.7460 - val_loss: 4.8577 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1312 - acc: 0.8571 - val_loss: 4.7933 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1703 - acc: 0.7937 - val_loss: 4.7321 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0950 - acc: 0.7937 - val_loss: 4.6855 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0033 - acc: 0.8095 - val_loss: 4.6282 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1629 - acc: 0.6508 - val_loss: 4.5810 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0019 - acc: 0.7619 - val_loss: 4.5478 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8673 - acc: 0.8413 - val_loss: 4.4927 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8123 - acc: 0.8571 - val_loss: 4.4365 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8856 - acc: 0.7460 - val_loss: 4.3872 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.7933 - acc: 0.7460 - val_loss: 4.3418 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.7625 - acc: 0.7778 - val_loss: 4.2990 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.6020 - acc: 0.9048 - val_loss: 4.2545 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.6051 - acc: 0.8571 - val_loss: 4.2148 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.4777 - acc: 0.8889 - val_loss: 4.1783 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5933 - acc: 0.8095 - val_loss: 4.1423 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5499 - acc: 0.8254 - val_loss: 4.1137 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3968 - acc: 0.9048 - val_loss: 4.0779 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5440 - acc: 0.8095 - val_loss: 4.0432 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.4125 - acc: 0.9048 - val_loss: 4.0139 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3563 - acc: 0.9048 - val_loss: 3.9870 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3778 - acc: 0.8889 - val_loss: 3.9570 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2695 - acc: 0.9365 - val_loss: 3.9262 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3704 - acc: 0.8730 - val_loss: 3.8972 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1920 - acc: 0.9048 - val_loss: 3.8734 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1442 - acc: 0.9365 - val_loss: 3.8442 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1614 - acc: 0.9365 - val_loss: 3.8113 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2103 - acc: 0.8254 - val_loss: 3.7831 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2099 - acc: 0.8413 - val_loss: 3.7623 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0524 - acc: 0.9365 - val_loss: 3.7381 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1891 - acc: 0.8254 - val_loss: 3.7236 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0965 - acc: 0.8889 - val_loss: 3.7112 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0073 - acc: 0.8889 - val_loss: 3.6928 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.9747 - acc: 0.9206 - val_loss: 3.6685 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0255 - acc: 0.9206 - val_loss: 3.6418 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8926 - acc: 0.9524 - val_loss: 3.6135 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0181 - acc: 0.8889 - val_loss: 3.5909 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8826 - acc: 0.9365 - val_loss: 3.5710 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.9053 - acc: 0.9524 - val_loss: 3.5495 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8480 - acc: 0.9048 - val_loss: 3.5311 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7472 - acc: 0.9524 - val_loss: 3.5104 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8056 - acc: 0.9206 - val_loss: 3.4967 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8680 - acc: 0.8571 - val_loss: 3.4858 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8543 - acc: 0.9048 - val_loss: 3.4753 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8278 - acc: 0.9365 - val_loss: 3.4570 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7670 - acc: 0.9365 - val_loss: 3.4342 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6562 - acc: 0.9841 - val_loss: 3.4117 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7184 - acc: 0.9365 - val_loss: 3.3907 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7107 - acc: 0.9365 - val_loss: 3.3760 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6607 - acc: 0.9206 - val_loss: 3.3603 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6666 - acc: 0.9206 - val_loss: 3.3448 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7002 - acc: 0.9048 - val_loss: 3.3325 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5492 - acc: 0.9841 - val_loss: 3.3135 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6265 - acc: 0.9048 - val_loss: 3.2962 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5689 - acc: 0.9683 - val_loss: 3.2819 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6233 - acc: 0.8730 - val_loss: 3.2677 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5707 - acc: 0.8889 - val_loss: 3.2577 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5497 - acc: 0.9206 - val_loss: 3.2445 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5194 - acc: 0.9365 - val_loss: 3.2330 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4056 - acc: 0.9841 - val_loss: 3.2189 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5197 - acc: 0.9206 - val_loss: 3.2006 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4061 - acc: 0.9683 - val_loss: 3.1880 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3392 - acc: 0.9841 - val_loss: 3.1688 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4472 - acc: 0.9683 - val_loss: 3.1531 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4603 - acc: 0.9048 - val_loss: 3.1422 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4462 - acc: 0.9365 - val_loss: 3.1364 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3180 - acc: 0.9841 - val_loss: 3.1249 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3175 - acc: 0.9683 - val_loss: 3.1103 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3979 - acc: 0.9524 - val_loss: 3.1000 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3761 - acc: 0.9524 - val_loss: 3.0910 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4303 - acc: 0.9206 - val_loss: 3.0775 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3228 - acc: 0.9524 - val_loss: 3.0635 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3528 - acc: 0.9365 - val_loss: 3.0531 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2608 - acc: 0.9524 - val_loss: 3.0424 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2827 - acc: 0.9524 - val_loss: 3.0306 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2730 - acc: 0.9206 - val_loss: 3.0168 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2212 - acc: 0.9841 - val_loss: 3.0025 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3885 - acc: 0.9048 - val_loss: 2.9973 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2393 - acc: 0.9524 - val_loss: 2.9937 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2323 - acc: 0.9683 - val_loss: 2.9833 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2439 - acc: 0.9524 - val_loss: 2.9703 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1519 - acc: 0.9841 - val_loss: 2.9577 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1106 - acc: 1.0000 - val_loss: 2.9407 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1422 - acc: 0.9683 - val_loss: 2.9258 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1427 - acc: 0.9683 - val_loss: 2.9174 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3213 - acc: 0.8571 - val_loss: 2.9157 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1618 - acc: 0.9841 - val_loss: 2.9140 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2285 - acc: 0.9524 - val_loss: 2.9110 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1531 - acc: 0.9683 - val_loss: 2.8991 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1358 - acc: 0.9524 - val_loss: 2.8854 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1452 - acc: 0.9365 - val_loss: 2.8741 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0833 - acc: 1.0000 - val_loss: 2.8634 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1121 - acc: 0.9365 - val_loss: 2.8534 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0129 - acc: 0.9683 - val_loss: 2.8391 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0660 - acc: 0.9841 - val_loss: 2.8267 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0262 - acc: 0.9841 - val_loss: 2.8177 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0268 - acc: 0.9365 - val_loss: 2.8069 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0196 - acc: 0.9524 - val_loss: 2.7977 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0094 - acc: 0.9524 - val_loss: 2.7875 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9798 - acc: 0.9841 - val_loss: 2.7782 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9450 - acc: 0.9683 - val_loss: 2.7690 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0173 - acc: 0.9524 - val_loss: 2.7658 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9474 - acc: 0.9841 - val_loss: 2.7576 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9107 - acc: 0.9841 - val_loss: 2.7457 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9112 - acc: 0.9683 - val_loss: 2.7361 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9785 - acc: 0.9365 - val_loss: 2.7328 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9742 - acc: 0.9524 - val_loss: 2.7284 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9650 - acc: 0.9524 - val_loss: 2.7231 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9504 - acc: 0.9683 - val_loss: 2.7184 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9234 - acc: 0.9841 - val_loss: 2.7075 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8798 - acc: 0.9683 - val_loss: 2.6979 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8441 - acc: 1.0000 - val_loss: 2.6877 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8782 - acc: 0.9683 - val_loss: 2.6755 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8266 - acc: 0.9841 - val_loss: 2.6679 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7671 - acc: 1.0000 - val_loss: 2.6560 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8336 - acc: 0.9524 - val_loss: 2.6484 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8297 - acc: 1.0000 - val_loss: 2.6423 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8156 - acc: 0.9841 - val_loss: 2.6334 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8115 - acc: 0.9524 - val_loss: 2.6283 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8097 - acc: 0.9524 - val_loss: 2.6239 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7681 - acc: 0.9683 - val_loss: 2.6117 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7529 - acc: 1.0000 - val_loss: 2.6017 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7397 - acc: 1.0000 - val_loss: 2.5941 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7753 - acc: 0.9524 - val_loss: 2.5862 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7149 - acc: 0.9841 - val_loss: 2.5805 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7733 - acc: 0.9683 - val_loss: 2.5762 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7438 - acc: 1.0000 - val_loss: 2.5683 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7136 - acc: 0.9683 - val_loss: 2.5612 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6855 - acc: 0.9841 - val_loss: 2.5549 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7333 - acc: 0.9683 - val_loss: 2.5474 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7649 - acc: 0.9524 - val_loss: 2.5420 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7732 - acc: 0.9206 - val_loss: 2.5410 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7493 - acc: 0.9841 - val_loss: 2.5394 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6675 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8093 - acc: 0.9683 - val_loss: 2.5275 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6614 - acc: 0.9841 - val_loss: 2.5230 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7242 - acc: 0.9524 - val_loss: 2.5182 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6602 - acc: 0.9683 - val_loss: 2.5101 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6857 - acc: 0.9683 - val_loss: 2.5034 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6767 - acc: 0.9524 - val_loss: 2.4956 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6390 - acc: 1.0000 - val_loss: 2.4845 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5797 - acc: 1.0000 - val_loss: 2.4730 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6214 - acc: 0.9841 - val_loss: 2.4647 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6064 - acc: 0.9841 - val_loss: 2.4601 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6153 - acc: 1.0000 - val_loss: 2.4504 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6800 - acc: 0.9206 - val_loss: 2.4489 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6219 - acc: 0.9841 - val_loss: 2.4488 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6293 - acc: 0.9683 - val_loss: 2.4437 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6410 - acc: 0.9841 - val_loss: 2.4369 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6038 - acc: 0.9841 - val_loss: 2.4342 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6185 - acc: 0.9683 - val_loss: 2.4305 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5474 - acc: 0.9683 - val_loss: 2.4197 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5588 - acc: 0.9841 - val_loss: 2.4078 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5978 - acc: 0.9683 - val_loss: 2.4026 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5470 - acc: 0.9524 - val_loss: 2.3984 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4990 - acc: 0.9841 - val_loss: 2.3898 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4985 - acc: 1.0000 - val_loss: 2.3790 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5775 - acc: 0.9683 - val_loss: 2.3750 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6027 - acc: 0.9206 - val_loss: 2.3803 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5180 - acc: 1.0000 - val_loss: 2.3791 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5253 - acc: 0.9683 - val_loss: 2.3674 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4768 - acc: 0.9841 - val_loss: 2.3549 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4605 - acc: 1.0000 - val_loss: 2.3440 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5200 - acc: 0.9524 - val_loss: 2.3359 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5321 - acc: 0.9841 - val_loss: 2.3390 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4669 - acc: 1.0000 - val_loss: 2.3342 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4863 - acc: 1.0000 - val_loss: 2.3268 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5060 - acc: 0.9841 - val_loss: 2.3203 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4725 - acc: 0.9524 - val_loss: 2.3208 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4509 - acc: 0.9841 - val_loss: 2.3135 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4544 - acc: 0.9841 - val_loss: 2.3102 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4354 - acc: 0.9841 - val_loss: 2.3029 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5309 - acc: 0.9683 - val_loss: 2.3054 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4361 - acc: 1.0000 - val_loss: 2.3062 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4104 - acc: 1.0000 - val_loss: 2.2980 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4147 - acc: 1.0000 - val_loss: 2.2870 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3494 - acc: 1.0000 - val_loss: 2.2734 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3726 - acc: 1.0000 - val_loss: 2.2645 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3849 - acc: 0.9683 - val_loss: 2.2529 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4672 - acc: 0.9365 - val_loss: 2.2551 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3911 - acc: 0.9841 - val_loss: 2.2554 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3609 - acc: 1.0000 - val_loss: 2.2470 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3423 - acc: 1.0000 - val_loss: 2.2346 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3485 - acc: 1.0000 - val_loss: 2.2253 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3577 - acc: 0.9841 - val_loss: 2.2183 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3332 - acc: 1.0000 - val_loss: 2.2181 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3526 - acc: 1.0000 - val_loss: 2.2189 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3576 - acc: 0.9841 - val_loss: 2.2220 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3430 - acc: 0.9683 - val_loss: 2.2187 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3751 - acc: 0.9683 - val_loss: 2.2125 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4137 - acc: 0.9841 - val_loss: 2.2119 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3431 - acc: 0.9841 - val_loss: 2.2093 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3299 - acc: 0.9841 - val_loss: 2.2060 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3058 - acc: 1.0000 - val_loss: 2.1953 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3804 - acc: 0.9841 - val_loss: 2.1867 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3195 - acc: 0.9841 - val_loss: 2.1875 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2840 - acc: 0.9841 - val_loss: 2.1812 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2819 - acc: 0.9841 - val_loss: 2.1736 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3006 - acc: 0.9841 - val_loss: 2.1668 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2780 - acc: 0.9841 - val_loss: 2.1644 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3036 - acc: 0.9683 - val_loss: 2.1603 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3089 - acc: 0.9841 - val_loss: 2.1578 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2611 - acc: 1.0000 - val_loss: 2.1540 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2778 - acc: 1.0000 - val_loss: 2.1449 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2831 - acc: 0.9841 - val_loss: 2.1403 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2971 - acc: 0.9841 - val_loss: 2.1405 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2458 - acc: 1.0000 - val_loss: 2.1373 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3268 - acc: 0.9365 - val_loss: 2.1367 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2831 - acc: 0.9683 - val_loss: 2.1335 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2524 - acc: 0.9841 - val_loss: 2.1270 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2359 - acc: 1.0000 - val_loss: 2.1224 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3023 - acc: 0.9683 - val_loss: 2.1221 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2306 - acc: 0.9841 - val_loss: 2.1218 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2681 - acc: 0.9841 - val_loss: 2.1217 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2154 - acc: 0.9841 - val_loss: 2.1157 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2607 - acc: 1.0000 - val_loss: 2.1070 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1875 - acc: 0.9683 - val_loss: 2.1033 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2687 - acc: 0.9683 - val_loss: 2.0974 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1892 - acc: 1.0000 - val_loss: 2.0941 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2184 - acc: 1.0000 - val_loss: 2.0867 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2280 - acc: 1.0000 - val_loss: 2.0858 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2326 - acc: 1.0000 - val_loss: 2.0845 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1809 - acc: 0.9841 - val_loss: 2.0816 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1955 - acc: 1.0000 - val_loss: 2.0758 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2455 - acc: 0.9365 - val_loss: 2.0743 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1880 - acc: 1.0000 - val_loss: 2.0658 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2074 - acc: 0.9841 - val_loss: 2.0578 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1391 - acc: 0.9841 - val_loss: 2.0483 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1238 - acc: 0.9841 - val_loss: 2.0355 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1899 - acc: 0.9683 - val_loss: 2.0367 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2469 - acc: 0.9524 - val_loss: 2.0437 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2299 - acc: 0.9524 - val_loss: 2.0490 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1585 - acc: 1.0000 - val_loss: 2.0473 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1207 - acc: 0.9841 - val_loss: 2.0382 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1479 - acc: 1.0000 - val_loss: 2.0265 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1412 - acc: 0.9841 - val_loss: 2.0144 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2079 - acc: 0.9524 - val_loss: 2.0155 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1833 - acc: 0.9683 - val_loss: 2.0181 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1918 - acc: 0.9683 - val_loss: 2.0168 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1151 - acc: 1.0000 - val_loss: 2.0164 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1217 - acc: 1.0000 - val_loss: 2.0092 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0691 - acc: 1.0000 - val_loss: 1.9992 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0969 - acc: 0.9841 - val_loss: 1.9867 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1294 - acc: 0.9683 - val_loss: 1.9837 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0860 - acc: 1.0000 - val_loss: 1.9891 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1806 - acc: 0.9841 - val_loss: 1.9929 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1073 - acc: 1.0000 - val_loss: 1.9940 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1073 - acc: 0.9841 - val_loss: 1.9878 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0736 - acc: 0.9841 - val_loss: 1.9772 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0359 - acc: 1.0000 - val_loss: 1.9666 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0740 - acc: 0.9841 - val_loss: 1.9636 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0953 - acc: 0.9683 - val_loss: 1.9609 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1138 - acc: 0.9683 - val_loss: 1.9603 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1141 - acc: 0.9683 - val_loss: 1.9617 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0662 - acc: 1.0000 - val_loss: 1.9557 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0516 - acc: 1.0000 - val_loss: 1.9483 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0701 - acc: 0.9841 - val_loss: 1.9394 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.1153 - acc: 0.9524 - val_loss: 1.9409 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 2.0723 - acc: 0.9841 - val_loss: 1.9387 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0352 - acc: 1.0000 - val_loss: 1.9386 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0573 - acc: 0.9841 - val_loss: 1.9368 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0583 - acc: 0.9683 - val_loss: 1.9360 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0257 - acc: 0.9841 - val_loss: 1.9323 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0611 - acc: 0.9841 - val_loss: 1.9251 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0186 - acc: 0.9841 - val_loss: 1.9174 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0298 - acc: 0.9841 - val_loss: 1.9139 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0360 - acc: 0.9841 - val_loss: 1.9092 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0087 - acc: 1.0000 - val_loss: 1.9064 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0174 - acc: 1.0000 - val_loss: 1.9013 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0031 - acc: 1.0000 - val_loss: 1.8982 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0310 - acc: 0.9841 - val_loss: 1.8939 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0278 - acc: 0.9841 - val_loss: 1.8916 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9821 - acc: 0.9841 - val_loss: 1.8913 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0161 - acc: 0.9841 - val_loss: 1.8890 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0731 - acc: 0.9524 - val_loss: 1.8916 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0354 - acc: 0.9841 - val_loss: 1.8979 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0580 - acc: 0.9683 - val_loss: 1.8956 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0806 - acc: 0.9524 - val_loss: 1.8975 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9748 - acc: 1.0000 - val_loss: 1.8943 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9551 - acc: 1.0000 - val_loss: 1.8818 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0633 - acc: 0.9683 - val_loss: 1.8761 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9737 - acc: 0.9841 - val_loss: 1.8762 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9667 - acc: 1.0000 - val_loss: 1.8674 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0154 - acc: 0.9524 - val_loss: 1.8615 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9636 - acc: 0.9841 - val_loss: 1.8634 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9677 - acc: 1.0000 - val_loss: 1.8588 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9638 - acc: 0.9841 - val_loss: 1.8534 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9304 - acc: 1.0000 - val_loss: 1.8450 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9842 - acc: 0.9841 - val_loss: 1.8434 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9892 - acc: 0.9841 - val_loss: 1.8463 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9518 - acc: 1.0000 - val_loss: 1.8442 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9563 - acc: 0.9841 - val_loss: 1.8392 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0060 - acc: 0.9524 - val_loss: 1.8353 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9587 - acc: 0.9841 - val_loss: 1.8385 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9751 - acc: 0.9841 - val_loss: 1.8353 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8960 - acc: 1.0000 - val_loss: 1.8291 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9600 - acc: 0.9524 - val_loss: 1.8211 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9373 - acc: 0.9841 - val_loss: 1.8167 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9317 - acc: 0.9841 - val_loss: 1.8117 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9248 - acc: 0.9841 - val_loss: 1.8124 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9327 - acc: 0.9524 - val_loss: 1.8111 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9583 - acc: 0.9841 - val_loss: 1.8108 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8907 - acc: 1.0000 - val_loss: 1.8085 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8895 - acc: 1.0000 - val_loss: 1.7977 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9315 - acc: 0.9524 - val_loss: 1.7918 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9214 - acc: 0.9841 - val_loss: 1.7926 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8863 - acc: 1.0000 - val_loss: 1.7868 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8984 - acc: 1.0000 - val_loss: 1.7814 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8785 - acc: 0.9841 - val_loss: 1.7824 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8856 - acc: 1.0000 - val_loss: 1.7788 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9085 - acc: 0.9841 - val_loss: 1.7759 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9396 - acc: 0.9524 - val_loss: 1.7740 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8911 - acc: 1.0000 - val_loss: 1.7723 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8644 - acc: 0.9683 - val_loss: 1.7711 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8619 - acc: 0.9841 - val_loss: 1.7632 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8857 - acc: 1.0000 - val_loss: 1.7617 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8731 - acc: 1.0000 - val_loss: 1.7592 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8627 - acc: 0.9841 - val_loss: 1.7565 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8732 - acc: 0.9524 - val_loss: 1.7509 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8178 - acc: 1.0000 - val_loss: 1.7471 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8227 - acc: 1.0000 - val_loss: 1.7371 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8178 - acc: 1.0000 - val_loss: 1.7289 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7948 - acc: 1.0000 - val_loss: 1.7217 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8451 - acc: 1.0000 - val_loss: 1.7197 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8047 - acc: 1.0000 - val_loss: 1.7186 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8032 - acc: 0.9841 - val_loss: 1.7124 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7959 - acc: 1.0000 - val_loss: 1.7091 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8231 - acc: 0.9841 - val_loss: 1.7129 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8015 - acc: 1.0000 - val_loss: 1.7071 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7898 - acc: 1.0000 - val_loss: 1.7028 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7904 - acc: 0.9841 - val_loss: 1.7022 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7605 - acc: 1.0000 - val_loss: 1.6989 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8183 - acc: 0.9683 - val_loss: 1.6957 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7928 - acc: 0.9841 - val_loss: 1.6987 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7832 - acc: 0.9841 - val_loss: 1.6977 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7649 - acc: 1.0000 - val_loss: 1.6928 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7480 - acc: 1.0000 - val_loss: 1.6836 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7770 - acc: 1.0000 - val_loss: 1.6771 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8034 - acc: 0.9683 - val_loss: 1.6753 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7450 - acc: 1.0000 - val_loss: 1.6771 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7658 - acc: 1.0000 - val_loss: 1.6725 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7855 - acc: 1.0000 - val_loss: 1.6678 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7444 - acc: 1.0000 - val_loss: 1.6622 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7464 - acc: 0.9841 - val_loss: 1.6576 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7971 - acc: 0.9841 - val_loss: 1.6603 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7967 - acc: 1.0000 - val_loss: 1.6630 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7802 - acc: 0.9841 - val_loss: 1.6608 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7514 - acc: 0.9841 - val_loss: 1.6612 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7517 - acc: 0.9841 - val_loss: 1.6539 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7958 - acc: 0.9683 - val_loss: 1.6495 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7433 - acc: 1.0000 - val_loss: 1.6565 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7994 - acc: 0.9841 - val_loss: 1.6591 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7787 - acc: 1.0000 - val_loss: 1.6588 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7449 - acc: 0.9841 - val_loss: 1.6578 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7433 - acc: 0.9841 - val_loss: 1.6509 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7295 - acc: 1.0000 - val_loss: 1.6442 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7081 - acc: 0.9841 - val_loss: 1.6348 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7563 - acc: 0.9683 - val_loss: 1.6339 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7729 - acc: 0.9683 - val_loss: 1.6354 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8153 - acc: 0.9683 - val_loss: 1.6489 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7731 - acc: 0.9841 - val_loss: 1.6599 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7565 - acc: 1.0000 - val_loss: 1.6556 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7485 - acc: 0.9841 - val_loss: 1.6476 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7308 - acc: 1.0000 - val_loss: 1.6441 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7268 - acc: 1.0000 - val_loss: 1.6335 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7921 - acc: 0.9524 - val_loss: 1.6295 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7172 - acc: 1.0000 - val_loss: 1.6308 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6992 - acc: 0.9841 - val_loss: 1.6246 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7569 - acc: 0.9683 - val_loss: 1.6220 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7067 - acc: 0.9841 - val_loss: 1.6201 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6834 - acc: 1.0000 - val_loss: 1.6128 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7013 - acc: 1.0000 - val_loss: 1.6067 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6979 - acc: 1.0000 - val_loss: 1.6010 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6508 - acc: 1.0000 - val_loss: 1.5934 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7307 - acc: 1.0000 - val_loss: 1.5930 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6976 - acc: 0.9841 - val_loss: 1.5957 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7052 - acc: 0.9841 - val_loss: 1.5961 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7140 - acc: 0.9841 - val_loss: 1.5929 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6978 - acc: 0.9841 - val_loss: 1.5924 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6729 - acc: 0.9841 - val_loss: 1.5897 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6425 - acc: 1.0000 - val_loss: 1.5813 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7417 - acc: 0.9524 - val_loss: 1.5868 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6895 - acc: 0.9841 - val_loss: 1.5880 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7011 - acc: 0.9841 - val_loss: 1.5809 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7235 - acc: 0.9683 - val_loss: 1.5779 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6401 - acc: 1.0000 - val_loss: 1.5725 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6484 - acc: 0.9683 - val_loss: 1.5607 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6510 - acc: 1.0000 - val_loss: 1.5576 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6485 - acc: 1.0000 - val_loss: 1.5569 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6334 - acc: 1.0000 - val_loss: 1.5519 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7070 - acc: 0.9683 - val_loss: 1.5565 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6413 - acc: 1.0000 - val_loss: 1.5570 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6677 - acc: 0.9841 - val_loss: 1.5532 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6577 - acc: 0.9841 - val_loss: 1.5559 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6478 - acc: 0.9841 - val_loss: 1.5576 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6298 - acc: 0.9683 - val_loss: 1.5532 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6422 - acc: 1.0000 - val_loss: 1.5455 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6136 - acc: 1.0000 - val_loss: 1.5356 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6729 - acc: 0.9683 - val_loss: 1.5386 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6146 - acc: 1.0000 - val_loss: 1.5363 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6570 - acc: 0.9841 - val_loss: 1.5390 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6371 - acc: 1.0000 - val_loss: 1.5360 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5979 - acc: 1.0000 - val_loss: 1.5271 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6252 - acc: 0.9683 - val_loss: 1.5246 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6407 - acc: 0.9841 - val_loss: 1.5220 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5840 - acc: 1.0000 - val_loss: 1.5132 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6182 - acc: 1.0000 - val_loss: 1.5100 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6238 - acc: 0.9683 - val_loss: 1.5123 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5906 - acc: 1.0000 - val_loss: 1.5157 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6366 - acc: 0.9841 - val_loss: 1.5149 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5966 - acc: 0.9841 - val_loss: 1.5158 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5989 - acc: 0.9841 - val_loss: 1.5105 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5543 - acc: 1.0000 - val_loss: 1.5024 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6128 - acc: 0.9841 - val_loss: 1.4967 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6123 - acc: 0.9841 - val_loss: 1.5008 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6094 - acc: 0.9683 - val_loss: 1.5033 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6227 - acc: 0.9841 - val_loss: 1.5006 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6180 - acc: 0.9841 - val_loss: 1.5003 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5876 - acc: 1.0000 - val_loss: 1.4981 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6300 - acc: 0.9841 - val_loss: 1.4975 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5670 - acc: 0.9841 - val_loss: 1.4932 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5867 - acc: 1.0000 - val_loss: 1.4915 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5671 - acc: 0.9683 - val_loss: 1.4854 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6690 - acc: 0.9683 - val_loss: 1.4931 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5771 - acc: 1.0000 - val_loss: 1.4964 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5757 - acc: 0.9841 - val_loss: 1.4951 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6089 - acc: 0.9683 - val_loss: 1.4903 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6145 - acc: 0.9841 - val_loss: 1.4847 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5966 - acc: 0.9683 - val_loss: 1.4797 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5708 - acc: 1.0000 - val_loss: 1.4763 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5571 - acc: 1.0000 - val_loss: 1.4686 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5599 - acc: 1.0000 - val_loss: 1.4637 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5666 - acc: 0.9841 - val_loss: 1.4599 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5914 - acc: 0.9683 - val_loss: 1.4651 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5666 - acc: 0.9841 - val_loss: 1.4689 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6172 - acc: 0.9841 - val_loss: 1.4694 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5577 - acc: 0.9841 - val_loss: 1.4639 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5710 - acc: 0.9841 - val_loss: 1.4558 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5981 - acc: 0.9841 - val_loss: 1.4532 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5455 - acc: 1.0000 - val_loss: 1.4526 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5022 - acc: 1.0000 - val_loss: 1.4437 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5159 - acc: 1.0000 - val_loss: 1.4370 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5721 - acc: 1.0000 - val_loss: 1.4387 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5480 - acc: 0.9841 - val_loss: 1.4406 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu Test Accuracy: 0.878\n",
      "my_model_neu Test f-measure: 0.768\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_input_tensor = Input(shape=(scaled_train_data_words.shape[1],) , name='words')\n",
    "ngram_input_tensor = Input(shape=(scaled_train_data_ngrams.shape[1],) , name='n_grams')\n",
    "\n",
    "neu0 = Sequential()\n",
    "neu0.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "\n",
    "neu1 = Sequential()\n",
    "neu1.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "\n",
    "output_tensor_0 = neu0(ngram_input_tensor)\n",
    "output_tensor_1 = neu1(word_input_tensor)\n",
    "\n",
    "\n",
    "# conv_input_tensor = Input(shape=(maxlen,) , name='convnets')\n",
    "\n",
    "# conv_1d_s3_model = Sequential()\n",
    "# conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_s3_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_0 = conv_1d_s3_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "# conv_1d_s1_model = Sequential()\n",
    "# conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_s1_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_1 = conv_1d_s1_model(conv_input_tensor)\n",
    "\n",
    "# conv_1d_complex_model = Sequential()\n",
    "# conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_complex_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_2 = conv_1d_complex_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "\n",
    "concatenated = layers.concatenate([output_tensor_0,\n",
    "                                   output_tensor_1,\n",
    "#                                    conv_output_tensor_0,\n",
    "#                                    conv_output_tensor_1,\n",
    "#                                    conv_output_tensor_2,\n",
    "                                  ], axis=-1)\n",
    "\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "concatenated = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model([ngram_input_tensor, word_input_tensor,\n",
    "#                conv_input_tensor\n",
    "              ], concatenated)\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "print(X_scaled_train_data_ngrams.shape, X_scaled_train_data_words.shape, y_train.shape) \n",
    "history = model.fit([X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
    "#                      X_train\n",
    "                    ], y_train,\n",
    "                    validation_data=([X_scaled_val_data_ngrams, X_scaled_val_data_words,\n",
    "#                                       X_val\n",
    "                                     ], y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu.h5')\n",
    "yhat = l_model.predict([scaled_test_data_ngrams, scaled_test_data_words])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_neu Test Accuracy: 0.853\n",
      "my_model_neu Test f-measure: 0.668\n"
     ]
    }
   ],
   "source": [
    "l_model = load_model('my_model_neu.h5')\n",
    "yhat = l_model.predict([scaled_test_data_ngrams, scaled_test_data_words])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "\n",
    "print('my_model_neu Test Accuracy: %.3f' % acc)\n",
    "\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "convnet_words (InputLayer)      (None, 1053)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 64)           72451       convnet_words[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 64)           69521       convnet_words[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 64)           72378       convnet_words[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 192)          0           sequential_11[1][0]              \n",
      "                                                                 sequential_12[1][0]              \n",
      "                                                                 sequential_13[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 128)          24704       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 128)          0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 9)            1161        dropout_30[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 240,215\n",
      "Trainable params: 239,959\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/300\n",
      " - 9s - loss: 4.1205 - acc: 0.0635 - val_loss: 4.0365 - val_acc: 0.1336\n",
      "Epoch 2/300\n",
      " - 2s - loss: 4.1029 - acc: 0.2063 - val_loss: 4.0073 - val_acc: 0.1601\n",
      "Epoch 3/300\n",
      " - 2s - loss: 4.0155 - acc: 0.1429 - val_loss: 3.9801 - val_acc: 0.1786\n",
      "Epoch 4/300\n",
      " - 2s - loss: 3.9800 - acc: 0.1746 - val_loss: 3.9549 - val_acc: 0.2288\n",
      "Epoch 5/300\n",
      " - 2s - loss: 3.9583 - acc: 0.1270 - val_loss: 3.9311 - val_acc: 0.2222\n",
      "Epoch 6/300\n",
      " - 2s - loss: 3.9670 - acc: 0.1270 - val_loss: 3.9076 - val_acc: 0.2288\n",
      "Epoch 7/300\n",
      " - 2s - loss: 3.9452 - acc: 0.1111 - val_loss: 3.8843 - val_acc: 0.2368\n",
      "Epoch 8/300\n",
      " - 2s - loss: 3.8963 - acc: 0.1270 - val_loss: 3.8629 - val_acc: 0.2183\n",
      "Epoch 9/300\n",
      " - 2s - loss: 3.8710 - acc: 0.1429 - val_loss: 3.8405 - val_acc: 0.2262\n",
      "Epoch 10/300\n",
      " - 2s - loss: 3.8835 - acc: 0.1270 - val_loss: 3.8189 - val_acc: 0.1799\n",
      "Epoch 11/300\n",
      " - 2s - loss: 3.8812 - acc: 0.1111 - val_loss: 3.7974 - val_acc: 0.1971\n",
      "Epoch 12/300\n",
      " - 2s - loss: 3.8129 - acc: 0.1111 - val_loss: 3.7762 - val_acc: 0.2407\n",
      "Epoch 13/300\n",
      " - 2s - loss: 3.8052 - acc: 0.0952 - val_loss: 3.7554 - val_acc: 0.2474\n",
      "Epoch 14/300\n",
      " - 2s - loss: 3.7991 - acc: 0.1746 - val_loss: 3.7347 - val_acc: 0.2725\n",
      "Epoch 15/300\n",
      " - 2s - loss: 3.7535 - acc: 0.1270 - val_loss: 3.7141 - val_acc: 0.3452\n",
      "Epoch 16/300\n",
      " - 2s - loss: 3.6919 - acc: 0.2540 - val_loss: 3.6938 - val_acc: 0.3426\n",
      "Epoch 17/300\n",
      " - 2s - loss: 3.7283 - acc: 0.1270 - val_loss: 3.6730 - val_acc: 0.3730\n",
      "Epoch 18/300\n",
      " - 2s - loss: 3.6677 - acc: 0.2063 - val_loss: 3.6528 - val_acc: 0.4087\n",
      "Epoch 19/300\n",
      " - 2s - loss: 3.6985 - acc: 0.1270 - val_loss: 3.6331 - val_acc: 0.4206\n",
      "Epoch 20/300\n",
      " - 2s - loss: 3.6776 - acc: 0.1429 - val_loss: 3.6138 - val_acc: 0.3968\n",
      "Epoch 21/300\n",
      " - 2s - loss: 3.6416 - acc: 0.1746 - val_loss: 3.5943 - val_acc: 0.3783\n",
      "Epoch 22/300\n",
      " - 2s - loss: 3.6359 - acc: 0.1270 - val_loss: 3.5742 - val_acc: 0.3836\n",
      "Epoch 23/300\n",
      " - 2s - loss: 3.5657 - acc: 0.2063 - val_loss: 3.5548 - val_acc: 0.3677\n",
      "Epoch 24/300\n",
      " - 2s - loss: 3.5779 - acc: 0.1429 - val_loss: 3.5354 - val_acc: 0.3320\n",
      "Epoch 25/300\n",
      " - 2s - loss: 3.5522 - acc: 0.1746 - val_loss: 3.5159 - val_acc: 0.3267\n",
      "Epoch 26/300\n",
      " - 2s - loss: 3.5494 - acc: 0.1587 - val_loss: 3.4966 - val_acc: 0.3122\n",
      "Epoch 27/300\n",
      " - 2s - loss: 3.4822 - acc: 0.2381 - val_loss: 3.4772 - val_acc: 0.4061\n",
      "Epoch 28/300\n",
      " - 2s - loss: 3.4766 - acc: 0.1905 - val_loss: 3.4582 - val_acc: 0.4511\n",
      "Epoch 29/300\n",
      " - 2s - loss: 3.5149 - acc: 0.1905 - val_loss: 3.4391 - val_acc: 0.5741\n",
      "Epoch 30/300\n",
      " - 2s - loss: 3.4264 - acc: 0.3492 - val_loss: 3.4204 - val_acc: 0.5384\n",
      "Epoch 31/300\n",
      " - 2s - loss: 3.4306 - acc: 0.3016 - val_loss: 3.4010 - val_acc: 0.5767\n",
      "Epoch 32/300\n",
      " - 2s - loss: 3.4131 - acc: 0.2381 - val_loss: 3.3816 - val_acc: 0.5767\n",
      "Epoch 33/300\n",
      " - 2s - loss: 3.3522 - acc: 0.2857 - val_loss: 3.3622 - val_acc: 0.6892\n",
      "Epoch 34/300\n",
      " - 2s - loss: 3.3915 - acc: 0.2063 - val_loss: 3.3431 - val_acc: 0.7156\n",
      "Epoch 35/300\n",
      " - 2s - loss: 3.3430 - acc: 0.3333 - val_loss: 3.3235 - val_acc: 0.7341\n",
      "Epoch 36/300\n",
      " - 2s - loss: 3.3229 - acc: 0.2381 - val_loss: 3.3033 - val_acc: 0.7526\n",
      "Epoch 37/300\n",
      " - 2s - loss: 3.2901 - acc: 0.3333 - val_loss: 3.2834 - val_acc: 0.7870\n",
      "Epoch 38/300\n",
      " - 2s - loss: 3.3276 - acc: 0.1746 - val_loss: 3.2637 - val_acc: 0.7474\n",
      "Epoch 39/300\n",
      " - 2s - loss: 3.2573 - acc: 0.2857 - val_loss: 3.2430 - val_acc: 0.7725\n",
      "Epoch 40/300\n",
      " - 2s - loss: 3.2597 - acc: 0.3651 - val_loss: 3.2206 - val_acc: 0.7368\n",
      "Epoch 41/300\n",
      " - 2s - loss: 3.2465 - acc: 0.3333 - val_loss: 3.1996 - val_acc: 0.7315\n",
      "Epoch 42/300\n",
      " - 2s - loss: 3.2136 - acc: 0.2698 - val_loss: 3.1807 - val_acc: 0.7381\n",
      "Epoch 43/300\n",
      " - 2s - loss: 3.2113 - acc: 0.2698 - val_loss: 3.1606 - val_acc: 0.7474\n",
      "Epoch 44/300\n",
      " - 2s - loss: 3.1392 - acc: 0.3333 - val_loss: 3.1402 - val_acc: 0.8042\n",
      "Epoch 45/300\n",
      " - 2s - loss: 3.1301 - acc: 0.3651 - val_loss: 3.1217 - val_acc: 0.7870\n",
      "Epoch 46/300\n",
      " - 2s - loss: 3.1416 - acc: 0.3810 - val_loss: 3.1020 - val_acc: 0.7937\n",
      "Epoch 47/300\n",
      " - 2s - loss: 3.1042 - acc: 0.3492 - val_loss: 3.0810 - val_acc: 0.8320\n",
      "Epoch 48/300\n",
      " - 2s - loss: 3.0356 - acc: 0.4603 - val_loss: 3.0603 - val_acc: 0.8320\n",
      "Epoch 49/300\n",
      " - 2s - loss: 3.0806 - acc: 0.3492 - val_loss: 3.0395 - val_acc: 0.8466\n",
      "Epoch 50/300\n",
      " - 2s - loss: 3.0114 - acc: 0.4603 - val_loss: 3.0186 - val_acc: 0.8717\n",
      "Epoch 51/300\n",
      " - 2s - loss: 3.0082 - acc: 0.4286 - val_loss: 2.9984 - val_acc: 0.8611\n",
      "Epoch 52/300\n",
      " - 2s - loss: 2.9767 - acc: 0.4921 - val_loss: 2.9783 - val_acc: 0.8452\n",
      "Epoch 53/300\n",
      " - 2s - loss: 2.9840 - acc: 0.4286 - val_loss: 2.9576 - val_acc: 0.8399\n",
      "Epoch 54/300\n",
      " - 2s - loss: 2.9477 - acc: 0.4603 - val_loss: 2.9371 - val_acc: 0.8585\n",
      "Epoch 55/300\n",
      " - 2s - loss: 2.9961 - acc: 0.3492 - val_loss: 2.9158 - val_acc: 0.8770\n",
      "Epoch 56/300\n",
      " - 2s - loss: 2.9515 - acc: 0.3968 - val_loss: 2.8949 - val_acc: 0.8664\n",
      "Epoch 57/300\n",
      " - 2s - loss: 2.8405 - acc: 0.5714 - val_loss: 2.8733 - val_acc: 0.8704\n",
      "Epoch 58/300\n",
      " - 2s - loss: 2.8373 - acc: 0.5714 - val_loss: 2.8512 - val_acc: 0.8929\n",
      "Epoch 59/300\n",
      " - 2s - loss: 2.8370 - acc: 0.5079 - val_loss: 2.8300 - val_acc: 0.9008\n",
      "Epoch 60/300\n",
      " - 2s - loss: 2.8805 - acc: 0.4127 - val_loss: 2.8092 - val_acc: 0.8862\n",
      "Epoch 61/300\n",
      " - 2s - loss: 2.7935 - acc: 0.5556 - val_loss: 2.7887 - val_acc: 0.9074\n",
      "Epoch 62/300\n",
      " - 2s - loss: 2.7629 - acc: 0.5873 - val_loss: 2.7687 - val_acc: 0.9061\n",
      "Epoch 63/300\n",
      " - 2s - loss: 2.7574 - acc: 0.4762 - val_loss: 2.7471 - val_acc: 0.9233\n",
      "Epoch 64/300\n",
      " - 2s - loss: 2.7559 - acc: 0.5238 - val_loss: 2.7273 - val_acc: 0.9272\n",
      "Epoch 65/300\n",
      " - 2s - loss: 2.7348 - acc: 0.5873 - val_loss: 2.7072 - val_acc: 0.9312\n",
      "Epoch 66/300\n",
      " - 2s - loss: 2.7185 - acc: 0.5556 - val_loss: 2.6872 - val_acc: 0.9325\n",
      "Epoch 67/300\n",
      " - 2s - loss: 2.7061 - acc: 0.5714 - val_loss: 2.6663 - val_acc: 0.9365\n",
      "Epoch 68/300\n",
      " - 2s - loss: 2.7114 - acc: 0.5238 - val_loss: 2.6458 - val_acc: 0.9484\n",
      "Epoch 69/300\n",
      " - 2s - loss: 2.6525 - acc: 0.5397 - val_loss: 2.6252 - val_acc: 0.9550\n",
      "Epoch 70/300\n",
      " - 2s - loss: 2.5928 - acc: 0.6349 - val_loss: 2.6031 - val_acc: 0.9392\n",
      "Epoch 71/300\n",
      " - 2s - loss: 2.6134 - acc: 0.5238 - val_loss: 2.5808 - val_acc: 0.9378\n",
      "Epoch 72/300\n",
      " - 2s - loss: 2.5655 - acc: 0.6032 - val_loss: 2.5583 - val_acc: 0.9418\n",
      "Epoch 73/300\n",
      " - 2s - loss: 2.5300 - acc: 0.6825 - val_loss: 2.5367 - val_acc: 0.9153\n",
      "Epoch 74/300\n",
      " - 2s - loss: 2.5214 - acc: 0.5873 - val_loss: 2.5158 - val_acc: 0.8955\n",
      "Epoch 75/300\n",
      " - 2s - loss: 2.4761 - acc: 0.6349 - val_loss: 2.4940 - val_acc: 0.9153\n",
      "Epoch 76/300\n",
      " - 2s - loss: 2.5709 - acc: 0.4762 - val_loss: 2.4731 - val_acc: 0.9074\n",
      "Epoch 77/300\n",
      " - 2s - loss: 2.4120 - acc: 0.6667 - val_loss: 2.4517 - val_acc: 0.9127\n",
      "Epoch 78/300\n",
      " - 2s - loss: 2.4451 - acc: 0.6825 - val_loss: 2.4283 - val_acc: 0.9299\n",
      "Epoch 79/300\n",
      " - 2s - loss: 2.4067 - acc: 0.5873 - val_loss: 2.4053 - val_acc: 0.9365\n",
      "Epoch 80/300\n",
      " - 2s - loss: 2.4006 - acc: 0.5873 - val_loss: 2.3814 - val_acc: 0.9550\n",
      "Epoch 81/300\n",
      " - 2s - loss: 2.4278 - acc: 0.5556 - val_loss: 2.3595 - val_acc: 0.9511\n",
      "Epoch 82/300\n",
      " - 2s - loss: 2.2646 - acc: 0.6667 - val_loss: 2.3371 - val_acc: 0.9431\n",
      "Epoch 83/300\n",
      " - 2s - loss: 2.3614 - acc: 0.6032 - val_loss: 2.3147 - val_acc: 0.9656\n",
      "Epoch 84/300\n",
      " - 2s - loss: 2.2629 - acc: 0.6825 - val_loss: 2.2930 - val_acc: 0.9418\n",
      "Epoch 85/300\n",
      " - 2s - loss: 2.2531 - acc: 0.6667 - val_loss: 2.2705 - val_acc: 0.9431\n",
      "Epoch 86/300\n",
      " - 2s - loss: 2.2927 - acc: 0.6349 - val_loss: 2.2492 - val_acc: 0.9405\n",
      "Epoch 87/300\n",
      " - 2s - loss: 2.2838 - acc: 0.6667 - val_loss: 2.2274 - val_acc: 0.9669\n",
      "Epoch 88/300\n",
      " - 2s - loss: 2.2179 - acc: 0.6190 - val_loss: 2.2037 - val_acc: 0.9696\n",
      "Epoch 89/300\n",
      " - 2s - loss: 2.2338 - acc: 0.6190 - val_loss: 2.1820 - val_acc: 0.9722\n",
      "Epoch 90/300\n",
      " - 2s - loss: 2.2138 - acc: 0.6349 - val_loss: 2.1602 - val_acc: 0.9775\n",
      "Epoch 91/300\n",
      " - 2s - loss: 2.1869 - acc: 0.6032 - val_loss: 2.1387 - val_acc: 0.9762\n",
      "Epoch 92/300\n",
      " - 2s - loss: 2.1992 - acc: 0.6667 - val_loss: 2.1177 - val_acc: 0.9762\n",
      "Epoch 93/300\n",
      " - 2s - loss: 2.1433 - acc: 0.6508 - val_loss: 2.0975 - val_acc: 0.9775\n",
      "Epoch 94/300\n",
      " - 2s - loss: 2.0975 - acc: 0.6667 - val_loss: 2.0763 - val_acc: 0.9775\n",
      "Epoch 95/300\n",
      " - 2s - loss: 2.0868 - acc: 0.6667 - val_loss: 2.0558 - val_acc: 0.9722\n",
      "Epoch 96/300\n",
      " - 2s - loss: 2.0357 - acc: 0.6825 - val_loss: 2.0333 - val_acc: 0.9735\n",
      "Epoch 97/300\n",
      " - 2s - loss: 2.0485 - acc: 0.7460 - val_loss: 2.0107 - val_acc: 0.9775\n",
      "Epoch 98/300\n",
      " - 2s - loss: 2.0077 - acc: 0.7460 - val_loss: 1.9890 - val_acc: 0.9775\n",
      "Epoch 99/300\n",
      " - 2s - loss: 1.9689 - acc: 0.7460 - val_loss: 1.9668 - val_acc: 0.9802\n",
      "Epoch 100/300\n",
      " - 2s - loss: 1.9060 - acc: 0.7937 - val_loss: 1.9465 - val_acc: 0.9749\n",
      "Epoch 101/300\n",
      " - 2s - loss: 2.0229 - acc: 0.6032 - val_loss: 1.9279 - val_acc: 0.9762\n",
      "Epoch 102/300\n",
      " - 2s - loss: 1.9285 - acc: 0.7619 - val_loss: 1.9079 - val_acc: 0.9775\n",
      "Epoch 103/300\n",
      " - 2s - loss: 1.9770 - acc: 0.7143 - val_loss: 1.8876 - val_acc: 0.9762\n",
      "Epoch 104/300\n",
      " - 2s - loss: 1.8281 - acc: 0.8095 - val_loss: 1.8674 - val_acc: 0.9775\n",
      "Epoch 105/300\n",
      " - 2s - loss: 1.8679 - acc: 0.7460 - val_loss: 1.8461 - val_acc: 0.9775\n",
      "Epoch 106/300\n",
      " - 2s - loss: 1.8119 - acc: 0.7937 - val_loss: 1.8260 - val_acc: 0.9788\n",
      "Epoch 107/300\n",
      " - 2s - loss: 1.8084 - acc: 0.7778 - val_loss: 1.8057 - val_acc: 0.9775\n",
      "Epoch 108/300\n",
      " - 2s - loss: 1.8069 - acc: 0.7619 - val_loss: 1.7860 - val_acc: 0.9788\n",
      "Epoch 109/300\n",
      " - 2s - loss: 1.8103 - acc: 0.8095 - val_loss: 1.7667 - val_acc: 0.9788\n",
      "Epoch 110/300\n",
      " - 2s - loss: 1.7774 - acc: 0.8095 - val_loss: 1.7453 - val_acc: 0.9788\n",
      "Epoch 111/300\n",
      " - 2s - loss: 1.7277 - acc: 0.8413 - val_loss: 1.7250 - val_acc: 0.9775\n",
      "Epoch 112/300\n",
      " - 2s - loss: 1.7601 - acc: 0.7460 - val_loss: 1.7045 - val_acc: 0.9775\n",
      "Epoch 113/300\n",
      " - 2s - loss: 1.7053 - acc: 0.7937 - val_loss: 1.6852 - val_acc: 0.9802\n",
      "Epoch 114/300\n",
      " - 2s - loss: 1.7243 - acc: 0.7937 - val_loss: 1.6662 - val_acc: 0.9788\n",
      "Epoch 115/300\n",
      " - 2s - loss: 1.6349 - acc: 0.7937 - val_loss: 1.6486 - val_acc: 0.9775\n",
      "Epoch 116/300\n",
      " - 2s - loss: 1.5785 - acc: 0.8730 - val_loss: 1.6276 - val_acc: 0.9775\n",
      "Epoch 117/300\n",
      " - 2s - loss: 1.7051 - acc: 0.7619 - val_loss: 1.6090 - val_acc: 0.9788\n",
      "Epoch 118/300\n",
      " - 2s - loss: 1.6545 - acc: 0.7937 - val_loss: 1.5889 - val_acc: 0.9802\n",
      "Epoch 119/300\n",
      " - 2s - loss: 1.6155 - acc: 0.7937 - val_loss: 1.5712 - val_acc: 0.9802\n",
      "Epoch 120/300\n",
      " - 2s - loss: 1.5136 - acc: 0.8889 - val_loss: 1.5525 - val_acc: 0.9828\n",
      "Epoch 121/300\n",
      " - 2s - loss: 1.6337 - acc: 0.7619 - val_loss: 1.5349 - val_acc: 0.9841\n",
      "Epoch 122/300\n",
      " - 2s - loss: 1.5268 - acc: 0.8571 - val_loss: 1.5179 - val_acc: 0.9815\n",
      "Epoch 123/300\n",
      " - 2s - loss: 1.5883 - acc: 0.7460 - val_loss: 1.5001 - val_acc: 0.9788\n",
      "Epoch 124/300\n",
      " - 2s - loss: 1.4930 - acc: 0.8413 - val_loss: 1.4817 - val_acc: 0.9788\n",
      "Epoch 125/300\n",
      " - 2s - loss: 1.4514 - acc: 0.9048 - val_loss: 1.4626 - val_acc: 0.9788\n",
      "Epoch 126/300\n",
      " - 2s - loss: 1.4111 - acc: 0.9048 - val_loss: 1.4456 - val_acc: 0.9788\n",
      "Epoch 127/300\n",
      " - 2s - loss: 1.5102 - acc: 0.8413 - val_loss: 1.4269 - val_acc: 0.9788\n",
      "Epoch 128/300\n",
      " - 2s - loss: 1.4391 - acc: 0.8730 - val_loss: 1.4092 - val_acc: 0.9802\n",
      "Epoch 129/300\n",
      " - 2s - loss: 1.4123 - acc: 0.8730 - val_loss: 1.3918 - val_acc: 0.9802\n",
      "Epoch 130/300\n",
      " - 2s - loss: 1.3981 - acc: 0.8889 - val_loss: 1.3722 - val_acc: 0.9854\n",
      "Epoch 131/300\n",
      " - 2s - loss: 1.4613 - acc: 0.8889 - val_loss: 1.3555 - val_acc: 0.9854\n",
      "Epoch 132/300\n",
      " - 2s - loss: 1.4383 - acc: 0.8730 - val_loss: 1.3399 - val_acc: 0.9841\n",
      "Epoch 133/300\n",
      " - 2s - loss: 1.3349 - acc: 0.9048 - val_loss: 1.3231 - val_acc: 0.9841\n",
      "Epoch 134/300\n",
      " - 2s - loss: 1.3437 - acc: 0.9206 - val_loss: 1.3071 - val_acc: 0.9802\n",
      "Epoch 135/300\n",
      " - 2s - loss: 1.3083 - acc: 0.9048 - val_loss: 1.2895 - val_acc: 0.9828\n",
      "Epoch 136/300\n",
      " - 2s - loss: 1.2925 - acc: 0.8889 - val_loss: 1.2733 - val_acc: 0.9841\n",
      "Epoch 137/300\n",
      " - 2s - loss: 1.2997 - acc: 0.9048 - val_loss: 1.2556 - val_acc: 0.9841\n",
      "Epoch 138/300\n",
      " - 2s - loss: 1.2695 - acc: 0.9048 - val_loss: 1.2398 - val_acc: 0.9815\n",
      "Epoch 139/300\n",
      " - 2s - loss: 1.2105 - acc: 0.9365 - val_loss: 1.2221 - val_acc: 0.9815\n",
      "Epoch 140/300\n",
      " - 2s - loss: 1.2811 - acc: 0.8571 - val_loss: 1.2071 - val_acc: 0.9802\n",
      "Epoch 141/300\n",
      " - 2s - loss: 1.3297 - acc: 0.8571 - val_loss: 1.1949 - val_acc: 0.9802\n",
      "Epoch 142/300\n",
      " - 2s - loss: 1.2446 - acc: 0.9206 - val_loss: 1.1796 - val_acc: 0.9788\n",
      "Epoch 143/300\n",
      " - 2s - loss: 1.1722 - acc: 0.9365 - val_loss: 1.1655 - val_acc: 0.9802\n",
      "Epoch 144/300\n",
      " - 2s - loss: 1.1569 - acc: 0.9206 - val_loss: 1.1514 - val_acc: 0.9802\n",
      "Epoch 145/300\n",
      " - 2s - loss: 1.1809 - acc: 0.8730 - val_loss: 1.1382 - val_acc: 0.9802\n",
      "Epoch 146/300\n",
      " - 2s - loss: 1.1466 - acc: 0.9048 - val_loss: 1.1245 - val_acc: 0.9802\n",
      "Epoch 147/300\n",
      " - 2s - loss: 1.0565 - acc: 0.9206 - val_loss: 1.1071 - val_acc: 0.9802\n",
      "Epoch 148/300\n",
      " - 2s - loss: 1.1304 - acc: 0.9048 - val_loss: 1.0932 - val_acc: 0.9854\n",
      "Epoch 149/300\n",
      " - 2s - loss: 1.1517 - acc: 0.9365 - val_loss: 1.0802 - val_acc: 0.9868\n",
      "Epoch 150/300\n",
      " - 2s - loss: 1.1256 - acc: 0.9841 - val_loss: 1.0680 - val_acc: 0.9868\n",
      "Epoch 151/300\n",
      " - 2s - loss: 1.0491 - acc: 0.9048 - val_loss: 1.0553 - val_acc: 0.9868\n",
      "Epoch 152/300\n",
      " - 2s - loss: 1.0947 - acc: 0.9841 - val_loss: 1.0406 - val_acc: 0.9881\n",
      "Epoch 153/300\n",
      " - 2s - loss: 1.0297 - acc: 0.9683 - val_loss: 1.0248 - val_acc: 0.9907\n",
      "Epoch 154/300\n",
      " - 2s - loss: 1.0532 - acc: 0.9365 - val_loss: 1.0127 - val_acc: 0.9881\n",
      "Epoch 155/300\n",
      " - 2s - loss: 1.0699 - acc: 0.9048 - val_loss: 1.0008 - val_acc: 0.9854\n",
      "Epoch 156/300\n",
      " - 2s - loss: 1.0380 - acc: 0.9206 - val_loss: 0.9901 - val_acc: 0.9854\n",
      "Epoch 157/300\n",
      " - 2s - loss: 0.9909 - acc: 0.9524 - val_loss: 0.9815 - val_acc: 0.9881\n",
      "Epoch 158/300\n",
      " - 2s - loss: 1.0023 - acc: 0.9206 - val_loss: 0.9721 - val_acc: 0.9868\n",
      "Epoch 159/300\n",
      " - 2s - loss: 1.0584 - acc: 0.9365 - val_loss: 0.9593 - val_acc: 0.9828\n",
      "Epoch 160/300\n",
      " - 2s - loss: 1.0113 - acc: 0.9048 - val_loss: 0.9457 - val_acc: 0.9868\n",
      "Epoch 161/300\n",
      " - 2s - loss: 0.9375 - acc: 0.9524 - val_loss: 0.9352 - val_acc: 0.9868\n",
      "Epoch 162/300\n",
      " - 2s - loss: 0.9359 - acc: 0.9524 - val_loss: 0.9216 - val_acc: 0.9894\n",
      "Epoch 163/300\n",
      " - 2s - loss: 0.9640 - acc: 0.9365 - val_loss: 0.9109 - val_acc: 0.9894\n",
      "Epoch 164/300\n",
      " - 2s - loss: 0.9773 - acc: 0.9365 - val_loss: 0.9015 - val_acc: 0.9894\n",
      "Epoch 165/300\n",
      " - 2s - loss: 0.8962 - acc: 0.9365 - val_loss: 0.8926 - val_acc: 0.9894\n",
      "Epoch 166/300\n",
      " - 2s - loss: 0.9275 - acc: 0.9206 - val_loss: 0.8833 - val_acc: 0.9894\n",
      "Epoch 167/300\n",
      " - 2s - loss: 0.9533 - acc: 0.9048 - val_loss: 0.8738 - val_acc: 0.9894\n",
      "Epoch 168/300\n",
      " - 2s - loss: 0.9299 - acc: 0.9365 - val_loss: 0.8649 - val_acc: 0.9881\n",
      "Epoch 169/300\n",
      " - 2s - loss: 1.0244 - acc: 0.8730 - val_loss: 0.8524 - val_acc: 0.9894\n",
      "Epoch 170/300\n",
      " - 2s - loss: 0.8564 - acc: 0.9841 - val_loss: 0.8423 - val_acc: 0.9907\n",
      "Epoch 171/300\n",
      " - 2s - loss: 0.8628 - acc: 0.9206 - val_loss: 0.8316 - val_acc: 0.9947\n",
      "Epoch 172/300\n",
      " - 2s - loss: 0.8945 - acc: 0.9206 - val_loss: 0.8220 - val_acc: 0.9974\n",
      "Epoch 173/300\n",
      " - 2s - loss: 0.8422 - acc: 1.0000 - val_loss: 0.8143 - val_acc: 0.9947\n",
      "Epoch 174/300\n",
      " - 2s - loss: 0.9214 - acc: 0.9524 - val_loss: 0.8081 - val_acc: 0.9921\n",
      "Epoch 175/300\n",
      " - 2s - loss: 0.8797 - acc: 0.9841 - val_loss: 0.7995 - val_acc: 0.9934\n",
      "Epoch 176/300\n",
      " - 2s - loss: 0.7471 - acc: 0.9841 - val_loss: 0.7904 - val_acc: 0.9947\n",
      "Epoch 177/300\n",
      " - 2s - loss: 0.8440 - acc: 0.9524 - val_loss: 0.7822 - val_acc: 0.9974\n",
      "Epoch 178/300\n",
      " - 2s - loss: 0.8825 - acc: 0.9683 - val_loss: 0.7746 - val_acc: 0.9947\n",
      "Epoch 179/300\n",
      " - 2s - loss: 0.7684 - acc: 0.9683 - val_loss: 0.7673 - val_acc: 0.9934\n",
      "Epoch 180/300\n",
      " - 2s - loss: 0.8426 - acc: 0.9524 - val_loss: 0.7624 - val_acc: 0.9907\n",
      "Epoch 181/300\n",
      " - 2s - loss: 0.7604 - acc: 0.9683 - val_loss: 0.7532 - val_acc: 0.9947\n",
      "Epoch 182/300\n",
      " - 2s - loss: 0.8233 - acc: 0.9365 - val_loss: 0.7463 - val_acc: 0.9947\n",
      "Epoch 183/300\n",
      " - 2s - loss: 0.8046 - acc: 0.9206 - val_loss: 0.7393 - val_acc: 0.9974\n",
      "Epoch 184/300\n",
      " - 2s - loss: 0.7464 - acc: 0.9683 - val_loss: 0.7331 - val_acc: 0.9974\n",
      "Epoch 185/300\n",
      " - 2s - loss: 0.8083 - acc: 0.9683 - val_loss: 0.7261 - val_acc: 0.9974\n",
      "Epoch 186/300\n",
      " - 2s - loss: 0.7470 - acc: 0.9683 - val_loss: 0.7192 - val_acc: 0.9974\n",
      "Epoch 187/300\n",
      " - 2s - loss: 0.7129 - acc: 1.0000 - val_loss: 0.7118 - val_acc: 0.9974\n",
      "Epoch 188/300\n",
      " - 2s - loss: 0.7636 - acc: 0.9841 - val_loss: 0.7046 - val_acc: 0.9987\n",
      "Epoch 189/300\n",
      " - 2s - loss: 0.7272 - acc: 0.9683 - val_loss: 0.6986 - val_acc: 1.0000\n",
      "Epoch 190/300\n",
      " - 2s - loss: 0.7393 - acc: 0.9841 - val_loss: 0.6928 - val_acc: 1.0000\n",
      "Epoch 191/300\n",
      " - 2s - loss: 0.7358 - acc: 0.9683 - val_loss: 0.6873 - val_acc: 1.0000\n",
      "Epoch 192/300\n",
      " - 2s - loss: 0.7933 - acc: 0.9524 - val_loss: 0.6823 - val_acc: 0.9987\n",
      "Epoch 193/300\n",
      " - 2s - loss: 0.7243 - acc: 0.9524 - val_loss: 0.6760 - val_acc: 0.9974\n",
      "Epoch 194/300\n",
      " - 2s - loss: 0.6872 - acc: 1.0000 - val_loss: 0.6699 - val_acc: 0.9974\n",
      "Epoch 195/300\n",
      " - 2s - loss: 0.7150 - acc: 0.9683 - val_loss: 0.6634 - val_acc: 0.9974\n",
      "Epoch 196/300\n",
      " - 2s - loss: 0.6638 - acc: 1.0000 - val_loss: 0.6572 - val_acc: 0.9987\n",
      "Epoch 197/300\n",
      " - 2s - loss: 0.6310 - acc: 1.0000 - val_loss: 0.6521 - val_acc: 0.9987\n",
      "Epoch 198/300\n",
      " - 2s - loss: 0.6380 - acc: 0.9841 - val_loss: 0.6485 - val_acc: 0.9974\n",
      "Epoch 199/300\n",
      " - 2s - loss: 0.6646 - acc: 1.0000 - val_loss: 0.6431 - val_acc: 0.9974\n",
      "Epoch 200/300\n",
      " - 2s - loss: 0.7114 - acc: 0.9841 - val_loss: 0.6358 - val_acc: 0.9987\n",
      "Epoch 201/300\n",
      " - 2s - loss: 0.6035 - acc: 1.0000 - val_loss: 0.6296 - val_acc: 0.9987\n",
      "Epoch 202/300\n",
      " - 2s - loss: 0.6284 - acc: 0.9841 - val_loss: 0.6248 - val_acc: 0.9987\n",
      "Epoch 203/300\n",
      " - 2s - loss: 0.6298 - acc: 1.0000 - val_loss: 0.6206 - val_acc: 0.9987\n",
      "Epoch 204/300\n",
      " - 2s - loss: 0.6937 - acc: 1.0000 - val_loss: 0.6162 - val_acc: 0.9987\n",
      "Epoch 205/300\n",
      " - 2s - loss: 0.5964 - acc: 0.9841 - val_loss: 0.6109 - val_acc: 0.9987\n",
      "Epoch 206/300\n",
      " - 2s - loss: 0.6533 - acc: 1.0000 - val_loss: 0.6065 - val_acc: 1.0000\n",
      "Epoch 207/300\n",
      " - 2s - loss: 0.6406 - acc: 1.0000 - val_loss: 0.6027 - val_acc: 0.9987\n",
      "Epoch 208/300\n",
      " - 2s - loss: 0.6222 - acc: 1.0000 - val_loss: 0.5979 - val_acc: 1.0000\n",
      "Epoch 209/300\n",
      " - 2s - loss: 0.5858 - acc: 1.0000 - val_loss: 0.5939 - val_acc: 1.0000\n",
      "Epoch 210/300\n",
      " - 2s - loss: 0.5992 - acc: 0.9841 - val_loss: 0.5900 - val_acc: 1.0000\n",
      "Epoch 211/300\n",
      " - 2s - loss: 0.6640 - acc: 0.9206 - val_loss: 0.5850 - val_acc: 1.0000\n",
      "Epoch 212/300\n",
      " - 2s - loss: 0.5878 - acc: 1.0000 - val_loss: 0.5816 - val_acc: 1.0000\n",
      "Epoch 213/300\n",
      " - 2s - loss: 0.5813 - acc: 1.0000 - val_loss: 0.5786 - val_acc: 1.0000\n",
      "Epoch 214/300\n",
      " - 2s - loss: 0.6277 - acc: 0.9683 - val_loss: 0.5764 - val_acc: 0.9987\n",
      "Epoch 215/300\n",
      " - 2s - loss: 0.6850 - acc: 0.9683 - val_loss: 0.5746 - val_acc: 1.0000\n",
      "Epoch 216/300\n",
      " - 2s - loss: 0.5964 - acc: 1.0000 - val_loss: 0.5715 - val_acc: 1.0000\n",
      "Epoch 217/300\n",
      " - 2s - loss: 0.5983 - acc: 0.9841 - val_loss: 0.5675 - val_acc: 1.0000\n",
      "Epoch 218/300\n",
      " - 2s - loss: 0.6240 - acc: 0.9683 - val_loss: 0.5635 - val_acc: 1.0000\n",
      "Epoch 219/300\n",
      " - 2s - loss: 0.6041 - acc: 1.0000 - val_loss: 0.5599 - val_acc: 1.0000\n",
      "Epoch 220/300\n",
      " - 2s - loss: 0.5646 - acc: 1.0000 - val_loss: 0.5563 - val_acc: 1.0000\n",
      "Epoch 221/300\n",
      " - 2s - loss: 0.5608 - acc: 1.0000 - val_loss: 0.5527 - val_acc: 1.0000\n",
      "Epoch 222/300\n",
      " - 2s - loss: 0.5646 - acc: 1.0000 - val_loss: 0.5498 - val_acc: 1.0000\n",
      "Epoch 223/300\n",
      " - 2s - loss: 0.6014 - acc: 0.9683 - val_loss: 0.5484 - val_acc: 0.9987\n",
      "Epoch 224/300\n",
      " - 2s - loss: 0.5635 - acc: 0.9841 - val_loss: 0.5498 - val_acc: 0.9987\n",
      "Epoch 225/300\n",
      " - 2s - loss: 0.5469 - acc: 1.0000 - val_loss: 0.5490 - val_acc: 0.9987\n",
      "Epoch 226/300\n",
      " - 2s - loss: 0.5599 - acc: 0.9841 - val_loss: 0.5434 - val_acc: 1.0000\n",
      "Epoch 227/300\n",
      " - 2s - loss: 0.5582 - acc: 0.9841 - val_loss: 0.5406 - val_acc: 1.0000\n",
      "Epoch 228/300\n",
      " - 2s - loss: 0.5339 - acc: 1.0000 - val_loss: 0.5378 - val_acc: 1.0000\n",
      "Epoch 229/300\n",
      " - 2s - loss: 0.5432 - acc: 1.0000 - val_loss: 0.5350 - val_acc: 1.0000\n",
      "Epoch 230/300\n",
      " - 2s - loss: 0.5917 - acc: 1.0000 - val_loss: 0.5328 - val_acc: 1.0000\n",
      "Epoch 231/300\n",
      " - 2s - loss: 0.5593 - acc: 1.0000 - val_loss: 0.5292 - val_acc: 1.0000\n",
      "Epoch 232/300\n",
      " - 2s - loss: 0.5700 - acc: 1.0000 - val_loss: 0.5256 - val_acc: 1.0000\n",
      "Epoch 233/300\n",
      " - 2s - loss: 0.5470 - acc: 1.0000 - val_loss: 0.5229 - val_acc: 1.0000\n",
      "Epoch 234/300\n",
      " - 2s - loss: 0.5429 - acc: 1.0000 - val_loss: 0.5208 - val_acc: 1.0000\n",
      "Epoch 235/300\n",
      " - 2s - loss: 0.5624 - acc: 0.9841 - val_loss: 0.5187 - val_acc: 1.0000\n",
      "Epoch 236/300\n",
      " - 2s - loss: 0.5390 - acc: 1.0000 - val_loss: 0.5161 - val_acc: 1.0000\n",
      "Epoch 237/300\n",
      " - 2s - loss: 0.5406 - acc: 1.0000 - val_loss: 0.5144 - val_acc: 1.0000\n",
      "Epoch 238/300\n",
      " - 2s - loss: 0.5551 - acc: 0.9841 - val_loss: 0.5134 - val_acc: 1.0000\n",
      "Epoch 239/300\n",
      " - 2s - loss: 0.5318 - acc: 1.0000 - val_loss: 0.5121 - val_acc: 1.0000\n",
      "Epoch 240/300\n",
      " - 2s - loss: 0.5275 - acc: 1.0000 - val_loss: 0.5094 - val_acc: 1.0000\n",
      "Epoch 241/300\n",
      " - 2s - loss: 0.5213 - acc: 1.0000 - val_loss: 0.5071 - val_acc: 1.0000\n",
      "Epoch 242/300\n",
      " - 2s - loss: 0.5268 - acc: 0.9841 - val_loss: 0.5049 - val_acc: 1.0000\n",
      "Epoch 243/300\n",
      " - 2s - loss: 0.5141 - acc: 1.0000 - val_loss: 0.5030 - val_acc: 1.0000\n",
      "Epoch 244/300\n",
      " - 2s - loss: 0.5214 - acc: 1.0000 - val_loss: 0.5009 - val_acc: 1.0000\n",
      "Epoch 245/300\n",
      " - 2s - loss: 0.5451 - acc: 0.9841 - val_loss: 0.4982 - val_acc: 1.0000\n",
      "Epoch 246/300\n",
      " - 2s - loss: 0.4930 - acc: 1.0000 - val_loss: 0.4960 - val_acc: 1.0000\n",
      "Epoch 247/300\n",
      " - 2s - loss: 0.5150 - acc: 0.9841 - val_loss: 0.4944 - val_acc: 1.0000\n",
      "Epoch 248/300\n",
      " - 2s - loss: 0.5258 - acc: 1.0000 - val_loss: 0.4949 - val_acc: 1.0000\n",
      "Epoch 249/300\n",
      " - 2s - loss: 0.5403 - acc: 1.0000 - val_loss: 0.4950 - val_acc: 1.0000\n",
      "Epoch 250/300\n",
      " - 2s - loss: 0.5038 - acc: 1.0000 - val_loss: 0.4947 - val_acc: 1.0000\n",
      "Epoch 251/300\n",
      " - 2s - loss: 0.5270 - acc: 0.9683 - val_loss: 0.4916 - val_acc: 1.0000\n",
      "Epoch 252/300\n",
      " - 2s - loss: 0.5209 - acc: 1.0000 - val_loss: 0.4882 - val_acc: 1.0000\n",
      "Epoch 253/300\n",
      " - 2s - loss: 0.4889 - acc: 1.0000 - val_loss: 0.4860 - val_acc: 1.0000\n",
      "Epoch 254/300\n",
      " - 2s - loss: 0.5036 - acc: 1.0000 - val_loss: 0.4845 - val_acc: 1.0000\n",
      "Epoch 255/300\n",
      " - 2s - loss: 0.5195 - acc: 0.9841 - val_loss: 0.4849 - val_acc: 1.0000\n",
      "Epoch 256/300\n",
      " - 2s - loss: 0.5019 - acc: 1.0000 - val_loss: 0.4842 - val_acc: 1.0000\n",
      "Epoch 257/300\n",
      " - 2s - loss: 0.5091 - acc: 0.9841 - val_loss: 0.4836 - val_acc: 1.0000\n",
      "Epoch 258/300\n",
      " - 2s - loss: 0.4871 - acc: 1.0000 - val_loss: 0.4835 - val_acc: 1.0000\n",
      "Epoch 259/300\n",
      " - 2s - loss: 0.4939 - acc: 1.0000 - val_loss: 0.4817 - val_acc: 1.0000\n",
      "Epoch 260/300\n",
      " - 2s - loss: 0.4755 - acc: 1.0000 - val_loss: 0.4792 - val_acc: 1.0000\n",
      "Epoch 261/300\n",
      " - 2s - loss: 0.5035 - acc: 1.0000 - val_loss: 0.4769 - val_acc: 1.0000\n",
      "Epoch 262/300\n",
      " - 2s - loss: 0.5070 - acc: 1.0000 - val_loss: 0.4745 - val_acc: 1.0000\n",
      "Epoch 263/300\n",
      " - 2s - loss: 0.5146 - acc: 1.0000 - val_loss: 0.4736 - val_acc: 1.0000\n",
      "Epoch 264/300\n",
      " - 2s - loss: 0.4812 - acc: 1.0000 - val_loss: 0.4727 - val_acc: 1.0000\n",
      "Epoch 265/300\n",
      " - 2s - loss: 0.4699 - acc: 1.0000 - val_loss: 0.4714 - val_acc: 1.0000\n",
      "Epoch 266/300\n",
      " - 2s - loss: 0.4864 - acc: 0.9841 - val_loss: 0.4694 - val_acc: 1.0000\n",
      "Epoch 267/300\n",
      " - 2s - loss: 0.4607 - acc: 1.0000 - val_loss: 0.4684 - val_acc: 1.0000\n",
      "Epoch 268/300\n",
      " - 2s - loss: 0.4623 - acc: 1.0000 - val_loss: 0.4678 - val_acc: 1.0000\n",
      "Epoch 269/300\n",
      " - 2s - loss: 0.5048 - acc: 0.9841 - val_loss: 0.4667 - val_acc: 1.0000\n",
      "Epoch 270/300\n",
      " - 2s - loss: 0.4769 - acc: 0.9841 - val_loss: 0.4662 - val_acc: 1.0000\n",
      "Epoch 271/300\n",
      " - 2s - loss: 0.4698 - acc: 1.0000 - val_loss: 0.4661 - val_acc: 1.0000\n",
      "Epoch 272/300\n",
      " - 2s - loss: 0.4746 - acc: 1.0000 - val_loss: 0.4653 - val_acc: 1.0000\n",
      "Epoch 273/300\n",
      " - 2s - loss: 0.4784 - acc: 1.0000 - val_loss: 0.4636 - val_acc: 1.0000\n",
      "Epoch 274/300\n",
      " - 2s - loss: 0.4708 - acc: 1.0000 - val_loss: 0.4618 - val_acc: 1.0000\n",
      "Epoch 275/300\n",
      " - 2s - loss: 0.4976 - acc: 1.0000 - val_loss: 0.4608 - val_acc: 1.0000\n",
      "Epoch 276/300\n",
      " - 2s - loss: 0.4619 - acc: 1.0000 - val_loss: 0.4611 - val_acc: 1.0000\n",
      "Epoch 277/300\n",
      " - 2s - loss: 0.4893 - acc: 1.0000 - val_loss: 0.4607 - val_acc: 1.0000\n",
      "Epoch 278/300\n",
      " - 2s - loss: 0.4500 - acc: 1.0000 - val_loss: 0.4596 - val_acc: 1.0000\n",
      "Epoch 279/300\n",
      " - 2s - loss: 0.4589 - acc: 1.0000 - val_loss: 0.4582 - val_acc: 1.0000\n",
      "Epoch 280/300\n",
      " - 2s - loss: 0.4591 - acc: 1.0000 - val_loss: 0.4572 - val_acc: 1.0000\n",
      "Epoch 281/300\n",
      " - 2s - loss: 0.4830 - acc: 1.0000 - val_loss: 0.4562 - val_acc: 1.0000\n",
      "Epoch 282/300\n",
      " - 2s - loss: 0.4694 - acc: 1.0000 - val_loss: 0.4547 - val_acc: 1.0000\n",
      "Epoch 283/300\n",
      " - 2s - loss: 0.4605 - acc: 1.0000 - val_loss: 0.4544 - val_acc: 1.0000\n",
      "Epoch 284/300\n",
      " - 2s - loss: 0.4616 - acc: 0.9841 - val_loss: 0.4539 - val_acc: 1.0000\n",
      "Epoch 285/300\n",
      " - 2s - loss: 0.4776 - acc: 1.0000 - val_loss: 0.4528 - val_acc: 1.0000\n",
      "Epoch 286/300\n",
      " - 2s - loss: 0.4427 - acc: 1.0000 - val_loss: 0.4520 - val_acc: 1.0000\n",
      "Epoch 287/300\n",
      " - 2s - loss: 0.4611 - acc: 1.0000 - val_loss: 0.4508 - val_acc: 1.0000\n",
      "Epoch 288/300\n",
      " - 2s - loss: 0.4708 - acc: 1.0000 - val_loss: 0.4501 - val_acc: 1.0000\n",
      "Epoch 289/300\n",
      " - 2s - loss: 0.4780 - acc: 1.0000 - val_loss: 0.4502 - val_acc: 1.0000\n",
      "Epoch 290/300\n",
      " - 2s - loss: 0.4654 - acc: 1.0000 - val_loss: 0.4496 - val_acc: 1.0000\n",
      "Epoch 291/300\n",
      " - 2s - loss: 0.4763 - acc: 0.9841 - val_loss: 0.4491 - val_acc: 1.0000\n",
      "Epoch 292/300\n",
      " - 2s - loss: 0.4389 - acc: 1.0000 - val_loss: 0.4479 - val_acc: 1.0000\n",
      "Epoch 293/300\n",
      " - 2s - loss: 0.4557 - acc: 1.0000 - val_loss: 0.4465 - val_acc: 1.0000\n",
      "Epoch 294/300\n",
      " - 2s - loss: 0.4637 - acc: 1.0000 - val_loss: 0.4455 - val_acc: 1.0000\n",
      "Epoch 295/300\n",
      " - 2s - loss: 0.4491 - acc: 1.0000 - val_loss: 0.4445 - val_acc: 1.0000\n",
      "Epoch 296/300\n",
      " - 2s - loss: 0.4504 - acc: 1.0000 - val_loss: 0.4435 - val_acc: 1.0000\n",
      "Epoch 297/300\n",
      " - 2s - loss: 0.4449 - acc: 1.0000 - val_loss: 0.4427 - val_acc: 1.0000\n",
      "Epoch 298/300\n",
      " - 2s - loss: 0.4557 - acc: 0.9841 - val_loss: 0.4418 - val_acc: 1.0000\n",
      "Epoch 299/300\n",
      " - 2s - loss: 0.4464 - acc: 1.0000 - val_loss: 0.4416 - val_acc: 1.0000\n",
      "Epoch 300/300\n",
      " - 2s - loss: 0.4414 - acc: 1.0000 - val_loss: 0.4411 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_convnet Test Accuracy: 0.372\n",
      "my_model_neu Test f-measure: 0.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "convnet_input_tensor = Input(shape=(maxlen,) , name='convnet_words')\n",
    "\n",
    "conv_1d_s3_model = Sequential()\n",
    "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s3_model.add(layers.Dropout(0.3))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "conv_1d_s3_model.add(layers.MaxPooling1D(2))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s3_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s3_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s3_model.layers[0].trainable = False\n",
    "conv_output_tensor_0 = conv_1d_s3_model(convnet_input_tensor)\n",
    "\n",
    "\n",
    "conv_1d_s1_model = Sequential()\n",
    "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s1_model.add(layers.Dropout(0.3))\n",
    "conv_1d_s1_model.add(layers.SeparableConv1D(64, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s1_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s1_model.layers[0].trainable = False\n",
    "conv_output_tensor_1 = conv_1d_s1_model(convnet_input_tensor)\n",
    "\n",
    "conv_1d_complex_model = Sequential()\n",
    "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_complex_model.add(layers.Dropout(0.3))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(64, 2, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_complex_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_complex_model.layers[0].trainable = False\n",
    "conv_output_tensor_2 = conv_1d_complex_model(convnet_input_tensor)\n",
    "\n",
    "# x = layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen)(word_input_tensor)\n",
    "# x = layers.Conv1D(128, 5, activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# y = layers.Conv1D(128, 10, activation='relu', padding='same')(x)\n",
    "# added = layers.add([y, x])\n",
    "# added = layers.GlobalMaxPooling1D()(added)\n",
    "\n",
    "concatenated = layers.concatenate([conv_output_tensor_0,\n",
    "                                   conv_output_tensor_1,\n",
    "                                   conv_output_tensor_2,\n",
    "#                                    ,added\n",
    "                                  ], axis=-1)\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "# concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "# concatenated = layers.Dropout(0.3)(concatenated)\n",
    "answer = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model(convnet_input_tensor, answer)\n",
    "model.summary()\n",
    "\n",
    "# model.layers[0].set_weights([w2d.word_embedding])\n",
    "# model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=300,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_convnet,\n",
    "                    verbose= 2\n",
    "                   )\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "l_model = load_model('my_model_convnet.h5')\n",
    "yhat = l_model.predict(test_data)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_convnet Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_convnet Test Accuracy: 0.434\n"
     ]
    }
   ],
   "source": [
    "l_model = load_model('my_model_convnet.h5')\n",
    "yhat = l_model.predict(test_data)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_convnet Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(model_names_list):\n",
    "    all_models = list()\n",
    "    for model_name in model_names_list:\n",
    "        # define filename for this ensemble\n",
    "#         filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "        filename = model_name + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "\n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer.name = 'ensemble_' + str(i + 1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "#     print(ensemble_visible)\n",
    "#     ensemble_visible = [[ngram_input_tensor, word_input_tensor], convnet_input_tensor]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "#     ensemble_outputs = [concatenated, answer]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = layers.Dense(128, activation='relu')(merge)\n",
    "    hidden = layers.Dropout(0.3)(hidden)\n",
    "    output = layers.Dense(len(set(train_labels)), activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    # plot graph of ensemble\n",
    "#     plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=3e-4), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy, valX, valy):\n",
    "    # prepare input data\n",
    "#     X = [inputX for _ in range(len(model.input))]\n",
    "    # encode output data\n",
    "#     inputy_enc = to_categorical(inputy)\n",
    "    # fit model\n",
    "    model.fit(inputX, inputy, validation_data=(valX, valy), batch_size=class_size,\n",
    "              callbacks=callbacks_list_stacked, epochs=300, verbose=1)\n",
    "    \n",
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "#     X = [inputX for _ in range(len(model.input))]\n",
    "    # make prediction\n",
    "    return model.predict(inputX, verbose=0)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded my_model_neu_ngrams.h5\n",
      ">loaded my_model_neu_words.h5\n",
      "Loaded 2 models\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/300\n",
      "63/63 [==============================] - 3s 41ms/step - loss: 3.9079 - acc: 0.1429 - val_loss: 3.8629 - val_acc: 0.3333\n",
      "Epoch 2/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.8379 - acc: 0.2857 - val_loss: 3.8268 - val_acc: 0.3333\n",
      "Epoch 3/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.8053 - acc: 0.3175 - val_loss: 3.7904 - val_acc: 0.3333\n",
      "Epoch 4/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.8103 - acc: 0.3016 - val_loss: 3.7539 - val_acc: 0.4444\n",
      "Epoch 5/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.7466 - acc: 0.4286 - val_loss: 3.7172 - val_acc: 0.4444\n",
      "Epoch 6/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.7234 - acc: 0.4127 - val_loss: 3.6803 - val_acc: 0.5556\n",
      "Epoch 7/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.7067 - acc: 0.5238 - val_loss: 3.6451 - val_acc: 0.8889\n",
      "Epoch 8/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.6851 - acc: 0.6032 - val_loss: 3.6103 - val_acc: 0.8889\n",
      "Epoch 9/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.6432 - acc: 0.6825 - val_loss: 3.5755 - val_acc: 0.8889\n",
      "Epoch 10/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.5963 - acc: 0.7778 - val_loss: 3.5402 - val_acc: 0.8889\n",
      "Epoch 11/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.5514 - acc: 0.8413 - val_loss: 3.5043 - val_acc: 1.0000\n",
      "Epoch 12/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.5543 - acc: 0.8413 - val_loss: 3.4686 - val_acc: 1.0000\n",
      "Epoch 13/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.5240 - acc: 0.8730 - val_loss: 3.4323 - val_acc: 1.0000\n",
      "Epoch 14/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.4832 - acc: 0.9206 - val_loss: 3.3953 - val_acc: 1.0000\n",
      "Epoch 15/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.4640 - acc: 0.9206 - val_loss: 3.3582 - val_acc: 1.0000\n",
      "Epoch 16/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.4159 - acc: 0.9524 - val_loss: 3.3208 - val_acc: 1.0000\n",
      "Epoch 17/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.3819 - acc: 0.9683 - val_loss: 3.2833 - val_acc: 1.0000\n",
      "Epoch 18/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.3426 - acc: 0.9524 - val_loss: 3.2458 - val_acc: 1.0000\n",
      "Epoch 19/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.3110 - acc: 0.9841 - val_loss: 3.2086 - val_acc: 1.0000\n",
      "Epoch 20/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.2599 - acc: 1.0000 - val_loss: 3.1700 - val_acc: 1.0000\n",
      "Epoch 21/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.2185 - acc: 1.0000 - val_loss: 3.1308 - val_acc: 1.0000\n",
      "Epoch 22/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.1960 - acc: 1.0000 - val_loss: 3.0916 - val_acc: 1.0000\n",
      "Epoch 23/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.1943 - acc: 0.9841 - val_loss: 3.0529 - val_acc: 1.0000\n",
      "Epoch 24/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.1473 - acc: 1.0000 - val_loss: 3.0147 - val_acc: 1.0000\n",
      "Epoch 25/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.0750 - acc: 1.0000 - val_loss: 2.9753 - val_acc: 1.0000\n",
      "Epoch 26/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.1069 - acc: 1.0000 - val_loss: 2.9364 - val_acc: 1.0000\n",
      "Epoch 27/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.0101 - acc: 0.9841 - val_loss: 2.8976 - val_acc: 1.0000\n",
      "Epoch 28/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.9976 - acc: 1.0000 - val_loss: 2.8590 - val_acc: 1.0000\n",
      "Epoch 29/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.9608 - acc: 1.0000 - val_loss: 2.8201 - val_acc: 1.0000\n",
      "Epoch 30/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.9142 - acc: 1.0000 - val_loss: 2.7811 - val_acc: 1.0000\n",
      "Epoch 31/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.9468 - acc: 1.0000 - val_loss: 2.7430 - val_acc: 1.0000\n",
      "Epoch 32/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.8606 - acc: 1.0000 - val_loss: 2.7056 - val_acc: 1.0000\n",
      "Epoch 33/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.7703 - acc: 1.0000 - val_loss: 2.6682 - val_acc: 1.0000\n",
      "Epoch 34/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.7842 - acc: 1.0000 - val_loss: 2.6309 - val_acc: 1.0000\n",
      "Epoch 35/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.7269 - acc: 1.0000 - val_loss: 2.5941 - val_acc: 1.0000\n",
      "Epoch 36/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.6940 - acc: 1.0000 - val_loss: 2.5583 - val_acc: 1.0000\n",
      "Epoch 37/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.6677 - acc: 1.0000 - val_loss: 2.5232 - val_acc: 1.0000\n",
      "Epoch 38/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.5977 - acc: 0.9841 - val_loss: 2.4889 - val_acc: 1.0000\n",
      "Epoch 39/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.6452 - acc: 1.0000 - val_loss: 2.4557 - val_acc: 1.0000\n",
      "Epoch 40/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.5706 - acc: 1.0000 - val_loss: 2.4228 - val_acc: 1.0000\n",
      "Epoch 41/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.5259 - acc: 1.0000 - val_loss: 2.3906 - val_acc: 1.0000\n",
      "Epoch 42/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.5384 - acc: 0.9841 - val_loss: 2.3599 - val_acc: 1.0000\n",
      "Epoch 43/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.4973 - acc: 0.9841 - val_loss: 2.3306 - val_acc: 1.0000\n",
      "Epoch 44/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.4476 - acc: 1.0000 - val_loss: 2.3018 - val_acc: 1.0000\n",
      "Epoch 45/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.4600 - acc: 1.0000 - val_loss: 2.2739 - val_acc: 1.0000\n",
      "Epoch 46/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.3822 - acc: 1.0000 - val_loss: 2.2470 - val_acc: 1.0000\n",
      "Epoch 47/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.3739 - acc: 1.0000 - val_loss: 2.2214 - val_acc: 1.0000\n",
      "Epoch 48/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.3558 - acc: 1.0000 - val_loss: 2.1966 - val_acc: 1.0000\n",
      "Epoch 49/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.2835 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 1.0000\n",
      "Epoch 50/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.2936 - acc: 1.0000 - val_loss: 2.1500 - val_acc: 1.0000\n",
      "Epoch 51/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.2827 - acc: 1.0000 - val_loss: 2.1284 - val_acc: 1.0000\n",
      "Epoch 52/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.2337 - acc: 1.0000 - val_loss: 2.1079 - val_acc: 1.0000\n",
      "Epoch 53/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.2847 - acc: 1.0000 - val_loss: 2.0885 - val_acc: 1.0000\n",
      "Epoch 54/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.2131 - acc: 1.0000 - val_loss: 2.0700 - val_acc: 1.0000\n",
      "Epoch 55/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1784 - acc: 1.0000 - val_loss: 2.0522 - val_acc: 1.0000\n",
      "Epoch 56/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1504 - acc: 1.0000 - val_loss: 2.0355 - val_acc: 1.0000\n",
      "Epoch 57/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1629 - acc: 1.0000 - val_loss: 2.0194 - val_acc: 1.0000\n",
      "Epoch 58/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1129 - acc: 1.0000 - val_loss: 2.0043 - val_acc: 1.0000\n",
      "Epoch 59/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1814 - acc: 1.0000 - val_loss: 1.9902 - val_acc: 1.0000\n",
      "Epoch 60/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1340 - acc: 1.0000 - val_loss: 1.9768 - val_acc: 1.0000\n",
      "Epoch 61/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0672 - acc: 1.0000 - val_loss: 1.9645 - val_acc: 1.0000\n",
      "Epoch 62/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1050 - acc: 1.0000 - val_loss: 1.9526 - val_acc: 1.0000\n",
      "Epoch 63/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0811 - acc: 1.0000 - val_loss: 1.9411 - val_acc: 1.0000\n",
      "Epoch 64/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0561 - acc: 1.0000 - val_loss: 1.9304 - val_acc: 1.0000\n",
      "Epoch 65/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0767 - acc: 1.0000 - val_loss: 1.9203 - val_acc: 1.0000\n",
      "Epoch 66/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0704 - acc: 0.9841 - val_loss: 1.9108 - val_acc: 1.0000\n",
      "Epoch 67/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0335 - acc: 1.0000 - val_loss: 1.9017 - val_acc: 1.0000\n",
      "Epoch 68/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0658 - acc: 0.9841 - val_loss: 1.8931 - val_acc: 1.0000\n",
      "Epoch 69/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0332 - acc: 0.9841 - val_loss: 1.8849 - val_acc: 1.0000\n",
      "Epoch 70/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0120 - acc: 0.9841 - val_loss: 1.8775 - val_acc: 1.0000\n",
      "Epoch 71/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9616 - acc: 1.0000 - val_loss: 1.8705 - val_acc: 1.0000\n",
      "Epoch 72/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9745 - acc: 1.0000 - val_loss: 1.8637 - val_acc: 1.0000\n",
      "Epoch 73/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9367 - acc: 1.0000 - val_loss: 1.8573 - val_acc: 1.0000\n",
      "Epoch 74/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9939 - acc: 1.0000 - val_loss: 1.8512 - val_acc: 1.0000\n",
      "Epoch 75/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9349 - acc: 1.0000 - val_loss: 1.8452 - val_acc: 1.0000\n",
      "Epoch 76/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9978 - acc: 0.9841 - val_loss: 1.8397 - val_acc: 1.0000\n",
      "Epoch 77/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9700 - acc: 1.0000 - val_loss: 1.8343 - val_acc: 1.0000\n",
      "Epoch 78/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9068 - acc: 1.0000 - val_loss: 1.8293 - val_acc: 1.0000\n",
      "Epoch 79/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9176 - acc: 1.0000 - val_loss: 1.8248 - val_acc: 1.0000\n",
      "Epoch 80/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9316 - acc: 1.0000 - val_loss: 1.8204 - val_acc: 1.0000\n",
      "Epoch 81/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9404 - acc: 1.0000 - val_loss: 1.8161 - val_acc: 1.0000\n",
      "Epoch 82/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9268 - acc: 1.0000 - val_loss: 1.8120 - val_acc: 1.0000\n",
      "Epoch 83/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9180 - acc: 1.0000 - val_loss: 1.8082 - val_acc: 1.0000\n",
      "Epoch 84/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8742 - acc: 1.0000 - val_loss: 1.8045 - val_acc: 1.0000\n",
      "Epoch 85/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9180 - acc: 0.9841 - val_loss: 1.8011 - val_acc: 1.0000\n",
      "Epoch 86/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9039 - acc: 0.9841 - val_loss: 1.7979 - val_acc: 1.0000\n",
      "Epoch 87/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8993 - acc: 1.0000 - val_loss: 1.7947 - val_acc: 1.0000\n",
      "Epoch 88/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8621 - acc: 1.0000 - val_loss: 1.7918 - val_acc: 1.0000\n",
      "Epoch 89/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8608 - acc: 1.0000 - val_loss: 1.7890 - val_acc: 1.0000\n",
      "Epoch 90/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8811 - acc: 0.9841 - val_loss: 1.7866 - val_acc: 1.0000\n",
      "Epoch 91/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8795 - acc: 1.0000 - val_loss: 1.7841 - val_acc: 1.0000\n",
      "Epoch 92/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8710 - acc: 1.0000 - val_loss: 1.7817 - val_acc: 1.0000\n",
      "Epoch 93/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8711 - acc: 1.0000 - val_loss: 1.7794 - val_acc: 1.0000\n",
      "Epoch 94/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8630 - acc: 1.0000 - val_loss: 1.7771 - val_acc: 1.0000\n",
      "Epoch 95/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8583 - acc: 1.0000 - val_loss: 1.7749 - val_acc: 1.0000\n",
      "Epoch 96/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8466 - acc: 1.0000 - val_loss: 1.7729 - val_acc: 1.0000\n",
      "Epoch 97/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8820 - acc: 0.9841 - val_loss: 1.7708 - val_acc: 1.0000\n",
      "Epoch 98/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8687 - acc: 1.0000 - val_loss: 1.7689 - val_acc: 1.0000\n",
      "Epoch 99/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8408 - acc: 1.0000 - val_loss: 1.7671 - val_acc: 1.0000\n",
      "Epoch 100/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8353 - acc: 1.0000 - val_loss: 1.7654 - val_acc: 1.0000\n",
      "Epoch 101/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8589 - acc: 1.0000 - val_loss: 1.7637 - val_acc: 1.0000\n",
      "Epoch 102/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8396 - acc: 1.0000 - val_loss: 1.7621 - val_acc: 1.0000\n",
      "Epoch 103/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8398 - acc: 1.0000 - val_loss: 1.7605 - val_acc: 1.0000\n",
      "Epoch 104/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8271 - acc: 1.0000 - val_loss: 1.7591 - val_acc: 1.0000\n",
      "Epoch 105/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8364 - acc: 1.0000 - val_loss: 1.7577 - val_acc: 1.0000\n",
      "Epoch 106/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8139 - acc: 1.0000 - val_loss: 1.7564 - val_acc: 1.0000\n",
      "Epoch 107/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8398 - acc: 1.0000 - val_loss: 1.7551 - val_acc: 1.0000\n",
      "Epoch 108/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8126 - acc: 1.0000 - val_loss: 1.7539 - val_acc: 1.0000\n",
      "Epoch 109/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8369 - acc: 1.0000 - val_loss: 1.7527 - val_acc: 1.0000\n",
      "Epoch 110/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8424 - acc: 0.9841 - val_loss: 1.7516 - val_acc: 1.0000\n",
      "Epoch 111/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8014 - acc: 1.0000 - val_loss: 1.7505 - val_acc: 1.0000\n",
      "Epoch 112/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7951 - acc: 1.0000 - val_loss: 1.7495 - val_acc: 1.0000\n",
      "Epoch 113/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8189 - acc: 1.0000 - val_loss: 1.7486 - val_acc: 1.0000\n",
      "Epoch 114/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8039 - acc: 1.0000 - val_loss: 1.7476 - val_acc: 1.0000\n",
      "Epoch 115/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8504 - acc: 1.0000 - val_loss: 1.7467 - val_acc: 1.0000\n",
      "Epoch 116/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7988 - acc: 1.0000 - val_loss: 1.7459 - val_acc: 1.0000\n",
      "Epoch 117/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7960 - acc: 1.0000 - val_loss: 1.7451 - val_acc: 1.0000\n",
      "Epoch 118/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8140 - acc: 1.0000 - val_loss: 1.7443 - val_acc: 1.0000\n",
      "Epoch 119/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8011 - acc: 1.0000 - val_loss: 1.7435 - val_acc: 1.0000\n",
      "Epoch 120/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8146 - acc: 0.9841 - val_loss: 1.7428 - val_acc: 1.0000\n",
      "Epoch 121/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7852 - acc: 1.0000 - val_loss: 1.7421 - val_acc: 1.0000\n",
      "Epoch 122/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8257 - acc: 0.9841 - val_loss: 1.7414 - val_acc: 1.0000\n",
      "Epoch 123/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8020 - acc: 1.0000 - val_loss: 1.7407 - val_acc: 1.0000\n",
      "Epoch 124/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8276 - acc: 1.0000 - val_loss: 1.7400 - val_acc: 1.0000\n",
      "Epoch 125/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7785 - acc: 1.0000 - val_loss: 1.7394 - val_acc: 1.0000\n",
      "Epoch 126/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7745 - acc: 1.0000 - val_loss: 1.7388 - val_acc: 1.0000\n",
      "Epoch 127/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7615 - acc: 1.0000 - val_loss: 1.7382 - val_acc: 1.0000\n",
      "Epoch 128/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8024 - acc: 0.9841 - val_loss: 1.7377 - val_acc: 1.0000\n",
      "Epoch 129/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8280 - acc: 0.9841 - val_loss: 1.7372 - val_acc: 1.0000\n",
      "Epoch 130/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7790 - acc: 1.0000 - val_loss: 1.7367 - val_acc: 1.0000\n",
      "Epoch 131/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8034 - acc: 0.9841 - val_loss: 1.7362 - val_acc: 1.0000\n",
      "Epoch 132/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7862 - acc: 1.0000 - val_loss: 1.7357 - val_acc: 1.0000\n",
      "Epoch 133/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7830 - acc: 1.0000 - val_loss: 1.7353 - val_acc: 1.0000\n",
      "Epoch 134/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7739 - acc: 1.0000 - val_loss: 1.7348 - val_acc: 1.0000\n",
      "Epoch 135/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7628 - acc: 1.0000 - val_loss: 1.7344 - val_acc: 1.0000\n",
      "Epoch 136/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7997 - acc: 1.0000 - val_loss: 1.7339 - val_acc: 1.0000\n",
      "Epoch 137/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7896 - acc: 1.0000 - val_loss: 1.7335 - val_acc: 1.0000\n",
      "Epoch 138/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8001 - acc: 1.0000 - val_loss: 1.7330 - val_acc: 1.0000\n",
      "Epoch 139/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7749 - acc: 1.0000 - val_loss: 1.7326 - val_acc: 1.0000\n",
      "Epoch 140/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7740 - acc: 1.0000 - val_loss: 1.7322 - val_acc: 1.0000\n",
      "Epoch 141/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7690 - acc: 1.0000 - val_loss: 1.7318 - val_acc: 1.0000\n",
      "Epoch 142/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7638 - acc: 1.0000 - val_loss: 1.7315 - val_acc: 1.0000\n",
      "Epoch 143/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7779 - acc: 1.0000 - val_loss: 1.7312 - val_acc: 1.0000\n",
      "Epoch 144/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7454 - acc: 1.0000 - val_loss: 1.7308 - val_acc: 1.0000\n",
      "Epoch 145/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7680 - acc: 1.0000 - val_loss: 1.7305 - val_acc: 1.0000\n",
      "Epoch 146/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8011 - acc: 0.9841 - val_loss: 1.7302 - val_acc: 1.0000\n",
      "Epoch 147/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7709 - acc: 1.0000 - val_loss: 1.7299 - val_acc: 1.0000\n",
      "Epoch 148/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7577 - acc: 1.0000 - val_loss: 1.7296 - val_acc: 1.0000\n",
      "Epoch 149/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7761 - acc: 1.0000 - val_loss: 1.7293 - val_acc: 1.0000\n",
      "Epoch 150/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7844 - acc: 0.9841 - val_loss: 1.7290 - val_acc: 1.0000\n",
      "Epoch 151/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7734 - acc: 1.0000 - val_loss: 1.7287 - val_acc: 1.0000\n",
      "Epoch 152/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7637 - acc: 1.0000 - val_loss: 1.7284 - val_acc: 1.0000\n",
      "Epoch 153/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7757 - acc: 1.0000 - val_loss: 1.7282 - val_acc: 1.0000\n",
      "Epoch 154/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7517 - acc: 1.0000 - val_loss: 1.7279 - val_acc: 1.0000\n",
      "Epoch 155/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7783 - acc: 1.0000 - val_loss: 1.7277 - val_acc: 1.0000\n",
      "Epoch 156/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7901 - acc: 0.9841 - val_loss: 1.7274 - val_acc: 1.0000\n",
      "Epoch 157/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7516 - acc: 1.0000 - val_loss: 1.7272 - val_acc: 1.0000\n",
      "Epoch 158/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7651 - acc: 1.0000 - val_loss: 1.7269 - val_acc: 1.0000\n",
      "Epoch 159/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7571 - acc: 1.0000 - val_loss: 1.7267 - val_acc: 1.0000\n",
      "Epoch 160/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7733 - acc: 1.0000 - val_loss: 1.7265 - val_acc: 1.0000\n",
      "Epoch 161/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7504 - acc: 1.0000 - val_loss: 1.7263 - val_acc: 1.0000\n",
      "Epoch 162/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7452 - acc: 1.0000 - val_loss: 1.7261 - val_acc: 1.0000\n",
      "Epoch 163/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7722 - acc: 1.0000 - val_loss: 1.7259 - val_acc: 1.0000\n",
      "Epoch 164/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7449 - acc: 1.0000 - val_loss: 1.7257 - val_acc: 1.0000\n",
      "Epoch 165/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7558 - acc: 1.0000 - val_loss: 1.7255 - val_acc: 1.0000\n",
      "Epoch 166/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7521 - acc: 1.0000 - val_loss: 1.7253 - val_acc: 1.0000\n",
      "Epoch 167/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7697 - acc: 1.0000 - val_loss: 1.7251 - val_acc: 1.0000\n",
      "Epoch 168/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7716 - acc: 1.0000 - val_loss: 1.7249 - val_acc: 1.0000\n",
      "Epoch 169/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7539 - acc: 1.0000 - val_loss: 1.7248 - val_acc: 1.0000\n",
      "Epoch 170/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7629 - acc: 1.0000 - val_loss: 1.7246 - val_acc: 1.0000\n",
      "Epoch 171/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7611 - acc: 1.0000 - val_loss: 1.7244 - val_acc: 1.0000\n",
      "Epoch 172/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7515 - acc: 1.0000 - val_loss: 1.7242 - val_acc: 1.0000\n",
      "Epoch 173/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7638 - acc: 1.0000 - val_loss: 1.7240 - val_acc: 1.0000\n",
      "Epoch 174/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7712 - acc: 1.0000 - val_loss: 1.7238 - val_acc: 1.0000\n",
      "Epoch 175/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7490 - acc: 1.0000 - val_loss: 1.7237 - val_acc: 1.0000\n",
      "Epoch 176/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7980 - acc: 0.9841 - val_loss: 1.7235 - val_acc: 1.0000\n",
      "Epoch 177/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7649 - acc: 1.0000 - val_loss: 1.7234 - val_acc: 1.0000\n",
      "Epoch 178/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7610 - acc: 1.0000 - val_loss: 1.7232 - val_acc: 1.0000\n",
      "Epoch 179/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7454 - acc: 1.0000 - val_loss: 1.7231 - val_acc: 1.0000\n",
      "Epoch 180/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7332 - acc: 1.0000 - val_loss: 1.7230 - val_acc: 1.0000\n",
      "Epoch 181/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7646 - acc: 1.0000 - val_loss: 1.7229 - val_acc: 1.0000\n",
      "Epoch 182/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7497 - acc: 1.0000 - val_loss: 1.7227 - val_acc: 1.0000\n",
      "Epoch 183/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7389 - acc: 1.0000 - val_loss: 1.7226 - val_acc: 1.0000\n",
      "Epoch 184/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7801 - acc: 0.9841 - val_loss: 1.7225 - val_acc: 1.0000\n",
      "Epoch 185/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7412 - acc: 1.0000 - val_loss: 1.7224 - val_acc: 1.0000\n",
      "Epoch 186/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7491 - acc: 1.0000 - val_loss: 1.7223 - val_acc: 1.0000\n",
      "Epoch 187/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7543 - acc: 1.0000 - val_loss: 1.7221 - val_acc: 1.0000\n",
      "Epoch 188/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7630 - acc: 1.0000 - val_loss: 1.7220 - val_acc: 1.0000\n",
      "Epoch 189/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7683 - acc: 0.9841 - val_loss: 1.7219 - val_acc: 1.0000\n",
      "Epoch 190/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7902 - acc: 0.9841 - val_loss: 1.7218 - val_acc: 1.0000\n",
      "Epoch 191/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7432 - acc: 1.0000 - val_loss: 1.7217 - val_acc: 1.0000\n",
      "Epoch 192/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7504 - acc: 1.0000 - val_loss: 1.7216 - val_acc: 1.0000\n",
      "Epoch 193/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7456 - acc: 1.0000 - val_loss: 1.7215 - val_acc: 1.0000\n",
      "Epoch 194/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7495 - acc: 1.0000 - val_loss: 1.7214 - val_acc: 1.0000\n",
      "Epoch 195/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7470 - acc: 1.0000 - val_loss: 1.7213 - val_acc: 1.0000\n",
      "Epoch 196/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7573 - acc: 0.9841 - val_loss: 1.7212 - val_acc: 1.0000\n",
      "Epoch 197/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7434 - acc: 1.0000 - val_loss: 1.7211 - val_acc: 1.0000\n",
      "Epoch 198/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7401 - acc: 1.0000 - val_loss: 1.7211 - val_acc: 1.0000\n",
      "Epoch 199/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7490 - acc: 1.0000 - val_loss: 1.7210 - val_acc: 1.0000\n",
      "Epoch 200/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7611 - acc: 1.0000 - val_loss: 1.7209 - val_acc: 1.0000\n",
      "Epoch 201/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7431 - acc: 1.0000 - val_loss: 1.7208 - val_acc: 1.0000\n",
      "Epoch 202/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7457 - acc: 1.0000 - val_loss: 1.7207 - val_acc: 1.0000\n",
      "Epoch 203/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7378 - acc: 1.0000 - val_loss: 1.7206 - val_acc: 1.0000\n",
      "Epoch 204/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7438 - acc: 1.0000 - val_loss: 1.7205 - val_acc: 1.0000\n",
      "Epoch 205/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7513 - acc: 1.0000 - val_loss: 1.7204 - val_acc: 1.0000\n",
      "Epoch 206/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7473 - acc: 1.0000 - val_loss: 1.7203 - val_acc: 1.0000\n",
      "Epoch 207/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7339 - acc: 1.0000 - val_loss: 1.7203 - val_acc: 1.0000\n",
      "Epoch 208/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7466 - acc: 1.0000 - val_loss: 1.7202 - val_acc: 1.0000\n",
      "Epoch 209/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7659 - acc: 0.9841 - val_loss: 1.7201 - val_acc: 1.0000\n",
      "Epoch 210/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7446 - acc: 1.0000 - val_loss: 1.7200 - val_acc: 1.0000\n",
      "Epoch 211/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7447 - acc: 1.0000 - val_loss: 1.7200 - val_acc: 1.0000\n",
      "Epoch 212/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7398 - acc: 1.0000 - val_loss: 1.7199 - val_acc: 1.0000\n",
      "Epoch 213/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7377 - acc: 1.0000 - val_loss: 1.7198 - val_acc: 1.0000\n",
      "Epoch 214/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7443 - acc: 0.9841 - val_loss: 1.7198 - val_acc: 1.0000\n",
      "Epoch 215/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7459 - acc: 1.0000 - val_loss: 1.7197 - val_acc: 1.0000\n",
      "Epoch 216/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7385 - acc: 1.0000 - val_loss: 1.7197 - val_acc: 1.0000\n",
      "Epoch 217/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7404 - acc: 1.0000 - val_loss: 1.7196 - val_acc: 1.0000\n",
      "Epoch 218/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7400 - acc: 1.0000 - val_loss: 1.7195 - val_acc: 1.0000\n",
      "Epoch 219/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7371 - acc: 1.0000 - val_loss: 1.7195 - val_acc: 1.0000\n",
      "Epoch 220/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7374 - acc: 1.0000 - val_loss: 1.7194 - val_acc: 1.0000\n",
      "Epoch 221/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7409 - acc: 1.0000 - val_loss: 1.7194 - val_acc: 1.0000\n",
      "Epoch 222/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7299 - acc: 1.0000 - val_loss: 1.7193 - val_acc: 1.0000\n",
      "Epoch 223/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7389 - acc: 1.0000 - val_loss: 1.7192 - val_acc: 1.0000\n",
      "Epoch 224/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7631 - acc: 0.9841 - val_loss: 1.7192 - val_acc: 1.0000\n",
      "Epoch 225/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7397 - acc: 1.0000 - val_loss: 1.7191 - val_acc: 1.0000\n",
      "Epoch 226/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7402 - acc: 1.0000 - val_loss: 1.7191 - val_acc: 1.0000\n",
      "Epoch 227/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7357 - acc: 1.0000 - val_loss: 1.7190 - val_acc: 1.0000\n",
      "Epoch 228/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7411 - acc: 1.0000 - val_loss: 1.7190 - val_acc: 1.0000\n",
      "Epoch 229/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7430 - acc: 1.0000 - val_loss: 1.7189 - val_acc: 1.0000\n",
      "Epoch 230/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7376 - acc: 1.0000 - val_loss: 1.7189 - val_acc: 1.0000\n",
      "Epoch 231/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7399 - acc: 1.0000 - val_loss: 1.7188 - val_acc: 1.0000\n",
      "Epoch 232/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7370 - acc: 1.0000 - val_loss: 1.7188 - val_acc: 1.0000\n",
      "Epoch 233/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7333 - acc: 1.0000 - val_loss: 1.7187 - val_acc: 1.0000\n",
      "Epoch 234/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7361 - acc: 1.0000 - val_loss: 1.7187 - val_acc: 1.0000\n",
      "Epoch 235/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7345 - acc: 1.0000 - val_loss: 1.7186 - val_acc: 1.0000\n",
      "Epoch 236/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7637 - acc: 1.0000 - val_loss: 1.7186 - val_acc: 1.0000\n",
      "Epoch 237/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7390 - acc: 1.0000 - val_loss: 1.7185 - val_acc: 1.0000\n",
      "Epoch 238/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7278 - acc: 1.0000 - val_loss: 1.7185 - val_acc: 1.0000\n",
      "Epoch 239/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7313 - acc: 1.0000 - val_loss: 1.7184 - val_acc: 1.0000\n",
      "Epoch 240/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7287 - acc: 1.0000 - val_loss: 1.7184 - val_acc: 1.0000\n",
      "Epoch 241/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7532 - acc: 1.0000 - val_loss: 1.7184 - val_acc: 1.0000\n",
      "Epoch 242/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7272 - acc: 1.0000 - val_loss: 1.7183 - val_acc: 1.0000\n",
      "Epoch 243/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7324 - acc: 1.0000 - val_loss: 1.7183 - val_acc: 1.0000\n",
      "Epoch 244/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7330 - acc: 1.0000 - val_loss: 1.7183 - val_acc: 1.0000\n",
      "Epoch 245/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7287 - acc: 1.0000 - val_loss: 1.7182 - val_acc: 1.0000\n",
      "Epoch 246/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7332 - acc: 1.0000 - val_loss: 1.7182 - val_acc: 1.0000\n",
      "Epoch 247/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7461 - acc: 1.0000 - val_loss: 1.7182 - val_acc: 1.0000\n",
      "Epoch 248/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7263 - acc: 1.0000 - val_loss: 1.7181 - val_acc: 1.0000\n",
      "Epoch 249/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7477 - acc: 1.0000 - val_loss: 1.7181 - val_acc: 1.0000\n",
      "Epoch 250/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7290 - acc: 1.0000 - val_loss: 1.7181 - val_acc: 1.0000\n",
      "Epoch 251/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7790 - acc: 1.0000 - val_loss: 1.7180 - val_acc: 1.0000\n",
      "Epoch 252/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7290 - acc: 1.0000 - val_loss: 1.7180 - val_acc: 1.0000\n",
      "Epoch 253/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7344 - acc: 1.0000 - val_loss: 1.7180 - val_acc: 1.0000\n",
      "Epoch 254/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7361 - acc: 1.0000 - val_loss: 1.7179 - val_acc: 1.0000\n",
      "Epoch 255/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7748 - acc: 0.9841 - val_loss: 1.7179 - val_acc: 1.0000\n",
      "Epoch 256/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7292 - acc: 1.0000 - val_loss: 1.7179 - val_acc: 1.0000\n",
      "Epoch 257/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7275 - acc: 1.0000 - val_loss: 1.7178 - val_acc: 1.0000\n",
      "Epoch 258/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7367 - acc: 1.0000 - val_loss: 1.7178 - val_acc: 1.0000\n",
      "Epoch 259/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7316 - acc: 1.0000 - val_loss: 1.7178 - val_acc: 1.0000\n",
      "Epoch 260/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7769 - acc: 0.9841 - val_loss: 1.7177 - val_acc: 1.0000\n",
      "Epoch 261/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7289 - acc: 1.0000 - val_loss: 1.7177 - val_acc: 1.0000\n",
      "Epoch 262/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7283 - acc: 1.0000 - val_loss: 1.7177 - val_acc: 1.0000\n",
      "Epoch 263/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7297 - acc: 1.0000 - val_loss: 1.7177 - val_acc: 1.0000\n",
      "Epoch 264/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7400 - acc: 1.0000 - val_loss: 1.7176 - val_acc: 1.0000\n",
      "Epoch 265/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7341 - acc: 1.0000 - val_loss: 1.7176 - val_acc: 1.0000\n",
      "Epoch 266/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7334 - acc: 1.0000 - val_loss: 1.7176 - val_acc: 1.0000\n",
      "Epoch 267/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7451 - acc: 1.0000 - val_loss: 1.7176 - val_acc: 1.0000\n",
      "Epoch 268/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7257 - acc: 1.0000 - val_loss: 1.7175 - val_acc: 1.0000\n",
      "Epoch 269/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7268 - acc: 1.0000 - val_loss: 1.7175 - val_acc: 1.0000\n",
      "Epoch 270/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7315 - acc: 1.0000 - val_loss: 1.7175 - val_acc: 1.0000\n",
      "Epoch 271/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7313 - acc: 1.0000 - val_loss: 1.7175 - val_acc: 1.0000\n",
      "Epoch 272/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7255 - acc: 1.0000 - val_loss: 1.7174 - val_acc: 1.0000\n",
      "Epoch 273/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7494 - acc: 0.9841 - val_loss: 1.7174 - val_acc: 1.0000\n",
      "Epoch 274/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7754 - acc: 0.9841 - val_loss: 1.7174 - val_acc: 1.0000\n",
      "Epoch 275/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7275 - acc: 1.0000 - val_loss: 1.7174 - val_acc: 1.0000\n",
      "Epoch 276/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7283 - acc: 1.0000 - val_loss: 1.7174 - val_acc: 1.0000\n",
      "Epoch 277/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7326 - acc: 1.0000 - val_loss: 1.7173 - val_acc: 1.0000\n",
      "Epoch 278/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7258 - acc: 1.0000 - val_loss: 1.7173 - val_acc: 1.0000\n",
      "Epoch 279/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7233 - acc: 1.0000 - val_loss: 1.7173 - val_acc: 1.0000\n",
      "Epoch 280/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7252 - acc: 1.0000 - val_loss: 1.7173 - val_acc: 1.0000\n",
      "Epoch 281/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7603 - acc: 0.9683 - val_loss: 1.7173 - val_acc: 1.0000\n",
      "Epoch 282/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7317 - acc: 1.0000 - val_loss: 1.7172 - val_acc: 1.0000\n",
      "Epoch 283/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7366 - acc: 1.0000 - val_loss: 1.7172 - val_acc: 1.0000\n",
      "Epoch 284/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7345 - acc: 1.0000 - val_loss: 1.7172 - val_acc: 1.0000\n",
      "Epoch 285/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7363 - acc: 1.0000 - val_loss: 1.7172 - val_acc: 1.0000\n",
      "Epoch 286/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7273 - acc: 1.0000 - val_loss: 1.7172 - val_acc: 1.0000\n",
      "Epoch 287/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7324 - acc: 1.0000 - val_loss: 1.7171 - val_acc: 1.0000\n",
      "Epoch 288/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7327 - acc: 1.0000 - val_loss: 1.7171 - val_acc: 1.0000\n",
      "Epoch 289/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7256 - acc: 1.0000 - val_loss: 1.7171 - val_acc: 1.0000\n",
      "Epoch 290/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7395 - acc: 1.0000 - val_loss: 1.7171 - val_acc: 1.0000\n",
      "Epoch 291/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7290 - acc: 1.0000 - val_loss: 1.7171 - val_acc: 1.0000\n",
      "Epoch 292/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7279 - acc: 1.0000 - val_loss: 1.7170 - val_acc: 1.0000\n",
      "Epoch 293/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7248 - acc: 1.0000 - val_loss: 1.7170 - val_acc: 1.0000\n",
      "Epoch 294/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7366 - acc: 1.0000 - val_loss: 1.7170 - val_acc: 1.0000\n",
      "Epoch 295/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7380 - acc: 1.0000 - val_loss: 1.7170 - val_acc: 1.0000\n",
      "Epoch 296/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7247 - acc: 1.0000 - val_loss: 1.7170 - val_acc: 1.0000\n",
      "Epoch 297/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7482 - acc: 0.9841 - val_loss: 1.7169 - val_acc: 1.0000\n",
      "Epoch 298/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7451 - acc: 1.0000 - val_loss: 1.7169 - val_acc: 1.0000\n",
      "Epoch 299/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7317 - acc: 1.0000 - val_loss: 1.7169 - val_acc: 1.0000\n",
      "Epoch 300/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7275 - acc: 1.0000 - val_loss: 1.7169 - val_acc: 1.0000\n",
      "Stacked Test Accuracy: 0.912\n",
      "my_model_neu Test f-measure: 0.796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "members = load_all_models(['my_model_neu_ngrams', 'my_model_neu_words'\n",
    "#                            , 'my_model_convnet'\n",
    "                          ])\n",
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, [X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
    "#                                   X_train\n",
    "                                 ], y_train, [X_scaled_val_data_ngrams, X_scaled_val_data_words, \n",
    "#                                               X_val\n",
    "                                             ], y_val)\n",
    "final_model = load_model('my_model_stacked.h5')\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, [scaled_test_data_ngrams, scaled_test_data_words\n",
    "#                                              , test_data\n",
    "                                            ])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded my_model_neu_ngrams.h5\n",
      ">loaded my_model_neu_words.h5\n",
      ">loaded my_model_convnet.h5\n",
      "Loaded 3 models\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/300\n",
      "63/63 [==============================] - 5s 80ms/step - loss: 3.9155 - acc: 0.1587 - val_loss: 3.8627 - val_acc: 0.1111\n",
      "Epoch 2/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.8519 - acc: 0.2381 - val_loss: 3.8124 - val_acc: 0.3333\n",
      "Epoch 3/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.7983 - acc: 0.3016 - val_loss: 3.7636 - val_acc: 0.5556\n",
      "Epoch 4/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.7734 - acc: 0.4127 - val_loss: 3.7152 - val_acc: 0.6667\n",
      "Epoch 5/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 3.7127 - acc: 0.4444 - val_loss: 3.6685 - val_acc: 0.6667\n",
      "Epoch 6/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.6766 - acc: 0.6667 - val_loss: 3.6220 - val_acc: 0.7778\n",
      "Epoch 7/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.6140 - acc: 0.7460 - val_loss: 3.5761 - val_acc: 0.9921\n",
      "Epoch 8/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.5909 - acc: 0.8254 - val_loss: 3.5298 - val_acc: 1.0000\n",
      "Epoch 9/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.5386 - acc: 0.8571 - val_loss: 3.4832 - val_acc: 1.0000\n",
      "Epoch 10/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 3.5147 - acc: 0.8413 - val_loss: 3.4371 - val_acc: 1.0000\n",
      "Epoch 11/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.4861 - acc: 0.8571 - val_loss: 3.3915 - val_acc: 1.0000\n",
      "Epoch 12/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.4316 - acc: 0.9524 - val_loss: 3.3459 - val_acc: 1.0000\n",
      "Epoch 13/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.4038 - acc: 0.9524 - val_loss: 3.3006 - val_acc: 1.0000\n",
      "Epoch 14/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.3223 - acc: 0.9365 - val_loss: 3.2549 - val_acc: 1.0000\n",
      "Epoch 15/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 3.2934 - acc: 0.9841 - val_loss: 3.2088 - val_acc: 1.0000\n",
      "Epoch 16/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 3.2675 - acc: 0.9683 - val_loss: 3.1628 - val_acc: 1.0000\n",
      "Epoch 17/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 3.2367 - acc: 1.0000 - val_loss: 3.1177 - val_acc: 1.0000\n",
      "Epoch 18/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 3.1879 - acc: 1.0000 - val_loss: 3.0726 - val_acc: 1.0000\n",
      "Epoch 19/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 3.1290 - acc: 1.0000 - val_loss: 3.0272 - val_acc: 1.0000\n",
      "Epoch 20/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 3.0888 - acc: 1.0000 - val_loss: 2.9812 - val_acc: 1.0000\n",
      "Epoch 21/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 3.0720 - acc: 1.0000 - val_loss: 2.9360 - val_acc: 1.0000\n",
      "Epoch 22/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.9906 - acc: 0.9841 - val_loss: 2.8913 - val_acc: 1.0000\n",
      "Epoch 23/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.9656 - acc: 1.0000 - val_loss: 2.8467 - val_acc: 1.0000\n",
      "Epoch 24/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.9020 - acc: 1.0000 - val_loss: 2.8022 - val_acc: 1.0000\n",
      "Epoch 25/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.8285 - acc: 1.0000 - val_loss: 2.7579 - val_acc: 1.0000\n",
      "Epoch 26/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.8023 - acc: 1.0000 - val_loss: 2.7139 - val_acc: 1.0000\n",
      "Epoch 27/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.8173 - acc: 0.9841 - val_loss: 2.6715 - val_acc: 1.0000\n",
      "Epoch 28/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.7563 - acc: 1.0000 - val_loss: 2.6303 - val_acc: 1.0000\n",
      "Epoch 29/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.7269 - acc: 1.0000 - val_loss: 2.5900 - val_acc: 1.0000\n",
      "Epoch 30/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.6796 - acc: 1.0000 - val_loss: 2.5503 - val_acc: 1.0000\n",
      "Epoch 31/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.6416 - acc: 1.0000 - val_loss: 2.5115 - val_acc: 1.0000\n",
      "Epoch 32/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.5664 - acc: 1.0000 - val_loss: 2.4743 - val_acc: 1.0000\n",
      "Epoch 33/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 2.5432 - acc: 1.0000 - val_loss: 2.4384 - val_acc: 1.0000\n",
      "Epoch 34/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.4998 - acc: 1.0000 - val_loss: 2.4037 - val_acc: 1.0000\n",
      "Epoch 35/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.4711 - acc: 1.0000 - val_loss: 2.3701 - val_acc: 1.0000\n",
      "Epoch 36/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 2.4562 - acc: 1.0000 - val_loss: 2.3378 - val_acc: 1.0000\n",
      "Epoch 37/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.4452 - acc: 1.0000 - val_loss: 2.3065 - val_acc: 1.0000\n",
      "Epoch 38/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.4221 - acc: 1.0000 - val_loss: 2.2770 - val_acc: 1.0000\n",
      "Epoch 39/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.3598 - acc: 1.0000 - val_loss: 2.2490 - val_acc: 1.0000\n",
      "Epoch 40/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.3561 - acc: 1.0000 - val_loss: 2.2221 - val_acc: 1.0000\n",
      "Epoch 41/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 2.3045 - acc: 1.0000 - val_loss: 2.1964 - val_acc: 1.0000\n",
      "Epoch 42/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.2870 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 1.0000\n",
      "Epoch 43/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.2779 - acc: 1.0000 - val_loss: 2.1491 - val_acc: 1.0000\n",
      "Epoch 44/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.2371 - acc: 1.0000 - val_loss: 2.1274 - val_acc: 1.0000\n",
      "Epoch 45/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 2.2079 - acc: 1.0000 - val_loss: 2.1072 - val_acc: 1.0000\n",
      "Epoch 46/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.1813 - acc: 1.0000 - val_loss: 2.0879 - val_acc: 1.0000\n",
      "Epoch 47/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.1543 - acc: 1.0000 - val_loss: 2.0696 - val_acc: 1.0000\n",
      "Epoch 48/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.1424 - acc: 1.0000 - val_loss: 2.0529 - val_acc: 1.0000\n",
      "Epoch 49/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.1122 - acc: 1.0000 - val_loss: 2.0373 - val_acc: 1.0000\n",
      "Epoch 50/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.0814 - acc: 1.0000 - val_loss: 2.0227 - val_acc: 1.0000\n",
      "Epoch 51/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 2.0982 - acc: 1.0000 - val_loss: 2.0090 - val_acc: 1.0000\n",
      "Epoch 52/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.0900 - acc: 1.0000 - val_loss: 1.9958 - val_acc: 1.0000\n",
      "Epoch 53/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.0626 - acc: 1.0000 - val_loss: 1.9837 - val_acc: 1.0000\n",
      "Epoch 54/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.0407 - acc: 1.0000 - val_loss: 1.9723 - val_acc: 1.0000\n",
      "Epoch 55/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.0520 - acc: 1.0000 - val_loss: 1.9616 - val_acc: 1.0000\n",
      "Epoch 56/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.0304 - acc: 1.0000 - val_loss: 1.9514 - val_acc: 1.0000\n",
      "Epoch 57/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.0213 - acc: 1.0000 - val_loss: 1.9418 - val_acc: 1.0000\n",
      "Epoch 58/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.0250 - acc: 1.0000 - val_loss: 1.9328 - val_acc: 1.0000\n",
      "Epoch 59/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 2.0230 - acc: 1.0000 - val_loss: 1.9243 - val_acc: 1.0000\n",
      "Epoch 60/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9915 - acc: 1.0000 - val_loss: 1.9164 - val_acc: 1.0000\n",
      "Epoch 61/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9791 - acc: 1.0000 - val_loss: 1.9091 - val_acc: 1.0000\n",
      "Epoch 62/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9715 - acc: 1.0000 - val_loss: 1.9022 - val_acc: 1.0000\n",
      "Epoch 63/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.9761 - acc: 1.0000 - val_loss: 1.8957 - val_acc: 1.0000\n",
      "Epoch 64/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9502 - acc: 1.0000 - val_loss: 1.8896 - val_acc: 1.0000\n",
      "Epoch 65/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9731 - acc: 1.0000 - val_loss: 1.8839 - val_acc: 1.0000\n",
      "Epoch 66/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9471 - acc: 1.0000 - val_loss: 1.8784 - val_acc: 1.0000\n",
      "Epoch 67/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9234 - acc: 1.0000 - val_loss: 1.8734 - val_acc: 1.0000\n",
      "Epoch 68/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.9535 - acc: 1.0000 - val_loss: 1.8686 - val_acc: 1.0000\n",
      "Epoch 69/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.9114 - acc: 1.0000 - val_loss: 1.8642 - val_acc: 1.0000\n",
      "Epoch 70/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9204 - acc: 1.0000 - val_loss: 1.8599 - val_acc: 1.0000\n",
      "Epoch 71/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9123 - acc: 1.0000 - val_loss: 1.8560 - val_acc: 1.0000\n",
      "Epoch 72/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9047 - acc: 1.0000 - val_loss: 1.8522 - val_acc: 1.0000\n",
      "Epoch 73/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9058 - acc: 1.0000 - val_loss: 1.8486 - val_acc: 1.0000\n",
      "Epoch 74/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8891 - acc: 1.0000 - val_loss: 1.8453 - val_acc: 1.0000\n",
      "Epoch 75/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.9046 - acc: 1.0000 - val_loss: 1.8420 - val_acc: 1.0000\n",
      "Epoch 76/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8890 - acc: 1.0000 - val_loss: 1.8389 - val_acc: 1.0000\n",
      "Epoch 77/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.9028 - acc: 1.0000 - val_loss: 1.8359 - val_acc: 1.0000\n",
      "Epoch 78/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8822 - acc: 1.0000 - val_loss: 1.8330 - val_acc: 1.0000\n",
      "Epoch 79/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8681 - acc: 1.0000 - val_loss: 1.8304 - val_acc: 1.0000\n",
      "Epoch 80/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8742 - acc: 1.0000 - val_loss: 1.8280 - val_acc: 1.0000\n",
      "Epoch 81/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8551 - acc: 1.0000 - val_loss: 1.8258 - val_acc: 1.0000\n",
      "Epoch 82/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8658 - acc: 1.0000 - val_loss: 1.8236 - val_acc: 1.0000\n",
      "Epoch 83/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8628 - acc: 1.0000 - val_loss: 1.8215 - val_acc: 1.0000\n",
      "Epoch 84/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8598 - acc: 1.0000 - val_loss: 1.8195 - val_acc: 1.0000\n",
      "Epoch 85/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8518 - acc: 1.0000 - val_loss: 1.8177 - val_acc: 1.0000\n",
      "Epoch 86/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.8524 - acc: 1.0000 - val_loss: 1.8159 - val_acc: 1.0000\n",
      "Epoch 87/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8382 - acc: 1.0000 - val_loss: 1.8143 - val_acc: 1.0000\n",
      "Epoch 88/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8481 - acc: 1.0000 - val_loss: 1.8127 - val_acc: 1.0000\n",
      "Epoch 89/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8618 - acc: 1.0000 - val_loss: 1.8112 - val_acc: 1.0000\n",
      "Epoch 90/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.8395 - acc: 1.0000 - val_loss: 1.8097 - val_acc: 1.0000\n",
      "Epoch 91/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.8324 - acc: 1.0000 - val_loss: 1.8083 - val_acc: 1.0000\n",
      "Epoch 92/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8439 - acc: 1.0000 - val_loss: 1.8070 - val_acc: 1.0000\n",
      "Epoch 93/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8501 - acc: 1.0000 - val_loss: 1.8056 - val_acc: 1.0000\n",
      "Epoch 94/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8517 - acc: 1.0000 - val_loss: 1.8043 - val_acc: 1.0000\n",
      "Epoch 95/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8485 - acc: 1.0000 - val_loss: 1.8030 - val_acc: 1.0000\n",
      "Epoch 96/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8377 - acc: 1.0000 - val_loss: 1.8018 - val_acc: 1.0000\n",
      "Epoch 97/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8380 - acc: 1.0000 - val_loss: 1.8007 - val_acc: 1.0000\n",
      "Epoch 98/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8250 - acc: 1.0000 - val_loss: 1.7996 - val_acc: 1.0000\n",
      "Epoch 99/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8395 - acc: 1.0000 - val_loss: 1.7985 - val_acc: 1.0000\n",
      "Epoch 100/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.8305 - acc: 1.0000 - val_loss: 1.7975 - val_acc: 1.0000\n",
      "Epoch 101/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8205 - acc: 1.0000 - val_loss: 1.7965 - val_acc: 1.0000\n",
      "Epoch 102/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8307 - acc: 1.0000 - val_loss: 1.7956 - val_acc: 1.0000\n",
      "Epoch 103/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8290 - acc: 1.0000 - val_loss: 1.7947 - val_acc: 1.0000\n",
      "Epoch 104/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8230 - acc: 1.0000 - val_loss: 1.7939 - val_acc: 1.0000\n",
      "Epoch 105/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8209 - acc: 1.0000 - val_loss: 1.7931 - val_acc: 1.0000\n",
      "Epoch 106/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8116 - acc: 1.0000 - val_loss: 1.7923 - val_acc: 1.0000\n",
      "Epoch 107/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8159 - acc: 1.0000 - val_loss: 1.7916 - val_acc: 1.0000\n",
      "Epoch 108/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8255 - acc: 1.0000 - val_loss: 1.7909 - val_acc: 1.0000\n",
      "Epoch 109/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8160 - acc: 1.0000 - val_loss: 1.7902 - val_acc: 1.0000\n",
      "Epoch 110/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8189 - acc: 1.0000 - val_loss: 1.7896 - val_acc: 1.0000\n",
      "Epoch 111/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8166 - acc: 1.0000 - val_loss: 1.7889 - val_acc: 1.0000\n",
      "Epoch 112/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8253 - acc: 1.0000 - val_loss: 1.7883 - val_acc: 1.0000\n",
      "Epoch 113/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8093 - acc: 1.0000 - val_loss: 1.7877 - val_acc: 1.0000\n",
      "Epoch 114/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8085 - acc: 1.0000 - val_loss: 1.7871 - val_acc: 1.0000\n",
      "Epoch 115/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.8083 - acc: 1.0000 - val_loss: 1.7866 - val_acc: 1.0000\n",
      "Epoch 116/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7956 - acc: 1.0000 - val_loss: 1.7861 - val_acc: 1.0000\n",
      "Epoch 117/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8211 - acc: 1.0000 - val_loss: 1.7856 - val_acc: 1.0000\n",
      "Epoch 118/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8032 - acc: 1.0000 - val_loss: 1.7852 - val_acc: 1.0000\n",
      "Epoch 119/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8011 - acc: 1.0000 - val_loss: 1.7847 - val_acc: 1.0000\n",
      "Epoch 120/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8070 - acc: 1.0000 - val_loss: 1.7843 - val_acc: 1.0000\n",
      "Epoch 121/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8069 - acc: 1.0000 - val_loss: 1.7838 - val_acc: 1.0000\n",
      "Epoch 122/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8005 - acc: 1.0000 - val_loss: 1.7834 - val_acc: 1.0000\n",
      "Epoch 123/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8061 - acc: 1.0000 - val_loss: 1.7830 - val_acc: 1.0000\n",
      "Epoch 124/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7930 - acc: 1.0000 - val_loss: 1.7826 - val_acc: 1.0000\n",
      "Epoch 125/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8141 - acc: 1.0000 - val_loss: 1.7822 - val_acc: 1.0000\n",
      "Epoch 126/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8046 - acc: 1.0000 - val_loss: 1.7818 - val_acc: 1.0000\n",
      "Epoch 127/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7998 - acc: 1.0000 - val_loss: 1.7814 - val_acc: 1.0000\n",
      "Epoch 128/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.8051 - acc: 1.0000 - val_loss: 1.7811 - val_acc: 1.0000\n",
      "Epoch 129/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.8044 - acc: 1.0000 - val_loss: 1.7807 - val_acc: 1.0000\n",
      "Epoch 130/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.8054 - acc: 1.0000 - val_loss: 1.7803 - val_acc: 1.0000\n",
      "Epoch 131/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7967 - acc: 1.0000 - val_loss: 1.7800 - val_acc: 1.0000\n",
      "Epoch 132/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7950 - acc: 1.0000 - val_loss: 1.7797 - val_acc: 1.0000\n",
      "Epoch 133/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7935 - acc: 1.0000 - val_loss: 1.7794 - val_acc: 1.0000\n",
      "Epoch 134/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7858 - acc: 1.0000 - val_loss: 1.7791 - val_acc: 1.0000\n",
      "Epoch 135/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.8019 - acc: 1.0000 - val_loss: 1.7789 - val_acc: 1.0000\n",
      "Epoch 136/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7971 - acc: 1.0000 - val_loss: 1.7786 - val_acc: 1.0000\n",
      "Epoch 137/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7927 - acc: 1.0000 - val_loss: 1.7783 - val_acc: 1.0000\n",
      "Epoch 138/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7966 - acc: 1.0000 - val_loss: 1.7781 - val_acc: 1.0000\n",
      "Epoch 139/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7908 - acc: 1.0000 - val_loss: 1.7778 - val_acc: 1.0000\n",
      "Epoch 140/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7911 - acc: 1.0000 - val_loss: 1.7776 - val_acc: 1.0000\n",
      "Epoch 141/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7938 - acc: 1.0000 - val_loss: 1.7773 - val_acc: 1.0000\n",
      "Epoch 142/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7909 - acc: 1.0000 - val_loss: 1.7771 - val_acc: 1.0000\n",
      "Epoch 143/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7953 - acc: 1.0000 - val_loss: 1.7769 - val_acc: 1.0000\n",
      "Epoch 144/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7887 - acc: 1.0000 - val_loss: 1.7766 - val_acc: 1.0000\n",
      "Epoch 145/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7878 - acc: 1.0000 - val_loss: 1.7764 - val_acc: 1.0000\n",
      "Epoch 146/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7897 - acc: 1.0000 - val_loss: 1.7762 - val_acc: 1.0000\n",
      "Epoch 147/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7890 - acc: 1.0000 - val_loss: 1.7760 - val_acc: 1.0000\n",
      "Epoch 148/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7935 - acc: 1.0000 - val_loss: 1.7758 - val_acc: 1.0000\n",
      "Epoch 149/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7955 - acc: 1.0000 - val_loss: 1.7756 - val_acc: 1.0000\n",
      "Epoch 150/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7896 - acc: 1.0000 - val_loss: 1.7754 - val_acc: 1.0000\n",
      "Epoch 151/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7862 - acc: 1.0000 - val_loss: 1.7753 - val_acc: 1.0000\n",
      "Epoch 152/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7913 - acc: 1.0000 - val_loss: 1.7751 - val_acc: 1.0000\n",
      "Epoch 153/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7865 - acc: 1.0000 - val_loss: 1.7749 - val_acc: 1.0000\n",
      "Epoch 154/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7834 - acc: 1.0000 - val_loss: 1.7747 - val_acc: 1.0000\n",
      "Epoch 155/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7973 - acc: 1.0000 - val_loss: 1.7746 - val_acc: 1.0000\n",
      "Epoch 156/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7879 - acc: 1.0000 - val_loss: 1.7744 - val_acc: 1.0000\n",
      "Epoch 157/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7904 - acc: 1.0000 - val_loss: 1.7742 - val_acc: 1.0000\n",
      "Epoch 158/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7852 - acc: 1.0000 - val_loss: 1.7741 - val_acc: 1.0000\n",
      "Epoch 159/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7861 - acc: 1.0000 - val_loss: 1.7739 - val_acc: 1.0000\n",
      "Epoch 160/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7829 - acc: 1.0000 - val_loss: 1.7738 - val_acc: 1.0000\n",
      "Epoch 161/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7977 - acc: 1.0000 - val_loss: 1.7736 - val_acc: 1.0000\n",
      "Epoch 162/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7870 - acc: 1.0000 - val_loss: 1.7735 - val_acc: 1.0000\n",
      "Epoch 163/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7884 - acc: 1.0000 - val_loss: 1.7733 - val_acc: 1.0000\n",
      "Epoch 164/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7856 - acc: 1.0000 - val_loss: 1.7732 - val_acc: 1.0000\n",
      "Epoch 165/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7795 - acc: 1.0000 - val_loss: 1.7731 - val_acc: 1.0000\n",
      "Epoch 166/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7829 - acc: 1.0000 - val_loss: 1.7729 - val_acc: 1.0000\n",
      "Epoch 167/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7858 - acc: 1.0000 - val_loss: 1.7728 - val_acc: 1.0000\n",
      "Epoch 168/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7858 - acc: 1.0000 - val_loss: 1.7727 - val_acc: 1.0000\n",
      "Epoch 169/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7891 - acc: 1.0000 - val_loss: 1.7726 - val_acc: 1.0000\n",
      "Epoch 170/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7800 - acc: 1.0000 - val_loss: 1.7724 - val_acc: 1.0000\n",
      "Epoch 171/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7814 - acc: 1.0000 - val_loss: 1.7723 - val_acc: 1.0000\n",
      "Epoch 172/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7824 - acc: 1.0000 - val_loss: 1.7722 - val_acc: 1.0000\n",
      "Epoch 173/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7830 - acc: 1.0000 - val_loss: 1.7721 - val_acc: 1.0000\n",
      "Epoch 174/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7826 - acc: 1.0000 - val_loss: 1.7720 - val_acc: 1.0000\n",
      "Epoch 175/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7822 - acc: 1.0000 - val_loss: 1.7719 - val_acc: 1.0000\n",
      "Epoch 176/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7881 - acc: 1.0000 - val_loss: 1.7718 - val_acc: 1.0000\n",
      "Epoch 177/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7795 - acc: 1.0000 - val_loss: 1.7717 - val_acc: 1.0000\n",
      "Epoch 178/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7840 - acc: 1.0000 - val_loss: 1.7716 - val_acc: 1.0000\n",
      "Epoch 179/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7791 - acc: 1.0000 - val_loss: 1.7715 - val_acc: 1.0000\n",
      "Epoch 180/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7826 - acc: 1.0000 - val_loss: 1.7714 - val_acc: 1.0000\n",
      "Epoch 181/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7892 - acc: 1.0000 - val_loss: 1.7713 - val_acc: 1.0000\n",
      "Epoch 182/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7791 - acc: 1.0000 - val_loss: 1.7712 - val_acc: 1.0000\n",
      "Epoch 183/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7822 - acc: 1.0000 - val_loss: 1.7711 - val_acc: 1.0000\n",
      "Epoch 184/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7786 - acc: 1.0000 - val_loss: 1.7710 - val_acc: 1.0000\n",
      "Epoch 185/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7796 - acc: 1.0000 - val_loss: 1.7710 - val_acc: 1.0000\n",
      "Epoch 186/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7798 - acc: 1.0000 - val_loss: 1.7709 - val_acc: 1.0000\n",
      "Epoch 187/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7787 - acc: 1.0000 - val_loss: 1.7708 - val_acc: 1.0000\n",
      "Epoch 188/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7858 - acc: 1.0000 - val_loss: 1.7707 - val_acc: 1.0000\n",
      "Epoch 189/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7792 - acc: 1.0000 - val_loss: 1.7706 - val_acc: 1.0000\n",
      "Epoch 190/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7782 - acc: 1.0000 - val_loss: 1.7706 - val_acc: 1.0000\n",
      "Epoch 191/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7828 - acc: 1.0000 - val_loss: 1.7705 - val_acc: 1.0000\n",
      "Epoch 192/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7768 - acc: 1.0000 - val_loss: 1.7704 - val_acc: 1.0000\n",
      "Epoch 193/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7807 - acc: 1.0000 - val_loss: 1.7703 - val_acc: 1.0000\n",
      "Epoch 194/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7799 - acc: 1.0000 - val_loss: 1.7703 - val_acc: 1.0000\n",
      "Epoch 195/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7759 - acc: 1.0000 - val_loss: 1.7702 - val_acc: 1.0000\n",
      "Epoch 196/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7801 - acc: 1.0000 - val_loss: 1.7701 - val_acc: 1.0000\n",
      "Epoch 197/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7770 - acc: 1.0000 - val_loss: 1.7701 - val_acc: 1.0000\n",
      "Epoch 198/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7788 - acc: 1.0000 - val_loss: 1.7700 - val_acc: 1.0000\n",
      "Epoch 199/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7780 - acc: 1.0000 - val_loss: 1.7700 - val_acc: 1.0000\n",
      "Epoch 200/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7793 - acc: 1.0000 - val_loss: 1.7699 - val_acc: 1.0000\n",
      "Epoch 201/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7746 - acc: 1.0000 - val_loss: 1.7698 - val_acc: 1.0000\n",
      "Epoch 202/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7770 - acc: 1.0000 - val_loss: 1.7698 - val_acc: 1.0000\n",
      "Epoch 203/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7739 - acc: 1.0000 - val_loss: 1.7697 - val_acc: 1.0000\n",
      "Epoch 204/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7831 - acc: 1.0000 - val_loss: 1.7697 - val_acc: 1.0000\n",
      "Epoch 205/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7776 - acc: 1.0000 - val_loss: 1.7696 - val_acc: 1.0000\n",
      "Epoch 206/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7765 - acc: 1.0000 - val_loss: 1.7696 - val_acc: 1.0000\n",
      "Epoch 207/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7739 - acc: 1.0000 - val_loss: 1.7695 - val_acc: 1.0000\n",
      "Epoch 208/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7746 - acc: 1.0000 - val_loss: 1.7695 - val_acc: 1.0000\n",
      "Epoch 209/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7820 - acc: 1.0000 - val_loss: 1.7694 - val_acc: 1.0000\n",
      "Epoch 210/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7784 - acc: 1.0000 - val_loss: 1.7693 - val_acc: 1.0000\n",
      "Epoch 211/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7755 - acc: 1.0000 - val_loss: 1.7693 - val_acc: 1.0000\n",
      "Epoch 212/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7765 - acc: 1.0000 - val_loss: 1.7692 - val_acc: 1.0000\n",
      "Epoch 213/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7753 - acc: 1.0000 - val_loss: 1.7692 - val_acc: 1.0000\n",
      "Epoch 214/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7745 - acc: 1.0000 - val_loss: 1.7692 - val_acc: 1.0000\n",
      "Epoch 215/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7735 - acc: 1.0000 - val_loss: 1.7691 - val_acc: 1.0000\n",
      "Epoch 216/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7743 - acc: 1.0000 - val_loss: 1.7691 - val_acc: 1.0000\n",
      "Epoch 217/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7789 - acc: 1.0000 - val_loss: 1.7690 - val_acc: 1.0000\n",
      "Epoch 218/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7864 - acc: 1.0000 - val_loss: 1.7690 - val_acc: 1.0000\n",
      "Epoch 219/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7796 - acc: 1.0000 - val_loss: 1.7689 - val_acc: 1.0000\n",
      "Epoch 220/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7768 - acc: 1.0000 - val_loss: 1.7689 - val_acc: 1.0000\n",
      "Epoch 221/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7759 - acc: 1.0000 - val_loss: 1.7688 - val_acc: 1.0000\n",
      "Epoch 222/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7748 - acc: 1.0000 - val_loss: 1.7688 - val_acc: 1.0000\n",
      "Epoch 223/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7731 - acc: 1.0000 - val_loss: 1.7687 - val_acc: 1.0000\n",
      "Epoch 224/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7788 - acc: 1.0000 - val_loss: 1.7687 - val_acc: 1.0000\n",
      "Epoch 225/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7723 - acc: 1.0000 - val_loss: 1.7687 - val_acc: 1.0000\n",
      "Epoch 226/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7810 - acc: 1.0000 - val_loss: 1.7686 - val_acc: 1.0000\n",
      "Epoch 227/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7754 - acc: 1.0000 - val_loss: 1.7686 - val_acc: 1.0000\n",
      "Epoch 228/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7731 - acc: 1.0000 - val_loss: 1.7685 - val_acc: 1.0000\n",
      "Epoch 229/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7769 - acc: 1.0000 - val_loss: 1.7685 - val_acc: 1.0000\n",
      "Epoch 230/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7753 - acc: 1.0000 - val_loss: 1.7685 - val_acc: 1.0000\n",
      "Epoch 231/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7769 - acc: 1.0000 - val_loss: 1.7684 - val_acc: 1.0000\n",
      "Epoch 232/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7721 - acc: 1.0000 - val_loss: 1.7684 - val_acc: 1.0000\n",
      "Epoch 233/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7750 - acc: 1.0000 - val_loss: 1.7684 - val_acc: 1.0000\n",
      "Epoch 234/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7722 - acc: 1.0000 - val_loss: 1.7683 - val_acc: 1.0000\n",
      "Epoch 235/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7751 - acc: 1.0000 - val_loss: 1.7683 - val_acc: 1.0000\n",
      "Epoch 236/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7755 - acc: 1.0000 - val_loss: 1.7683 - val_acc: 1.0000\n",
      "Epoch 237/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7714 - acc: 1.0000 - val_loss: 1.7682 - val_acc: 1.0000\n",
      "Epoch 238/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7720 - acc: 1.0000 - val_loss: 1.7682 - val_acc: 1.0000\n",
      "Epoch 239/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7728 - acc: 1.0000 - val_loss: 1.7682 - val_acc: 1.0000\n",
      "Epoch 240/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7734 - acc: 1.0000 - val_loss: 1.7681 - val_acc: 1.0000\n",
      "Epoch 241/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7724 - acc: 1.0000 - val_loss: 1.7681 - val_acc: 1.0000\n",
      "Epoch 242/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7724 - acc: 1.0000 - val_loss: 1.7681 - val_acc: 1.0000\n",
      "Epoch 243/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7747 - acc: 1.0000 - val_loss: 1.7681 - val_acc: 1.0000\n",
      "Epoch 244/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7728 - acc: 1.0000 - val_loss: 1.7680 - val_acc: 1.0000\n",
      "Epoch 245/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7722 - acc: 1.0000 - val_loss: 1.7680 - val_acc: 1.0000\n",
      "Epoch 246/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7716 - acc: 1.0000 - val_loss: 1.7680 - val_acc: 1.0000\n",
      "Epoch 247/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7730 - acc: 1.0000 - val_loss: 1.7680 - val_acc: 1.0000\n",
      "Epoch 248/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7746 - acc: 1.0000 - val_loss: 1.7679 - val_acc: 1.0000\n",
      "Epoch 249/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7724 - acc: 1.0000 - val_loss: 1.7679 - val_acc: 1.0000\n",
      "Epoch 250/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7729 - acc: 1.0000 - val_loss: 1.7679 - val_acc: 1.0000\n",
      "Epoch 251/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7735 - acc: 1.0000 - val_loss: 1.7678 - val_acc: 1.0000\n",
      "Epoch 252/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7776 - acc: 1.0000 - val_loss: 1.7678 - val_acc: 1.0000\n",
      "Epoch 253/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7705 - acc: 1.0000 - val_loss: 1.7678 - val_acc: 1.0000\n",
      "Epoch 254/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7703 - acc: 1.0000 - val_loss: 1.7678 - val_acc: 1.0000\n",
      "Epoch 255/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7714 - acc: 1.0000 - val_loss: 1.7678 - val_acc: 1.0000\n",
      "Epoch 256/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7730 - acc: 1.0000 - val_loss: 1.7677 - val_acc: 1.0000\n",
      "Epoch 257/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7716 - acc: 1.0000 - val_loss: 1.7677 - val_acc: 1.0000\n",
      "Epoch 258/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7738 - acc: 1.0000 - val_loss: 1.7677 - val_acc: 1.0000\n",
      "Epoch 259/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7729 - acc: 1.0000 - val_loss: 1.7677 - val_acc: 1.0000\n",
      "Epoch 260/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7716 - acc: 1.0000 - val_loss: 1.7676 - val_acc: 1.0000\n",
      "Epoch 261/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7706 - acc: 1.0000 - val_loss: 1.7676 - val_acc: 1.0000\n",
      "Epoch 262/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7716 - acc: 1.0000 - val_loss: 1.7676 - val_acc: 1.0000\n",
      "Epoch 263/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7740 - acc: 1.0000 - val_loss: 1.7676 - val_acc: 1.0000\n",
      "Epoch 264/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7731 - acc: 1.0000 - val_loss: 1.7676 - val_acc: 1.0000\n",
      "Epoch 265/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7722 - acc: 1.0000 - val_loss: 1.7675 - val_acc: 1.0000\n",
      "Epoch 266/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7724 - acc: 1.0000 - val_loss: 1.7675 - val_acc: 1.0000\n",
      "Epoch 267/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7702 - acc: 1.0000 - val_loss: 1.7675 - val_acc: 1.0000\n",
      "Epoch 268/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7711 - acc: 1.0000 - val_loss: 1.7675 - val_acc: 1.0000\n",
      "Epoch 269/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7733 - acc: 1.0000 - val_loss: 1.7674 - val_acc: 1.0000\n",
      "Epoch 270/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7728 - acc: 1.0000 - val_loss: 1.7674 - val_acc: 1.0000\n",
      "Epoch 271/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7716 - acc: 1.0000 - val_loss: 1.7674 - val_acc: 1.0000\n",
      "Epoch 272/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7710 - acc: 1.0000 - val_loss: 1.7674 - val_acc: 1.0000\n",
      "Epoch 273/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7719 - acc: 1.0000 - val_loss: 1.7674 - val_acc: 1.0000\n",
      "Epoch 274/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7700 - acc: 1.0000 - val_loss: 1.7673 - val_acc: 1.0000\n",
      "Epoch 275/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7719 - acc: 1.0000 - val_loss: 1.7673 - val_acc: 1.0000\n",
      "Epoch 276/300\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.7709 - acc: 1.0000 - val_loss: 1.7673 - val_acc: 1.0000\n",
      "Epoch 277/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7723 - acc: 1.0000 - val_loss: 1.7673 - val_acc: 1.0000\n",
      "Epoch 278/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7716 - acc: 1.0000 - val_loss: 1.7673 - val_acc: 1.0000\n",
      "Epoch 279/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7696 - acc: 1.0000 - val_loss: 1.7673 - val_acc: 1.0000\n",
      "Epoch 280/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7699 - acc: 1.0000 - val_loss: 1.7672 - val_acc: 1.0000\n",
      "Epoch 281/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7710 - acc: 1.0000 - val_loss: 1.7672 - val_acc: 1.0000\n",
      "Epoch 282/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7735 - acc: 1.0000 - val_loss: 1.7672 - val_acc: 1.0000\n",
      "Epoch 283/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7712 - acc: 1.0000 - val_loss: 1.7672 - val_acc: 1.0000\n",
      "Epoch 284/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7707 - acc: 1.0000 - val_loss: 1.7672 - val_acc: 1.0000\n",
      "Epoch 285/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7714 - acc: 1.0000 - val_loss: 1.7672 - val_acc: 1.0000\n",
      "Epoch 286/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7715 - acc: 1.0000 - val_loss: 1.7671 - val_acc: 1.0000\n",
      "Epoch 287/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7720 - acc: 1.0000 - val_loss: 1.7671 - val_acc: 1.0000\n",
      "Epoch 288/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7728 - acc: 1.0000 - val_loss: 1.7671 - val_acc: 1.0000\n",
      "Epoch 289/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7708 - acc: 1.0000 - val_loss: 1.7671 - val_acc: 1.0000\n",
      "Epoch 290/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7710 - acc: 1.0000 - val_loss: 1.7671 - val_acc: 1.0000\n",
      "Epoch 291/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7724 - acc: 1.0000 - val_loss: 1.7671 - val_acc: 1.0000\n",
      "Epoch 292/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7702 - acc: 1.0000 - val_loss: 1.7670 - val_acc: 1.0000\n",
      "Epoch 293/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7725 - acc: 1.0000 - val_loss: 1.7670 - val_acc: 1.0000\n",
      "Epoch 294/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7699 - acc: 1.0000 - val_loss: 1.7670 - val_acc: 1.0000\n",
      "Epoch 295/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7721 - acc: 1.0000 - val_loss: 1.7670 - val_acc: 1.0000\n",
      "Epoch 296/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7699 - acc: 1.0000 - val_loss: 1.7670 - val_acc: 1.0000\n",
      "Epoch 297/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7710 - acc: 1.0000 - val_loss: 1.7670 - val_acc: 1.0000\n",
      "Epoch 298/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7702 - acc: 1.0000 - val_loss: 1.7670 - val_acc: 1.0000\n",
      "Epoch 299/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7712 - acc: 1.0000 - val_loss: 1.7670 - val_acc: 1.0000\n",
      "Epoch 300/300\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 1.7701 - acc: 1.0000 - val_loss: 1.7669 - val_acc: 1.0000\n",
      "Stacked Test Accuracy: 0.880\n",
      "my_model_neu Test f-measure: 0.745\n"
     ]
    }
   ],
   "source": [
    "\n",
    "members = load_all_models(['my_model_neu_ngrams', 'my_model_neu_words'\n",
    "                           , 'my_model_convnet'\n",
    "                          ])\n",
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, [X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
    "                                  X_train\n",
    "                                 ], y_train, [X_scaled_val_data_ngrams, X_scaled_val_data_words, \n",
    "                                              X_val\n",
    "                                             ], y_val)\n",
    "final_model = load_model('my_model_stacked.h5')\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, [scaled_test_data_ngrams, scaled_test_data_words\n",
    "                                             , test_data\n",
    "                                            ])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_neu Test f-measure: 0.668\n"
     ]
    }
   ],
   "source": [
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
