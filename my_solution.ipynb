{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Reading general data of the problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading general data of the problems, done!\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import division\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "# multiprocessing.set_start_method('spawn')\n",
    "from MyUtils import clean_folder, read_files, shuffle_docs, shuffle_docs2\n",
    "from Word2Dim import Word2Dim\n",
    "\n",
    "dataset_path = '.' + os.sep + 'pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23'\n",
    "outpath = '.' + os.sep + 'dev_out'\n",
    "\n",
    "clean_folder(outpath)\n",
    "\n",
    "infocollection = dataset_path + os.sep + 'collection-info.json'\n",
    "problems = []\n",
    "language = []\n",
    "with open(infocollection, 'r') as f:\n",
    "    for attrib in json.load(f):\n",
    "        problems.append(attrib['problem-name'])\n",
    "        language.append(attrib['language'])\n",
    "print('Reading general data of the problems, done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reading problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc count to process:  819\n",
      "process_doc, done!\n",
      "word_set, ready!\n",
      "fit_transform_texts is done!\n",
      "doc count to process:  561\n",
      "Reading problem 1, done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "problem = problems[0]\n",
    "index = 0\n",
    "\n",
    "# used for n_gram extraction and word indexing, a threshold which prevent words appearing lower than this value to be counted in calculations\n",
    "tf = 5\n",
    "\n",
    "\n",
    "infoproblem = dataset_path + os.sep + problem + os.sep + 'problem-info.json'\n",
    "candidates = []\n",
    "with open(infoproblem, 'r') as f:\n",
    "    fj = json.load(f)\n",
    "    unk_folder = fj['unknown-folder']\n",
    "    for attrib in fj['candidate-authors']:\n",
    "        candidates.append(attrib['author-name'])\n",
    "\n",
    "candidates.sort()\n",
    "# Building training set\n",
    "train_docs = []\n",
    "for candidate in candidates:\n",
    "    train_docs.extend(read_files(dataset_path + os.sep + problem, candidate))\n",
    "train_texts = [text for i, (text, label) in enumerate(train_docs)]\n",
    "train_labels = [label for i, (text, label) in enumerate(train_docs)]\n",
    "initial_train_size = len(train_labels)\n",
    "\n",
    "train_texts, train_labels = shuffle_docs(train_texts, train_labels)\n",
    "validation_size = len(train_texts) - initial_train_size\n",
    "class_size = int(initial_train_size / len(set(train_labels)))\n",
    "\n",
    "# train_texts, train_labels, validation_start_index, class_size = shuffle_docs2(train_texts, train_labels)\n",
    "\n",
    "index_2_label_dict = {i: l for i, l in enumerate(set(train_labels))}\n",
    "label_2_index_dict = {l: i for i, l in enumerate(set(train_labels))}\n",
    "\n",
    "label_2_index_dict[u'<UNK>'] = len(index_2_label_dict.keys())\n",
    "index_2_label_dict[len(index_2_label_dict.keys())] = u'<UNK>'\n",
    "\n",
    "train_labels = [label_2_index_dict[v] for v in train_labels]\n",
    "w2d = Word2Dim(lang= language[index])\n",
    "train_tokenized_with_pos, train_tokenized_indexed = w2d.fit_transform_texts(train_texts, train_labels, tf= tf)\n",
    "\n",
    "maxlen = len(max(train_tokenized_indexed, key=len))  # We will cut the texts after # words\n",
    "embedding_dim = w2d.word_embedding.shape[1]\n",
    "\n",
    "# preparing test set\n",
    "ground_truth_file = dataset_path + os.sep + problem + os.sep + 'ground-truth.json'\n",
    "gt = {}\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    for attrib in json.load(f)['ground_truth']:\n",
    "        gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "test_docs = read_files(dataset_path + os.sep + problem, unk_folder, gt)\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs)]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs)]\n",
    "\n",
    "# # Filter validation to known authors\n",
    "# test_texts = [text for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "# test_labels = [label for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "\n",
    "test_labels = [label_2_index_dict[v] for v in test_labels]\n",
    "\n",
    "test_tokenized_with_pos, test_tokenized_indexed = w2d.transform(test_texts)\n",
    "print(\"Reading problem 1, done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Extraction for Neural Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from MyUtils import extract_n_grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "n = 3\n",
    "vocabulary = extract_n_grams(train_docs, n, tf)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(n, n), lowercase=False, vocabulary=vocabulary)\n",
    "n_gram_train_data = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "n_gram_train_data = n_gram_train_data.astype(float)\n",
    "\n",
    "for i, v in enumerate(train_texts):\n",
    "    n_gram_train_data[i] = n_gram_train_data[i] / len(train_texts[i])\n",
    "n_gram_test_data = vectorizer.transform(test_texts)\n",
    "n_gram_test_data = n_gram_test_data.astype(float)\n",
    "for i, v in enumerate(test_texts):\n",
    "    n_gram_test_data[i] = n_gram_test_data[i] / len(test_texts[i])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_ngrams = max_abs_scaler.fit_transform(n_gram_train_data)\n",
    "scaled_test_data_ngrams = max_abs_scaler.transform(n_gram_test_data)\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_words = max_abs_scaler.fit_transform(w2d.get_texts_vectorized_and_normalized(train_tokenized_indexed)[:, 1:])\n",
    "scaled_test_data_words = max_abs_scaler.transform(w2d.get_texts_vectorized_and_normalized(test_tokenized_indexed)[:, 1:])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(819, 7623)\n",
      "(561, 7623)\n",
      "7623\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train_data_words.shape)\n",
    "print(scaled_test_data_words.shape)\n",
    "print(len(w2d.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 9)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import layers, Input, callbacks\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "callbacks_list_neu = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_neu_ngrams = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu_ngrams.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_neu_words = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu_words.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_convnet = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=35,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_convnet.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=10,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_stacked = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_stacked.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "train_data = pad_sequences(train_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "test_data = pad_sequences(test_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "# X_scaled_train_data_words, X_scaled_val_data_words, _, _ = train_test_split(scaled_train_data_words, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "# X_scaled_train_data_ngrams, X_scaled_val_data_ngrams, _, _ = train_test_split(scaled_train_data_ngrams, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "\n",
    "# train_val_split_index = validation_start_index\n",
    "train_val_split_index = initial_train_size\n",
    "y_train, y_val = train_labels[:train_val_split_index], train_labels[train_val_split_index:]\n",
    "X_train, X_val = train_data[:train_val_split_index], train_data[train_val_split_index:]\n",
    "X_scaled_train_data_words, X_scaled_val_data_words = scaled_train_data_words[:train_val_split_index], scaled_train_data_words[train_val_split_index:]\n",
    "X_scaled_train_data_ngrams, X_scaled_val_data_ngrams = scaled_train_data_ngrams[:train_val_split_index], scaled_train_data_ngrams[train_val_split_index:]\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "# y_test = to_categorical(test_labels)\n",
    "# print(X_train.shape)\n",
    "\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                132896    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 139,753\n",
      "Trainable params: 139,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 5.5498 - acc: 0.2063 - val_loss: 5.5152 - val_acc: 0.1336\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.4848 - acc: 0.1429 - val_loss: 5.4601 - val_acc: 0.1376\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.4456 - acc: 0.1587 - val_loss: 5.4096 - val_acc: 0.1614\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.3945 - acc: 0.2222 - val_loss: 5.3623 - val_acc: 0.1931\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.3317 - acc: 0.1746 - val_loss: 5.3144 - val_acc: 0.2262\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.2978 - acc: 0.2222 - val_loss: 5.2675 - val_acc: 0.2553\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.2578 - acc: 0.1587 - val_loss: 5.2220 - val_acc: 0.2976\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.1953 - acc: 0.1905 - val_loss: 5.1760 - val_acc: 0.3519\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.1492 - acc: 0.2540 - val_loss: 5.1276 - val_acc: 0.4127\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.0874 - acc: 0.2222 - val_loss: 5.0797 - val_acc: 0.4061\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.0662 - acc: 0.2063 - val_loss: 5.0300 - val_acc: 0.3704\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.0502 - acc: 0.2540 - val_loss: 4.9804 - val_acc: 0.4101\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.0177 - acc: 0.2540 - val_loss: 4.9352 - val_acc: 0.4709\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.9205 - acc: 0.2698 - val_loss: 4.8893 - val_acc: 0.5172\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.9284 - acc: 0.2540 - val_loss: 4.8403 - val_acc: 0.5265\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.8452 - acc: 0.3175 - val_loss: 4.7916 - val_acc: 0.5582\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.8025 - acc: 0.2857 - val_loss: 4.7437 - val_acc: 0.6111\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.7494 - acc: 0.2381 - val_loss: 4.6906 - val_acc: 0.5952\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.7187 - acc: 0.3492 - val_loss: 4.6405 - val_acc: 0.6495\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6132 - acc: 0.4286 - val_loss: 4.5873 - val_acc: 0.6521\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6443 - acc: 0.2857 - val_loss: 4.5410 - val_acc: 0.6601\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6027 - acc: 0.3810 - val_loss: 4.4998 - val_acc: 0.6786\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.5792 - acc: 0.3333 - val_loss: 4.4537 - val_acc: 0.6772\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4885 - acc: 0.3810 - val_loss: 4.4086 - val_acc: 0.6825\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4494 - acc: 0.4444 - val_loss: 4.3641 - val_acc: 0.6852\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.5023 - acc: 0.4286 - val_loss: 4.3206 - val_acc: 0.6984\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4043 - acc: 0.3651 - val_loss: 4.2714 - val_acc: 0.7037\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.3332 - acc: 0.3968 - val_loss: 4.2242 - val_acc: 0.7063\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.3056 - acc: 0.4921 - val_loss: 4.1790 - val_acc: 0.7037\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2357 - acc: 0.4444 - val_loss: 4.1274 - val_acc: 0.7077\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.1672 - acc: 0.4286 - val_loss: 4.0792 - val_acc: 0.7103\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.1457 - acc: 0.5079 - val_loss: 4.0224 - val_acc: 0.7235\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.1033 - acc: 0.5238 - val_loss: 3.9698 - val_acc: 0.7315\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0996 - acc: 0.4286 - val_loss: 3.9241 - val_acc: 0.7302\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0503 - acc: 0.5238 - val_loss: 3.8787 - val_acc: 0.7513\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0216 - acc: 0.5238 - val_loss: 3.8312 - val_acc: 0.7540\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0625 - acc: 0.3968 - val_loss: 3.7871 - val_acc: 0.7646\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8907 - acc: 0.5556 - val_loss: 3.7453 - val_acc: 0.7844\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9873 - acc: 0.4603 - val_loss: 3.7031 - val_acc: 0.7857\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.7951 - acc: 0.5238 - val_loss: 3.6627 - val_acc: 0.7950\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8296 - acc: 0.5397 - val_loss: 3.6244 - val_acc: 0.8280\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.7559 - acc: 0.5714 - val_loss: 3.5836 - val_acc: 0.8638\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.7172 - acc: 0.6349 - val_loss: 3.5388 - val_acc: 0.9034\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6949 - acc: 0.5397 - val_loss: 3.4970 - val_acc: 0.9061\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6443 - acc: 0.6825 - val_loss: 3.4551 - val_acc: 0.8889\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.7568 - acc: 0.5397 - val_loss: 3.4196 - val_acc: 0.9034\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.5933 - acc: 0.6508 - val_loss: 3.3804 - val_acc: 0.9392\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6364 - acc: 0.5714 - val_loss: 3.3371 - val_acc: 0.9722\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6223 - acc: 0.5714 - val_loss: 3.3048 - val_acc: 0.9868\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.5934 - acc: 0.5714 - val_loss: 3.2760 - val_acc: 0.9921\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.5115 - acc: 0.6825 - val_loss: 3.2428 - val_acc: 0.9934\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4711 - acc: 0.7143 - val_loss: 3.2018 - val_acc: 0.9934\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4181 - acc: 0.6032 - val_loss: 3.1554 - val_acc: 0.9974\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4018 - acc: 0.6984 - val_loss: 3.1128 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4070 - acc: 0.6349 - val_loss: 3.0824 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3709 - acc: 0.6984 - val_loss: 3.0496 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3722 - acc: 0.7302 - val_loss: 3.0164 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3426 - acc: 0.6349 - val_loss: 2.9793 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3443 - acc: 0.6508 - val_loss: 2.9465 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2677 - acc: 0.6825 - val_loss: 2.9155 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2136 - acc: 0.6984 - val_loss: 2.8843 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1834 - acc: 0.6984 - val_loss: 2.8460 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2536 - acc: 0.6667 - val_loss: 2.8178 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1783 - acc: 0.6667 - val_loss: 2.7948 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1226 - acc: 0.7143 - val_loss: 2.7637 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9946 - acc: 0.7937 - val_loss: 2.7296 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0524 - acc: 0.7460 - val_loss: 2.6932 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1054 - acc: 0.7302 - val_loss: 2.6635 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0389 - acc: 0.8095 - val_loss: 2.6290 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0339 - acc: 0.6667 - val_loss: 2.6032 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0011 - acc: 0.7302 - val_loss: 2.5798 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9034 - acc: 0.7302 - val_loss: 2.5621 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8926 - acc: 0.7302 - val_loss: 2.5360 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8855 - acc: 0.8095 - val_loss: 2.5095 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0524 - acc: 0.6508 - val_loss: 2.4925 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8935 - acc: 0.7937 - val_loss: 2.4729 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8813 - acc: 0.7778 - val_loss: 2.4460 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8760 - acc: 0.7143 - val_loss: 2.4160 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7881 - acc: 0.7778 - val_loss: 2.3993 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7675 - acc: 0.7937 - val_loss: 2.3807 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7071 - acc: 0.8254 - val_loss: 2.3502 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7024 - acc: 0.7937 - val_loss: 2.3237 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7723 - acc: 0.7937 - val_loss: 2.3024 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6684 - acc: 0.8254 - val_loss: 2.2894 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7039 - acc: 0.7619 - val_loss: 2.2804 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5602 - acc: 0.8730 - val_loss: 2.2617 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7909 - acc: 0.7143 - val_loss: 2.2431 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5490 - acc: 0.8730 - val_loss: 2.2272 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8301 - acc: 0.7460 - val_loss: 2.2126 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6109 - acc: 0.7937 - val_loss: 2.1982 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5487 - acc: 0.8571 - val_loss: 2.1782 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6375 - acc: 0.8413 - val_loss: 2.1590 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5494 - acc: 0.8413 - val_loss: 2.1395 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5715 - acc: 0.8254 - val_loss: 2.1203 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5043 - acc: 0.8889 - val_loss: 2.1047 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4109 - acc: 0.8889 - val_loss: 2.0877 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4892 - acc: 0.8571 - val_loss: 2.0706 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5441 - acc: 0.8254 - val_loss: 2.0578 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4448 - acc: 0.9206 - val_loss: 2.0518 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5808 - acc: 0.8095 - val_loss: 2.0450 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3295 - acc: 0.9048 - val_loss: 2.0342 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5484 - acc: 0.8571 - val_loss: 2.0190 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3803 - acc: 0.8889 - val_loss: 2.0031 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4372 - acc: 0.8730 - val_loss: 1.9911 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4679 - acc: 0.7778 - val_loss: 1.9816 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3698 - acc: 0.8730 - val_loss: 1.9685 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3867 - acc: 0.8254 - val_loss: 1.9532 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3334 - acc: 0.9206 - val_loss: 1.9398 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2706 - acc: 0.8889 - val_loss: 1.9296 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3134 - acc: 0.8571 - val_loss: 1.9155 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2853 - acc: 0.8730 - val_loss: 1.9046 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2687 - acc: 0.9206 - val_loss: 1.8932 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3615 - acc: 0.8254 - val_loss: 1.8826 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2137 - acc: 0.9365 - val_loss: 1.8735 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3070 - acc: 0.8730 - val_loss: 1.8663 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2069 - acc: 0.8730 - val_loss: 1.8563 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2646 - acc: 0.8730 - val_loss: 1.8462 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1592 - acc: 0.8730 - val_loss: 1.8350 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1805 - acc: 0.8413 - val_loss: 1.8244 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1856 - acc: 0.9206 - val_loss: 1.8127 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1835 - acc: 0.9048 - val_loss: 1.8028 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1924 - acc: 0.8571 - val_loss: 1.7974 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2228 - acc: 0.7937 - val_loss: 1.7957 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1078 - acc: 0.8889 - val_loss: 1.7881 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1069 - acc: 0.8889 - val_loss: 1.7777 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1005 - acc: 0.8571 - val_loss: 1.7691 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1791 - acc: 0.8571 - val_loss: 1.7629 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1379 - acc: 0.8730 - val_loss: 1.7529 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1439 - acc: 0.9365 - val_loss: 1.7447 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1299 - acc: 0.8571 - val_loss: 1.7359 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0846 - acc: 0.8889 - val_loss: 1.7246 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0430 - acc: 0.9206 - val_loss: 1.7132 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0815 - acc: 0.9048 - val_loss: 1.7064 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9109 - acc: 0.9206 - val_loss: 1.6985 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0628 - acc: 0.8889 - val_loss: 1.6898 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0001 - acc: 0.8889 - val_loss: 1.6846 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9332 - acc: 0.9683 - val_loss: 1.6775 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9820 - acc: 0.9206 - val_loss: 1.6698 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9898 - acc: 0.9206 - val_loss: 1.6622 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0473 - acc: 0.9206 - val_loss: 1.6564 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9871 - acc: 0.9365 - val_loss: 1.6503 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9715 - acc: 0.9206 - val_loss: 1.6438 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9729 - acc: 0.9365 - val_loss: 1.6365 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9634 - acc: 0.8730 - val_loss: 1.6307 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9512 - acc: 0.9206 - val_loss: 1.6252 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8217 - acc: 0.9524 - val_loss: 1.6170 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8597 - acc: 0.9683 - val_loss: 1.6087 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0127 - acc: 0.8730 - val_loss: 1.6028 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8764 - acc: 0.9365 - val_loss: 1.5973 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8613 - acc: 0.9206 - val_loss: 1.5904 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8371 - acc: 0.9206 - val_loss: 1.5849 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8903 - acc: 0.9206 - val_loss: 1.5796 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8607 - acc: 0.9048 - val_loss: 1.5734 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8960 - acc: 0.8571 - val_loss: 1.5683 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8279 - acc: 0.9524 - val_loss: 1.5624 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7812 - acc: 0.9841 - val_loss: 1.5526 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8332 - acc: 0.9365 - val_loss: 1.5454 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9584 - acc: 0.8571 - val_loss: 1.5422 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8820 - acc: 0.9365 - val_loss: 1.5389 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8097 - acc: 0.8889 - val_loss: 1.5328 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8401 - acc: 0.9365 - val_loss: 1.5254 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9142 - acc: 0.8889 - val_loss: 1.5268 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7926 - acc: 0.9524 - val_loss: 1.5225 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7908 - acc: 0.9524 - val_loss: 1.5154 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7996 - acc: 0.9365 - val_loss: 1.5058 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7636 - acc: 0.9524 - val_loss: 1.4970 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7923 - acc: 0.9365 - val_loss: 1.4913 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7471 - acc: 0.9206 - val_loss: 1.4852 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6961 - acc: 0.9683 - val_loss: 1.4801 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7906 - acc: 0.8730 - val_loss: 1.4754 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8717 - acc: 0.8571 - val_loss: 1.4712 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7438 - acc: 0.9206 - val_loss: 1.4680 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7926 - acc: 0.9048 - val_loss: 1.4645 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8182 - acc: 0.9206 - val_loss: 1.4617 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7002 - acc: 0.9524 - val_loss: 1.4552 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7118 - acc: 0.9524 - val_loss: 1.4467 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7937 - acc: 0.8889 - val_loss: 1.4412 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7878 - acc: 0.9048 - val_loss: 1.4398 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7421 - acc: 0.9206 - val_loss: 1.4353 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6752 - acc: 0.9365 - val_loss: 1.4292 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6516 - acc: 0.9048 - val_loss: 1.4230 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6920 - acc: 0.9365 - val_loss: 1.4171 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6660 - acc: 0.9683 - val_loss: 1.4110 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6159 - acc: 0.9524 - val_loss: 1.4050 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6377 - acc: 0.9048 - val_loss: 1.4012 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7617 - acc: 0.9048 - val_loss: 1.3976 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6197 - acc: 0.9365 - val_loss: 1.3939 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7494 - acc: 0.8889 - val_loss: 1.3922 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7466 - acc: 0.9048 - val_loss: 1.3912 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5915 - acc: 0.9841 - val_loss: 1.3861 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7178 - acc: 0.8730 - val_loss: 1.3814 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6959 - acc: 0.8889 - val_loss: 1.3776 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6432 - acc: 0.9206 - val_loss: 1.3734 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6344 - acc: 0.9683 - val_loss: 1.3694 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6364 - acc: 0.9365 - val_loss: 1.3644 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6658 - acc: 0.9048 - val_loss: 1.3598 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6665 - acc: 0.9365 - val_loss: 1.3572 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5703 - acc: 0.9524 - val_loss: 1.3527 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5484 - acc: 1.0000 - val_loss: 1.3472 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5468 - acc: 0.9524 - val_loss: 1.3426 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5899 - acc: 0.9683 - val_loss: 1.3358 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4821 - acc: 0.9841 - val_loss: 1.3297 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4847 - acc: 0.9524 - val_loss: 1.3255 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6216 - acc: 0.9365 - val_loss: 1.3243 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6473 - acc: 0.9048 - val_loss: 1.3236 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5458 - acc: 0.9841 - val_loss: 1.3215 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5874 - acc: 0.8889 - val_loss: 1.3185 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5511 - acc: 0.9206 - val_loss: 1.3141 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5451 - acc: 0.9048 - val_loss: 1.3082 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5314 - acc: 0.9524 - val_loss: 1.3028 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5894 - acc: 0.9048 - val_loss: 1.2991 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4551 - acc: 0.9683 - val_loss: 1.2966 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4991 - acc: 0.9683 - val_loss: 1.2913 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5156 - acc: 0.9524 - val_loss: 1.2870 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6020 - acc: 0.9365 - val_loss: 1.2841 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5225 - acc: 0.9683 - val_loss: 1.2808 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5375 - acc: 0.9365 - val_loss: 1.2788 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5085 - acc: 0.9365 - val_loss: 1.2748 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5667 - acc: 0.9206 - val_loss: 1.2726 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4828 - acc: 0.9524 - val_loss: 1.2677 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4969 - acc: 0.9683 - val_loss: 1.2626 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4859 - acc: 0.9683 - val_loss: 1.2583 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5384 - acc: 0.9365 - val_loss: 1.2551 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4912 - acc: 0.9365 - val_loss: 1.2548 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5641 - acc: 0.9206 - val_loss: 1.2528 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4828 - acc: 0.9683 - val_loss: 1.2495 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4849 - acc: 0.9524 - val_loss: 1.2482 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4508 - acc: 0.9683 - val_loss: 1.2451 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4417 - acc: 0.9524 - val_loss: 1.2388 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5291 - acc: 0.9048 - val_loss: 1.2348 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4223 - acc: 0.9841 - val_loss: 1.2305 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4902 - acc: 0.9365 - val_loss: 1.2268 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4102 - acc: 0.9206 - val_loss: 1.2267 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4578 - acc: 0.9206 - val_loss: 1.2234 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5089 - acc: 0.9048 - val_loss: 1.2193 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4705 - acc: 0.9683 - val_loss: 1.2148 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3778 - acc: 0.9841 - val_loss: 1.2109 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4047 - acc: 0.9365 - val_loss: 1.2087 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4036 - acc: 0.9841 - val_loss: 1.2063 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4038 - acc: 0.9524 - val_loss: 1.2043 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5480 - acc: 0.8889 - val_loss: 1.2022 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5003 - acc: 0.8889 - val_loss: 1.2026 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3690 - acc: 0.9841 - val_loss: 1.1999 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3764 - acc: 0.9841 - val_loss: 1.1937 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4215 - acc: 0.9683 - val_loss: 1.1903 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4243 - acc: 0.9524 - val_loss: 1.1885 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4793 - acc: 0.9048 - val_loss: 1.1870 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4295 - acc: 0.9206 - val_loss: 1.1863 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4230 - acc: 0.9683 - val_loss: 1.1833 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4916 - acc: 0.9206 - val_loss: 1.1829 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4389 - acc: 0.9206 - val_loss: 1.1818 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4186 - acc: 0.9365 - val_loss: 1.1810 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4283 - acc: 0.9365 - val_loss: 1.1766 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3910 - acc: 0.9365 - val_loss: 1.1739 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3935 - acc: 0.9524 - val_loss: 1.1690 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3672 - acc: 0.9524 - val_loss: 1.1661 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4133 - acc: 0.9365 - val_loss: 1.1638 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4429 - acc: 0.9365 - val_loss: 1.1636 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3941 - acc: 0.9683 - val_loss: 1.1634 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3386 - acc: 0.9841 - val_loss: 1.1592 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4729 - acc: 0.9048 - val_loss: 1.1577 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3236 - acc: 0.9683 - val_loss: 1.1555 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4432 - acc: 0.9365 - val_loss: 1.1526 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4852 - acc: 0.9206 - val_loss: 1.1516 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3005 - acc: 0.9841 - val_loss: 1.1479 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3446 - acc: 0.9524 - val_loss: 1.1428 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3926 - acc: 0.9048 - val_loss: 1.1399 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3159 - acc: 0.9841 - val_loss: 1.1378 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4269 - acc: 0.9365 - val_loss: 1.1376 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3545 - acc: 0.9524 - val_loss: 1.1354 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4014 - acc: 0.9365 - val_loss: 1.1330 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3735 - acc: 0.9206 - val_loss: 1.1327 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2871 - acc: 0.9841 - val_loss: 1.1308 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4304 - acc: 0.9048 - val_loss: 1.1300 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4070 - acc: 0.9206 - val_loss: 1.1293 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3677 - acc: 0.9206 - val_loss: 1.1258 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2624 - acc: 1.0000 - val_loss: 1.1213 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2927 - acc: 0.9841 - val_loss: 1.1171 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2856 - acc: 0.9365 - val_loss: 1.1129 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3365 - acc: 0.9524 - val_loss: 1.1119 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3043 - acc: 0.9683 - val_loss: 1.1112 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3155 - acc: 0.9524 - val_loss: 1.1088 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4036 - acc: 0.9365 - val_loss: 1.1106 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3505 - acc: 0.9524 - val_loss: 1.1112 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3128 - acc: 1.0000 - val_loss: 1.1081 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2214 - acc: 1.0000 - val_loss: 1.1028 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3188 - acc: 0.9524 - val_loss: 1.1000 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3200 - acc: 0.9365 - val_loss: 1.0987 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2260 - acc: 0.9841 - val_loss: 1.0949 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2918 - acc: 0.9841 - val_loss: 1.0919 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3175 - acc: 0.9524 - val_loss: 1.0909 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3114 - acc: 0.9365 - val_loss: 1.0896 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3305 - acc: 0.9365 - val_loss: 1.0880 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3228 - acc: 0.9524 - val_loss: 1.0899 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3153 - acc: 0.9841 - val_loss: 1.0890 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3149 - acc: 0.9365 - val_loss: 1.0855 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4186 - acc: 0.8889 - val_loss: 1.0854 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3283 - acc: 0.9206 - val_loss: 1.0891 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3008 - acc: 0.9683 - val_loss: 1.0867 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3309 - acc: 0.9365 - val_loss: 1.0819 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3027 - acc: 0.9841 - val_loss: 1.0779 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2784 - acc: 0.9524 - val_loss: 1.0745 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2127 - acc: 0.9841 - val_loss: 1.0700 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2701 - acc: 0.9841 - val_loss: 1.0674 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2490 - acc: 0.9841 - val_loss: 1.0684 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2895 - acc: 0.9683 - val_loss: 1.0678 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2377 - acc: 0.9524 - val_loss: 1.0639 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2117 - acc: 0.9683 - val_loss: 1.0601 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2856 - acc: 0.9524 - val_loss: 1.0593 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2121 - acc: 0.9683 - val_loss: 1.0620 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2858 - acc: 0.9524 - val_loss: 1.0597 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1732 - acc: 1.0000 - val_loss: 1.0581 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2540 - acc: 0.9683 - val_loss: 1.0561 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4074 - acc: 0.8889 - val_loss: 1.0556 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2121 - acc: 0.9841 - val_loss: 1.0553 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2097 - acc: 0.9841 - val_loss: 1.0526 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2385 - acc: 0.9841 - val_loss: 1.0508 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2683 - acc: 0.9524 - val_loss: 1.0507 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2242 - acc: 0.9683 - val_loss: 1.0487 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2342 - acc: 0.9683 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2318 - acc: 0.9683 - val_loss: 1.0446 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2612 - acc: 0.9524 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2341 - acc: 0.9683 - val_loss: 1.0433 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3237 - acc: 0.8889 - val_loss: 1.0453 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2596 - acc: 0.9524 - val_loss: 1.0443 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1700 - acc: 0.9841 - val_loss: 1.0393 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1719 - acc: 0.9841 - val_loss: 1.0339 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1791 - acc: 0.9841 - val_loss: 1.0292 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2537 - acc: 0.9206 - val_loss: 1.0302 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2370 - acc: 0.9683 - val_loss: 1.0312 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1921 - acc: 0.9841 - val_loss: 1.0297 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2534 - acc: 0.9841 - val_loss: 1.0276 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2218 - acc: 0.9365 - val_loss: 1.0266 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1877 - acc: 0.9841 - val_loss: 1.0272 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2129 - acc: 0.9683 - val_loss: 1.0268 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2157 - acc: 0.9524 - val_loss: 1.0230 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2418 - acc: 0.9365 - val_loss: 1.0212 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1987 - acc: 0.9524 - val_loss: 1.0216 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3289 - acc: 0.9206 - val_loss: 1.0199 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1405 - acc: 0.9841 - val_loss: 1.0196 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2012 - acc: 0.9683 - val_loss: 1.0171 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2118 - acc: 0.9683 - val_loss: 1.0146 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1292 - acc: 1.0000 - val_loss: 1.0114 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2130 - acc: 0.9524 - val_loss: 1.0099 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2146 - acc: 0.9524 - val_loss: 1.0111 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1823 - acc: 0.9683 - val_loss: 1.0092 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1547 - acc: 0.9683 - val_loss: 1.0076 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2727 - acc: 0.9365 - val_loss: 1.0097 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1473 - acc: 0.9841 - val_loss: 1.0084 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2302 - acc: 0.9524 - val_loss: 1.0055 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1592 - acc: 0.9841 - val_loss: 1.0052 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1626 - acc: 0.9524 - val_loss: 1.0020 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1623 - acc: 0.9683 - val_loss: 1.0005 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2103 - acc: 0.9524 - val_loss: 0.9989 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1117 - acc: 0.9841 - val_loss: 0.9958 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2047 - acc: 0.9365 - val_loss: 0.9922 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1812 - acc: 0.9683 - val_loss: 0.9931 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1110 - acc: 0.9683 - val_loss: 0.9952 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2078 - acc: 0.9365 - val_loss: 0.9942 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2028 - acc: 0.9524 - val_loss: 0.9928 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1638 - acc: 0.9683 - val_loss: 0.9921 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0982 - acc: 0.9841 - val_loss: 0.9892 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1494 - acc: 0.9524 - val_loss: 0.9867 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0990 - acc: 0.9683 - val_loss: 0.9891 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2218 - acc: 0.9524 - val_loss: 0.9881 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1504 - acc: 1.0000 - val_loss: 0.9867 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1686 - acc: 0.9524 - val_loss: 0.9847 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1341 - acc: 0.9841 - val_loss: 0.9832 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1955 - acc: 0.9365 - val_loss: 0.9832 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1630 - acc: 0.9841 - val_loss: 0.9833 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0701 - acc: 1.0000 - val_loss: 0.9802 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1119 - acc: 0.9841 - val_loss: 0.9751 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1592 - acc: 0.9683 - val_loss: 0.9747 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1009 - acc: 0.9841 - val_loss: 0.9727 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1595 - acc: 0.9683 - val_loss: 0.9708 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1996 - acc: 0.9365 - val_loss: 0.9750 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2264 - acc: 0.9365 - val_loss: 0.9759 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2335 - acc: 0.9683 - val_loss: 0.9742 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1256 - acc: 0.9841 - val_loss: 0.9733 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2226 - acc: 0.9365 - val_loss: 0.9737 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1118 - acc: 0.9683 - val_loss: 0.9736 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1803 - acc: 0.9206 - val_loss: 0.9711 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1333 - acc: 0.9683 - val_loss: 0.9709 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1513 - acc: 0.9524 - val_loss: 0.9680 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0910 - acc: 0.9683 - val_loss: 0.9667 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1367 - acc: 0.9683 - val_loss: 0.9646 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1046 - acc: 0.9841 - val_loss: 0.9615 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0598 - acc: 0.9841 - val_loss: 0.9576 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1346 - acc: 0.9683 - val_loss: 0.9579 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1160 - acc: 1.0000 - val_loss: 0.9570 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1504 - acc: 0.9683 - val_loss: 0.9569 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1801 - acc: 0.9365 - val_loss: 0.9566 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1531 - acc: 0.9683 - val_loss: 0.9553 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0878 - acc: 0.9683 - val_loss: 0.9545 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1152 - acc: 0.9683 - val_loss: 0.9535 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1646 - acc: 0.9524 - val_loss: 0.9532 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1417 - acc: 0.9524 - val_loss: 0.9536 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0941 - acc: 1.0000 - val_loss: 0.9532 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1947 - acc: 0.9683 - val_loss: 0.9511 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1057 - acc: 0.9841 - val_loss: 0.9500 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1301 - acc: 0.9365 - val_loss: 0.9498 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0445 - acc: 1.0000 - val_loss: 0.9487 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1184 - acc: 0.9683 - val_loss: 0.9456 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0893 - acc: 0.9841 - val_loss: 0.9416 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1082 - acc: 0.9524 - val_loss: 0.9395 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0782 - acc: 0.9841 - val_loss: 0.9374 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1481 - acc: 0.9206 - val_loss: 0.9367 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2211 - acc: 0.9206 - val_loss: 0.9424 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0391 - acc: 0.9841 - val_loss: 0.9446 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0477 - acc: 1.0000 - val_loss: 0.9402 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1037 - acc: 0.9683 - val_loss: 0.9371 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0914 - acc: 1.0000 - val_loss: 0.9362 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0604 - acc: 1.0000 - val_loss: 0.9340 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0566 - acc: 0.9683 - val_loss: 0.9320 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1310 - acc: 0.9524 - val_loss: 0.9311 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0920 - acc: 0.9683 - val_loss: 0.9310 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0654 - acc: 0.9683 - val_loss: 0.9299 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0943 - acc: 0.9841 - val_loss: 0.9292 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0483 - acc: 0.9841 - val_loss: 0.9268 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1408 - acc: 0.9524 - val_loss: 0.9266 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0389 - acc: 1.0000 - val_loss: 0.9254 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0692 - acc: 1.0000 - val_loss: 0.9237 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1371 - acc: 0.9206 - val_loss: 0.9238 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0666 - acc: 0.9841 - val_loss: 0.9241 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0740 - acc: 0.9365 - val_loss: 0.9235 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0807 - acc: 0.9524 - val_loss: 0.9232 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0636 - acc: 1.0000 - val_loss: 0.9198 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0417 - acc: 0.9841 - val_loss: 0.9182 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0824 - acc: 0.9683 - val_loss: 0.9182 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0099 - acc: 1.0000 - val_loss: 0.9161 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0836 - acc: 0.9683 - val_loss: 0.9155 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0758 - acc: 0.9841 - val_loss: 0.9161 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0498 - acc: 0.9683 - val_loss: 0.9152 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1761 - acc: 0.9206 - val_loss: 0.9160 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0492 - acc: 1.0000 - val_loss: 0.9175 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0760 - acc: 0.9683 - val_loss: 0.9145 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0940 - acc: 0.9841 - val_loss: 0.9151 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0368 - acc: 0.9841 - val_loss: 0.9141 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1047 - acc: 0.9524 - val_loss: 0.9135 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0789 - acc: 1.0000 - val_loss: 0.9121 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0483 - acc: 0.9841 - val_loss: 0.9114 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0157 - acc: 0.9841 - val_loss: 0.9086 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1376 - acc: 0.9683 - val_loss: 0.9081 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0651 - acc: 0.9841 - val_loss: 0.9094 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1027 - acc: 0.9683 - val_loss: 0.9084 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0485 - acc: 0.9683 - val_loss: 0.9076 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0450 - acc: 0.9524 - val_loss: 0.9067 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0305 - acc: 0.9683 - val_loss: 0.9067 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0442 - acc: 0.9841 - val_loss: 0.9040 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0353 - acc: 1.0000 - val_loss: 0.9022 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9892 - acc: 1.0000 - val_loss: 0.8987 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0732 - acc: 0.9524 - val_loss: 0.8970 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0593 - acc: 0.9683 - val_loss: 0.8976 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0488 - acc: 0.9841 - val_loss: 0.8977 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0487 - acc: 0.9683 - val_loss: 0.8978 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1031 - acc: 0.9683 - val_loss: 0.8986 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0503 - acc: 0.9683 - val_loss: 0.9019 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0469 - acc: 0.9683 - val_loss: 0.9013 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1374 - acc: 0.9524 - val_loss: 0.9021 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1780 - acc: 0.9365 - val_loss: 0.9025 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0562 - acc: 0.9683 - val_loss: 0.9016 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0700 - acc: 0.9524 - val_loss: 0.8996 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0039 - acc: 0.9841 - val_loss: 0.8961 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0307 - acc: 0.9524 - val_loss: 0.8921 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0592 - acc: 0.9524 - val_loss: 0.8901 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0644 - acc: 0.9683 - val_loss: 0.8902 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0825 - acc: 0.9365 - val_loss: 0.8910 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0578 - acc: 0.9683 - val_loss: 0.8908 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0442 - acc: 0.9683 - val_loss: 0.8909 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0305 - acc: 0.9683 - val_loss: 0.8909 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9955 - acc: 0.9683 - val_loss: 0.8886 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9758 - acc: 0.9841 - val_loss: 0.8847 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0100 - acc: 0.9841 - val_loss: 0.8829 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0753 - acc: 0.9683 - val_loss: 0.8824 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0293 - acc: 0.9524 - val_loss: 0.8836 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9895 - acc: 0.9841 - val_loss: 0.8836 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1175 - acc: 0.9524 - val_loss: 0.8815 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0013 - acc: 0.9841 - val_loss: 0.8795 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0759 - acc: 0.9683 - val_loss: 0.8794 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0964 - acc: 0.9524 - val_loss: 0.8815 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0017 - acc: 1.0000 - val_loss: 0.8820 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0004 - acc: 1.0000 - val_loss: 0.8787 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0587 - acc: 0.9524 - val_loss: 0.8761 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9646 - acc: 1.0000 - val_loss: 0.8746 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0102 - acc: 0.9841 - val_loss: 0.8726 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9399 - acc: 1.0000 - val_loss: 0.8723 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0172 - acc: 0.9524 - val_loss: 0.8708 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0204 - acc: 0.9841 - val_loss: 0.8684 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0243 - acc: 1.0000 - val_loss: 0.8703 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0101 - acc: 0.9841 - val_loss: 0.8709 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9768 - acc: 0.9683 - val_loss: 0.8684 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9353 - acc: 0.9841 - val_loss: 0.8666 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9767 - acc: 0.9683 - val_loss: 0.8646 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9569 - acc: 1.0000 - val_loss: 0.8639 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9937 - acc: 0.9841 - val_loss: 0.8632 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9860 - acc: 0.9841 - val_loss: 0.8621 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9812 - acc: 0.9683 - val_loss: 0.8618 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1286 - acc: 0.9524 - val_loss: 0.8671 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0697 - acc: 0.9524 - val_loss: 0.8731 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0744 - acc: 0.9524 - val_loss: 0.8754 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu_ngrams Test Accuracy: 0.742\n",
      "my_model_neu Test f-measure: 0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "neu_ng = Sequential()\n",
    "neu_ng.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "neu_ng.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "neu_ng.summary()\n",
    "history = neu_ng.fit(X_scaled_train_data_ngrams, y_train,\n",
    "                    validation_data=(X_scaled_val_data_ngrams, y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu_ngrams,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu_ngrams.h5')\n",
    "yhat = l_model.predict(scaled_test_data_ngrams)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_ngrams Test Accuracy: %.3f' % acc)\n",
    "\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_neu_ngrams Test Accuracy: 0.825\n"
     ]
    }
   ],
   "source": [
    "l_model = load_model('my_model_neu_ngrams.h5')\n",
    "yhat = l_model.predict(scaled_test_data_ngrams)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_ngrams Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 32)                243968    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 250,825\n",
      "Trainable params: 250,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 6.3986 - acc: 0.1587 - val_loss: 6.2980 - val_acc: 0.1323\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 6.2391 - acc: 0.0952 - val_loss: 6.1583 - val_acc: 0.1389\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 6.0852 - acc: 0.1905 - val_loss: 6.0325 - val_acc: 0.1627\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.9844 - acc: 0.1111 - val_loss: 5.9146 - val_acc: 0.1772\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.8485 - acc: 0.1587 - val_loss: 5.8025 - val_acc: 0.2090\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.7513 - acc: 0.1746 - val_loss: 5.6946 - val_acc: 0.2249\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.6576 - acc: 0.2381 - val_loss: 5.5911 - val_acc: 0.2434\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.5296 - acc: 0.2063 - val_loss: 5.4917 - val_acc: 0.2606\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.4349 - acc: 0.2857 - val_loss: 5.3950 - val_acc: 0.2910\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.3159 - acc: 0.3175 - val_loss: 5.3010 - val_acc: 0.3228\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.2718 - acc: 0.2698 - val_loss: 5.2095 - val_acc: 0.3466\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.1732 - acc: 0.2540 - val_loss: 5.1236 - val_acc: 0.3651\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.1174 - acc: 0.2540 - val_loss: 5.0426 - val_acc: 0.3862\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.0138 - acc: 0.3016 - val_loss: 4.9627 - val_acc: 0.3968\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.9436 - acc: 0.2698 - val_loss: 4.8842 - val_acc: 0.4074\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.8511 - acc: 0.2857 - val_loss: 4.8079 - val_acc: 0.4206\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.7632 - acc: 0.2698 - val_loss: 4.7356 - val_acc: 0.4272\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.7007 - acc: 0.2540 - val_loss: 4.6628 - val_acc: 0.4392\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.6431 - acc: 0.3175 - val_loss: 4.5930 - val_acc: 0.4696\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.5237 - acc: 0.3968 - val_loss: 4.5221 - val_acc: 0.4749\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.5107 - acc: 0.3175 - val_loss: 4.4540 - val_acc: 0.4921\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.4378 - acc: 0.3333 - val_loss: 4.3904 - val_acc: 0.5198\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.4045 - acc: 0.3651 - val_loss: 4.3273 - val_acc: 0.5000\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.3294 - acc: 0.3810 - val_loss: 4.2667 - val_acc: 0.5212\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.2980 - acc: 0.3175 - val_loss: 4.2087 - val_acc: 0.5450\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.3105 - acc: 0.3810 - val_loss: 4.1531 - val_acc: 0.5622\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.2022 - acc: 0.3968 - val_loss: 4.0998 - val_acc: 0.5714\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.1465 - acc: 0.3492 - val_loss: 4.0433 - val_acc: 0.6005\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.1152 - acc: 0.4127 - val_loss: 3.9890 - val_acc: 0.6349\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.0256 - acc: 0.5238 - val_loss: 3.9341 - val_acc: 0.6587\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.9130 - acc: 0.5238 - val_loss: 3.8812 - val_acc: 0.6812\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.9183 - acc: 0.4603 - val_loss: 3.8273 - val_acc: 0.6746\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.9072 - acc: 0.4444 - val_loss: 3.7753 - val_acc: 0.6733\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.8258 - acc: 0.5873 - val_loss: 3.7245 - val_acc: 0.7130\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.7449 - acc: 0.5556 - val_loss: 3.6715 - val_acc: 0.7579\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.7746 - acc: 0.5397 - val_loss: 3.6203 - val_acc: 0.8188\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.7584 - acc: 0.5238 - val_loss: 3.5706 - val_acc: 0.8360\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6679 - acc: 0.5397 - val_loss: 3.5212 - val_acc: 0.8386\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6649 - acc: 0.5556 - val_loss: 3.4719 - val_acc: 0.8611\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5153 - acc: 0.5873 - val_loss: 3.4239 - val_acc: 0.8836\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5729 - acc: 0.5873 - val_loss: 3.3783 - val_acc: 0.8902\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4445 - acc: 0.6508 - val_loss: 3.3312 - val_acc: 0.8995\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5281 - acc: 0.5397 - val_loss: 3.2837 - val_acc: 0.8995\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3344 - acc: 0.6984 - val_loss: 3.2380 - val_acc: 0.9034\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3909 - acc: 0.6349 - val_loss: 3.1949 - val_acc: 0.9034\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3919 - acc: 0.6032 - val_loss: 3.1497 - val_acc: 0.9048\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.2802 - acc: 0.6984 - val_loss: 3.1054 - val_acc: 0.9034\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3096 - acc: 0.5873 - val_loss: 3.0594 - val_acc: 0.9101\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.2459 - acc: 0.6984 - val_loss: 3.0173 - val_acc: 0.9140\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.2097 - acc: 0.6190 - val_loss: 2.9755 - val_acc: 0.9140\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.2076 - acc: 0.6825 - val_loss: 2.9362 - val_acc: 0.9193\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1607 - acc: 0.6825 - val_loss: 2.8961 - val_acc: 0.9193\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1314 - acc: 0.5873 - val_loss: 2.8581 - val_acc: 0.9233\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0231 - acc: 0.6667 - val_loss: 2.8179 - val_acc: 0.9405\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0626 - acc: 0.6667 - val_loss: 2.7779 - val_acc: 0.9511\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0262 - acc: 0.6984 - val_loss: 2.7395 - val_acc: 0.9603\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0455 - acc: 0.7143 - val_loss: 2.7041 - val_acc: 0.9630\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.0089 - acc: 0.6508 - val_loss: 2.6686 - val_acc: 0.9669\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9034 - acc: 0.6667 - val_loss: 2.6322 - val_acc: 0.9656\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9057 - acc: 0.6984 - val_loss: 2.5966 - val_acc: 0.9656\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.8726 - acc: 0.7143 - val_loss: 2.5589 - val_acc: 0.9683\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.8577 - acc: 0.6984 - val_loss: 2.5215 - val_acc: 0.9775\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.8829 - acc: 0.6984 - val_loss: 2.4900 - val_acc: 0.9881\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.8204 - acc: 0.6508 - val_loss: 2.4624 - val_acc: 0.9934\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7638 - acc: 0.7619 - val_loss: 2.4311 - val_acc: 0.9960\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7383 - acc: 0.6984 - val_loss: 2.3997 - val_acc: 0.9960\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7112 - acc: 0.7619 - val_loss: 2.3684 - val_acc: 0.9960\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6324 - acc: 0.7937 - val_loss: 2.3377 - val_acc: 0.9947\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7106 - acc: 0.7302 - val_loss: 2.3074 - val_acc: 0.9947\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6548 - acc: 0.7143 - val_loss: 2.2779 - val_acc: 0.9974\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6297 - acc: 0.7302 - val_loss: 2.2520 - val_acc: 0.9974\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5834 - acc: 0.7143 - val_loss: 2.2273 - val_acc: 0.9987\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5052 - acc: 0.8254 - val_loss: 2.2022 - val_acc: 0.9987\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5452 - acc: 0.7460 - val_loss: 2.1778 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6205 - acc: 0.6825 - val_loss: 2.1548 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5178 - acc: 0.7619 - val_loss: 2.1315 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4852 - acc: 0.7619 - val_loss: 2.1077 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4821 - acc: 0.7619 - val_loss: 2.0831 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3723 - acc: 0.7778 - val_loss: 2.0607 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4273 - acc: 0.8254 - val_loss: 2.0377 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3119 - acc: 0.8571 - val_loss: 2.0128 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3025 - acc: 0.8413 - val_loss: 1.9912 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3778 - acc: 0.8571 - val_loss: 1.9702 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2251 - acc: 0.8571 - val_loss: 1.9517 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2766 - acc: 0.8571 - val_loss: 1.9332 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1314 - acc: 0.9365 - val_loss: 1.9124 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3660 - acc: 0.8095 - val_loss: 1.8953 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1904 - acc: 0.8413 - val_loss: 1.8796 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4157 - acc: 0.8095 - val_loss: 1.8635 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1732 - acc: 0.8730 - val_loss: 1.8483 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1490 - acc: 0.8730 - val_loss: 1.8312 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2076 - acc: 0.8730 - val_loss: 1.8135 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1345 - acc: 0.8413 - val_loss: 1.7981 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2008 - acc: 0.8730 - val_loss: 1.7821 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1330 - acc: 0.8095 - val_loss: 1.7681 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1266 - acc: 0.8095 - val_loss: 1.7545 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1064 - acc: 0.9048 - val_loss: 1.7398 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2054 - acc: 0.8095 - val_loss: 1.7275 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1399 - acc: 0.8889 - val_loss: 1.7173 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1444 - acc: 0.7619 - val_loss: 1.7044 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9916 - acc: 0.9048 - val_loss: 1.6902 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1850 - acc: 0.8730 - val_loss: 1.6782 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0296 - acc: 0.8730 - val_loss: 1.6668 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0605 - acc: 0.8889 - val_loss: 1.6558 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1292 - acc: 0.8095 - val_loss: 1.6469 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0145 - acc: 0.8730 - val_loss: 1.6374 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0503 - acc: 0.8095 - val_loss: 1.6277 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0445 - acc: 0.8571 - val_loss: 1.6190 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8690 - acc: 0.9683 - val_loss: 1.6093 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9308 - acc: 0.9206 - val_loss: 1.5949 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9109 - acc: 0.8889 - val_loss: 1.5840 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9487 - acc: 0.8730 - val_loss: 1.5761 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9944 - acc: 0.8889 - val_loss: 1.5700 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8892 - acc: 0.9206 - val_loss: 1.5632 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9263 - acc: 0.9048 - val_loss: 1.5547 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8725 - acc: 0.9048 - val_loss: 1.5448 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9183 - acc: 0.8413 - val_loss: 1.5373 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8914 - acc: 0.9048 - val_loss: 1.5284 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7517 - acc: 0.9206 - val_loss: 1.5176 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8576 - acc: 0.9048 - val_loss: 1.5068 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8896 - acc: 0.8730 - val_loss: 1.5000 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8241 - acc: 0.8730 - val_loss: 1.4941 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9644 - acc: 0.8254 - val_loss: 1.4920 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8063 - acc: 0.9206 - val_loss: 1.4899 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7460 - acc: 0.8889 - val_loss: 1.4807 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7642 - acc: 0.9048 - val_loss: 1.4734 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8772 - acc: 0.8254 - val_loss: 1.4700 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8844 - acc: 0.9206 - val_loss: 1.4646 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7665 - acc: 0.9365 - val_loss: 1.4569 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8147 - acc: 0.9365 - val_loss: 1.4486 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7524 - acc: 0.9524 - val_loss: 1.4419 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7453 - acc: 0.9524 - val_loss: 1.4346 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7837 - acc: 0.9048 - val_loss: 1.4281 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6545 - acc: 0.9206 - val_loss: 1.4225 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7808 - acc: 0.9048 - val_loss: 1.4178 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7501 - acc: 0.9206 - val_loss: 1.4159 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6609 - acc: 0.9683 - val_loss: 1.4080 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7004 - acc: 0.9206 - val_loss: 1.3982 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7053 - acc: 0.9524 - val_loss: 1.3922 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7166 - acc: 0.9365 - val_loss: 1.3872 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6822 - acc: 0.9048 - val_loss: 1.3822 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6808 - acc: 0.9365 - val_loss: 1.3768 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7130 - acc: 0.9048 - val_loss: 1.3727 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6537 - acc: 0.8889 - val_loss: 1.3684 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6988 - acc: 0.9365 - val_loss: 1.3657 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5584 - acc: 0.9841 - val_loss: 1.3610 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6368 - acc: 0.9048 - val_loss: 1.3538 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6684 - acc: 0.9206 - val_loss: 1.3489 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6676 - acc: 0.9048 - val_loss: 1.3474 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6290 - acc: 0.9206 - val_loss: 1.3442 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5715 - acc: 0.9683 - val_loss: 1.3399 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5863 - acc: 0.9683 - val_loss: 1.3334 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6592 - acc: 0.8889 - val_loss: 1.3281 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6505 - acc: 0.9206 - val_loss: 1.3263 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5930 - acc: 0.9365 - val_loss: 1.3254 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5714 - acc: 0.9365 - val_loss: 1.3193 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5685 - acc: 0.9365 - val_loss: 1.3125 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6752 - acc: 0.9048 - val_loss: 1.3092 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6345 - acc: 0.9365 - val_loss: 1.3098 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4876 - acc: 0.9683 - val_loss: 1.3054 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6186 - acc: 0.9206 - val_loss: 1.2979 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6281 - acc: 0.9048 - val_loss: 1.2951 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5966 - acc: 0.9524 - val_loss: 1.2907 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5640 - acc: 0.9048 - val_loss: 1.2889 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5345 - acc: 0.9683 - val_loss: 1.2866 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5460 - acc: 0.9524 - val_loss: 1.2822 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5559 - acc: 0.9524 - val_loss: 1.2776 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5817 - acc: 0.9206 - val_loss: 1.2735 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4838 - acc: 0.9683 - val_loss: 1.2706 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5460 - acc: 0.9524 - val_loss: 1.2665 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5866 - acc: 0.9206 - val_loss: 1.2643 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5549 - acc: 0.9365 - val_loss: 1.2628 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5353 - acc: 0.9206 - val_loss: 1.2605 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6645 - acc: 0.8571 - val_loss: 1.2599 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5000 - acc: 0.9365 - val_loss: 1.2570 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5448 - acc: 0.9206 - val_loss: 1.2525 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5229 - acc: 0.9206 - val_loss: 1.2509 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6149 - acc: 0.9048 - val_loss: 1.2535 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5948 - acc: 0.8889 - val_loss: 1.2536 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4802 - acc: 0.9683 - val_loss: 1.2497 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4275 - acc: 0.9841 - val_loss: 1.2432 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4603 - acc: 0.9524 - val_loss: 1.2357 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5091 - acc: 0.9206 - val_loss: 1.2302 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5114 - acc: 0.9365 - val_loss: 1.2269 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4387 - acc: 0.9524 - val_loss: 1.2241 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4992 - acc: 0.9206 - val_loss: 1.2207 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4086 - acc: 0.9365 - val_loss: 1.2178 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5009 - acc: 0.9206 - val_loss: 1.2159 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6086 - acc: 0.8413 - val_loss: 1.2188 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4545 - acc: 0.9365 - val_loss: 1.2177 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5211 - acc: 0.9048 - val_loss: 1.2154 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5114 - acc: 0.9048 - val_loss: 1.2122 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4809 - acc: 0.9365 - val_loss: 1.2095 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5158 - acc: 0.9683 - val_loss: 1.2068 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4647 - acc: 0.9365 - val_loss: 1.2027 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5388 - acc: 0.9524 - val_loss: 1.2004 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5202 - acc: 0.9365 - val_loss: 1.1990 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3945 - acc: 0.9683 - val_loss: 1.1948 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5045 - acc: 0.9048 - val_loss: 1.1938 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4195 - acc: 0.9683 - val_loss: 1.1935 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4652 - acc: 0.9683 - val_loss: 1.1901 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3245 - acc: 0.9841 - val_loss: 1.1838 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3733 - acc: 0.9524 - val_loss: 1.1810 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4542 - acc: 0.9365 - val_loss: 1.1778 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4664 - acc: 0.9206 - val_loss: 1.1760 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3783 - acc: 0.9683 - val_loss: 1.1744 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3959 - acc: 0.9683 - val_loss: 1.1721 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4182 - acc: 0.9841 - val_loss: 1.1686 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3971 - acc: 0.9524 - val_loss: 1.1657 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4115 - acc: 0.9365 - val_loss: 1.1622 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4780 - acc: 0.9048 - val_loss: 1.1590 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3318 - acc: 0.9365 - val_loss: 1.1592 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3997 - acc: 0.9524 - val_loss: 1.1565 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3880 - acc: 0.9365 - val_loss: 1.1543 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5128 - acc: 0.8889 - val_loss: 1.1537 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3641 - acc: 0.9683 - val_loss: 1.1521 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3899 - acc: 0.9683 - val_loss: 1.1512 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4270 - acc: 0.9365 - val_loss: 1.1487 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3898 - acc: 0.9683 - val_loss: 1.1474 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3841 - acc: 0.9206 - val_loss: 1.1432 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3791 - acc: 0.9683 - val_loss: 1.1427 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3473 - acc: 0.9524 - val_loss: 1.1408 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4409 - acc: 0.9365 - val_loss: 1.1382 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3759 - acc: 0.9841 - val_loss: 1.1390 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4425 - acc: 0.9365 - val_loss: 1.1379 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3833 - acc: 0.9206 - val_loss: 1.1351 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3967 - acc: 0.9206 - val_loss: 1.1324 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3187 - acc: 0.9841 - val_loss: 1.1301 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3851 - acc: 0.9841 - val_loss: 1.1277 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4709 - acc: 0.8889 - val_loss: 1.1316 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3279 - acc: 0.9841 - val_loss: 1.1306 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3663 - acc: 0.9365 - val_loss: 1.1259 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3586 - acc: 0.9206 - val_loss: 1.1234 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3157 - acc: 0.9683 - val_loss: 1.1198 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3742 - acc: 0.9524 - val_loss: 1.1176 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4379 - acc: 0.9365 - val_loss: 1.1174 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2659 - acc: 1.0000 - val_loss: 1.1153 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3027 - acc: 1.0000 - val_loss: 1.1117 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3270 - acc: 0.9683 - val_loss: 1.1100 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3553 - acc: 0.9524 - val_loss: 1.1098 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4256 - acc: 0.9048 - val_loss: 1.1138 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3829 - acc: 0.9206 - val_loss: 1.1170 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3140 - acc: 0.9683 - val_loss: 1.1144 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3412 - acc: 0.9365 - val_loss: 1.1112 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3497 - acc: 0.9683 - val_loss: 1.1070 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3006 - acc: 0.9683 - val_loss: 1.1037 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3139 - acc: 0.9683 - val_loss: 1.1006 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3552 - acc: 0.9524 - val_loss: 1.0994 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3373 - acc: 0.9365 - val_loss: 1.0981 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3166 - acc: 0.9365 - val_loss: 1.0959 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2979 - acc: 0.9683 - val_loss: 1.0940 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3031 - acc: 0.9683 - val_loss: 1.0907 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3111 - acc: 0.9683 - val_loss: 1.0874 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2966 - acc: 0.9524 - val_loss: 1.0852 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3190 - acc: 0.9524 - val_loss: 1.0840 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2878 - acc: 0.9524 - val_loss: 1.0832 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3383 - acc: 0.9524 - val_loss: 1.0820 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3458 - acc: 0.9524 - val_loss: 1.0842 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2803 - acc: 0.9365 - val_loss: 1.0862 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2799 - acc: 0.9683 - val_loss: 1.0830 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4144 - acc: 0.9206 - val_loss: 1.0828 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2620 - acc: 0.9683 - val_loss: 1.0823 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3395 - acc: 0.9206 - val_loss: 1.0822 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4714 - acc: 0.8413 - val_loss: 1.0862 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2370 - acc: 0.9841 - val_loss: 1.0850 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2877 - acc: 0.9683 - val_loss: 1.0814 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3354 - acc: 0.9524 - val_loss: 1.0782 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2469 - acc: 0.9841 - val_loss: 1.0751 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3624 - acc: 0.9524 - val_loss: 1.0728 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2912 - acc: 0.9683 - val_loss: 1.0709 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3301 - acc: 0.8889 - val_loss: 1.0703 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2789 - acc: 0.9524 - val_loss: 1.0698 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2256 - acc: 0.9841 - val_loss: 1.0672 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2707 - acc: 0.9683 - val_loss: 1.0632 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2852 - acc: 0.9683 - val_loss: 1.0630 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2361 - acc: 0.9841 - val_loss: 1.0589 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2107 - acc: 1.0000 - val_loss: 1.0537 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2019 - acc: 0.9841 - val_loss: 1.0516 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2360 - acc: 0.9841 - val_loss: 1.0495 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2285 - acc: 0.9683 - val_loss: 1.0482 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2239 - acc: 0.9841 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2514 - acc: 0.9524 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2820 - acc: 0.9683 - val_loss: 1.0449 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2806 - acc: 0.9365 - val_loss: 1.0466 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3060 - acc: 0.9524 - val_loss: 1.0494 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2407 - acc: 0.9524 - val_loss: 1.0492 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2887 - acc: 0.9841 - val_loss: 1.0492 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2806 - acc: 0.9524 - val_loss: 1.0506 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1815 - acc: 0.9841 - val_loss: 1.0465 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2536 - acc: 0.9683 - val_loss: 1.0438 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2634 - acc: 0.9048 - val_loss: 1.0419 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2836 - acc: 0.9683 - val_loss: 1.0410 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2569 - acc: 0.9841 - val_loss: 1.0410 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2876 - acc: 0.9683 - val_loss: 1.0457 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2889 - acc: 0.9365 - val_loss: 1.0470 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2347 - acc: 0.9841 - val_loss: 1.0440 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3163 - acc: 0.9524 - val_loss: 1.0425 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2934 - acc: 0.9524 - val_loss: 1.0413 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2459 - acc: 0.9841 - val_loss: 1.0418 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2354 - acc: 0.9524 - val_loss: 1.0397 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2643 - acc: 0.9524 - val_loss: 1.0355 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2574 - acc: 0.9683 - val_loss: 1.0332 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1906 - acc: 0.9683 - val_loss: 1.0308 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2079 - acc: 0.9524 - val_loss: 1.0275 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2473 - acc: 0.9524 - val_loss: 1.0253 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2109 - acc: 0.9524 - val_loss: 1.0247 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2276 - acc: 0.9683 - val_loss: 1.0241 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1873 - acc: 0.9524 - val_loss: 1.0218 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2920 - acc: 0.9206 - val_loss: 1.0220 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1806 - acc: 0.9524 - val_loss: 1.0228 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2369 - acc: 0.9365 - val_loss: 1.0201 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1695 - acc: 0.9841 - val_loss: 1.0209 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2973 - acc: 0.9206 - val_loss: 1.0230 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3555 - acc: 0.9365 - val_loss: 1.0234 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1815 - acc: 0.9683 - val_loss: 1.0221 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1842 - acc: 0.9841 - val_loss: 1.0179 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2339 - acc: 0.9841 - val_loss: 1.0174 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2581 - acc: 0.9048 - val_loss: 1.0169 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1870 - acc: 0.9683 - val_loss: 1.0153 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2109 - acc: 0.9683 - val_loss: 1.0138 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1809 - acc: 1.0000 - val_loss: 1.0124 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1295 - acc: 0.9841 - val_loss: 1.0092 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2169 - acc: 0.9524 - val_loss: 1.0065 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2626 - acc: 0.9524 - val_loss: 1.0097 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2326 - acc: 0.9524 - val_loss: 1.0117 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0933 - acc: 1.0000 - val_loss: 1.0092 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1140 - acc: 0.9841 - val_loss: 1.0027 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1671 - acc: 0.9683 - val_loss: 0.9985 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2402 - acc: 0.9365 - val_loss: 1.0005 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2202 - acc: 0.9524 - val_loss: 1.0041 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1481 - acc: 0.9841 - val_loss: 1.0043 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2678 - acc: 0.9524 - val_loss: 1.0033 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2358 - acc: 0.9365 - val_loss: 1.0051 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1474 - acc: 0.9841 - val_loss: 1.0058 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2171 - acc: 0.9524 - val_loss: 1.0028 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1154 - acc: 1.0000 - val_loss: 0.9985 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1619 - acc: 0.9841 - val_loss: 0.9933 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1824 - acc: 0.9683 - val_loss: 0.9914 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2924 - acc: 0.8889 - val_loss: 0.9951 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1595 - acc: 0.9206 - val_loss: 0.9989 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1818 - acc: 0.9683 - val_loss: 0.9976 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1944 - acc: 0.9365 - val_loss: 0.9933 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1108 - acc: 1.0000 - val_loss: 0.9906 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1532 - acc: 0.9683 - val_loss: 0.9893 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1607 - acc: 0.9524 - val_loss: 0.9876 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1884 - acc: 0.9683 - val_loss: 0.9860 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1471 - acc: 1.0000 - val_loss: 0.9854 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2250 - acc: 0.9524 - val_loss: 0.9862 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0896 - acc: 1.0000 - val_loss: 0.9832 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1733 - acc: 0.9683 - val_loss: 0.9810 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1354 - acc: 0.9683 - val_loss: 0.9791 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1142 - acc: 0.9841 - val_loss: 0.9755 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0873 - acc: 0.9841 - val_loss: 0.9741 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1558 - acc: 0.9841 - val_loss: 0.9739 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1053 - acc: 1.0000 - val_loss: 0.9728 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2337 - acc: 0.9206 - val_loss: 0.9719 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1949 - acc: 0.9524 - val_loss: 0.9759 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0976 - acc: 0.9683 - val_loss: 0.9788 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2059 - acc: 0.9683 - val_loss: 0.9786 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1225 - acc: 0.9841 - val_loss: 0.9759 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1497 - acc: 0.9683 - val_loss: 0.9761 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1118 - acc: 0.9841 - val_loss: 0.9721 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1167 - acc: 0.9841 - val_loss: 0.9667 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0553 - acc: 1.0000 - val_loss: 0.9627 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1683 - acc: 0.9524 - val_loss: 0.9630 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1454 - acc: 0.9524 - val_loss: 0.9634 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1693 - acc: 0.9683 - val_loss: 0.9646 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1507 - acc: 0.9524 - val_loss: 0.9645 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1915 - acc: 0.9524 - val_loss: 0.9660 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1651 - acc: 0.9365 - val_loss: 0.9671 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1133 - acc: 0.9524 - val_loss: 0.9680 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1409 - acc: 0.9365 - val_loss: 0.9650 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1570 - acc: 0.9524 - val_loss: 0.9638 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1034 - acc: 0.9683 - val_loss: 0.9619 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0981 - acc: 0.9841 - val_loss: 0.9586 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1368 - acc: 0.9841 - val_loss: 0.9579 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2264 - acc: 0.9524 - val_loss: 0.9595 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2141 - acc: 0.9365 - val_loss: 0.9610 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0960 - acc: 1.0000 - val_loss: 0.9617 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1837 - acc: 0.9683 - val_loss: 0.9615 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1205 - acc: 0.9841 - val_loss: 0.9620 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1580 - acc: 0.9841 - val_loss: 0.9607 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1566 - acc: 0.9524 - val_loss: 0.9603 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1422 - acc: 0.9683 - val_loss: 0.9592 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0400 - acc: 1.0000 - val_loss: 0.9562 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1245 - acc: 0.9841 - val_loss: 0.9536 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1212 - acc: 0.9683 - val_loss: 0.9526 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0655 - acc: 0.9841 - val_loss: 0.9510 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1080 - acc: 0.9841 - val_loss: 0.9482 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0370 - acc: 1.0000 - val_loss: 0.9453 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0653 - acc: 0.9683 - val_loss: 0.9417 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1345 - acc: 0.9683 - val_loss: 0.9423 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1284 - acc: 0.9683 - val_loss: 0.9432 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0971 - acc: 0.9683 - val_loss: 0.9444 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1046 - acc: 0.9365 - val_loss: 0.9459 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1787 - acc: 0.9524 - val_loss: 0.9488 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1726 - acc: 0.9365 - val_loss: 0.9536 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1121 - acc: 0.9683 - val_loss: 0.9522 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1833 - acc: 0.9524 - val_loss: 0.9500 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0589 - acc: 0.9841 - val_loss: 0.9470 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0796 - acc: 1.0000 - val_loss: 0.9423 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0473 - acc: 1.0000 - val_loss: 0.9391 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0941 - acc: 0.9524 - val_loss: 0.9364 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1322 - acc: 0.9683 - val_loss: 0.9354 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1091 - acc: 0.9524 - val_loss: 0.9353 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0769 - acc: 0.9683 - val_loss: 0.9345 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1436 - acc: 0.9206 - val_loss: 0.9341 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1322 - acc: 0.9841 - val_loss: 0.9365 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0516 - acc: 0.9841 - val_loss: 0.9347 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0549 - acc: 0.9683 - val_loss: 0.9305 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1010 - acc: 1.0000 - val_loss: 0.9305 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0985 - acc: 0.9365 - val_loss: 0.9308 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0684 - acc: 1.0000 - val_loss: 0.9290 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1073 - acc: 0.9683 - val_loss: 0.9298 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1196 - acc: 0.9683 - val_loss: 0.9307 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0591 - acc: 0.9841 - val_loss: 0.9292 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0605 - acc: 0.9841 - val_loss: 0.9249 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0976 - acc: 0.9524 - val_loss: 0.9252 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0167 - acc: 0.9841 - val_loss: 0.9219 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1377 - acc: 0.9524 - val_loss: 0.9239 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0394 - acc: 0.9841 - val_loss: 0.9236 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1052 - acc: 0.9683 - val_loss: 0.9225 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1134 - acc: 0.9206 - val_loss: 0.9245 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1036 - acc: 0.9683 - val_loss: 0.9265 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0415 - acc: 0.9683 - val_loss: 0.9261 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0597 - acc: 0.9683 - val_loss: 0.9243 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0724 - acc: 1.0000 - val_loss: 0.9214 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0687 - acc: 0.9524 - val_loss: 0.9225 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1001 - acc: 0.9683 - val_loss: 0.9213 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0447 - acc: 1.0000 - val_loss: 0.9184 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0259 - acc: 0.9841 - val_loss: 0.9153 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0873 - acc: 0.9683 - val_loss: 0.9164 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0900 - acc: 0.9524 - val_loss: 0.9157 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1252 - acc: 0.9683 - val_loss: 0.9163 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0256 - acc: 1.0000 - val_loss: 0.9146 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0255 - acc: 0.9841 - val_loss: 0.9102 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1117 - acc: 0.9524 - val_loss: 0.9101 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0030 - acc: 0.9841 - val_loss: 0.9105 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0852 - acc: 0.9683 - val_loss: 0.9094 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0437 - acc: 0.9841 - val_loss: 0.9086 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0405 - acc: 0.9841 - val_loss: 0.9069 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9976 - acc: 1.0000 - val_loss: 0.9046 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1116 - acc: 0.9524 - val_loss: 0.9062 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0692 - acc: 0.9841 - val_loss: 0.9113 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1047 - acc: 0.9683 - val_loss: 0.9142 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0324 - acc: 1.0000 - val_loss: 0.9132 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0537 - acc: 0.9841 - val_loss: 0.9097 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0160 - acc: 1.0000 - val_loss: 0.9079 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0689 - acc: 0.9683 - val_loss: 0.9026 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0640 - acc: 0.9683 - val_loss: 0.9041 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0382 - acc: 0.9841 - val_loss: 0.9019 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0466 - acc: 0.9524 - val_loss: 0.8991 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0060 - acc: 0.9683 - val_loss: 0.8966 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0322 - acc: 0.9841 - val_loss: 0.8965 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0232 - acc: 0.9683 - val_loss: 0.8958 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0491 - acc: 0.9841 - val_loss: 0.8959 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0553 - acc: 0.9841 - val_loss: 0.8958 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0141 - acc: 1.0000 - val_loss: 0.8956 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0790 - acc: 0.9841 - val_loss: 0.8985 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1594 - acc: 0.9206 - val_loss: 0.9018 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0591 - acc: 0.9683 - val_loss: 0.9026 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0335 - acc: 0.9841 - val_loss: 0.9009 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0029 - acc: 1.0000 - val_loss: 0.8979 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0646 - acc: 0.9524 - val_loss: 0.8965 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0628 - acc: 0.9524 - val_loss: 0.8957 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0544 - acc: 0.9524 - val_loss: 0.8948 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0337 - acc: 0.9683 - val_loss: 0.8938 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0832 - acc: 0.9683 - val_loss: 0.8923 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0125 - acc: 0.9683 - val_loss: 0.8917 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0261 - acc: 0.9683 - val_loss: 0.8892 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0130 - acc: 0.9365 - val_loss: 0.8868 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9873 - acc: 0.9841 - val_loss: 0.8840 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9907 - acc: 0.9841 - val_loss: 0.8827 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0567 - acc: 0.9841 - val_loss: 0.8823 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0157 - acc: 0.9841 - val_loss: 0.8843 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9988 - acc: 0.9841 - val_loss: 0.8856 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1102 - acc: 0.9524 - val_loss: 0.8887 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0060 - acc: 1.0000 - val_loss: 0.8881 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0709 - acc: 0.9683 - val_loss: 0.8892 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0989 - acc: 0.9524 - val_loss: 0.8935 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0596 - acc: 0.9524 - val_loss: 0.8940 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0290 - acc: 1.0000 - val_loss: 0.8914 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0186 - acc: 1.0000 - val_loss: 0.8881 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0071 - acc: 0.9683 - val_loss: 0.8844 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0035 - acc: 0.9841 - val_loss: 0.8820 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9678 - acc: 0.9841 - val_loss: 0.8795 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0240 - acc: 0.9841 - val_loss: 0.8771 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0252 - acc: 0.9683 - val_loss: 0.8751 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9971 - acc: 1.0000 - val_loss: 0.8760 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0239 - acc: 0.9841 - val_loss: 0.8782 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9998 - acc: 0.9683 - val_loss: 0.8767 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9672 - acc: 1.0000 - val_loss: 0.8735 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9802 - acc: 0.9841 - val_loss: 0.8681 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9644 - acc: 1.0000 - val_loss: 0.8659 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0008 - acc: 0.9841 - val_loss: 0.8667 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9974 - acc: 0.9841 - val_loss: 0.8668 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9760 - acc: 1.0000 - val_loss: 0.8685 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1023 - acc: 0.9683 - val_loss: 0.8735 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1242 - acc: 0.9206 - val_loss: 0.8833 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0534 - acc: 0.9683 - val_loss: 0.8874 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu_words Test Accuracy: 0.754\n",
      "my_model_neu Test f-measure: 0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "neu_wo = Sequential()\n",
    "neu_wo.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "\n",
    "neu_wo.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "neu_wo.summary()\n",
    "history = neu_wo.fit(X_scaled_train_data_words, y_train,\n",
    "                    validation_data=(X_scaled_val_data_words, y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu_words,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu_words.h5')\n",
    "yhat = l_model.predict( scaled_test_data_words)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_words Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "n_grams (InputLayer)            (None, 4152)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "words (InputLayer)              (None, 7623)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 64)           139168      n_grams[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 64)           250240      words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           sequential_4[1][0]               \n",
      "                                                                 sequential_5[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          16512       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 128)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 9)            1161        dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 407,081\n",
      "Trainable params: 407,081\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(63, 4152) (63, 7623) (63, 9)\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 11.1390 - acc: 0.0794 - val_loss: 10.9800 - val_acc: 0.1164\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.8978 - acc: 0.1746 - val_loss: 10.7778 - val_acc: 0.1177\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.6965 - acc: 0.1270 - val_loss: 10.5908 - val_acc: 0.1270\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.5163 - acc: 0.0794 - val_loss: 10.4118 - val_acc: 0.1349\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.3314 - acc: 0.0952 - val_loss: 10.2393 - val_acc: 0.1402\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 10.1639 - acc: 0.1587 - val_loss: 10.0738 - val_acc: 0.1481\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.9866 - acc: 0.1587 - val_loss: 9.9132 - val_acc: 0.1468\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.8838 - acc: 0.1270 - val_loss: 9.7562 - val_acc: 0.1561\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.6910 - acc: 0.1905 - val_loss: 9.6028 - val_acc: 0.1759\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.5307 - acc: 0.2222 - val_loss: 9.4529 - val_acc: 0.1878\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.3954 - acc: 0.1587 - val_loss: 9.3079 - val_acc: 0.2050\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.2633 - acc: 0.1587 - val_loss: 9.1660 - val_acc: 0.2302\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.1173 - acc: 0.2063 - val_loss: 9.0282 - val_acc: 0.2685\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 9.0175 - acc: 0.1587 - val_loss: 8.8941 - val_acc: 0.2778\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.8679 - acc: 0.2063 - val_loss: 8.7645 - val_acc: 0.2778\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.7598 - acc: 0.1429 - val_loss: 8.6373 - val_acc: 0.2765\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.5959 - acc: 0.2381 - val_loss: 8.5123 - val_acc: 0.3082\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.4877 - acc: 0.1905 - val_loss: 8.3888 - val_acc: 0.3452\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.3533 - acc: 0.2857 - val_loss: 8.2668 - val_acc: 0.3836\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.2367 - acc: 0.2222 - val_loss: 8.1467 - val_acc: 0.4193\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 8.1527 - acc: 0.2063 - val_loss: 8.0279 - val_acc: 0.4683\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.9649 - acc: 0.3175 - val_loss: 7.9094 - val_acc: 0.5714\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.9489 - acc: 0.2063 - val_loss: 7.7960 - val_acc: 0.6376\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.8427 - acc: 0.3016 - val_loss: 7.6879 - val_acc: 0.6733\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.6817 - acc: 0.3968 - val_loss: 7.5827 - val_acc: 0.6958\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.6255 - acc: 0.2857 - val_loss: 7.4784 - val_acc: 0.7262\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.4745 - acc: 0.3333 - val_loss: 7.3624 - val_acc: 0.7262\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.4031 - acc: 0.3492 - val_loss: 7.2574 - val_acc: 0.7421\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.3071 - acc: 0.4444 - val_loss: 7.1495 - val_acc: 0.7989\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.1936 - acc: 0.4444 - val_loss: 7.0463 - val_acc: 0.8135\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.1566 - acc: 0.3333 - val_loss: 6.9502 - val_acc: 0.8730\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.0687 - acc: 0.4762 - val_loss: 6.8611 - val_acc: 0.9193\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 7.0233 - acc: 0.5079 - val_loss: 6.7664 - val_acc: 0.9325\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.8847 - acc: 0.3968 - val_loss: 6.6716 - val_acc: 0.9339\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.7909 - acc: 0.5079 - val_loss: 6.5701 - val_acc: 0.9312\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.7306 - acc: 0.4603 - val_loss: 6.4753 - val_acc: 0.9577\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.6389 - acc: 0.5714 - val_loss: 6.3863 - val_acc: 0.9683\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.6084 - acc: 0.5556 - val_loss: 6.3004 - val_acc: 0.9788\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.4585 - acc: 0.5873 - val_loss: 6.2096 - val_acc: 0.9881\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.3246 - acc: 0.6508 - val_loss: 6.1092 - val_acc: 0.9947\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.2782 - acc: 0.5873 - val_loss: 6.0177 - val_acc: 0.9868\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.2055 - acc: 0.6190 - val_loss: 5.9297 - val_acc: 0.9987\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.1875 - acc: 0.5873 - val_loss: 5.8436 - val_acc: 1.0000\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.0736 - acc: 0.6032 - val_loss: 5.7622 - val_acc: 1.0000\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.9016 - acc: 0.7302 - val_loss: 5.6802 - val_acc: 1.0000\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 6.0003 - acc: 0.5873 - val_loss: 5.5969 - val_acc: 0.9987\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.8926 - acc: 0.6190 - val_loss: 5.5219 - val_acc: 0.9987\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.8001 - acc: 0.7460 - val_loss: 5.4391 - val_acc: 1.0000\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.7757 - acc: 0.6508 - val_loss: 5.3660 - val_acc: 1.0000\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.8112 - acc: 0.5238 - val_loss: 5.3107 - val_acc: 1.0000\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.6218 - acc: 0.7460 - val_loss: 5.2513 - val_acc: 1.0000\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.5569 - acc: 0.7460 - val_loss: 5.1733 - val_acc: 1.0000\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.3974 - acc: 0.7619 - val_loss: 5.0956 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.5195 - acc: 0.6508 - val_loss: 5.0340 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.4593 - acc: 0.7302 - val_loss: 4.9689 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.3652 - acc: 0.6984 - val_loss: 4.9132 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.2807 - acc: 0.7460 - val_loss: 4.8577 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1312 - acc: 0.8571 - val_loss: 4.7933 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1703 - acc: 0.7937 - val_loss: 4.7321 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0950 - acc: 0.7937 - val_loss: 4.6855 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0033 - acc: 0.8095 - val_loss: 4.6282 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1629 - acc: 0.6508 - val_loss: 4.5810 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0019 - acc: 0.7619 - val_loss: 4.5478 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8673 - acc: 0.8413 - val_loss: 4.4927 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8123 - acc: 0.8571 - val_loss: 4.4365 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8856 - acc: 0.7460 - val_loss: 4.3872 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.7933 - acc: 0.7460 - val_loss: 4.3418 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.7625 - acc: 0.7778 - val_loss: 4.2990 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.6020 - acc: 0.9048 - val_loss: 4.2545 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.6051 - acc: 0.8571 - val_loss: 4.2148 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.4777 - acc: 0.8889 - val_loss: 4.1783 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5933 - acc: 0.8095 - val_loss: 4.1423 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5499 - acc: 0.8254 - val_loss: 4.1137 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3968 - acc: 0.9048 - val_loss: 4.0779 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5440 - acc: 0.8095 - val_loss: 4.0432 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.4125 - acc: 0.9048 - val_loss: 4.0139 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3563 - acc: 0.9048 - val_loss: 3.9870 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3778 - acc: 0.8889 - val_loss: 3.9570 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2695 - acc: 0.9365 - val_loss: 3.9262 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3704 - acc: 0.8730 - val_loss: 3.8972 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1920 - acc: 0.9048 - val_loss: 3.8734 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1442 - acc: 0.9365 - val_loss: 3.8442 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1614 - acc: 0.9365 - val_loss: 3.8113 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2103 - acc: 0.8254 - val_loss: 3.7831 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2099 - acc: 0.8413 - val_loss: 3.7623 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0524 - acc: 0.9365 - val_loss: 3.7381 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1891 - acc: 0.8254 - val_loss: 3.7236 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0965 - acc: 0.8889 - val_loss: 3.7112 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0073 - acc: 0.8889 - val_loss: 3.6928 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.9747 - acc: 0.9206 - val_loss: 3.6685 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0255 - acc: 0.9206 - val_loss: 3.6418 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8926 - acc: 0.9524 - val_loss: 3.6135 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0181 - acc: 0.8889 - val_loss: 3.5909 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8826 - acc: 0.9365 - val_loss: 3.5710 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.9053 - acc: 0.9524 - val_loss: 3.5495 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8480 - acc: 0.9048 - val_loss: 3.5311 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7472 - acc: 0.9524 - val_loss: 3.5104 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8056 - acc: 0.9206 - val_loss: 3.4967 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8680 - acc: 0.8571 - val_loss: 3.4858 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8543 - acc: 0.9048 - val_loss: 3.4753 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8278 - acc: 0.9365 - val_loss: 3.4570 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7670 - acc: 0.9365 - val_loss: 3.4342 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6562 - acc: 0.9841 - val_loss: 3.4117 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7184 - acc: 0.9365 - val_loss: 3.3907 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7107 - acc: 0.9365 - val_loss: 3.3760 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6607 - acc: 0.9206 - val_loss: 3.3603 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6666 - acc: 0.9206 - val_loss: 3.3448 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7002 - acc: 0.9048 - val_loss: 3.3325 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5492 - acc: 0.9841 - val_loss: 3.3135 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6265 - acc: 0.9048 - val_loss: 3.2962 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5689 - acc: 0.9683 - val_loss: 3.2819 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6233 - acc: 0.8730 - val_loss: 3.2677 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5707 - acc: 0.8889 - val_loss: 3.2577 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5497 - acc: 0.9206 - val_loss: 3.2445 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5194 - acc: 0.9365 - val_loss: 3.2330 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4056 - acc: 0.9841 - val_loss: 3.2189 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5197 - acc: 0.9206 - val_loss: 3.2006 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4061 - acc: 0.9683 - val_loss: 3.1880 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3392 - acc: 0.9841 - val_loss: 3.1688 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4472 - acc: 0.9683 - val_loss: 3.1531 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4603 - acc: 0.9048 - val_loss: 3.1422 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4462 - acc: 0.9365 - val_loss: 3.1364 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3180 - acc: 0.9841 - val_loss: 3.1249 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3175 - acc: 0.9683 - val_loss: 3.1103 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3979 - acc: 0.9524 - val_loss: 3.1000 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3761 - acc: 0.9524 - val_loss: 3.0910 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4303 - acc: 0.9206 - val_loss: 3.0775 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3228 - acc: 0.9524 - val_loss: 3.0635 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3528 - acc: 0.9365 - val_loss: 3.0531 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2608 - acc: 0.9524 - val_loss: 3.0424 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2827 - acc: 0.9524 - val_loss: 3.0306 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2730 - acc: 0.9206 - val_loss: 3.0168 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2212 - acc: 0.9841 - val_loss: 3.0025 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3885 - acc: 0.9048 - val_loss: 2.9973 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2393 - acc: 0.9524 - val_loss: 2.9937 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2323 - acc: 0.9683 - val_loss: 2.9833 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2439 - acc: 0.9524 - val_loss: 2.9703 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1519 - acc: 0.9841 - val_loss: 2.9577 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1106 - acc: 1.0000 - val_loss: 2.9407 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1422 - acc: 0.9683 - val_loss: 2.9258 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1427 - acc: 0.9683 - val_loss: 2.9174 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3213 - acc: 0.8571 - val_loss: 2.9157 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1618 - acc: 0.9841 - val_loss: 2.9140 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2285 - acc: 0.9524 - val_loss: 2.9110 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1531 - acc: 0.9683 - val_loss: 2.8991 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1358 - acc: 0.9524 - val_loss: 2.8854 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1452 - acc: 0.9365 - val_loss: 2.8741 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0833 - acc: 1.0000 - val_loss: 2.8634 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1121 - acc: 0.9365 - val_loss: 2.8534 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0129 - acc: 0.9683 - val_loss: 2.8391 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0660 - acc: 0.9841 - val_loss: 2.8267 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0262 - acc: 0.9841 - val_loss: 2.8177 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0268 - acc: 0.9365 - val_loss: 2.8069 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0196 - acc: 0.9524 - val_loss: 2.7977 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0094 - acc: 0.9524 - val_loss: 2.7875 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9798 - acc: 0.9841 - val_loss: 2.7782 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9450 - acc: 0.9683 - val_loss: 2.7690 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0173 - acc: 0.9524 - val_loss: 2.7658 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9474 - acc: 0.9841 - val_loss: 2.7576 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9107 - acc: 0.9841 - val_loss: 2.7457 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9112 - acc: 0.9683 - val_loss: 2.7361 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9785 - acc: 0.9365 - val_loss: 2.7328 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9742 - acc: 0.9524 - val_loss: 2.7284 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9650 - acc: 0.9524 - val_loss: 2.7231 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9504 - acc: 0.9683 - val_loss: 2.7184 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9234 - acc: 0.9841 - val_loss: 2.7075 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8798 - acc: 0.9683 - val_loss: 2.6979 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8441 - acc: 1.0000 - val_loss: 2.6877 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8782 - acc: 0.9683 - val_loss: 2.6755 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8266 - acc: 0.9841 - val_loss: 2.6679 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7671 - acc: 1.0000 - val_loss: 2.6560 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8336 - acc: 0.9524 - val_loss: 2.6484 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8297 - acc: 1.0000 - val_loss: 2.6423 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8156 - acc: 0.9841 - val_loss: 2.6334 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8115 - acc: 0.9524 - val_loss: 2.6283 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8097 - acc: 0.9524 - val_loss: 2.6239 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7681 - acc: 0.9683 - val_loss: 2.6117 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7529 - acc: 1.0000 - val_loss: 2.6017 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7397 - acc: 1.0000 - val_loss: 2.5941 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7753 - acc: 0.9524 - val_loss: 2.5862 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7149 - acc: 0.9841 - val_loss: 2.5805 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7733 - acc: 0.9683 - val_loss: 2.5762 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7438 - acc: 1.0000 - val_loss: 2.5683 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7136 - acc: 0.9683 - val_loss: 2.5612 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6855 - acc: 0.9841 - val_loss: 2.5549 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7333 - acc: 0.9683 - val_loss: 2.5474 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7649 - acc: 0.9524 - val_loss: 2.5420 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7732 - acc: 0.9206 - val_loss: 2.5410 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7493 - acc: 0.9841 - val_loss: 2.5394 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6675 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8093 - acc: 0.9683 - val_loss: 2.5275 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6614 - acc: 0.9841 - val_loss: 2.5230 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7242 - acc: 0.9524 - val_loss: 2.5182 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6602 - acc: 0.9683 - val_loss: 2.5101 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6857 - acc: 0.9683 - val_loss: 2.5034 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6767 - acc: 0.9524 - val_loss: 2.4956 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6390 - acc: 1.0000 - val_loss: 2.4845 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5797 - acc: 1.0000 - val_loss: 2.4730 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6214 - acc: 0.9841 - val_loss: 2.4647 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6064 - acc: 0.9841 - val_loss: 2.4601 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6153 - acc: 1.0000 - val_loss: 2.4504 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6800 - acc: 0.9206 - val_loss: 2.4489 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6219 - acc: 0.9841 - val_loss: 2.4488 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6293 - acc: 0.9683 - val_loss: 2.4437 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6410 - acc: 0.9841 - val_loss: 2.4369 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6038 - acc: 0.9841 - val_loss: 2.4342 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6185 - acc: 0.9683 - val_loss: 2.4305 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5474 - acc: 0.9683 - val_loss: 2.4197 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5588 - acc: 0.9841 - val_loss: 2.4078 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5978 - acc: 0.9683 - val_loss: 2.4026 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5470 - acc: 0.9524 - val_loss: 2.3984 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4990 - acc: 0.9841 - val_loss: 2.3898 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4985 - acc: 1.0000 - val_loss: 2.3790 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5775 - acc: 0.9683 - val_loss: 2.3750 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6027 - acc: 0.9206 - val_loss: 2.3803 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5180 - acc: 1.0000 - val_loss: 2.3791 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5253 - acc: 0.9683 - val_loss: 2.3674 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4768 - acc: 0.9841 - val_loss: 2.3549 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4605 - acc: 1.0000 - val_loss: 2.3440 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5200 - acc: 0.9524 - val_loss: 2.3359 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5321 - acc: 0.9841 - val_loss: 2.3390 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4669 - acc: 1.0000 - val_loss: 2.3342 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4863 - acc: 1.0000 - val_loss: 2.3268 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5060 - acc: 0.9841 - val_loss: 2.3203 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4725 - acc: 0.9524 - val_loss: 2.3208 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4509 - acc: 0.9841 - val_loss: 2.3135 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4544 - acc: 0.9841 - val_loss: 2.3102 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4354 - acc: 0.9841 - val_loss: 2.3029 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5309 - acc: 0.9683 - val_loss: 2.3054 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4361 - acc: 1.0000 - val_loss: 2.3062 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4104 - acc: 1.0000 - val_loss: 2.2980 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4147 - acc: 1.0000 - val_loss: 2.2870 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3494 - acc: 1.0000 - val_loss: 2.2734 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3726 - acc: 1.0000 - val_loss: 2.2645 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3849 - acc: 0.9683 - val_loss: 2.2529 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4672 - acc: 0.9365 - val_loss: 2.2551 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3911 - acc: 0.9841 - val_loss: 2.2554 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3609 - acc: 1.0000 - val_loss: 2.2470 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3423 - acc: 1.0000 - val_loss: 2.2346 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3485 - acc: 1.0000 - val_loss: 2.2253 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3577 - acc: 0.9841 - val_loss: 2.2183 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3332 - acc: 1.0000 - val_loss: 2.2181 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3526 - acc: 1.0000 - val_loss: 2.2189 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3576 - acc: 0.9841 - val_loss: 2.2220 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3430 - acc: 0.9683 - val_loss: 2.2187 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3751 - acc: 0.9683 - val_loss: 2.2125 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4137 - acc: 0.9841 - val_loss: 2.2119 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3431 - acc: 0.9841 - val_loss: 2.2093 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3299 - acc: 0.9841 - val_loss: 2.2060 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3058 - acc: 1.0000 - val_loss: 2.1953 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3804 - acc: 0.9841 - val_loss: 2.1867 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3195 - acc: 0.9841 - val_loss: 2.1875 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2840 - acc: 0.9841 - val_loss: 2.1812 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2819 - acc: 0.9841 - val_loss: 2.1736 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3006 - acc: 0.9841 - val_loss: 2.1668 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2780 - acc: 0.9841 - val_loss: 2.1644 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3036 - acc: 0.9683 - val_loss: 2.1603 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3089 - acc: 0.9841 - val_loss: 2.1578 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2611 - acc: 1.0000 - val_loss: 2.1540 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2778 - acc: 1.0000 - val_loss: 2.1449 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2831 - acc: 0.9841 - val_loss: 2.1403 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2971 - acc: 0.9841 - val_loss: 2.1405 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2458 - acc: 1.0000 - val_loss: 2.1373 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3268 - acc: 0.9365 - val_loss: 2.1367 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2831 - acc: 0.9683 - val_loss: 2.1335 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2524 - acc: 0.9841 - val_loss: 2.1270 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2359 - acc: 1.0000 - val_loss: 2.1224 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3023 - acc: 0.9683 - val_loss: 2.1221 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2306 - acc: 0.9841 - val_loss: 2.1218 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2681 - acc: 0.9841 - val_loss: 2.1217 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2154 - acc: 0.9841 - val_loss: 2.1157 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2607 - acc: 1.0000 - val_loss: 2.1070 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1875 - acc: 0.9683 - val_loss: 2.1033 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2687 - acc: 0.9683 - val_loss: 2.0974 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1892 - acc: 1.0000 - val_loss: 2.0941 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2184 - acc: 1.0000 - val_loss: 2.0867 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2280 - acc: 1.0000 - val_loss: 2.0858 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2326 - acc: 1.0000 - val_loss: 2.0845 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1809 - acc: 0.9841 - val_loss: 2.0816 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1955 - acc: 1.0000 - val_loss: 2.0758 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2455 - acc: 0.9365 - val_loss: 2.0743 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1880 - acc: 1.0000 - val_loss: 2.0658 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2074 - acc: 0.9841 - val_loss: 2.0578 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1391 - acc: 0.9841 - val_loss: 2.0483 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1238 - acc: 0.9841 - val_loss: 2.0355 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1899 - acc: 0.9683 - val_loss: 2.0367 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2469 - acc: 0.9524 - val_loss: 2.0437 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2299 - acc: 0.9524 - val_loss: 2.0490 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1585 - acc: 1.0000 - val_loss: 2.0473 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1207 - acc: 0.9841 - val_loss: 2.0382 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1479 - acc: 1.0000 - val_loss: 2.0265 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1412 - acc: 0.9841 - val_loss: 2.0144 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2079 - acc: 0.9524 - val_loss: 2.0155 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1833 - acc: 0.9683 - val_loss: 2.0181 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1918 - acc: 0.9683 - val_loss: 2.0168 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1151 - acc: 1.0000 - val_loss: 2.0164 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1217 - acc: 1.0000 - val_loss: 2.0092 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0691 - acc: 1.0000 - val_loss: 1.9992 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0969 - acc: 0.9841 - val_loss: 1.9867 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1294 - acc: 0.9683 - val_loss: 1.9837 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0860 - acc: 1.0000 - val_loss: 1.9891 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1806 - acc: 0.9841 - val_loss: 1.9929 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1073 - acc: 1.0000 - val_loss: 1.9940 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1073 - acc: 0.9841 - val_loss: 1.9878 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0736 - acc: 0.9841 - val_loss: 1.9772 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0359 - acc: 1.0000 - val_loss: 1.9666 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0740 - acc: 0.9841 - val_loss: 1.9636 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0953 - acc: 0.9683 - val_loss: 1.9609 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1138 - acc: 0.9683 - val_loss: 1.9603 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1141 - acc: 0.9683 - val_loss: 1.9617 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0662 - acc: 1.0000 - val_loss: 1.9557 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0516 - acc: 1.0000 - val_loss: 1.9483 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0701 - acc: 0.9841 - val_loss: 1.9394 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.1153 - acc: 0.9524 - val_loss: 1.9409 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 2.0723 - acc: 0.9841 - val_loss: 1.9387 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0352 - acc: 1.0000 - val_loss: 1.9386 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0573 - acc: 0.9841 - val_loss: 1.9368 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0583 - acc: 0.9683 - val_loss: 1.9360 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0257 - acc: 0.9841 - val_loss: 1.9323 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0611 - acc: 0.9841 - val_loss: 1.9251 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0186 - acc: 0.9841 - val_loss: 1.9174 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0298 - acc: 0.9841 - val_loss: 1.9139 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0360 - acc: 0.9841 - val_loss: 1.9092 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0087 - acc: 1.0000 - val_loss: 1.9064 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0174 - acc: 1.0000 - val_loss: 1.9013 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0031 - acc: 1.0000 - val_loss: 1.8982 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0310 - acc: 0.9841 - val_loss: 1.8939 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0278 - acc: 0.9841 - val_loss: 1.8916 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9821 - acc: 0.9841 - val_loss: 1.8913 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0161 - acc: 0.9841 - val_loss: 1.8890 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0731 - acc: 0.9524 - val_loss: 1.8916 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0354 - acc: 0.9841 - val_loss: 1.8979 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0580 - acc: 0.9683 - val_loss: 1.8956 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0806 - acc: 0.9524 - val_loss: 1.8975 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9748 - acc: 1.0000 - val_loss: 1.8943 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9551 - acc: 1.0000 - val_loss: 1.8818 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0633 - acc: 0.9683 - val_loss: 1.8761 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9737 - acc: 0.9841 - val_loss: 1.8762 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9667 - acc: 1.0000 - val_loss: 1.8674 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0154 - acc: 0.9524 - val_loss: 1.8615 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9636 - acc: 0.9841 - val_loss: 1.8634 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9677 - acc: 1.0000 - val_loss: 1.8588 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9638 - acc: 0.9841 - val_loss: 1.8534 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9304 - acc: 1.0000 - val_loss: 1.8450 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9842 - acc: 0.9841 - val_loss: 1.8434 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9892 - acc: 0.9841 - val_loss: 1.8463 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9518 - acc: 1.0000 - val_loss: 1.8442 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9563 - acc: 0.9841 - val_loss: 1.8392 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0060 - acc: 0.9524 - val_loss: 1.8353 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9587 - acc: 0.9841 - val_loss: 1.8385 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9751 - acc: 0.9841 - val_loss: 1.8353 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8960 - acc: 1.0000 - val_loss: 1.8291 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9600 - acc: 0.9524 - val_loss: 1.8211 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9373 - acc: 0.9841 - val_loss: 1.8167 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9317 - acc: 0.9841 - val_loss: 1.8117 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9248 - acc: 0.9841 - val_loss: 1.8124 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9327 - acc: 0.9524 - val_loss: 1.8111 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9583 - acc: 0.9841 - val_loss: 1.8108 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8907 - acc: 1.0000 - val_loss: 1.8085 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8895 - acc: 1.0000 - val_loss: 1.7977 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9315 - acc: 0.9524 - val_loss: 1.7918 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9214 - acc: 0.9841 - val_loss: 1.7926 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8863 - acc: 1.0000 - val_loss: 1.7868 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8984 - acc: 1.0000 - val_loss: 1.7814 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8785 - acc: 0.9841 - val_loss: 1.7824 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8856 - acc: 1.0000 - val_loss: 1.7788 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9085 - acc: 0.9841 - val_loss: 1.7759 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9396 - acc: 0.9524 - val_loss: 1.7740 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8911 - acc: 1.0000 - val_loss: 1.7723 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8644 - acc: 0.9683 - val_loss: 1.7711 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8619 - acc: 0.9841 - val_loss: 1.7632 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8857 - acc: 1.0000 - val_loss: 1.7617 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8731 - acc: 1.0000 - val_loss: 1.7592 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8627 - acc: 0.9841 - val_loss: 1.7565 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8732 - acc: 0.9524 - val_loss: 1.7509 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8178 - acc: 1.0000 - val_loss: 1.7471 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8227 - acc: 1.0000 - val_loss: 1.7371 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8178 - acc: 1.0000 - val_loss: 1.7289 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7948 - acc: 1.0000 - val_loss: 1.7217 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8451 - acc: 1.0000 - val_loss: 1.7197 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8047 - acc: 1.0000 - val_loss: 1.7186 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8032 - acc: 0.9841 - val_loss: 1.7124 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7959 - acc: 1.0000 - val_loss: 1.7091 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8231 - acc: 0.9841 - val_loss: 1.7129 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8015 - acc: 1.0000 - val_loss: 1.7071 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7898 - acc: 1.0000 - val_loss: 1.7028 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7904 - acc: 0.9841 - val_loss: 1.7022 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7605 - acc: 1.0000 - val_loss: 1.6989 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8183 - acc: 0.9683 - val_loss: 1.6957 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7928 - acc: 0.9841 - val_loss: 1.6987 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7832 - acc: 0.9841 - val_loss: 1.6977 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7649 - acc: 1.0000 - val_loss: 1.6928 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7480 - acc: 1.0000 - val_loss: 1.6836 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7770 - acc: 1.0000 - val_loss: 1.6771 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8034 - acc: 0.9683 - val_loss: 1.6753 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7450 - acc: 1.0000 - val_loss: 1.6771 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7658 - acc: 1.0000 - val_loss: 1.6725 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7855 - acc: 1.0000 - val_loss: 1.6678 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7444 - acc: 1.0000 - val_loss: 1.6622 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7464 - acc: 0.9841 - val_loss: 1.6576 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7971 - acc: 0.9841 - val_loss: 1.6603 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7967 - acc: 1.0000 - val_loss: 1.6630 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7802 - acc: 0.9841 - val_loss: 1.6608 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7514 - acc: 0.9841 - val_loss: 1.6612 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7517 - acc: 0.9841 - val_loss: 1.6539 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7958 - acc: 0.9683 - val_loss: 1.6495 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7433 - acc: 1.0000 - val_loss: 1.6565 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7994 - acc: 0.9841 - val_loss: 1.6591 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7787 - acc: 1.0000 - val_loss: 1.6588 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7449 - acc: 0.9841 - val_loss: 1.6578 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7433 - acc: 0.9841 - val_loss: 1.6509 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7295 - acc: 1.0000 - val_loss: 1.6442 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7081 - acc: 0.9841 - val_loss: 1.6348 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7563 - acc: 0.9683 - val_loss: 1.6339 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7729 - acc: 0.9683 - val_loss: 1.6354 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8153 - acc: 0.9683 - val_loss: 1.6489 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7731 - acc: 0.9841 - val_loss: 1.6599 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7565 - acc: 1.0000 - val_loss: 1.6556 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7485 - acc: 0.9841 - val_loss: 1.6476 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7308 - acc: 1.0000 - val_loss: 1.6441 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7268 - acc: 1.0000 - val_loss: 1.6335 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7921 - acc: 0.9524 - val_loss: 1.6295 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7172 - acc: 1.0000 - val_loss: 1.6308 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6992 - acc: 0.9841 - val_loss: 1.6246 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7569 - acc: 0.9683 - val_loss: 1.6220 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7067 - acc: 0.9841 - val_loss: 1.6201 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6834 - acc: 1.0000 - val_loss: 1.6128 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7013 - acc: 1.0000 - val_loss: 1.6067 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6979 - acc: 1.0000 - val_loss: 1.6010 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6508 - acc: 1.0000 - val_loss: 1.5934 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7307 - acc: 1.0000 - val_loss: 1.5930 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6976 - acc: 0.9841 - val_loss: 1.5957 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7052 - acc: 0.9841 - val_loss: 1.5961 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7140 - acc: 0.9841 - val_loss: 1.5929 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6978 - acc: 0.9841 - val_loss: 1.5924 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6729 - acc: 0.9841 - val_loss: 1.5897 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6425 - acc: 1.0000 - val_loss: 1.5813 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7417 - acc: 0.9524 - val_loss: 1.5868 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6895 - acc: 0.9841 - val_loss: 1.5880 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7011 - acc: 0.9841 - val_loss: 1.5809 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7235 - acc: 0.9683 - val_loss: 1.5779 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6401 - acc: 1.0000 - val_loss: 1.5725 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6484 - acc: 0.9683 - val_loss: 1.5607 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6510 - acc: 1.0000 - val_loss: 1.5576 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6485 - acc: 1.0000 - val_loss: 1.5569 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6334 - acc: 1.0000 - val_loss: 1.5519 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7070 - acc: 0.9683 - val_loss: 1.5565 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6413 - acc: 1.0000 - val_loss: 1.5570 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6677 - acc: 0.9841 - val_loss: 1.5532 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6577 - acc: 0.9841 - val_loss: 1.5559 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6478 - acc: 0.9841 - val_loss: 1.5576 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6298 - acc: 0.9683 - val_loss: 1.5532 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6422 - acc: 1.0000 - val_loss: 1.5455 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6136 - acc: 1.0000 - val_loss: 1.5356 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6729 - acc: 0.9683 - val_loss: 1.5386 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6146 - acc: 1.0000 - val_loss: 1.5363 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6570 - acc: 0.9841 - val_loss: 1.5390 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6371 - acc: 1.0000 - val_loss: 1.5360 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5979 - acc: 1.0000 - val_loss: 1.5271 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6252 - acc: 0.9683 - val_loss: 1.5246 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6407 - acc: 0.9841 - val_loss: 1.5220 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5840 - acc: 1.0000 - val_loss: 1.5132 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6182 - acc: 1.0000 - val_loss: 1.5100 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6238 - acc: 0.9683 - val_loss: 1.5123 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5906 - acc: 1.0000 - val_loss: 1.5157 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6366 - acc: 0.9841 - val_loss: 1.5149 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5966 - acc: 0.9841 - val_loss: 1.5158 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5989 - acc: 0.9841 - val_loss: 1.5105 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5543 - acc: 1.0000 - val_loss: 1.5024 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6128 - acc: 0.9841 - val_loss: 1.4967 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6123 - acc: 0.9841 - val_loss: 1.5008 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6094 - acc: 0.9683 - val_loss: 1.5033 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6227 - acc: 0.9841 - val_loss: 1.5006 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6180 - acc: 0.9841 - val_loss: 1.5003 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5876 - acc: 1.0000 - val_loss: 1.4981 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6300 - acc: 0.9841 - val_loss: 1.4975 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5670 - acc: 0.9841 - val_loss: 1.4932 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5867 - acc: 1.0000 - val_loss: 1.4915 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5671 - acc: 0.9683 - val_loss: 1.4854 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6690 - acc: 0.9683 - val_loss: 1.4931 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5771 - acc: 1.0000 - val_loss: 1.4964 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5757 - acc: 0.9841 - val_loss: 1.4951 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6089 - acc: 0.9683 - val_loss: 1.4903 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6145 - acc: 0.9841 - val_loss: 1.4847 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5966 - acc: 0.9683 - val_loss: 1.4797 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5708 - acc: 1.0000 - val_loss: 1.4763 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5571 - acc: 1.0000 - val_loss: 1.4686 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5599 - acc: 1.0000 - val_loss: 1.4637 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5666 - acc: 0.9841 - val_loss: 1.4599 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5914 - acc: 0.9683 - val_loss: 1.4651 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5666 - acc: 0.9841 - val_loss: 1.4689 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6172 - acc: 0.9841 - val_loss: 1.4694 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5577 - acc: 0.9841 - val_loss: 1.4639 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5710 - acc: 0.9841 - val_loss: 1.4558 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5981 - acc: 0.9841 - val_loss: 1.4532 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5455 - acc: 1.0000 - val_loss: 1.4526 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5022 - acc: 1.0000 - val_loss: 1.4437 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5159 - acc: 1.0000 - val_loss: 1.4370 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5721 - acc: 1.0000 - val_loss: 1.4387 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5480 - acc: 0.9841 - val_loss: 1.4406 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu Test Accuracy: 0.878\n",
      "my_model_neu Test f-measure: 0.768\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_input_tensor = Input(shape=(scaled_train_data_words.shape[1],) , name='words')\n",
    "ngram_input_tensor = Input(shape=(scaled_train_data_ngrams.shape[1],) , name='n_grams')\n",
    "\n",
    "neu0 = Sequential()\n",
    "neu0.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "\n",
    "neu1 = Sequential()\n",
    "neu1.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "\n",
    "output_tensor_0 = neu0(ngram_input_tensor)\n",
    "output_tensor_1 = neu1(word_input_tensor)\n",
    "\n",
    "\n",
    "# conv_input_tensor = Input(shape=(maxlen,) , name='convnets')\n",
    "\n",
    "# conv_1d_s3_model = Sequential()\n",
    "# conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_s3_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_0 = conv_1d_s3_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "# conv_1d_s1_model = Sequential()\n",
    "# conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_s1_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_1 = conv_1d_s1_model(conv_input_tensor)\n",
    "\n",
    "# conv_1d_complex_model = Sequential()\n",
    "# conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_complex_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_2 = conv_1d_complex_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "\n",
    "concatenated = layers.concatenate([output_tensor_0,\n",
    "                                   output_tensor_1,\n",
    "#                                    conv_output_tensor_0,\n",
    "#                                    conv_output_tensor_1,\n",
    "#                                    conv_output_tensor_2,\n",
    "                                  ], axis=-1)\n",
    "\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "concatenated = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model([ngram_input_tensor, word_input_tensor,\n",
    "#                conv_input_tensor\n",
    "              ], concatenated)\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "print(X_scaled_train_data_ngrams.shape, X_scaled_train_data_words.shape, y_train.shape) \n",
    "history = model.fit([X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
    "#                      X_train\n",
    "                    ], y_train,\n",
    "                    validation_data=([X_scaled_val_data_ngrams, X_scaled_val_data_words,\n",
    "#                                       X_val\n",
    "                                     ], y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu.h5')\n",
    "yhat = l_model.predict([scaled_test_data_ngrams, scaled_test_data_words])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_neu Test Accuracy: 0.853\n",
      "my_model_neu Test f-measure: 0.668\n"
     ]
    }
   ],
   "source": [
    "l_model = load_model('my_model_neu.h5')\n",
    "yhat = l_model.predict([scaled_test_data_ngrams, scaled_test_data_words])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "\n",
    "print('my_model_neu Test Accuracy: %.3f' % acc)\n",
    "\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "convnet_words (InputLayer)      (None, 1053)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 64)           72451       convnet_words[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 64)           69521       convnet_words[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 64)           72378       convnet_words[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 192)          0           sequential_11[1][0]              \n",
      "                                                                 sequential_12[1][0]              \n",
      "                                                                 sequential_13[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 128)          24704       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 128)          0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 9)            1161        dropout_30[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 240,215\n",
      "Trainable params: 239,959\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/300\n",
      " - 9s - loss: 4.1205 - acc: 0.0635 - val_loss: 4.0365 - val_acc: 0.1336\n",
      "Epoch 2/300\n",
      " - 2s - loss: 4.1029 - acc: 0.2063 - val_loss: 4.0073 - val_acc: 0.1601\n",
      "Epoch 3/300\n",
      " - 2s - loss: 4.0155 - acc: 0.1429 - val_loss: 3.9801 - val_acc: 0.1786\n",
      "Epoch 4/300\n",
      " - 2s - loss: 3.9800 - acc: 0.1746 - val_loss: 3.9549 - val_acc: 0.2288\n",
      "Epoch 5/300\n",
      " - 2s - loss: 3.9583 - acc: 0.1270 - val_loss: 3.9311 - val_acc: 0.2222\n",
      "Epoch 6/300\n",
      " - 2s - loss: 3.9670 - acc: 0.1270 - val_loss: 3.9076 - val_acc: 0.2288\n",
      "Epoch 7/300\n",
      " - 2s - loss: 3.9452 - acc: 0.1111 - val_loss: 3.8843 - val_acc: 0.2368\n",
      "Epoch 8/300\n",
      " - 2s - loss: 3.8963 - acc: 0.1270 - val_loss: 3.8629 - val_acc: 0.2183\n",
      "Epoch 9/300\n",
      " - 2s - loss: 3.8710 - acc: 0.1429 - val_loss: 3.8405 - val_acc: 0.2262\n",
      "Epoch 10/300\n",
      " - 2s - loss: 3.8835 - acc: 0.1270 - val_loss: 3.8189 - val_acc: 0.1799\n",
      "Epoch 11/300\n",
      " - 2s - loss: 3.8812 - acc: 0.1111 - val_loss: 3.7974 - val_acc: 0.1971\n",
      "Epoch 12/300\n",
      " - 2s - loss: 3.8129 - acc: 0.1111 - val_loss: 3.7762 - val_acc: 0.2407\n",
      "Epoch 13/300\n",
      " - 2s - loss: 3.8052 - acc: 0.0952 - val_loss: 3.7554 - val_acc: 0.2474\n",
      "Epoch 14/300\n",
      " - 2s - loss: 3.7991 - acc: 0.1746 - val_loss: 3.7347 - val_acc: 0.2725\n",
      "Epoch 15/300\n",
      " - 2s - loss: 3.7535 - acc: 0.1270 - val_loss: 3.7141 - val_acc: 0.3452\n",
      "Epoch 16/300\n",
      " - 2s - loss: 3.6919 - acc: 0.2540 - val_loss: 3.6938 - val_acc: 0.3426\n",
      "Epoch 17/300\n",
      " - 2s - loss: 3.7283 - acc: 0.1270 - val_loss: 3.6730 - val_acc: 0.3730\n",
      "Epoch 18/300\n",
      " - 2s - loss: 3.6677 - acc: 0.2063 - val_loss: 3.6528 - val_acc: 0.4087\n",
      "Epoch 19/300\n",
      " - 2s - loss: 3.6985 - acc: 0.1270 - val_loss: 3.6331 - val_acc: 0.4206\n",
      "Epoch 20/300\n",
      " - 2s - loss: 3.6776 - acc: 0.1429 - val_loss: 3.6138 - val_acc: 0.3968\n",
      "Epoch 21/300\n",
      " - 2s - loss: 3.6416 - acc: 0.1746 - val_loss: 3.5943 - val_acc: 0.3783\n",
      "Epoch 22/300\n",
      " - 2s - loss: 3.6359 - acc: 0.1270 - val_loss: 3.5742 - val_acc: 0.3836\n",
      "Epoch 23/300\n",
      " - 2s - loss: 3.5657 - acc: 0.2063 - val_loss: 3.5548 - val_acc: 0.3677\n",
      "Epoch 24/300\n",
      " - 2s - loss: 3.5779 - acc: 0.1429 - val_loss: 3.5354 - val_acc: 0.3320\n",
      "Epoch 25/300\n",
      " - 2s - loss: 3.5522 - acc: 0.1746 - val_loss: 3.5159 - val_acc: 0.3267\n",
      "Epoch 26/300\n",
      " - 2s - loss: 3.5494 - acc: 0.1587 - val_loss: 3.4966 - val_acc: 0.3122\n",
      "Epoch 27/300\n",
      " - 2s - loss: 3.4822 - acc: 0.2381 - val_loss: 3.4772 - val_acc: 0.4061\n",
      "Epoch 28/300\n",
      " - 2s - loss: 3.4766 - acc: 0.1905 - val_loss: 3.4582 - val_acc: 0.4511\n",
      "Epoch 29/300\n",
      " - 2s - loss: 3.5149 - acc: 0.1905 - val_loss: 3.4391 - val_acc: 0.5741\n",
      "Epoch 30/300\n",
      " - 2s - loss: 3.4264 - acc: 0.3492 - val_loss: 3.4204 - val_acc: 0.5384\n",
      "Epoch 31/300\n",
      " - 2s - loss: 3.4306 - acc: 0.3016 - val_loss: 3.4010 - val_acc: 0.5767\n",
      "Epoch 32/300\n",
      " - 2s - loss: 3.4131 - acc: 0.2381 - val_loss: 3.3816 - val_acc: 0.5767\n",
      "Epoch 33/300\n",
      " - 2s - loss: 3.3522 - acc: 0.2857 - val_loss: 3.3622 - val_acc: 0.6892\n",
      "Epoch 34/300\n",
      " - 2s - loss: 3.3915 - acc: 0.2063 - val_loss: 3.3431 - val_acc: 0.7156\n",
      "Epoch 35/300\n",
      " - 2s - loss: 3.3430 - acc: 0.3333 - val_loss: 3.3235 - val_acc: 0.7341\n",
      "Epoch 36/300\n",
      " - 2s - loss: 3.3229 - acc: 0.2381 - val_loss: 3.3033 - val_acc: 0.7526\n",
      "Epoch 37/300\n",
      " - 2s - loss: 3.2901 - acc: 0.3333 - val_loss: 3.2834 - val_acc: 0.7870\n",
      "Epoch 38/300\n",
      " - 2s - loss: 3.3276 - acc: 0.1746 - val_loss: 3.2637 - val_acc: 0.7474\n",
      "Epoch 39/300\n",
      " - 2s - loss: 3.2573 - acc: 0.2857 - val_loss: 3.2430 - val_acc: 0.7725\n",
      "Epoch 40/300\n",
      " - 2s - loss: 3.2597 - acc: 0.3651 - val_loss: 3.2206 - val_acc: 0.7368\n",
      "Epoch 41/300\n",
      " - 2s - loss: 3.2465 - acc: 0.3333 - val_loss: 3.1996 - val_acc: 0.7315\n",
      "Epoch 42/300\n",
      " - 2s - loss: 3.2136 - acc: 0.2698 - val_loss: 3.1807 - val_acc: 0.7381\n",
      "Epoch 43/300\n",
      " - 2s - loss: 3.2113 - acc: 0.2698 - val_loss: 3.1606 - val_acc: 0.7474\n",
      "Epoch 44/300\n",
      " - 2s - loss: 3.1392 - acc: 0.3333 - val_loss: 3.1402 - val_acc: 0.8042\n",
      "Epoch 45/300\n",
      " - 2s - loss: 3.1301 - acc: 0.3651 - val_loss: 3.1217 - val_acc: 0.7870\n",
      "Epoch 46/300\n",
      " - 2s - loss: 3.1416 - acc: 0.3810 - val_loss: 3.1020 - val_acc: 0.7937\n",
      "Epoch 47/300\n",
      " - 2s - loss: 3.1042 - acc: 0.3492 - val_loss: 3.0810 - val_acc: 0.8320\n",
      "Epoch 48/300\n",
      " - 2s - loss: 3.0356 - acc: 0.4603 - val_loss: 3.0603 - val_acc: 0.8320\n",
      "Epoch 49/300\n",
      " - 2s - loss: 3.0806 - acc: 0.3492 - val_loss: 3.0395 - val_acc: 0.8466\n",
      "Epoch 50/300\n",
      " - 2s - loss: 3.0114 - acc: 0.4603 - val_loss: 3.0186 - val_acc: 0.8717\n",
      "Epoch 51/300\n",
      " - 2s - loss: 3.0082 - acc: 0.4286 - val_loss: 2.9984 - val_acc: 0.8611\n",
      "Epoch 52/300\n",
      " - 2s - loss: 2.9767 - acc: 0.4921 - val_loss: 2.9783 - val_acc: 0.8452\n",
      "Epoch 53/300\n",
      " - 2s - loss: 2.9840 - acc: 0.4286 - val_loss: 2.9576 - val_acc: 0.8399\n",
      "Epoch 54/300\n",
      " - 2s - loss: 2.9477 - acc: 0.4603 - val_loss: 2.9371 - val_acc: 0.8585\n",
      "Epoch 55/300\n",
      " - 2s - loss: 2.9961 - acc: 0.3492 - val_loss: 2.9158 - val_acc: 0.8770\n",
      "Epoch 56/300\n",
      " - 2s - loss: 2.9515 - acc: 0.3968 - val_loss: 2.8949 - val_acc: 0.8664\n",
      "Epoch 57/300\n",
      " - 2s - loss: 2.8405 - acc: 0.5714 - val_loss: 2.8733 - val_acc: 0.8704\n",
      "Epoch 58/300\n",
      " - 2s - loss: 2.8373 - acc: 0.5714 - val_loss: 2.8512 - val_acc: 0.8929\n",
      "Epoch 59/300\n",
      " - 2s - loss: 2.8370 - acc: 0.5079 - val_loss: 2.8300 - val_acc: 0.9008\n",
      "Epoch 60/300\n",
      " - 2s - loss: 2.8805 - acc: 0.4127 - val_loss: 2.8092 - val_acc: 0.8862\n",
      "Epoch 61/300\n",
      " - 2s - loss: 2.7935 - acc: 0.5556 - val_loss: 2.7887 - val_acc: 0.9074\n",
      "Epoch 62/300\n",
      " - 2s - loss: 2.7629 - acc: 0.5873 - val_loss: 2.7687 - val_acc: 0.9061\n",
      "Epoch 63/300\n",
      " - 2s - loss: 2.7574 - acc: 0.4762 - val_loss: 2.7471 - val_acc: 0.9233\n",
      "Epoch 64/300\n",
      " - 2s - loss: 2.7559 - acc: 0.5238 - val_loss: 2.7273 - val_acc: 0.9272\n",
      "Epoch 65/300\n",
      " - 2s - loss: 2.7348 - acc: 0.5873 - val_loss: 2.7072 - val_acc: 0.9312\n",
      "Epoch 66/300\n",
      " - 2s - loss: 2.7185 - acc: 0.5556 - val_loss: 2.6872 - val_acc: 0.9325\n",
      "Epoch 67/300\n",
      " - 2s - loss: 2.7061 - acc: 0.5714 - val_loss: 2.6663 - val_acc: 0.9365\n",
      "Epoch 68/300\n",
      " - 2s - loss: 2.7114 - acc: 0.5238 - val_loss: 2.6458 - val_acc: 0.9484\n",
      "Epoch 69/300\n",
      " - 2s - loss: 2.6525 - acc: 0.5397 - val_loss: 2.6252 - val_acc: 0.9550\n",
      "Epoch 70/300\n",
      " - 2s - loss: 2.5928 - acc: 0.6349 - val_loss: 2.6031 - val_acc: 0.9392\n",
      "Epoch 71/300\n",
      " - 2s - loss: 2.6134 - acc: 0.5238 - val_loss: 2.5808 - val_acc: 0.9378\n",
      "Epoch 72/300\n",
      " - 2s - loss: 2.5655 - acc: 0.6032 - val_loss: 2.5583 - val_acc: 0.9418\n",
      "Epoch 73/300\n",
      " - 2s - loss: 2.5300 - acc: 0.6825 - val_loss: 2.5367 - val_acc: 0.9153\n",
      "Epoch 74/300\n",
      " - 2s - loss: 2.5214 - acc: 0.5873 - val_loss: 2.5158 - val_acc: 0.8955\n",
      "Epoch 75/300\n",
      " - 2s - loss: 2.4761 - acc: 0.6349 - val_loss: 2.4940 - val_acc: 0.9153\n",
      "Epoch 76/300\n",
      " - 2s - loss: 2.5709 - acc: 0.4762 - val_loss: 2.4731 - val_acc: 0.9074\n",
      "Epoch 77/300\n",
      " - 2s - loss: 2.4120 - acc: 0.6667 - val_loss: 2.4517 - val_acc: 0.9127\n",
      "Epoch 78/300\n",
      " - 2s - loss: 2.4451 - acc: 0.6825 - val_loss: 2.4283 - val_acc: 0.9299\n",
      "Epoch 79/300\n",
      " - 2s - loss: 2.4067 - acc: 0.5873 - val_loss: 2.4053 - val_acc: 0.9365\n",
      "Epoch 80/300\n",
      " - 2s - loss: 2.4006 - acc: 0.5873 - val_loss: 2.3814 - val_acc: 0.9550\n",
      "Epoch 81/300\n",
      " - 2s - loss: 2.4278 - acc: 0.5556 - val_loss: 2.3595 - val_acc: 0.9511\n",
      "Epoch 82/300\n",
      " - 2s - loss: 2.2646 - acc: 0.6667 - val_loss: 2.3371 - val_acc: 0.9431\n",
      "Epoch 83/300\n",
      " - 2s - loss: 2.3614 - acc: 0.6032 - val_loss: 2.3147 - val_acc: 0.9656\n",
      "Epoch 84/300\n",
      " - 2s - loss: 2.2629 - acc: 0.6825 - val_loss: 2.2930 - val_acc: 0.9418\n",
      "Epoch 85/300\n",
      " - 2s - loss: 2.2531 - acc: 0.6667 - val_loss: 2.2705 - val_acc: 0.9431\n",
      "Epoch 86/300\n",
      " - 2s - loss: 2.2927 - acc: 0.6349 - val_loss: 2.2492 - val_acc: 0.9405\n",
      "Epoch 87/300\n",
      " - 2s - loss: 2.2838 - acc: 0.6667 - val_loss: 2.2274 - val_acc: 0.9669\n",
      "Epoch 88/300\n",
      " - 2s - loss: 2.2179 - acc: 0.6190 - val_loss: 2.2037 - val_acc: 0.9696\n",
      "Epoch 89/300\n",
      " - 2s - loss: 2.2338 - acc: 0.6190 - val_loss: 2.1820 - val_acc: 0.9722\n",
      "Epoch 90/300\n",
      " - 2s - loss: 2.2138 - acc: 0.6349 - val_loss: 2.1602 - val_acc: 0.9775\n",
      "Epoch 91/300\n",
      " - 2s - loss: 2.1869 - acc: 0.6032 - val_loss: 2.1387 - val_acc: 0.9762\n",
      "Epoch 92/300\n",
      " - 2s - loss: 2.1992 - acc: 0.6667 - val_loss: 2.1177 - val_acc: 0.9762\n",
      "Epoch 93/300\n",
      " - 2s - loss: 2.1433 - acc: 0.6508 - val_loss: 2.0975 - val_acc: 0.9775\n",
      "Epoch 94/300\n",
      " - 2s - loss: 2.0975 - acc: 0.6667 - val_loss: 2.0763 - val_acc: 0.9775\n",
      "Epoch 95/300\n",
      " - 2s - loss: 2.0868 - acc: 0.6667 - val_loss: 2.0558 - val_acc: 0.9722\n",
      "Epoch 96/300\n",
      " - 2s - loss: 2.0357 - acc: 0.6825 - val_loss: 2.0333 - val_acc: 0.9735\n",
      "Epoch 97/300\n",
      " - 2s - loss: 2.0485 - acc: 0.7460 - val_loss: 2.0107 - val_acc: 0.9775\n",
      "Epoch 98/300\n",
      " - 2s - loss: 2.0077 - acc: 0.7460 - val_loss: 1.9890 - val_acc: 0.9775\n",
      "Epoch 99/300\n",
      " - 2s - loss: 1.9689 - acc: 0.7460 - val_loss: 1.9668 - val_acc: 0.9802\n",
      "Epoch 100/300\n",
      " - 2s - loss: 1.9060 - acc: 0.7937 - val_loss: 1.9465 - val_acc: 0.9749\n",
      "Epoch 101/300\n",
      " - 2s - loss: 2.0229 - acc: 0.6032 - val_loss: 1.9279 - val_acc: 0.9762\n",
      "Epoch 102/300\n",
      " - 2s - loss: 1.9285 - acc: 0.7619 - val_loss: 1.9079 - val_acc: 0.9775\n",
      "Epoch 103/300\n",
      " - 2s - loss: 1.9770 - acc: 0.7143 - val_loss: 1.8876 - val_acc: 0.9762\n",
      "Epoch 104/300\n",
      " - 2s - loss: 1.8281 - acc: 0.8095 - val_loss: 1.8674 - val_acc: 0.9775\n",
      "Epoch 105/300\n",
      " - 2s - loss: 1.8679 - acc: 0.7460 - val_loss: 1.8461 - val_acc: 0.9775\n",
      "Epoch 106/300\n",
      " - 2s - loss: 1.8119 - acc: 0.7937 - val_loss: 1.8260 - val_acc: 0.9788\n",
      "Epoch 107/300\n",
      " - 2s - loss: 1.8084 - acc: 0.7778 - val_loss: 1.8057 - val_acc: 0.9775\n",
      "Epoch 108/300\n",
      " - 2s - loss: 1.8069 - acc: 0.7619 - val_loss: 1.7860 - val_acc: 0.9788\n",
      "Epoch 109/300\n",
      " - 2s - loss: 1.8103 - acc: 0.8095 - val_loss: 1.7667 - val_acc: 0.9788\n",
      "Epoch 110/300\n",
      " - 2s - loss: 1.7774 - acc: 0.8095 - val_loss: 1.7453 - val_acc: 0.9788\n",
      "Epoch 111/300\n",
      " - 2s - loss: 1.7277 - acc: 0.8413 - val_loss: 1.7250 - val_acc: 0.9775\n",
      "Epoch 112/300\n",
      " - 2s - loss: 1.7601 - acc: 0.7460 - val_loss: 1.7045 - val_acc: 0.9775\n",
      "Epoch 113/300\n",
      " - 2s - loss: 1.7053 - acc: 0.7937 - val_loss: 1.6852 - val_acc: 0.9802\n",
      "Epoch 114/300\n",
      " - 2s - loss: 1.7243 - acc: 0.7937 - val_loss: 1.6662 - val_acc: 0.9788\n",
      "Epoch 115/300\n",
      " - 2s - loss: 1.6349 - acc: 0.7937 - val_loss: 1.6486 - val_acc: 0.9775\n",
      "Epoch 116/300\n",
      " - 2s - loss: 1.5785 - acc: 0.8730 - val_loss: 1.6276 - val_acc: 0.9775\n",
      "Epoch 117/300\n",
      " - 2s - loss: 1.7051 - acc: 0.7619 - val_loss: 1.6090 - val_acc: 0.9788\n",
      "Epoch 118/300\n",
      " - 2s - loss: 1.6545 - acc: 0.7937 - val_loss: 1.5889 - val_acc: 0.9802\n",
      "Epoch 119/300\n",
      " - 2s - loss: 1.6155 - acc: 0.7937 - val_loss: 1.5712 - val_acc: 0.9802\n",
      "Epoch 120/300\n",
      " - 2s - loss: 1.5136 - acc: 0.8889 - val_loss: 1.5525 - val_acc: 0.9828\n",
      "Epoch 121/300\n",
      " - 2s - loss: 1.6337 - acc: 0.7619 - val_loss: 1.5349 - val_acc: 0.9841\n",
      "Epoch 122/300\n",
      " - 2s - loss: 1.5268 - acc: 0.8571 - val_loss: 1.5179 - val_acc: 0.9815\n",
      "Epoch 123/300\n",
      " - 2s - loss: 1.5883 - acc: 0.7460 - val_loss: 1.5001 - val_acc: 0.9788\n",
      "Epoch 124/300\n",
      " - 2s - loss: 1.4930 - acc: 0.8413 - val_loss: 1.4817 - val_acc: 0.9788\n",
      "Epoch 125/300\n",
      " - 2s - loss: 1.4514 - acc: 0.9048 - val_loss: 1.4626 - val_acc: 0.9788\n",
      "Epoch 126/300\n",
      " - 2s - loss: 1.4111 - acc: 0.9048 - val_loss: 1.4456 - val_acc: 0.9788\n",
      "Epoch 127/300\n",
      " - 2s - loss: 1.5102 - acc: 0.8413 - val_loss: 1.4269 - val_acc: 0.9788\n",
      "Epoch 128/300\n",
      " - 2s - loss: 1.4391 - acc: 0.8730 - val_loss: 1.4092 - val_acc: 0.9802\n",
      "Epoch 129/300\n",
      " - 2s - loss: 1.4123 - acc: 0.8730 - val_loss: 1.3918 - val_acc: 0.9802\n",
      "Epoch 130/300\n",
      " - 2s - loss: 1.3981 - acc: 0.8889 - val_loss: 1.3722 - val_acc: 0.9854\n",
      "Epoch 131/300\n",
      " - 2s - loss: 1.4613 - acc: 0.8889 - val_loss: 1.3555 - val_acc: 0.9854\n",
      "Epoch 132/300\n",
      " - 2s - loss: 1.4383 - acc: 0.8730 - val_loss: 1.3399 - val_acc: 0.9841\n",
      "Epoch 133/300\n",
      " - 2s - loss: 1.3349 - acc: 0.9048 - val_loss: 1.3231 - val_acc: 0.9841\n",
      "Epoch 134/300\n",
      " - 2s - loss: 1.3437 - acc: 0.9206 - val_loss: 1.3071 - val_acc: 0.9802\n",
      "Epoch 135/300\n",
      " - 2s - loss: 1.3083 - acc: 0.9048 - val_loss: 1.2895 - val_acc: 0.9828\n",
      "Epoch 136/300\n",
      " - 2s - loss: 1.2925 - acc: 0.8889 - val_loss: 1.2733 - val_acc: 0.9841\n",
      "Epoch 137/300\n",
      " - 2s - loss: 1.2997 - acc: 0.9048 - val_loss: 1.2556 - val_acc: 0.9841\n",
      "Epoch 138/300\n",
      " - 2s - loss: 1.2695 - acc: 0.9048 - val_loss: 1.2398 - val_acc: 0.9815\n",
      "Epoch 139/300\n",
      " - 2s - loss: 1.2105 - acc: 0.9365 - val_loss: 1.2221 - val_acc: 0.9815\n",
      "Epoch 140/300\n",
      " - 2s - loss: 1.2811 - acc: 0.8571 - val_loss: 1.2071 - val_acc: 0.9802\n",
      "Epoch 141/300\n",
      " - 2s - loss: 1.3297 - acc: 0.8571 - val_loss: 1.1949 - val_acc: 0.9802\n",
      "Epoch 142/300\n",
      " - 2s - loss: 1.2446 - acc: 0.9206 - val_loss: 1.1796 - val_acc: 0.9788\n",
      "Epoch 143/300\n",
      " - 2s - loss: 1.1722 - acc: 0.9365 - val_loss: 1.1655 - val_acc: 0.9802\n",
      "Epoch 144/300\n",
      " - 2s - loss: 1.1569 - acc: 0.9206 - val_loss: 1.1514 - val_acc: 0.9802\n",
      "Epoch 145/300\n",
      " - 2s - loss: 1.1809 - acc: 0.8730 - val_loss: 1.1382 - val_acc: 0.9802\n",
      "Epoch 146/300\n",
      " - 2s - loss: 1.1466 - acc: 0.9048 - val_loss: 1.1245 - val_acc: 0.9802\n",
      "Epoch 147/300\n",
      " - 2s - loss: 1.0565 - acc: 0.9206 - val_loss: 1.1071 - val_acc: 0.9802\n",
      "Epoch 148/300\n",
      " - 2s - loss: 1.1304 - acc: 0.9048 - val_loss: 1.0932 - val_acc: 0.9854\n",
      "Epoch 149/300\n",
      " - 2s - loss: 1.1517 - acc: 0.9365 - val_loss: 1.0802 - val_acc: 0.9868\n",
      "Epoch 150/300\n",
      " - 2s - loss: 1.1256 - acc: 0.9841 - val_loss: 1.0680 - val_acc: 0.9868\n",
      "Epoch 151/300\n",
      " - 2s - loss: 1.0491 - acc: 0.9048 - val_loss: 1.0553 - val_acc: 0.9868\n",
      "Epoch 152/300\n",
      " - 2s - loss: 1.0947 - acc: 0.9841 - val_loss: 1.0406 - val_acc: 0.9881\n",
      "Epoch 153/300\n",
      " - 2s - loss: 1.0297 - acc: 0.9683 - val_loss: 1.0248 - val_acc: 0.9907\n",
      "Epoch 154/300\n",
      " - 2s - loss: 1.0532 - acc: 0.9365 - val_loss: 1.0127 - val_acc: 0.9881\n",
      "Epoch 155/300\n",
      " - 2s - loss: 1.0699 - acc: 0.9048 - val_loss: 1.0008 - val_acc: 0.9854\n",
      "Epoch 156/300\n",
      " - 2s - loss: 1.0380 - acc: 0.9206 - val_loss: 0.9901 - val_acc: 0.9854\n",
      "Epoch 157/300\n",
      " - 2s - loss: 0.9909 - acc: 0.9524 - val_loss: 0.9815 - val_acc: 0.9881\n",
      "Epoch 158/300\n",
      " - 2s - loss: 1.0023 - acc: 0.9206 - val_loss: 0.9721 - val_acc: 0.9868\n",
      "Epoch 159/300\n",
      " - 2s - loss: 1.0584 - acc: 0.9365 - val_loss: 0.9593 - val_acc: 0.9828\n",
      "Epoch 160/300\n",
      " - 2s - loss: 1.0113 - acc: 0.9048 - val_loss: 0.9457 - val_acc: 0.9868\n",
      "Epoch 161/300\n",
      " - 2s - loss: 0.9375 - acc: 0.9524 - val_loss: 0.9352 - val_acc: 0.9868\n",
      "Epoch 162/300\n",
      " - 2s - loss: 0.9359 - acc: 0.9524 - val_loss: 0.9216 - val_acc: 0.9894\n",
      "Epoch 163/300\n",
      " - 2s - loss: 0.9640 - acc: 0.9365 - val_loss: 0.9109 - val_acc: 0.9894\n",
      "Epoch 164/300\n",
      " - 2s - loss: 0.9773 - acc: 0.9365 - val_loss: 0.9015 - val_acc: 0.9894\n",
      "Epoch 165/300\n",
      " - 2s - loss: 0.8962 - acc: 0.9365 - val_loss: 0.8926 - val_acc: 0.9894\n",
      "Epoch 166/300\n",
      " - 2s - loss: 0.9275 - acc: 0.9206 - val_loss: 0.8833 - val_acc: 0.9894\n",
      "Epoch 167/300\n",
      " - 2s - loss: 0.9533 - acc: 0.9048 - val_loss: 0.8738 - val_acc: 0.9894\n",
      "Epoch 168/300\n",
      " - 2s - loss: 0.9299 - acc: 0.9365 - val_loss: 0.8649 - val_acc: 0.9881\n",
      "Epoch 169/300\n",
      " - 2s - loss: 1.0244 - acc: 0.8730 - val_loss: 0.8524 - val_acc: 0.9894\n",
      "Epoch 170/300\n",
      " - 2s - loss: 0.8564 - acc: 0.9841 - val_loss: 0.8423 - val_acc: 0.9907\n",
      "Epoch 171/300\n",
      " - 2s - loss: 0.8628 - acc: 0.9206 - val_loss: 0.8316 - val_acc: 0.9947\n",
      "Epoch 172/300\n",
      " - 2s - loss: 0.8945 - acc: 0.9206 - val_loss: 0.8220 - val_acc: 0.9974\n",
      "Epoch 173/300\n",
      " - 2s - loss: 0.8422 - acc: 1.0000 - val_loss: 0.8143 - val_acc: 0.9947\n",
      "Epoch 174/300\n",
      " - 2s - loss: 0.9214 - acc: 0.9524 - val_loss: 0.8081 - val_acc: 0.9921\n",
      "Epoch 175/300\n",
      " - 2s - loss: 0.8797 - acc: 0.9841 - val_loss: 0.7995 - val_acc: 0.9934\n",
      "Epoch 176/300\n",
      " - 2s - loss: 0.7471 - acc: 0.9841 - val_loss: 0.7904 - val_acc: 0.9947\n",
      "Epoch 177/300\n",
      " - 2s - loss: 0.8440 - acc: 0.9524 - val_loss: 0.7822 - val_acc: 0.9974\n",
      "Epoch 178/300\n",
      " - 2s - loss: 0.8825 - acc: 0.9683 - val_loss: 0.7746 - val_acc: 0.9947\n",
      "Epoch 179/300\n",
      " - 2s - loss: 0.7684 - acc: 0.9683 - val_loss: 0.7673 - val_acc: 0.9934\n",
      "Epoch 180/300\n",
      " - 2s - loss: 0.8426 - acc: 0.9524 - val_loss: 0.7624 - val_acc: 0.9907\n",
      "Epoch 181/300\n",
      " - 2s - loss: 0.7604 - acc: 0.9683 - val_loss: 0.7532 - val_acc: 0.9947\n",
      "Epoch 182/300\n",
      " - 2s - loss: 0.8233 - acc: 0.9365 - val_loss: 0.7463 - val_acc: 0.9947\n",
      "Epoch 183/300\n",
      " - 2s - loss: 0.8046 - acc: 0.9206 - val_loss: 0.7393 - val_acc: 0.9974\n",
      "Epoch 184/300\n",
      " - 2s - loss: 0.7464 - acc: 0.9683 - val_loss: 0.7331 - val_acc: 0.9974\n",
      "Epoch 185/300\n",
      " - 2s - loss: 0.8083 - acc: 0.9683 - val_loss: 0.7261 - val_acc: 0.9974\n",
      "Epoch 186/300\n",
      " - 2s - loss: 0.7470 - acc: 0.9683 - val_loss: 0.7192 - val_acc: 0.9974\n",
      "Epoch 187/300\n",
      " - 2s - loss: 0.7129 - acc: 1.0000 - val_loss: 0.7118 - val_acc: 0.9974\n",
      "Epoch 188/300\n",
      " - 2s - loss: 0.7636 - acc: 0.9841 - val_loss: 0.7046 - val_acc: 0.9987\n",
      "Epoch 189/300\n",
      " - 2s - loss: 0.7272 - acc: 0.9683 - val_loss: 0.6986 - val_acc: 1.0000\n",
      "Epoch 190/300\n",
      " - 2s - loss: 0.7393 - acc: 0.9841 - val_loss: 0.6928 - val_acc: 1.0000\n",
      "Epoch 191/300\n",
      " - 2s - loss: 0.7358 - acc: 0.9683 - val_loss: 0.6873 - val_acc: 1.0000\n",
      "Epoch 192/300\n",
      " - 2s - loss: 0.7933 - acc: 0.9524 - val_loss: 0.6823 - val_acc: 0.9987\n",
      "Epoch 193/300\n",
      " - 2s - loss: 0.7243 - acc: 0.9524 - val_loss: 0.6760 - val_acc: 0.9974\n",
      "Epoch 194/300\n",
      " - 2s - loss: 0.6872 - acc: 1.0000 - val_loss: 0.6699 - val_acc: 0.9974\n",
      "Epoch 195/300\n",
      " - 2s - loss: 0.7150 - acc: 0.9683 - val_loss: 0.6634 - val_acc: 0.9974\n",
      "Epoch 196/300\n",
      " - 2s - loss: 0.6638 - acc: 1.0000 - val_loss: 0.6572 - val_acc: 0.9987\n",
      "Epoch 197/300\n",
      " - 2s - loss: 0.6310 - acc: 1.0000 - val_loss: 0.6521 - val_acc: 0.9987\n",
      "Epoch 198/300\n",
      " - 2s - loss: 0.6380 - acc: 0.9841 - val_loss: 0.6485 - val_acc: 0.9974\n",
      "Epoch 199/300\n",
      " - 2s - loss: 0.6646 - acc: 1.0000 - val_loss: 0.6431 - val_acc: 0.9974\n",
      "Epoch 200/300\n",
      " - 2s - loss: 0.7114 - acc: 0.9841 - val_loss: 0.6358 - val_acc: 0.9987\n",
      "Epoch 201/300\n",
      " - 2s - loss: 0.6035 - acc: 1.0000 - val_loss: 0.6296 - val_acc: 0.9987\n",
      "Epoch 202/300\n",
      " - 2s - loss: 0.6284 - acc: 0.9841 - val_loss: 0.6248 - val_acc: 0.9987\n",
      "Epoch 203/300\n",
      " - 2s - loss: 0.6298 - acc: 1.0000 - val_loss: 0.6206 - val_acc: 0.9987\n",
      "Epoch 204/300\n",
      " - 2s - loss: 0.6937 - acc: 1.0000 - val_loss: 0.6162 - val_acc: 0.9987\n",
      "Epoch 205/300\n",
      " - 2s - loss: 0.5964 - acc: 0.9841 - val_loss: 0.6109 - val_acc: 0.9987\n",
      "Epoch 206/300\n",
      " - 2s - loss: 0.6533 - acc: 1.0000 - val_loss: 0.6065 - val_acc: 1.0000\n",
      "Epoch 207/300\n",
      " - 2s - loss: 0.6406 - acc: 1.0000 - val_loss: 0.6027 - val_acc: 0.9987\n",
      "Epoch 208/300\n",
      " - 2s - loss: 0.6222 - acc: 1.0000 - val_loss: 0.5979 - val_acc: 1.0000\n",
      "Epoch 209/300\n",
      " - 2s - loss: 0.5858 - acc: 1.0000 - val_loss: 0.5939 - val_acc: 1.0000\n",
      "Epoch 210/300\n",
      " - 2s - loss: 0.5992 - acc: 0.9841 - val_loss: 0.5900 - val_acc: 1.0000\n",
      "Epoch 211/300\n",
      " - 2s - loss: 0.6640 - acc: 0.9206 - val_loss: 0.5850 - val_acc: 1.0000\n",
      "Epoch 212/300\n",
      " - 2s - loss: 0.5878 - acc: 1.0000 - val_loss: 0.5816 - val_acc: 1.0000\n",
      "Epoch 213/300\n",
      " - 2s - loss: 0.5813 - acc: 1.0000 - val_loss: 0.5786 - val_acc: 1.0000\n",
      "Epoch 214/300\n",
      " - 2s - loss: 0.6277 - acc: 0.9683 - val_loss: 0.5764 - val_acc: 0.9987\n",
      "Epoch 215/300\n",
      " - 2s - loss: 0.6850 - acc: 0.9683 - val_loss: 0.5746 - val_acc: 1.0000\n",
      "Epoch 216/300\n",
      " - 2s - loss: 0.5964 - acc: 1.0000 - val_loss: 0.5715 - val_acc: 1.0000\n",
      "Epoch 217/300\n",
      " - 2s - loss: 0.5983 - acc: 0.9841 - val_loss: 0.5675 - val_acc: 1.0000\n",
      "Epoch 218/300\n",
      " - 2s - loss: 0.6240 - acc: 0.9683 - val_loss: 0.5635 - val_acc: 1.0000\n",
      "Epoch 219/300\n",
      " - 2s - loss: 0.6041 - acc: 1.0000 - val_loss: 0.5599 - val_acc: 1.0000\n",
      "Epoch 220/300\n",
      " - 2s - loss: 0.5646 - acc: 1.0000 - val_loss: 0.5563 - val_acc: 1.0000\n",
      "Epoch 221/300\n",
      " - 2s - loss: 0.5608 - acc: 1.0000 - val_loss: 0.5527 - val_acc: 1.0000\n",
      "Epoch 222/300\n",
      " - 2s - loss: 0.5646 - acc: 1.0000 - val_loss: 0.5498 - val_acc: 1.0000\n",
      "Epoch 223/300\n",
      " - 2s - loss: 0.6014 - acc: 0.9683 - val_loss: 0.5484 - val_acc: 0.9987\n",
      "Epoch 224/300\n",
      " - 2s - loss: 0.5635 - acc: 0.9841 - val_loss: 0.5498 - val_acc: 0.9987\n",
      "Epoch 225/300\n",
      " - 2s - loss: 0.5469 - acc: 1.0000 - val_loss: 0.5490 - val_acc: 0.9987\n",
      "Epoch 226/300\n",
      " - 2s - loss: 0.5599 - acc: 0.9841 - val_loss: 0.5434 - val_acc: 1.0000\n",
      "Epoch 227/300\n",
      " - 2s - loss: 0.5582 - acc: 0.9841 - val_loss: 0.5406 - val_acc: 1.0000\n",
      "Epoch 228/300\n",
      " - 2s - loss: 0.5339 - acc: 1.0000 - val_loss: 0.5378 - val_acc: 1.0000\n",
      "Epoch 229/300\n",
      " - 2s - loss: 0.5432 - acc: 1.0000 - val_loss: 0.5350 - val_acc: 1.0000\n",
      "Epoch 230/300\n",
      " - 2s - loss: 0.5917 - acc: 1.0000 - val_loss: 0.5328 - val_acc: 1.0000\n",
      "Epoch 231/300\n",
      " - 2s - loss: 0.5593 - acc: 1.0000 - val_loss: 0.5292 - val_acc: 1.0000\n",
      "Epoch 232/300\n",
      " - 2s - loss: 0.5700 - acc: 1.0000 - val_loss: 0.5256 - val_acc: 1.0000\n",
      "Epoch 233/300\n",
      " - 2s - loss: 0.5470 - acc: 1.0000 - val_loss: 0.5229 - val_acc: 1.0000\n",
      "Epoch 234/300\n",
      " - 2s - loss: 0.5429 - acc: 1.0000 - val_loss: 0.5208 - val_acc: 1.0000\n",
      "Epoch 235/300\n",
      " - 2s - loss: 0.5624 - acc: 0.9841 - val_loss: 0.5187 - val_acc: 1.0000\n",
      "Epoch 236/300\n",
      " - 2s - loss: 0.5390 - acc: 1.0000 - val_loss: 0.5161 - val_acc: 1.0000\n",
      "Epoch 237/300\n",
      " - 2s - loss: 0.5406 - acc: 1.0000 - val_loss: 0.5144 - val_acc: 1.0000\n",
      "Epoch 238/300\n",
      " - 2s - loss: 0.5551 - acc: 0.9841 - val_loss: 0.5134 - val_acc: 1.0000\n",
      "Epoch 239/300\n",
      " - 2s - loss: 0.5318 - acc: 1.0000 - val_loss: 0.5121 - val_acc: 1.0000\n",
      "Epoch 240/300\n",
      " - 2s - loss: 0.5275 - acc: 1.0000 - val_loss: 0.5094 - val_acc: 1.0000\n",
      "Epoch 241/300\n",
      " - 2s - loss: 0.5213 - acc: 1.0000 - val_loss: 0.5071 - val_acc: 1.0000\n",
      "Epoch 242/300\n",
      " - 2s - loss: 0.5268 - acc: 0.9841 - val_loss: 0.5049 - val_acc: 1.0000\n",
      "Epoch 243/300\n",
      " - 2s - loss: 0.5141 - acc: 1.0000 - val_loss: 0.5030 - val_acc: 1.0000\n",
      "Epoch 244/300\n",
      " - 2s - loss: 0.5214 - acc: 1.0000 - val_loss: 0.5009 - val_acc: 1.0000\n",
      "Epoch 245/300\n",
      " - 2s - loss: 0.5451 - acc: 0.9841 - val_loss: 0.4982 - val_acc: 1.0000\n",
      "Epoch 246/300\n",
      " - 2s - loss: 0.4930 - acc: 1.0000 - val_loss: 0.4960 - val_acc: 1.0000\n",
      "Epoch 247/300\n",
      " - 2s - loss: 0.5150 - acc: 0.9841 - val_loss: 0.4944 - val_acc: 1.0000\n",
      "Epoch 248/300\n",
      " - 2s - loss: 0.5258 - acc: 1.0000 - val_loss: 0.4949 - val_acc: 1.0000\n",
      "Epoch 249/300\n",
      " - 2s - loss: 0.5403 - acc: 1.0000 - val_loss: 0.4950 - val_acc: 1.0000\n",
      "Epoch 250/300\n",
      " - 2s - loss: 0.5038 - acc: 1.0000 - val_loss: 0.4947 - val_acc: 1.0000\n",
      "Epoch 251/300\n",
      " - 2s - loss: 0.5270 - acc: 0.9683 - val_loss: 0.4916 - val_acc: 1.0000\n",
      "Epoch 252/300\n",
      " - 2s - loss: 0.5209 - acc: 1.0000 - val_loss: 0.4882 - val_acc: 1.0000\n",
      "Epoch 253/300\n",
      " - 2s - loss: 0.4889 - acc: 1.0000 - val_loss: 0.4860 - val_acc: 1.0000\n",
      "Epoch 254/300\n",
      " - 2s - loss: 0.5036 - acc: 1.0000 - val_loss: 0.4845 - val_acc: 1.0000\n",
      "Epoch 255/300\n",
      " - 2s - loss: 0.5195 - acc: 0.9841 - val_loss: 0.4849 - val_acc: 1.0000\n",
      "Epoch 256/300\n",
      " - 2s - loss: 0.5019 - acc: 1.0000 - val_loss: 0.4842 - val_acc: 1.0000\n",
      "Epoch 257/300\n",
      " - 2s - loss: 0.5091 - acc: 0.9841 - val_loss: 0.4836 - val_acc: 1.0000\n",
      "Epoch 258/300\n",
      " - 2s - loss: 0.4871 - acc: 1.0000 - val_loss: 0.4835 - val_acc: 1.0000\n",
      "Epoch 259/300\n",
      " - 2s - loss: 0.4939 - acc: 1.0000 - val_loss: 0.4817 - val_acc: 1.0000\n",
      "Epoch 260/300\n",
      " - 2s - loss: 0.4755 - acc: 1.0000 - val_loss: 0.4792 - val_acc: 1.0000\n",
      "Epoch 261/300\n",
      " - 2s - loss: 0.5035 - acc: 1.0000 - val_loss: 0.4769 - val_acc: 1.0000\n",
      "Epoch 262/300\n",
      " - 2s - loss: 0.5070 - acc: 1.0000 - val_loss: 0.4745 - val_acc: 1.0000\n",
      "Epoch 263/300\n",
      " - 2s - loss: 0.5146 - acc: 1.0000 - val_loss: 0.4736 - val_acc: 1.0000\n",
      "Epoch 264/300\n",
      " - 2s - loss: 0.4812 - acc: 1.0000 - val_loss: 0.4727 - val_acc: 1.0000\n",
      "Epoch 265/300\n",
      " - 2s - loss: 0.4699 - acc: 1.0000 - val_loss: 0.4714 - val_acc: 1.0000\n",
      "Epoch 266/300\n",
      " - 2s - loss: 0.4864 - acc: 0.9841 - val_loss: 0.4694 - val_acc: 1.0000\n",
      "Epoch 267/300\n",
      " - 2s - loss: 0.4607 - acc: 1.0000 - val_loss: 0.4684 - val_acc: 1.0000\n",
      "Epoch 268/300\n",
      " - 2s - loss: 0.4623 - acc: 1.0000 - val_loss: 0.4678 - val_acc: 1.0000\n",
      "Epoch 269/300\n",
      " - 2s - loss: 0.5048 - acc: 0.9841 - val_loss: 0.4667 - val_acc: 1.0000\n",
      "Epoch 270/300\n",
      " - 2s - loss: 0.4769 - acc: 0.9841 - val_loss: 0.4662 - val_acc: 1.0000\n",
      "Epoch 271/300\n",
      " - 2s - loss: 0.4698 - acc: 1.0000 - val_loss: 0.4661 - val_acc: 1.0000\n",
      "Epoch 272/300\n",
      " - 2s - loss: 0.4746 - acc: 1.0000 - val_loss: 0.4653 - val_acc: 1.0000\n",
      "Epoch 273/300\n",
      " - 2s - loss: 0.4784 - acc: 1.0000 - val_loss: 0.4636 - val_acc: 1.0000\n",
      "Epoch 274/300\n",
      " - 2s - loss: 0.4708 - acc: 1.0000 - val_loss: 0.4618 - val_acc: 1.0000\n",
      "Epoch 275/300\n",
      " - 2s - loss: 0.4976 - acc: 1.0000 - val_loss: 0.4608 - val_acc: 1.0000\n",
      "Epoch 276/300\n",
      " - 2s - loss: 0.4619 - acc: 1.0000 - val_loss: 0.4611 - val_acc: 1.0000\n",
      "Epoch 277/300\n",
      " - 2s - loss: 0.4893 - acc: 1.0000 - val_loss: 0.4607 - val_acc: 1.0000\n",
      "Epoch 278/300\n",
      " - 2s - loss: 0.4500 - acc: 1.0000 - val_loss: 0.4596 - val_acc: 1.0000\n",
      "Epoch 279/300\n",
      " - 2s - loss: 0.4589 - acc: 1.0000 - val_loss: 0.4582 - val_acc: 1.0000\n",
      "Epoch 280/300\n",
      " - 2s - loss: 0.4591 - acc: 1.0000 - val_loss: 0.4572 - val_acc: 1.0000\n",
      "Epoch 281/300\n",
      " - 2s - loss: 0.4830 - acc: 1.0000 - val_loss: 0.4562 - val_acc: 1.0000\n",
      "Epoch 282/300\n",
      " - 2s - loss: 0.4694 - acc: 1.0000 - val_loss: 0.4547 - val_acc: 1.0000\n",
      "Epoch 283/300\n",
      " - 2s - loss: 0.4605 - acc: 1.0000 - val_loss: 0.4544 - val_acc: 1.0000\n",
      "Epoch 284/300\n",
      " - 2s - loss: 0.4616 - acc: 0.9841 - val_loss: 0.4539 - val_acc: 1.0000\n",
      "Epoch 285/300\n",
      " - 2s - loss: 0.4776 - acc: 1.0000 - val_loss: 0.4528 - val_acc: 1.0000\n",
      "Epoch 286/300\n",
      " - 2s - loss: 0.4427 - acc: 1.0000 - val_loss: 0.4520 - val_acc: 1.0000\n",
      "Epoch 287/300\n",
      " - 2s - loss: 0.4611 - acc: 1.0000 - val_loss: 0.4508 - val_acc: 1.0000\n",
      "Epoch 288/300\n",
      " - 2s - loss: 0.4708 - acc: 1.0000 - val_loss: 0.4501 - val_acc: 1.0000\n",
      "Epoch 289/300\n",
      " - 2s - loss: 0.4780 - acc: 1.0000 - val_loss: 0.4502 - val_acc: 1.0000\n",
      "Epoch 290/300\n",
      " - 2s - loss: 0.4654 - acc: 1.0000 - val_loss: 0.4496 - val_acc: 1.0000\n",
      "Epoch 291/300\n",
      " - 2s - loss: 0.4763 - acc: 0.9841 - val_loss: 0.4491 - val_acc: 1.0000\n",
      "Epoch 292/300\n",
      " - 2s - loss: 0.4389 - acc: 1.0000 - val_loss: 0.4479 - val_acc: 1.0000\n",
      "Epoch 293/300\n",
      " - 2s - loss: 0.4557 - acc: 1.0000 - val_loss: 0.4465 - val_acc: 1.0000\n",
      "Epoch 294/300\n",
      " - 2s - loss: 0.4637 - acc: 1.0000 - val_loss: 0.4455 - val_acc: 1.0000\n",
      "Epoch 295/300\n",
      " - 2s - loss: 0.4491 - acc: 1.0000 - val_loss: 0.4445 - val_acc: 1.0000\n",
      "Epoch 296/300\n",
      " - 2s - loss: 0.4504 - acc: 1.0000 - val_loss: 0.4435 - val_acc: 1.0000\n",
      "Epoch 297/300\n",
      " - 2s - loss: 0.4449 - acc: 1.0000 - val_loss: 0.4427 - val_acc: 1.0000\n",
      "Epoch 298/300\n",
      " - 2s - loss: 0.4557 - acc: 0.9841 - val_loss: 0.4418 - val_acc: 1.0000\n",
      "Epoch 299/300\n",
      " - 2s - loss: 0.4464 - acc: 1.0000 - val_loss: 0.4416 - val_acc: 1.0000\n",
      "Epoch 300/300\n",
      " - 2s - loss: 0.4414 - acc: 1.0000 - val_loss: 0.4411 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3XmcTfX/wPHXexYzGAxjKvtSKtsYY5JS9soSSrJFSSUt3+qr+iVapE2FJCqSUvbIkiwlI60YssuXGBnEGOvYZsZ8fn98Dk3Mhjtz5t55Px+P+5hz7/ncc97nHt73cz/ncz4fMcaglFLKt/i5HYBSSinP0+SulFI+SJO7Ukr5IE3uSinlgzS5K6WUD9LkrpRSPkiTu8qQiPiLSJKIVPRkWTeJyFUi4vG+vyLSQkTi0j3fLCI356TsRexrrIj0v9j3Z7Hd10TkM09vV7knwO0AlGeISFK6p0WAU8Bp5/nDxpiJF7I9Y8xpIMTTZQsCY8w1ntiOiDwIdDfGNEm37Qc9sW3l+zS5+whjzNnk6tQMHzTGLMqsvIgEGGNS8yI2pVTe02aZAsL52T1VRCaLyFGgu4jcICK/icghEdkjIiNEJNApHyAiRkQqO88nOOvni8hREflVRKpcaFlnfSsR+Z+IHBaR90XkZxHpmUncOYnxYRHZKiIHRWREuvf6i8i7IpIoIn8CLbP4fF4QkSnnvDZKRIY5yw+KyCbneP50atWZbSteRJo4y0VE5Asntg1AvQz2u83Z7gYRaee8XhsYCdzsNHntT/fZDkz3/j7OsSeKyCwRKZOTzyY7InKHE88hEVksItekW9dfRHaLyBER+SPdsTYQkVXO63tF5J2c7k/lAmOMPnzsAcQBLc557TUgGWiL/VIvDFwHXI/9BVcV+B/wuFM+ADBAZef5BGA/EA0EAlOBCRdR9jLgKNDeWdcXSAF6ZnIsOYlxNlACqAwcOHPswOPABqA8EAYstf/kM9xPVSAJKJpu2/uAaOd5W6eMAM2AE0CEs64FEJduW/FAE2d5CLAEKAlUAjaeU7YTUMY5J92cGC531j0ILDknzgnAQGf5VifGSCAY+ABYnJPPJoPjfw34zFmu7sTRzDlH/Z3PPRCoCewArnDKVgGqOssrgK7OcjHgerf/LxTkh9bcC5afjDFfG2PSjDEnjDErjDHLjDGpxphtwBigcRbvn26MiTXGpAATsUnlQsveDqw2xsx21r2L/SLIUA5jfNMYc9gYE4dNpGf21Ql41xgTb4xJBAZnsZ9twHrslw7ALcAhY0yss/5rY8w2Yy0GvgcyvGh6jk7Aa8aYg8aYHdjaePr9TjPG7HHOySTsF3N0DrYLcA8w1hiz2hhzEugHNBaR8unKZPbZZKULMMcYs9g5R4OB4tgv2VTsF0lNp2lvu/PZgf2SriYiYcaYo8aYZTk8DpULNLkXLDvTPxGRa0XkGxH5W0SOAIOA0lm8/+90y8fJ+iJqZmXLpo/DGGOwNd0M5TDGHO0LW+PMyiSgq7PcDfuldCaO20VkmYgcEJFD2FpzVp/VGWWyikFEeorIGqf54xBwbQ63C/b4zm7PGHMEOAiUS1fmQs5ZZttNw56jcsaYzcDT2POwz2nmu8Ipej9QA9gsIstFpHUOj0PlAk3uBcu53QBHY2urVxljigMvYZsdctMebDMJACIi/DsZnetSYtwDVEj3PLuumlOBFk7Ntz022SMihYHpwJvYJpNQ4NscxvF3ZjGISFXgQ+ARIMzZ7h/ptptdt83d2KaeM9srhm3+2ZWDuC5ku37Yc7YLwBgzwRjTENsk44/9XDDGbDbGdME2vQ0FZohI8CXGoi6SJveCrRhwGDgmItWBh/Ngn3OBKBFpKyIBwJNAeC7FOA14SkTKiUgY8FxWhY0xe4GfgE+BzcaYLc6qIKAQkACcFpHbgeYXEEN/EQkVex/A4+nWhWATeAL2e+5BbM39jL1A+TMXkDMwGXhARCJEJAibZH80xmT6S+gCYm4nIk2cfT+LvU6yTESqi0hTZ38nnMdp7AH0EJHSTk3/sHNsaZcYi7pImtwLtqeB+7D/cUdja665ykmgnYFhQCJwJfA7tl++p2P8ENs2vg57sW96Dt4zCXuBdFK6mA8B/wVmYi9KdsR+SeXEy9hfEHHAfODzdNtdC4wAljtlrgXSt1N/B2wB9opI+uaVM+9fgG0emem8vyK2Hf6SGGM2YD/zD7FfPC2Bdk77exDwNvY6yd/YXwovOG9tDWwS2xtrCNDZGJN8qfGoiyO2yVMpd4iIP7YZoKMx5ke341HKV2jNXeU5EWkpIiWcn/YvYntgLHc5LKV8iiZ35YabgG3Yn/YtgTuMMZk1yyilLoI2yyillA/SmrtSSvkg1wYOK126tKlcubJbu1dKKa+0cuXK/caYrLoPAy4m98qVKxMbG+vW7pVSyiuJSHZ3WgPaLKOUUj5Jk7tSSvkgTe5KKeWDdCYmpQqIlJQU4uPjOXnypNuhqBwIDg6mfPnyBAZmNrRQ1jS5K1VAxMfHU6xYMSpXrowdjFPlV8YYEhMTiY+Pp0qVKtm/IQPaLKNUAXHy5EnCwsI0sXsBESEsLOySfmVpcleqANHE7j0u9Vx5X3Lfvx+eegq03VAppTLlfcl98WJ47z1o1QqSdahopbxFYmIikZGRREZGcsUVV1CuXLmzz5Nz+H/5/vvvZ/PmzVmWGTVqFBMnTsyyTE7ddNNNrF692iPbymved0G1UydITIRHH4UlS+DWW92OSCmVA2FhYWcT5cCBAwkJCeGZZ575VxljDMYY/Pwyrnd++umn2e7nscceu/RgfYD31dwBunUDEVimk6sr5e22bt1KrVq16NOnD1FRUezZs4fevXsTHR1NzZo1GTRo0NmyZ2rSqamphIaG0q9fP+rUqcMNN9zAvn37AHjhhRcYPnz42fL9+vWjfv36XHPNNfzyyy8AHDt2jLvuuos6derQtWtXoqOjs62hT5gwgdq1a1OrVi369+8PQGpqKj169Dj7+ogRIwB49913qVGjBnXq1KF79+4e/8xywvtq7sDekyW4vHp1Te5KXaynngJPNzdERoKTVC/Uxo0b+fTTT/noo48AGDx4MKVKlSI1NZWmTZvSsWNHatSo8a/3HD58mMaNGzN48GD69u3LuHHj6Nev33nbNsawfPly5syZw6BBg1iwYAHvv/8+V1xxBTNmzGDNmjVERUVlGV98fDwvvPACsbGxlChRghYtWjB37lzCw8PZv38/69atA+DQoUMAvP322+zYsYNChQqdfS2v5bjmLiL+IvK7iJw3d6SIBInIVBHZKiLLRKSyJ4NMb8oUqFwZ1l19F/z2G+h49Ep5vSuvvJLrrrvu7PPJkycTFRVFVFQUmzZtYuPGjee9p3DhwrRq1QqAevXqERcXl+G2O3TocF6Zn376iS5dugBQp04datasmWV8y5Yto1mzZpQuXZrAwEC6devG0qVLueqqq9i8eTNPPvkkCxcupESJEgDUrFmT7t27M3HixIu+CelSXUjN/UlgE1A8g3UPAAeNMVeJSBfgLewkyB7XogUULw73rXyCZYlvErhpE5zzja6UysZF1rBzS9GiRc8ub9myhffee4/ly5cTGhpK9+7dM+zvXahQobPL/v7+pKamZrjtoKCg88pc6CRFmZUPCwtj7dq1zJ8/nxEjRjBjxgzGjBnDwoUL+eGHH5g9ezavvfYa69evx9/f/4L2ealyVHMXkfJAG2BsJkXaA+Od5elAc8mlDrWlS8Po0fD7ztK8EfASDB6cG7tRSrnkyJEjFCtWjOLFi7Nnzx4WLlzo8X3cdNNNTJs2DYB169Zl+MsgvQYNGhATE0NiYiKpqalMmTKFxo0bk5CQgDGGu+++m1deeYVVq1Zx+vRp4uPjadasGe+88w4JCQkcP37c48eQnZzW3IcD/wcUy2R9OWAngDEmVUQOA2HYOTLPEpHeQG+AihUrXky8ANxxB3TvDq9N6k/LLxpy/X3fQ1gY1KljL7QqpbxWVFQUNWrUoFatWlStWpWGDRt6fB//+c9/uPfee4mIiCAqKopatWqdbVLJSPny5Rk0aBBNmjTBGEPbtm1p06YNq1at4oEHHsAYg4jw1ltvkZqaSrdu3Th69ChpaWk899xzFCuWWerMRWe6HmX2AG4HPnCWmwBzMyizASif7vmfQFhW261Xr565FAcOGFOl0mlTLmCP2Uu4MWDMgAGXtE2lfNnGjRvdDiHfSElJMSdOnDDGGPO///3PVK5c2aSkpLgc1fkyOmdArMkmbxtjclRzbwi0E5HWQDBQXEQmGGPS9++JByoA8SISAJQADnjguydTJUvCjJl+3HjDZXQu/QPf1X+BgNdfh+bNoWnT3Ny1UsrLJSUl0bx5c1JTUzHGMHr0aAICvLLzYKaybXM3xjxvjClvjKkMdAEWn5PYAeYA9znLHZ0yud6NpW5dGD3GjyV/V6f/VVOhVClwulIppVRmQkNDWblyJWvWrGHt2rXc6oM3Q170TUwiMkhE2jlPPwHCRGQr0Bc4v7NpLrn3XnjkEXhnWACTrxsGs2bZ8WeUUqoAu6DfIcaYJcASZ/mldK+fBO72ZGAXYvhw2LAB7o/pQaWUj7mxe3d7YbVYMRg6FCpUcCs0pZRyhXcOP3COQoVgxgwoX8GPO4osJG7hH7B2LUyfDp995nZ4SimV53wiuYPt//7NN5ASWITbK63j8PLNUK8e5EIfWaWUyu98JrkDXHMNzJghbN5VjDt7hHCqeWs7RMHhw26HplSB16RJk/NuSBo+fDiPPvpolu8LCQkBYPfu3XTs2DHTbcfGxma5neHDh//rZqLWrVt7ZNyXgQMHMmTIkEvejqf5VHIHaNYMPv0UYmLg3hWPk3Y6zd71NGkSpKW5HZ5SBVbXrl2ZMmXKv16bMmUKXbt2zdH7y5Yty/Tp0y96/+cm93nz5hEaGnrR28vvfC65g7179Z13YNricJ66YTlm23a45x4YNszt0JQqsDp27MjcuXM5deoUAHFxcezevZubbrrpbL/zqKgoateuzezZs897f1xcHLVq1QLgxIkTdOnShYiICDp37syJEyfOlnvkkUfODhf88ssvAzBixAh2795N06ZNaercB1O5cmX2Oz3rhg0bRq1atahVq9bZ4YLj4uKoXr06Dz30EDVr1uTWW2/9134ysnr1aho0aEBERAR33nknBw8ePLv/GjVqEBERcXbAsh9++OHsZCV169bl6NGjF/3ZZsS3eu2n88wzsGcPDBsWTdk3ttFvQVMYNQr++1/I4wF8lMpv3BjxNywsjPr167NgwQLat2/PlClT6Ny5MyJCcHAwM2fOpHjx4uzfv58GDRrQrl27TOcR/fDDDylSpAhr165l7dq1/xqy9/XXX6dUqVKcPn2a5s2bs3btWp544gmGDRtGTEwMpUuX/te2Vq5cyaeffsqyZcswxnD99dfTuHFjSpYsyZYtW5g8eTIff/wxnTp1YsaMGVmOz37vvffy/vvv07hxY1566SVeeeUVhg8fzuDBg9m+fTtBQUFnm4KGDBnCqFGjaNiwIUlJSQQHB1/Ap509n6y5n/HOO3Zej+f7+/FZrXcgLg4WLHA7LKUKrPRNM+mbZIwx9O/fn4iICFq0aMGuXbvYu3dvpttZunTp2SQbERFBRETE2XXTpk0jKiqKunXrsmHDhmwHBfvpp5+48847KVq0KCEhIXTo0IEff/wRgCpVqhAZGQlkPaww2PHlDx06ROPGjQG47777WLp06dkY77nnHiZMmHD2TtiGDRvSt29fRowYwaFDhzx+h6zP1twB/Pxs+/u+ffDg6Ou4LORuWn/5JaSmwk032cHGlCqA3Brx94477qBv376sWrWKEydOnK1xT5w4kYSEBFauXElgYCCVK1fOcJjf9DKq1W/fvp0hQ4awYsUKSpYsSc+ePbPdTlY3058ZLhjskMHZNctk5ptvvmHp0qXMmTOHV199lQ0bNtCvXz/atGnDvHnzaNCgAYsWLeLaa6+9qO1nxKdr7mD7wH/1FdSpI9x94nN+m7TNXmB9/XW3Q1OqwAkJCaFJkyb06tXrXxdSDx8+zGWXXUZgYCAxMTHs2LEjy+00atTo7CTY69evZ+3atYAdLrho0aKUKFGCvXv3Mn/+/LPvKVasWIbt2o0aNWLWrFkcP36cY8eOMXPmTG6++eYLPrYSJUpQsmTJs7X+L774gsaNG5OWlsbOnTtp2rQpb7/9NocOHSIpKYk///yT2rVr89xzzxEdHc0ff/xxwfvMik/X3M8oVgzmzYObIlNo9fdsYmhK5MyZsH07dOgAPXq4HaJSBUbXrl3p0KHDv3rO3HPPPbRt25bo6GgiIyOzrcE+8sgj3H///URERBAZGUn9+vUBO6tS3bp1qVmz5nnDBffu3ZtWrVpRpkwZYmJizr4eFRVFz549z27jwQcfpG7dulk2wWRm/Pjx9OnTh+PHj1O1alU+/fRTTp8+Tffu3Tl8+DDGGP773/8SGhrKiy++SExMDP7+/tSoUePsrFKeInkwvleGoqOjTXb9Uj1tx4Ykbq5zhJN+RVia0oBr2WzbbubNg9tuy9NYlMprmzZtonr16m6HoS5ARudMRFYaY6Kze6/PN8ukV6lmCIs2lsUvtBgtWERc60ftnU9PPmnb4ZVSykcUqOQOcPXV8O0if44Xv4Lmm95n9zPDYPNm6NoV4uPdDk8ppTyiwCV3gIgIWPBdAPsS/Lhl6G3sf7AffP01PP+826EplavcaoZVF+5Sz1WBTO4A9evD3LmwbZtw26o3OdymGyxaBPqPX/mo4OBgEhMTNcF7AWMMiYmJl3RjU4HoLZOZxo1tN8n27aHNkTdZ+PdUio4ZY7tKXn652+Ep5VHly5cnPj6ehIQEt0NRORAcHEz58uUv+v0FqrdMZmbMgE6dDE3SFjOX2ylcuxosWwaFC7sdmlJK/YvHesuISLCILBeRNSKyQUReyaBMTxFJEJHVzuPBiw3cDXfdBZ99JsTQlA4VVnBq3WaoVAnGjnU7NKWUuig5aXM/BTQzxtQBIoGWItIgg3JTjTGRzsPrsmKPHvDxWD8W7KzF3dHbSS5eGkaOdDsspZS6KNkmd2MlOU8DnYdPXpF54AE7cOTXsWXpVngmqWvWw+7d0Lo1DBjgdnhKKZVjOeotIyL+IrIa2Ad8Z4xZlkGxu0RkrYhMFxGvnZH60Ufh3Xdhxvpr6MEXnH6yL8yfbxvmlVLKS+QouRtjThtjIoHyQH0RqXVOka+BysaYCGARMD6j7YhIbxGJFZHY/HzF/qmn4K3Bhil0pdf0VqQh9kYnZ+B9pZTK7y6on7sx5hCwBGh5zuuJxphTztOPgXqZvH+MMSbaGBMdHh5+EeHmnf97ThjU9xCfcx8PR8XaBL9ihdthKaVUjuSkt0y4iIQ6y4WBFsAf55Qpk+5pO2CTJ4N0y4tDQxkwAMauiuI/vI959DH4/Xe3w1JKqWzl5CamMsB4EfHHfhlMM8bMFZFBQKwxZg7whIi0A1KBA0DP3Ao4r736Kpw6BUOGPEbQn6cYen0DZMZ0aNvW7dCUUipTehNTDhgDTz10jBGfFKVf+FjeCH0HWTAf0tLgqqvcDk8pVYDk9CamAj38QE6JwPCPi3IqAAaPfpCghHgGXn89lCxpL7RmMomvUkq5pcAOHHahROCDD+D+Tkm8wkDe2P8QbNkCzgS4SimVn2jN/QL4+cHHk0JIXryQAfvfICjYj6fffRcaNdLau1IqX9HkfoH8/eGz9dEkP3yKZ2a/RqHZ/+E/L75ok/vOnfDZZ26HqJRSekH1YqWkwN13G2bPFkZLH3r7f2KvvCYmQokSboenlPJROodqLgsMhKlThda3JPOw+YjPUu+B06ch3azqSinlFk3ulyAoCGbMKUSL6EP0kk+ZFNQTJk6ErVvdDk0pVcBpcr9EwcEw+4dQGjcW7k0ey/TpBq69FoYOdTs0pVQBpsndA4oUsfNrX1/vNN0CprGkQT949llYudLt0JRSBZQmdw8JCYG53xbiqmp+3Ln+VTaVvNHOxfryy26HppQqgDS5e1DJkjBvHgQXFm7zX8SOItXt4DSJiW6HppQqYDS5e1jlynZuj6MpwTQ9Ood4U1Z70Cil8pwm91wQGQnffguJx4JoJkvYM0fHgVdK5S1N7rnkuutg/nxht185mk96gH0d+kDDhnb8YKWUymWa3HPRjTfCvOFbiEurQIuZj3Hwl40wdqzbYSmlCgBN7rms0eMRzPkmgM2FatGm2I8ce+1deyerUkrlIk3ueaBFq0AmTxaWJdXgrr9HkvzxeBgwwI5Fo5RSuUCTex7p0AHGDDvGQlpy76NFSXvjTdiwwe2wlFI+KicTZAeLyHIRWSMiG0TklQzKBInIVBHZKiLLRKRybgTr7R54qhhvlRvBVNOZJ3kPs/Bbt0NSSvmonNTcTwHNjDF1gEigpYg0OKfMA8BBY8xVwLvAW54N03f8X+9DPO0/nJH8hzdHl3Q7HKWUj8o2uRsryXka6DzObSxuD4x3lqcDzUV0aqIMPfccb2/tQPdrVzBgy/2MrTsSjh//Z/2RI+7FppTyGTlqcxcRfxFZDewDvjPGLDunSDlgJ4AxJhU4DIRlsJ3eIhIrIrEJCQmXFrm3CgrCr3JFxn19GS3Lr+fh1Y8wu/04O4PT5MkQHg5//OF2lEopL5ej5G6MOW2MiQTKA/VFpNY5RTKqpZ/XFcQYM8YYE22MiQ4PD7/waH1I4FWV+HJTLaLD4uiy6AF+vP8TePxxSE6GkSPdDk8p5eUuqLeMMeYQsARoec6qeKACgIgEACWAAx6Iz6eFhMA3Ky6jUtkU2vnNZd2BsnaKvvHj4ehRt8NTSnmxnPSWCReRUGe5MNACOLfdYA5wn7PcEVhs3Jqc1cuUrlKMhb8Up0jJIFr6L2LHy+MgKUnHgldKXZKc1NzLADEishZYgW1znysig0SknVPmEyBMRLYCfYF+uROub6pUCRbEBHMs5HJuG9WW/YRpH3il1CUJyK6AMWYtUDeD119Kt3wSuNuzoRUstWvb2ZxuuSWANv4LWPz7FxR1OyillNfSO1TzkZtvhilThNjTden4VVdSUoB166BWLfjrL7fDU0p5EU3u+cwdd8BHjSaz4GADevUypA1+2zbRfKt3syqlck6Tez700N2HeJUXmDBBeG5ypH3xt9/cDUop5VWybXNXLmjblgGzHuTvZZ8xJOlpLi8jPPPbOLejUkp5EU3u+VGlSsii73jvNOzrCs9+2ZfL96ymx5EjULy429EppbyANsvkY/7+8MUX0KzuAXrxCfNf/MXtkJRSXkKTez4XFAQzF4dSu+g2Oo64mWUvzoVly3SiD6VUljS5e4HioX7Mnwdl/BNo81oD/mhwHzz7rNthKaXyMU3uXuLyRtewcFNF/MNCuS3kZ3YNnQzPPKPzsSqlMqTJ3YtcWc2PBd8FcFBK0bLUCg4O/QSuuw4aNIBVq9wOTymVj2hy9zJ168KsWcL/ksrQ7urNnEg6bRP76NFuh6aUykc0uXuhZs3giy+En7dcRpfqa0htfxfMmqVNNEqpszS5e6lOnWDECJgzBx458Bpm3z74RbtKKqUsTe5e7PHH4YUXYOziK3nJ73WYOdPtkJRS+YQmdy83aBA8+CC8ltafkZ+FwLBhsHu322EppVymyd3LicCHH0L7ujt44uBApj39G7z4otthKaVcpsndBwQEwOQ5ITQMXkV3JvD9xL/hyJF/CmzeDMeOuRegUirPaXL3EYXLhzFnVz2uuTKVO05NYdWr39gVJ05AVBQMHepugEqpPJWTCbIriEiMiGwSkQ0i8mQGZZqIyGERWe08XspoWyp3lSwlLFhSmFJBx7h9WFP+2nQM1q6F48dt7V0pVWDkpOaeCjxtjKkONAAeE5EaGZT70RgT6TwGeTRKlWPlygvffLKXY2mFaVN3F4dHT7ErdJo+pQqUbJO7MWaPMWaVs3wU2ASUy+3A1MWrdU8dZrywmj9OVaHjp61JIQB27HA7LKVUHrqgNncRqQzUBZZlsPoGEVkjIvNFpGYm7+8tIrEiEpuQkHDBwaqca/FqYz5uN5dF3EIfPsLE78LOuK2UKghynNxFJASYATxljDlyzupVQCVjTB3gfWBWRtswxowxxkQbY6LDw8MvNmaVQz0/vJ6X/F5jHA/whumn/d+VKkBylNxFJBCb2CcaY746d70x5ogxJslZngcEikhpj0aqLlzZsgz8ows9muzkBV5n4oMxsHWr21EppfJATnrLCPAJsMkYMyyTMlc45RCR+s52Ez0ZqLo4Uu0qxo44ThNi6LWoKz+0G2pncfrgA/jtN7fDU0rlkpxMkN0Q6AGsE5HVzmv9gYoAxpiPgI7AIyKSCpwAuhij88DlF4WurMBXNOBGfuHOTa/zS+eXufbLVyEiAtascTs8pVQuyDa5G2N+AiSbMiOBkZ4KSnlYkSKU7NeHedcm0OCBy7j9y3tZxijCgoPdjkwplUv0DtWC4s03qXJfI2YuLsHOwKp0CvuelN3aY0kpX6XJvYC5sVEAH4/1Y3FiJE/s+j/MTTfD88+7HZZSysM0uRdA994Lz922mo9MH4b/HA0LFrgdklLKwzS5F1BvPPE3HfmSpxnK7HVVITnZ7ZCUUh6Uk94yygf5Va7I59TjLyrS7fTnLP1yG/WiDKSmQu3aboenlLpEWnMvqCpUoDAnmRNwF+Ek0PbRCuy8uZvtHjlwoNvRKaUukSb3gqpYMQgN5fIWtfmmSCeOHT3N7YmfcfSaaBg1CtLS3I5QKXUJNLkXZO++CwMGUPP5dkznbjZQk87Bs0ndfxAWL/73bE5KKa8ibt1IGh0dbWJjY13Zt8rAzp18PKMUvf9blEcZxUgeRypWhB9/hIoV3Y5OKeUQkZXGmOjsymnNXVkVKvDQU0V59ln4gMd4r0h/OHQIbr8dNm2CP/90O0Kl1AXQ5K7+ZfBg6ND+NH1PvMacJ7+HdeugRg1o3NgOOKaU8gqa3NW/+PnBF5P8iY4Wug6NZuUjYyEgAHbtguXL3Q5PKZVDmtzVeYoUgTlzoHRpuH3mA2yPTYRChWDaNLdDU0rlkCZ3laErroD58+HUKbiLvJuBAAAa+klEQVTt7uIkNOsMkybpVH1KeQlN7ipTNWrA3LkQHw+t/xxB0t9HYVaGMygqpfIZTe4qSzfeCFOnwu/bSnBX4XkkvzAIVq/O/o1KKVdpclfZatsWxowRvj3RiF47Xsbc1lKbZ5TK5zS5qxzp1Qtefx0mnurIgH1PwJdfwu7dboellMpETibIriAiMSKySUQ2iMiTGZQRERkhIltFZK2IROVOuMpNzz8PDz90mjfpz4f3/Ah16sDRo26HpZTKQE5q7qnA08aY6kAD4DERqXFOmVZANefRG/jQo1GqfEEERn7gz+3V/+RxRjJv/3UwZoxd+dVX0KmT3uikVD6RbXI3xuwxxqxylo8Cm4By5xRrD3xurN+AUBEp4/FolesCAmDy8iupU9efzv7TWTN4Ppw8aZtptKlGqXzjgtrcRaQyUBdYds6qcsDOdM/jOf8LABHpLSKxIhKbkKCTM3urkBD4+msoUdKP2/d/yu73voSNG+3Kb7+FmBh3A1RK5Ty5i0gIMAN4yhhz7liwksFbzvt9bowZY4yJNsZEh4eHX1ikKl8pVw7mfhvEQb8w2r5UlyN/ODX2Xr2gWTNbm1dKuSZHyV1EArGJfaIx5qsMisQDFdI9Lw/o73MfF1lX+PLxpaxNvoa2ydM5TuF/Vu7Y4V5gSqkc9ZYR4BNgkzFmWCbF5gD3Or1mGgCHjTF7PBinyqda9avDBLrzIzdzV9EFJBNoV2zf7m5gShVwOam5NwR6AM1EZLXzaC0ifUSkj1NmHrAN2Ap8DDyaO+GqfKdMGTpft50x9GbBsUZ0r/ILp/GDuDi3I1OqQAvIroAx5icyblNPX8YAj3kqKOVlnniCB+fM4UgDePrpaEL8PmHstj/0DjmlXKT//9Sl694dpk2jb1946SX4NK0nfT+vg/ngQ+33rpRLNLkrjxo4EJ6q+BXv7e3KwMf22ZmclFJ5TpO78igRGFbmHXrxCYN4mWF9490OSakCKds2d6UulDzQizHLenO0VCWe/r41xbr/wEO374HkZOjRw34DKKVyldbclec99BD+qclMGHWEVn4LeHjizUzpOgvuu88ODq+UynWa3FXu8PenUJcOTD98Kzdfn0KPgMl8c+UT8OyzeveqUnlAk7vKVUVC/Pj62yAiI4W7/hpGTPxV8NprdgTJI+eOYqGU8hRN7irXFS8OCxbAldX8aCdfs+z17+wIktOnux2aUj5Lk7vKE2Fh8N13wmWhybRiPusC6sLbb0ObNpCY6HZ4SvkcTe4qz5QtC4t+KESRkkHcEvQDWzafhnnzYNEi+PtvaN0a4rXrpFKeoMld5akqtUP47ueinA4qSotiy4ijEixfDrNmwfz58NRTboeolE/Q5K7yXPXqsPA7P474l6JJ0K/ELf0L9u61K5cuhbQ0dwNUygdocleuiIqC77+HIxSn8cqhbF++z65ISICFC90NTikfoMlduSYqCr5/+UeSTFEaz3uOP6/vBhUqwOuv2zb4K6+E3393O0ylvJImd+Wqug9F873cwnGK0GTDSLb2egN+/hlGjIBt2+C779wOUSmvpMlduat0aSJrprCYZpw0wTQZ05UtXAXjx9v1q1fDyJFw6JC7cSrlZTS5K/e1bUsE61g86CdOpfjT2O9HNu8OseumT4f//Ac+/9zdGJXyMprclfsGDoTPPqP2k82IiYHUgGCaEsMfXAMpKbbMypWuhqiUt8nJBNnjRGSfiKzPZH0TETmcbn7VlzwfpvJphQrZESP9/alVC5b0GEcafjSRpWyghi2jyV2pC5KTmvtnQMtsyvxojIl0HoMuPSxVkNVoUZYYmuJXJIhGLGVF5bth0yY4dswW+O03HVlSqWxkm9yNMUuBA3kQi1JWo0ZUvzqNH7/cS4nyxWn29yRi0hrBmjWwezfceCMMH+52lErla55qc79BRNaIyHwRqZlZIRHpLSKxIhKbkJDgoV0rn1O2LGzezJWtruanZYFUrAitmM/coZtt84wxeqOTUtnwRHJfBVQyxtQB3gdmZVbQGDPGGBNtjIkODw/3wK6VrytbFpb+EkDtkru486vuzPjY+RH588+2mcYYGDAAYmLcDVSpfOaSk7sx5ogxJslZngcEikjpS45MKUdYGCyaeZT6LKfz1/cwUbrbXjRLl8K6dfDGG9CunW2XV0oBHkjuInKFiJ3xWETqO9vUAbqVR5VoHMnCJoNpxFJ6mPGMKfpfePFF+OorWyA1FT74wN0glcpHArIrICKTgSZAaRGJB14GAgGMMR8BHYFHRCQVOAF0McaYXItYFVgh/Z/gmyVt6HjNeh7ePIx9K19gwMpXkMhIKFLEXnBVSgE5SO7GmK7ZrB8JjPRYREplpkULCk/7nFlNwujVF16c8Br7uIzh96Tit20rTJpk2+DtD0mlCjS9Q1V5DxG4+24Cw0MZPx769oX3eYJ7Yv9Lcs26cPgw7NjhdpRK5Qua3JVX8vODIUPgrbdgylSh7ed3k0RRO1zw7t1uh6eU6zS5K68lAv/3f/DJJ7AotgTNWEzC2Flwzz22eeZc48bZOVuVKgA0uSuv16sXzJwprAuM4sbi69m6ZCfMnWsT/Jkkn5gIjzxiBylTqgDQ5K58Qrt28P2SAA4GXsYN/stZdtfbULIkPPqoLTBxIiQn25mdTpxwN1il8oAmd+UzbrwRfvlFKF6hBE3TFjE7rBd89JG9e3X8eAgOtv3hdYRJVQBoclc+5eqr4bfl/kREB9EhbiijS/aDZ56BVatsswzAr7+6G6RSeUCTu/I54eHw/ffQqpXQ5+CbDFzVFgPQrZuddPvnn90OUalcp8ld+aSiRWHmTLj/9gReYSC9Ck3kZPW60KQJ/PADnD7tdohK5SpN7spnBQbCJ7PCeDn0PT5L7sZ1DfyJrXK3nWz799/dDk+pXKXJXfk08fdj4OaufDP9BAcPwo2v3MpIHsN8t8hOuj15stshKpUrxK0xvqKjo01sbKwr+1YF04EDcO+98M030LnQTD5OvpdiQSnwxx9QuTKkpdnmmsBAt0NVKlMistIYE51dOa25qwKjVCmYMwfefPJvvkxuR72g9azgOntj04kT0LQpREVBUpLboSp1yTS5qwLFzw/6Db+CmMWGk5dV5MbkGIbNqITp+zT8+CNs2GBHJFPKy2lyVwVSo6YBrFkjtIvYwdNJr/D42EhOduwODzxg72ZNSXE7RKUuiSZ3VWCVLAlfjj7A0wzhg9TeRP30HiurdYHjx6FFC3vLq1JeSpO7KtD8IiMYUmgAC7mVI6YYDQY0YxAvkrr0Z3sn69atMHWqndLvTOcDY+w4NUrlY5rcVcEWFAR163Jrxc2s2+BPp07CywyiKTH8RQXbvaZLF3jtNZgxw75n+HCoUAGOHnU3dqWykG1yF5FxIrJPRNZnsl5EZISIbBWRtSIS5fkwlcpFo0bBxImULCVMnAhfdF/A6oBo6vqtZcmvheCOO6BOHXj6aTvw2CefwL59dlq/rBhju1kq5YKc1Nw/A1pmsb4VUM159AY+vPSwlMpD9erBTTedfdr9i5as2liYy0qfphmL6VNqGgeefh3++gveecf2qPHzg9Gjs97unDlQvTps3pzLB6DU+bJN7saYpcCBLIq0Bz431m9AqIiU8VSASrmhWjVYvjWM//b1Y+z4QK7p25qxJZ4m5YVXbGJ/9lk7hMH27Zlv5Kef7N///S9vglYqHU+0uZcDdqZ7Hu+8dh4R6S0isSISm5CQ4IFdK5V7ihWDoUPtaMFXXy08dHgI1WQLo+//ldPdethCMTH271df2YlB0nehPDNu/M6dKJXXPJHcJYPXMhzTwBgzxhgTbYyJDg8P98Culcp9ERG2Ej53LpStX4E+n9TnpodrsKHUzTa5jxoFd90FH34IK1bYC61ffglnhtfQ5K5c4InkHg9USPe8PKDTzyufIgJt2tih4CdMgC1bhLqHFvPKjJokP9Mfop2hPn780faw6dTpn940mtyVCzyR3OcA9zq9ZhoAh40xezywXaXyHRG45x7YtAnuvnkvA0/0Iyotlt9emmengerXD2bNsndIgZ05JH1yP3gQevSA3Vr/UbkrJ10hJwO/AteISLyIPCAifUSkj1NkHrAN2Ap8DDyaa9EqlU+Eh8PEJeWYOxcOX1aNG9qFc8eRz1lBtO15s3evbZZp0cImd2NsG/ycObbq/9Zbbh+C8nE56S3T1RhTxhgTaIwpb4z5xBjzkTHmI2e9McY8Zoy50hhT2xij4/iqAqNNG9sz8pVXYOnRutRnBS3TvuGnZYG2i2WFCrBrF4wcaZtuBg2ybxw71o5BfOKEre3//be7B6J8jt6hqtQlKl4cXnoJ4nYFMvjVFFZtKc7NN9sRhFcRZYcqeOIJW3jbNqhY0Y5f8/338OabthafXZ95pS6QJnelPKR4CeG5FwKJi7MjFGzcCNHvdKIPH7IvuCK0bm0L3n+//bt+PXz8sV3euxc+/RQOH3YlduV7dCYmpXLJoUMw8GXDyFFQuDA83X0fj01vSvjSGdC8uR3XJi7OFi5Z0l5sfftte2G2QQO4/HJX41f5k87EpJTLQkNh+HvChg1Cy5bCKx9dTrnDG+k8sDqLSnchLW6HLdi+vU3sAGPG2LFsypSBPXtse8/6DId1UipLmtyVymXXXGPvadqwAR57DBYtglvWDaMaW3izxGD21G39T+GtW+1fY2yXyVdftePZZOT+++0k30plQJtllMpjJ0/CVz3n8PHUYiyhKf5+abRLm8VDxadx65Ev8W9Q3zbTzJ9v31CqlG2T37EDypa1bTybN8O118J118Hy5e4ekMpT2iyjVD4VHAzdOiYTQzM2PzGKvo+d4if/xrQ+MoWqbGNQ0beIb+qMXVO4sO0y+fXXdhyER53bSKZPt39jYyEx0Z0DUfma1tyVcsP27bbm/f33cNNNJCfD7Nnw8eD9fLeqNH5+huYSwz13neDOuQ9SvHCKTeJ+frad588/bR/M/fthyhTo3NntI1J5JKc1d03uSrnl5ElbjT/Htm0wbhxMmpDG9h1+BAek0D51Bt3LLOa2E7MIvKwkREbCffdB9+7QrBk0bmzHKW6ZbuqF2FgICLBllc/Q5K6UlzMGfvsNJn6WwtRxSexPLUnpsDTubG9o38Gf5s0h+NUB8MYb9g1XX21nfhKxQw8XKvTPhsAOf5CUZL8IlNfSNnelvJwI3HADjBwdyO7jJZkzB5o192PyNH9uvx3CwqDDmpcZX7gP+4PL20lBli2zb547958NpaXZvw89ZGv7OTFmDFSp8u/x6ZVX0eSulBcIDIS2bWHqVNvMvmCBzdPLVxei54kPuTz5L272+4khT/zFujVppA0Z9s+bt22z49v8/rvtcbNxo+2Xmd64cba//RljxtgbrNaty5PjU56nyV0pLxMUBLfdBh98YAecjI2FF14QjpaqxLMrOhER6UfpX2bTueoK5tCW5FXr7WiUZzRsCLVqwXPP/VOr/+ADWyYuzn4ZnJlFSrtZeq0AtwNQSl08ETv4ZL168MqTRdhRtQ5LDkey9Mr7mX2wMdOYQ+HOx7mBX7m56DvcfGw+DQ79RlGwQx0cOwYDBvyTzH/4wd4ZC1C0qG3mqVHD1uTfe8+2BSmvoBdUlfIlf/xhe+HUqUNKqrCw0O18y638WLoDaxLLYYwQQApRtZJpVGgZDVeNILp7dcpNGIwEBkLXrnZOwfLlbVfLtWvtsMQJCfaGqZ9/tm1ECQkwfjw8+aR9fi5j7DeP8ji9oKpUQXTttbbrowiBgXD7W40Y8dxuft9XjoMHhXlN3ubZ0p9RKLQII9Y35U5mUWHCm5Tx38ftpX5h4ISrmLktgjWtn+dw/Vvgr7/g1Ck7Dv2KFXaiEYD+/eHZZ22CP9e6dXZgncWL8/bY1b9ozV2pguTYMTu+fMmSnDwJq4csIvabvcRW6kDsCsOmbUGk4X+2eFhoKnUjDfXqB1Bv2nPUS11GlcfbIAP62/b6ihVh9Wr49Ve44gqoWxceeQQ++siuu/xyGDIEbrzRDrDTqpVN/Kmptg++umAe7ecuIi2B9wB/YKwxZvA563sC7wC7nJdGGmPGZrVNTe5K5T/Hjtn5YXfssNdW//gDVq2ylfEzvSKLc5hKhfYQflUJjm+Mo4TfURqlLaEW66n2ZBuqjnuBoPLhdkNBQRASAtdfD/Pm2Ruu7rwTnn/e1v6ff/78IKZMsUMeV65sn8fF2fF1ihfPo08hf/NYchcRf+B/wC1APLAC6GqM2ZiuTE8g2hjzeE4D1OSulPc4dcqOPLxyRRrrYk/xV0Iw+/cLRY/tY++Ww6w9Xu1sWSGNilekUK2aoVr5k1y1eAzVEn6hWuOyVI0ZSyFSoHRp26ezRw87tv2ePfbvihV26My2beGrr+xF3H797JfCfffBkSP2ukCJEnY2Kz+/DO/y9WWeTO43AAONMbc5z58HMMa8ma5MTzS5K1VgHTwIW1YdZctH37PlsoZsORTOli2wZYudtOQMPz/DZSVTKFvBn7KJ6ym773fKntpOWXZThj2UZTflCx8gPHkXElHb9s2/8ko7ls4ZN9wAQ4fCXXfZHj0PPGBr+V262PUJCTBzph2sJyTE/hJIf3E3Kcne8BUVlSefjad5Mrl3BFoaYx50nvcArk+fyJ3k/iaQgK3l/9cYszOr7WpyV8r3GWPHO9uyxQ5Vv3WrvZ9q9+4zD0NCwvm9agpxivLEU6RCGKZYcdiyhfDCx7gmIohrfhrLFewlOLwYhU8coEjSXopynKLNrqdk1ZKE/zwLv00bbNv+oUMwaRI0amR3eN11tuY/dartFXTjjf/s9IsvbPfQSZPsL4j77gN/f3sQu3dDuXJ5+MllzpPJ/W7gtnOSe31jzH/SlQkDkowxp0SkD9DJGNMsg231BnoDVKxYsd6OHTsu5JiUUj4oOdkOV38m4cfHw86JPxAfWIWT4RVtpTs1hT37Atj8PzhwIOsuloEkU6Z0KuWrFeayjTGEHE8gxP84IacOEHLXbYTMGE+IOUJIaAAh19UgJG49IUf3EHJwJyGn9hMSlErIqf0UmvIF0rmTHbtnwADo2RNef90m/ldftXPeFi9uf0V89529s2zNGjvhef36tgmpWDGPdwnN02aZc8r7AweMMSWy2q7W3JVSF2P/fvtr4MQJ+zh+HI4dPc2xJEicv4xdewPZVfY64uMhYVcyx3YmkpRciKS0IpwwhXO8nwBSCJFjhJijhASlEHIqkRCS/nmUCLBfCiQRUrY4IfGb7PKVVxBSwp+QVT8QckMEIS0aEHLl5YTEfE2RmlWQ+3vaaw4XyZPJPQDb1NIc2xtmBdDNGLMhXZkyxpg9zvKdwHPGmAZZbVeTu1IqT23bxunpMzkW2ZCkWg1ISuL8R+Ipktb8SdKRNJKmfkNS1dokhVxBUtUIu27HAZJOB5N03M8+9y9BUloRjMlZ7VxII4hTPNvwVwb9dF7jRs62kcPknm1HU2NMqog8DizEdoUcZ4zZICKDgFhjzBzgCRFpB6QCB4CeFxW1UkrllqpV8f+/pykOZN6pMgioYRe/qG7b3M8KADtwg+0X+tVX0KYNpqhw4kS6L4eho0lqeBtJ5a4hKeGEfW3r3ySFlifp76Oc3BTH9beG5tJB/kNvYlJKKS+iww8opVQBpsldKaV8kCZ3pZTyQZrclVLKB2lyV0opH6TJXSmlfJAmd6WU8kGa3JVSyge5dhOTiCQAFzNyWGlgv4fDcYseS/6kx5I/6bFYlYwx4dkVci25XywRic3J3VneQI8lf9JjyZ/0WC6MNssopZQP0uSulFI+yBuT+xi3A/AgPZb8SY8lf9JjuQBe1+aulFIqe95Yc1dKKZUNTe5KKeWDvCq5i0hLEdksIltFpJ/b8VwoEYkTkXUislpEYp3XSonIdyKyxflb0u04MyIi40Rkn4isT/dahrGLNcI5T2tFJMq9yM+XybEMFJFdzrlZLSKt06173jmWzSJymztRn09EKohIjIhsEpENIvKk87rXnZcsjsUbz0uwiCwXkTXOsbzivF5FRJY552WqiBRyXg9ynm911lf2SCDGGK94YKf4+xOoChQC1gA13I7rAo8hDih9zmtvA/2c5X7AW27HmUnsjYAoYH12sQOtgfmAAA2AZW7Hn4NjGQg8k0HZGs6/tSCgivNv0N/tY3BiKwNEOcvFsHMd1/DG85LFsXjjeREgxFkOBJY5n/c0oIvz+kfAI87yo8BHznIXYKon4vCmmnt9YKsxZpsxJhmYArR3OSZPaA+Md5bHA3e4GEumjDFLsfPjppdZ7O2Bz431GxAqImXyJtLsZXIsmWkPTDHGnDLGbAe2Yv8tus4Ys8cYs8pZPgpsAsrhhecli2PJTH4+L8YYk+Q8DXQeBmgGTHdeP/e8nDlf04HmIpKzGbez4E3JvRywM93zeLI++fmRAb4VkZUi0tt57XJjzB6w/8CBy1yL7sJlFru3nqvHneaKcemax7ziWJyf8nWxtUSvPi/nHAt44XkREX8RWQ3sA77D/rI4ZIxJdYqkj/fssTjrDwNhlxqDNyX3jL7JvK0fZ0NjTBTQCnhMRBq5HVAu8cZz9SFwJRAJ7AGGOq/n+2MRkRBgBvCUMeZIVkUzeC2/H4tXnhdjzGljTCRQHvuLonpGxZy/uXIs3pTc44EK6Z6XB3a7FMtFMcbsdv7uA2ZiT/reMz+Nnb/73IvwgmUWu9edK2PMXuc/ZBrwMf/8xM/XxyIigdhkONEY85Xzsleel4yOxVvPyxnGmEPAEmybe6iIBDir0sd79lic9SXIebNhprwpua8AqjlXnAthLzzMcTmmHBORoiJS7MwycCuwHnsM9znF7gNmuxPhRcks9jnAvU7vjAbA4TPNBPnVOW3Pd2LPDdhj6eL0aKgCVAOW53V8GXHaZT8BNhljhqVb5XXnJbNj8dLzEi4ioc5yYaAF9hpCDNDRKXbueTlzvjoCi41zdfWSuH1l+QKvQrfGXkX/ExjgdjwXGHtV7NX9NcCGM/Fj29a+B7Y4f0u5HWsm8U/G/ixOwdY0HsgsduzPzFHOeVoHRLsdfw6O5Qsn1rXOf7Yy6coPcI5lM9DK7fjTxXUT9uf7WmC182jtjecli2PxxvMSAfzuxLweeMl5vSr2C2gr8CUQ5Lwe7Dzf6qyv6ok4dPgBpZTyQd7ULKOUUiqHNLkrpZQP0uSulFI+SJO7Ukr5IE3uSinlgzS5K6WUD9LkrpRSPuj/AWUxre3O7ra/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_convnet Test Accuracy: 0.372\n",
      "my_model_neu Test f-measure: 0.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "convnet_input_tensor = Input(shape=(maxlen,) , name='convnet_words')\n",
    "\n",
    "conv_1d_s3_model = Sequential()\n",
    "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s3_model.add(layers.Dropout(0.3))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "conv_1d_s3_model.add(layers.MaxPooling1D(2))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s3_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s3_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s3_model.layers[0].trainable = False\n",
    "conv_output_tensor_0 = conv_1d_s3_model(convnet_input_tensor)\n",
    "\n",
    "\n",
    "conv_1d_s1_model = Sequential()\n",
    "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s1_model.add(layers.Dropout(0.3))\n",
    "conv_1d_s1_model.add(layers.SeparableConv1D(64, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s1_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s1_model.layers[0].trainable = False\n",
    "conv_output_tensor_1 = conv_1d_s1_model(convnet_input_tensor)\n",
    "\n",
    "conv_1d_complex_model = Sequential()\n",
    "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_complex_model.add(layers.Dropout(0.3))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(64, 2, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_complex_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_complex_model.layers[0].trainable = False\n",
    "conv_output_tensor_2 = conv_1d_complex_model(convnet_input_tensor)\n",
    "\n",
    "# x = layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen)(word_input_tensor)\n",
    "# x = layers.Conv1D(128, 5, activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# y = layers.Conv1D(128, 10, activation='relu', padding='same')(x)\n",
    "# added = layers.add([y, x])\n",
    "# added = layers.GlobalMaxPooling1D()(added)\n",
    "\n",
    "concatenated = layers.concatenate([conv_output_tensor_0,\n",
    "                                   conv_output_tensor_1,\n",
    "                                   conv_output_tensor_2,\n",
    "#                                    ,added\n",
    "                                  ], axis=-1)\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "# concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "# concatenated = layers.Dropout(0.3)(concatenated)\n",
    "answer = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model(convnet_input_tensor, answer)\n",
    "model.summary()\n",
    "\n",
    "# model.layers[0].set_weights([w2d.word_embedding])\n",
    "# model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=300,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_convnet,\n",
    "                    verbose= 2\n",
    "                   )\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "l_model = load_model('my_model_convnet.h5')\n",
    "yhat = l_model.predict(test_data)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_convnet Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_convnet Test Accuracy: 0.434\n"
     ]
    }
   ],
   "source": [
    "l_model = load_model('my_model_convnet.h5')\n",
    "yhat = l_model.predict(test_data)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_convnet Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(model_names_list):\n",
    "    all_models = list()\n",
    "    for model_name in model_names_list:\n",
    "        # define filename for this ensemble\n",
    "#         filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "        filename = model_name + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "\n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer.name = 'ensemble_' + str(i + 1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "#     print(ensemble_visible)\n",
    "#     ensemble_visible = [[ngram_input_tensor, word_input_tensor], convnet_input_tensor]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "#     ensemble_outputs = [concatenated, answer]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = layers.Dense(128, activation='relu')(merge)\n",
    "    hidden = layers.Dropout(0.3)(hidden)\n",
    "    output = layers.Dense(len(set(train_labels)), activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    # plot graph of ensemble\n",
    "#     plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=3e-4), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy, valX, valy):\n",
    "    # prepare input data\n",
    "#     X = [inputX for _ in range(len(model.input))]\n",
    "    # encode output data\n",
    "#     inputy_enc = to_categorical(inputy)\n",
    "    # fit model\n",
    "    model.fit(inputX, inputy, validation_data=(valX, valy), batch_size=class_size,\n",
    "              callbacks=callbacks_list_stacked, epochs=300, verbose=1)\n",
    "    \n",
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "#     X = [inputX for _ in range(len(model.input))]\n",
    "    # make prediction\n",
    "    return model.predict(inputX, verbose=0)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded my_model_neu_ngrams.h5\n",
      ">loaded my_model_neu_words.h5\n",
      "Loaded 2 models\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/300\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 3.9638 - acc: 0.0476 - val_loss: 3.9308 - val_acc: 0.1111\n",
      "Epoch 2/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.8945 - acc: 0.1111 - val_loss: 3.8935 - val_acc: 0.1111\n",
      "Epoch 3/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.8828 - acc: 0.0794 - val_loss: 3.8559 - val_acc: 0.1111\n",
      "Epoch 4/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.8690 - acc: 0.0952 - val_loss: 3.8181 - val_acc: 0.1111\n",
      "Epoch 5/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.8320 - acc: 0.1746 - val_loss: 3.7807 - val_acc: 0.1601\n",
      "Epoch 6/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7888 - acc: 0.1429 - val_loss: 3.7436 - val_acc: 0.2222\n",
      "Epoch 7/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.7554 - acc: 0.2698 - val_loss: 3.7072 - val_acc: 0.2222\n",
      "Epoch 8/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.7128 - acc: 0.3016 - val_loss: 3.6710 - val_acc: 0.2222\n",
      "Epoch 9/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.6937 - acc: 0.3016 - val_loss: 3.6353 - val_acc: 0.5556\n",
      "Epoch 10/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6631 - acc: 0.5238 - val_loss: 3.5994 - val_acc: 0.7778\n",
      "Epoch 11/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6199 - acc: 0.6190 - val_loss: 3.5630 - val_acc: 0.7778\n",
      "Epoch 12/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5890 - acc: 0.6667 - val_loss: 3.5264 - val_acc: 1.0000\n",
      "Epoch 13/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.5482 - acc: 0.8254 - val_loss: 3.4892 - val_acc: 1.0000\n",
      "Epoch 14/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5207 - acc: 0.7778 - val_loss: 3.4515 - val_acc: 1.0000\n",
      "Epoch 15/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.5129 - acc: 0.7778 - val_loss: 3.4140 - val_acc: 1.0000\n",
      "Epoch 16/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4747 - acc: 0.8413 - val_loss: 3.3776 - val_acc: 1.0000\n",
      "Epoch 17/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.4457 - acc: 0.9365 - val_loss: 3.3413 - val_acc: 1.0000\n",
      "Epoch 18/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.3922 - acc: 0.9841 - val_loss: 3.3049 - val_acc: 1.0000\n",
      "Epoch 19/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3388 - acc: 0.9524 - val_loss: 3.2669 - val_acc: 1.0000\n",
      "Epoch 20/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3429 - acc: 0.9524 - val_loss: 3.2288 - val_acc: 1.0000\n",
      "Epoch 21/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.2821 - acc: 0.9683 - val_loss: 3.1906 - val_acc: 1.0000\n",
      "Epoch 22/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.2816 - acc: 0.9841 - val_loss: 3.1522 - val_acc: 1.0000\n",
      "Epoch 23/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.2187 - acc: 0.9841 - val_loss: 3.1138 - val_acc: 1.0000\n",
      "Epoch 24/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.2016 - acc: 0.9841 - val_loss: 3.0752 - val_acc: 1.0000\n",
      "Epoch 25/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.1872 - acc: 0.9683 - val_loss: 3.0369 - val_acc: 1.0000\n",
      "Epoch 26/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.0947 - acc: 1.0000 - val_loss: 2.9975 - val_acc: 1.0000\n",
      "Epoch 27/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.0908 - acc: 1.0000 - val_loss: 2.9577 - val_acc: 1.0000\n",
      "Epoch 28/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0414 - acc: 1.0000 - val_loss: 2.9189 - val_acc: 1.0000\n",
      "Epoch 29/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.0587 - acc: 1.0000 - val_loss: 2.8806 - val_acc: 1.0000\n",
      "Epoch 30/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.9641 - acc: 1.0000 - val_loss: 2.8424 - val_acc: 1.0000\n",
      "Epoch 31/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.9146 - acc: 1.0000 - val_loss: 2.8036 - val_acc: 1.0000\n",
      "Epoch 32/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.9012 - acc: 1.0000 - val_loss: 2.7648 - val_acc: 1.0000\n",
      "Epoch 33/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.8809 - acc: 0.9683 - val_loss: 2.7265 - val_acc: 1.0000\n",
      "Epoch 34/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8331 - acc: 0.9683 - val_loss: 2.6887 - val_acc: 1.0000\n",
      "Epoch 35/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.8018 - acc: 1.0000 - val_loss: 2.6515 - val_acc: 1.0000\n",
      "Epoch 36/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.7907 - acc: 1.0000 - val_loss: 2.6151 - val_acc: 1.0000\n",
      "Epoch 37/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.7381 - acc: 1.0000 - val_loss: 2.5794 - val_acc: 1.0000\n",
      "Epoch 38/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.6885 - acc: 1.0000 - val_loss: 2.5446 - val_acc: 1.0000\n",
      "Epoch 39/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.6739 - acc: 1.0000 - val_loss: 2.5104 - val_acc: 1.0000\n",
      "Epoch 40/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.6578 - acc: 1.0000 - val_loss: 2.4771 - val_acc: 1.0000\n",
      "Epoch 41/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5982 - acc: 1.0000 - val_loss: 2.4449 - val_acc: 1.0000\n",
      "Epoch 42/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5831 - acc: 1.0000 - val_loss: 2.4134 - val_acc: 1.0000\n",
      "Epoch 43/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.5908 - acc: 0.9841 - val_loss: 2.3832 - val_acc: 1.0000\n",
      "Epoch 44/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.4797 - acc: 1.0000 - val_loss: 2.3539 - val_acc: 1.0000\n",
      "Epoch 45/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.4769 - acc: 1.0000 - val_loss: 2.3249 - val_acc: 1.0000\n",
      "Epoch 46/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.4183 - acc: 1.0000 - val_loss: 2.2969 - val_acc: 1.0000\n",
      "Epoch 47/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.4385 - acc: 1.0000 - val_loss: 2.2700 - val_acc: 1.0000\n",
      "Epoch 48/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.3851 - acc: 0.9841 - val_loss: 2.2443 - val_acc: 1.0000\n",
      "Epoch 49/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.3955 - acc: 1.0000 - val_loss: 2.2191 - val_acc: 1.0000\n",
      "Epoch 50/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3148 - acc: 1.0000 - val_loss: 2.1950 - val_acc: 1.0000\n",
      "Epoch 51/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.3448 - acc: 1.0000 - val_loss: 2.1722 - val_acc: 1.0000\n",
      "Epoch 52/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2795 - acc: 1.0000 - val_loss: 2.1503 - val_acc: 1.0000\n",
      "Epoch 53/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.2635 - acc: 1.0000 - val_loss: 2.1291 - val_acc: 1.0000\n",
      "Epoch 54/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.2730 - acc: 1.0000 - val_loss: 2.1087 - val_acc: 1.0000\n",
      "Epoch 55/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.2293 - acc: 1.0000 - val_loss: 2.0894 - val_acc: 1.0000\n",
      "Epoch 56/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.2056 - acc: 0.9841 - val_loss: 2.0709 - val_acc: 1.0000\n",
      "Epoch 57/300\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.1910 - acc: 1.0000 - val_loss: 2.0533 - val_acc: 1.0000\n",
      "Epoch 58/300\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 2.1951 - acc: 1.0000 - val_loss: 2.0367 - val_acc: 1.0000\n",
      "Epoch 59/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1976 - acc: 1.0000 - val_loss: 2.0205 - val_acc: 1.0000\n",
      "Epoch 60/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1460 - acc: 1.0000 - val_loss: 2.0056 - val_acc: 1.0000\n",
      "Epoch 61/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1489 - acc: 1.0000 - val_loss: 1.9915 - val_acc: 1.0000\n",
      "Epoch 62/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0867 - acc: 1.0000 - val_loss: 1.9781 - val_acc: 1.0000\n",
      "Epoch 63/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1393 - acc: 1.0000 - val_loss: 1.9653 - val_acc: 1.0000\n",
      "Epoch 64/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0602 - acc: 1.0000 - val_loss: 1.9532 - val_acc: 1.0000\n",
      "Epoch 65/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.1122 - acc: 1.0000 - val_loss: 1.9419 - val_acc: 1.0000\n",
      "Epoch 66/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0779 - acc: 1.0000 - val_loss: 1.9313 - val_acc: 1.0000\n",
      "Epoch 67/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0419 - acc: 1.0000 - val_loss: 1.9214 - val_acc: 1.0000\n",
      "Epoch 68/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0059 - acc: 1.0000 - val_loss: 1.9120 - val_acc: 1.0000\n",
      "Epoch 69/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0228 - acc: 1.0000 - val_loss: 1.9030 - val_acc: 1.0000\n",
      "Epoch 70/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0151 - acc: 1.0000 - val_loss: 1.8942 - val_acc: 1.0000\n",
      "Epoch 71/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0046 - acc: 1.0000 - val_loss: 1.8859 - val_acc: 1.0000\n",
      "Epoch 72/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9966 - acc: 1.0000 - val_loss: 1.8779 - val_acc: 1.0000\n",
      "Epoch 73/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9816 - acc: 1.0000 - val_loss: 1.8704 - val_acc: 1.0000\n",
      "Epoch 74/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0402 - acc: 0.9841 - val_loss: 1.8633 - val_acc: 1.0000\n",
      "Epoch 75/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.9329 - acc: 1.0000 - val_loss: 1.8568 - val_acc: 1.0000\n",
      "Epoch 76/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9644 - acc: 1.0000 - val_loss: 1.8505 - val_acc: 1.0000\n",
      "Epoch 77/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9453 - acc: 1.0000 - val_loss: 1.8446 - val_acc: 1.0000\n",
      "Epoch 78/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9356 - acc: 1.0000 - val_loss: 1.8391 - val_acc: 1.0000\n",
      "Epoch 79/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9522 - acc: 1.0000 - val_loss: 1.8336 - val_acc: 1.0000\n",
      "Epoch 80/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9625 - acc: 0.9841 - val_loss: 1.8284 - val_acc: 1.0000\n",
      "Epoch 81/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9181 - acc: 1.0000 - val_loss: 1.8234 - val_acc: 1.0000\n",
      "Epoch 82/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9193 - acc: 1.0000 - val_loss: 1.8187 - val_acc: 1.0000\n",
      "Epoch 83/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9762 - acc: 0.9683 - val_loss: 1.8143 - val_acc: 1.0000\n",
      "Epoch 84/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9138 - acc: 1.0000 - val_loss: 1.8101 - val_acc: 1.0000\n",
      "Epoch 85/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8820 - acc: 1.0000 - val_loss: 1.8061 - val_acc: 1.0000\n",
      "Epoch 86/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9073 - acc: 1.0000 - val_loss: 1.8024 - val_acc: 1.0000\n",
      "Epoch 87/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8899 - acc: 1.0000 - val_loss: 1.7989 - val_acc: 1.0000\n",
      "Epoch 88/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9207 - acc: 0.9841 - val_loss: 1.7955 - val_acc: 1.0000\n",
      "Epoch 89/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9006 - acc: 1.0000 - val_loss: 1.7923 - val_acc: 1.0000\n",
      "Epoch 90/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8498 - acc: 1.0000 - val_loss: 1.7893 - val_acc: 1.0000\n",
      "Epoch 91/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8704 - acc: 1.0000 - val_loss: 1.7864 - val_acc: 1.0000\n",
      "Epoch 92/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8643 - acc: 1.0000 - val_loss: 1.7837 - val_acc: 1.0000\n",
      "Epoch 93/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8653 - acc: 1.0000 - val_loss: 1.7810 - val_acc: 1.0000\n",
      "Epoch 94/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8830 - acc: 1.0000 - val_loss: 1.7784 - val_acc: 1.0000\n",
      "Epoch 95/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8548 - acc: 1.0000 - val_loss: 1.7759 - val_acc: 1.0000\n",
      "Epoch 96/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8473 - acc: 1.0000 - val_loss: 1.7736 - val_acc: 1.0000\n",
      "Epoch 97/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8303 - acc: 1.0000 - val_loss: 1.7714 - val_acc: 1.0000\n",
      "Epoch 98/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8167 - acc: 1.0000 - val_loss: 1.7693 - val_acc: 1.0000\n",
      "Epoch 99/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8688 - acc: 0.9841 - val_loss: 1.7674 - val_acc: 1.0000\n",
      "Epoch 100/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8251 - acc: 1.0000 - val_loss: 1.7656 - val_acc: 1.0000\n",
      "Epoch 101/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8516 - acc: 1.0000 - val_loss: 1.7637 - val_acc: 1.0000\n",
      "Epoch 102/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8156 - acc: 1.0000 - val_loss: 1.7620 - val_acc: 1.0000\n",
      "Epoch 103/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8298 - acc: 1.0000 - val_loss: 1.7603 - val_acc: 1.0000\n",
      "Epoch 104/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8252 - acc: 1.0000 - val_loss: 1.7586 - val_acc: 1.0000\n",
      "Epoch 105/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8399 - acc: 1.0000 - val_loss: 1.7569 - val_acc: 1.0000\n",
      "Epoch 106/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7943 - acc: 1.0000 - val_loss: 1.7554 - val_acc: 1.0000\n",
      "Epoch 107/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8097 - acc: 1.0000 - val_loss: 1.7540 - val_acc: 1.0000\n",
      "Epoch 108/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8292 - acc: 1.0000 - val_loss: 1.7527 - val_acc: 1.0000\n",
      "Epoch 109/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7984 - acc: 1.0000 - val_loss: 1.7513 - val_acc: 1.0000\n",
      "Epoch 110/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8013 - acc: 1.0000 - val_loss: 1.7501 - val_acc: 1.0000\n",
      "Epoch 111/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8580 - acc: 0.9841 - val_loss: 1.7488 - val_acc: 1.0000\n",
      "Epoch 112/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8153 - acc: 1.0000 - val_loss: 1.7476 - val_acc: 1.0000\n",
      "Epoch 113/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8196 - acc: 1.0000 - val_loss: 1.7464 - val_acc: 1.0000\n",
      "Epoch 114/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8012 - acc: 1.0000 - val_loss: 1.7453 - val_acc: 1.0000\n",
      "Epoch 115/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7939 - acc: 1.0000 - val_loss: 1.7442 - val_acc: 1.0000\n",
      "Epoch 116/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7869 - acc: 1.0000 - val_loss: 1.7432 - val_acc: 1.0000\n",
      "Epoch 117/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8142 - acc: 1.0000 - val_loss: 1.7422 - val_acc: 1.0000\n",
      "Epoch 118/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7913 - acc: 1.0000 - val_loss: 1.7413 - val_acc: 1.0000\n",
      "Epoch 119/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7822 - acc: 1.0000 - val_loss: 1.7404 - val_acc: 1.0000\n",
      "Epoch 120/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7741 - acc: 1.0000 - val_loss: 1.7395 - val_acc: 1.0000\n",
      "Epoch 121/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8061 - acc: 1.0000 - val_loss: 1.7387 - val_acc: 1.0000\n",
      "Epoch 122/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7827 - acc: 1.0000 - val_loss: 1.7379 - val_acc: 1.0000\n",
      "Epoch 123/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7879 - acc: 1.0000 - val_loss: 1.7371 - val_acc: 1.0000\n",
      "Epoch 124/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7679 - acc: 1.0000 - val_loss: 1.7364 - val_acc: 1.0000\n",
      "Epoch 125/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7856 - acc: 1.0000 - val_loss: 1.7357 - val_acc: 1.0000\n",
      "Epoch 126/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8117 - acc: 1.0000 - val_loss: 1.7349 - val_acc: 1.0000\n",
      "Epoch 127/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7659 - acc: 1.0000 - val_loss: 1.7342 - val_acc: 1.0000\n",
      "Epoch 128/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8166 - acc: 0.9841 - val_loss: 1.7335 - val_acc: 1.0000\n",
      "Epoch 129/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7945 - acc: 1.0000 - val_loss: 1.7329 - val_acc: 1.0000\n",
      "Epoch 130/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7750 - acc: 1.0000 - val_loss: 1.7322 - val_acc: 1.0000\n",
      "Epoch 131/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8165 - acc: 0.9841 - val_loss: 1.7316 - val_acc: 1.0000\n",
      "Epoch 132/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7724 - acc: 1.0000 - val_loss: 1.7311 - val_acc: 1.0000\n",
      "Epoch 133/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7777 - acc: 1.0000 - val_loss: 1.7305 - val_acc: 1.0000\n",
      "Epoch 134/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7817 - acc: 1.0000 - val_loss: 1.7300 - val_acc: 1.0000\n",
      "Epoch 135/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7725 - acc: 1.0000 - val_loss: 1.7294 - val_acc: 1.0000\n",
      "Epoch 136/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7910 - acc: 1.0000 - val_loss: 1.7289 - val_acc: 1.0000\n",
      "Epoch 137/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7682 - acc: 1.0000 - val_loss: 1.7284 - val_acc: 1.0000\n",
      "Epoch 138/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7708 - acc: 1.0000 - val_loss: 1.7279 - val_acc: 1.0000\n",
      "Epoch 139/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7774 - acc: 1.0000 - val_loss: 1.7274 - val_acc: 1.0000\n",
      "Epoch 140/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7649 - acc: 1.0000 - val_loss: 1.7269 - val_acc: 1.0000\n",
      "Epoch 141/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7795 - acc: 1.0000 - val_loss: 1.7265 - val_acc: 1.0000\n",
      "Epoch 142/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7689 - acc: 1.0000 - val_loss: 1.7260 - val_acc: 1.0000\n",
      "Epoch 143/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7795 - acc: 1.0000 - val_loss: 1.7256 - val_acc: 1.0000\n",
      "Epoch 144/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8165 - acc: 0.9841 - val_loss: 1.7251 - val_acc: 1.0000\n",
      "Epoch 145/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7781 - acc: 1.0000 - val_loss: 1.7247 - val_acc: 1.0000\n",
      "Epoch 146/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7648 - acc: 1.0000 - val_loss: 1.7243 - val_acc: 1.0000\n",
      "Epoch 147/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7897 - acc: 0.9841 - val_loss: 1.7239 - val_acc: 1.0000\n",
      "Epoch 148/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7502 - acc: 1.0000 - val_loss: 1.7236 - val_acc: 1.0000\n",
      "Epoch 149/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7518 - acc: 1.0000 - val_loss: 1.7233 - val_acc: 1.0000\n",
      "Epoch 150/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7638 - acc: 1.0000 - val_loss: 1.7229 - val_acc: 1.0000\n",
      "Epoch 151/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.8018 - acc: 0.9841 - val_loss: 1.7226 - val_acc: 1.0000\n",
      "Epoch 152/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7672 - acc: 1.0000 - val_loss: 1.7223 - val_acc: 1.0000\n",
      "Epoch 153/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7619 - acc: 1.0000 - val_loss: 1.7219 - val_acc: 1.0000\n",
      "Epoch 154/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7900 - acc: 0.9683 - val_loss: 1.7217 - val_acc: 1.0000\n",
      "Epoch 155/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7585 - acc: 1.0000 - val_loss: 1.7214 - val_acc: 1.0000\n",
      "Epoch 156/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7730 - acc: 0.9841 - val_loss: 1.7211 - val_acc: 1.0000\n",
      "Epoch 157/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7601 - acc: 1.0000 - val_loss: 1.7208 - val_acc: 1.0000\n",
      "Epoch 158/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7533 - acc: 1.0000 - val_loss: 1.7206 - val_acc: 1.0000\n",
      "Epoch 159/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7682 - acc: 1.0000 - val_loss: 1.7203 - val_acc: 1.0000\n",
      "Epoch 160/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7433 - acc: 1.0000 - val_loss: 1.7201 - val_acc: 1.0000\n",
      "Epoch 161/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7532 - acc: 1.0000 - val_loss: 1.7199 - val_acc: 1.0000\n",
      "Epoch 162/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8565 - acc: 0.9841 - val_loss: 1.7197 - val_acc: 1.0000\n",
      "Epoch 163/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7565 - acc: 1.0000 - val_loss: 1.7194 - val_acc: 1.0000\n",
      "Epoch 164/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7447 - acc: 1.0000 - val_loss: 1.7192 - val_acc: 1.0000\n",
      "Epoch 165/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7938 - acc: 0.9841 - val_loss: 1.7190 - val_acc: 1.0000\n",
      "Epoch 166/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7961 - acc: 0.9841 - val_loss: 1.7188 - val_acc: 1.0000\n",
      "Epoch 167/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7501 - acc: 1.0000 - val_loss: 1.7186 - val_acc: 1.0000\n",
      "Epoch 168/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7438 - acc: 1.0000 - val_loss: 1.7184 - val_acc: 1.0000\n",
      "Epoch 169/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7374 - acc: 1.0000 - val_loss: 1.7182 - val_acc: 1.0000\n",
      "Epoch 170/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7465 - acc: 1.0000 - val_loss: 1.7181 - val_acc: 1.0000\n",
      "Epoch 171/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7608 - acc: 1.0000 - val_loss: 1.7179 - val_acc: 1.0000\n",
      "Epoch 172/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7625 - acc: 1.0000 - val_loss: 1.7177 - val_acc: 1.0000\n",
      "Epoch 173/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7507 - acc: 1.0000 - val_loss: 1.7175 - val_acc: 1.0000\n",
      "Epoch 174/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7360 - acc: 1.0000 - val_loss: 1.7173 - val_acc: 1.0000\n",
      "Epoch 175/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7365 - acc: 1.0000 - val_loss: 1.7171 - val_acc: 1.0000\n",
      "Epoch 176/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7486 - acc: 1.0000 - val_loss: 1.7170 - val_acc: 1.0000\n",
      "Epoch 177/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7601 - acc: 1.0000 - val_loss: 1.7168 - val_acc: 1.0000\n",
      "Epoch 178/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7425 - acc: 1.0000 - val_loss: 1.7166 - val_acc: 1.0000\n",
      "Epoch 179/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7656 - acc: 0.9841 - val_loss: 1.7164 - val_acc: 1.0000\n",
      "Epoch 180/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7436 - acc: 1.0000 - val_loss: 1.7163 - val_acc: 1.0000\n",
      "Epoch 181/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7392 - acc: 1.0000 - val_loss: 1.7161 - val_acc: 1.0000\n",
      "Epoch 182/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7441 - acc: 1.0000 - val_loss: 1.7160 - val_acc: 1.0000\n",
      "Epoch 183/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7934 - acc: 0.9841 - val_loss: 1.7158 - val_acc: 1.0000\n",
      "Epoch 184/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7509 - acc: 1.0000 - val_loss: 1.7157 - val_acc: 1.0000\n",
      "Epoch 185/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7474 - acc: 1.0000 - val_loss: 1.7156 - val_acc: 1.0000\n",
      "Epoch 186/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7317 - acc: 1.0000 - val_loss: 1.7154 - val_acc: 1.0000\n",
      "Epoch 187/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7358 - acc: 1.0000 - val_loss: 1.7153 - val_acc: 1.0000\n",
      "Epoch 188/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7568 - acc: 0.9841 - val_loss: 1.7152 - val_acc: 1.0000\n",
      "Epoch 189/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7338 - acc: 1.0000 - val_loss: 1.7151 - val_acc: 1.0000\n",
      "Epoch 190/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7365 - acc: 1.0000 - val_loss: 1.7150 - val_acc: 1.0000\n",
      "Epoch 191/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7583 - acc: 1.0000 - val_loss: 1.7148 - val_acc: 1.0000\n",
      "Epoch 192/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7677 - acc: 1.0000 - val_loss: 1.7147 - val_acc: 1.0000\n",
      "Epoch 193/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7594 - acc: 0.9841 - val_loss: 1.7146 - val_acc: 1.0000\n",
      "Epoch 194/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7923 - acc: 1.0000 - val_loss: 1.7144 - val_acc: 1.0000\n",
      "Epoch 195/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7382 - acc: 1.0000 - val_loss: 1.7143 - val_acc: 1.0000\n",
      "Epoch 196/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7415 - acc: 1.0000 - val_loss: 1.7142 - val_acc: 1.0000\n",
      "Epoch 197/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7481 - acc: 1.0000 - val_loss: 1.7141 - val_acc: 1.0000\n",
      "Epoch 198/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7317 - acc: 1.0000 - val_loss: 1.7140 - val_acc: 1.0000\n",
      "Epoch 199/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7703 - acc: 0.9841 - val_loss: 1.7139 - val_acc: 1.0000\n",
      "Epoch 200/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7285 - acc: 1.0000 - val_loss: 1.7138 - val_acc: 1.0000\n",
      "Epoch 201/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7521 - acc: 0.9841 - val_loss: 1.7137 - val_acc: 1.0000\n",
      "Epoch 202/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7557 - acc: 1.0000 - val_loss: 1.7136 - val_acc: 1.0000\n",
      "Epoch 203/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7300 - acc: 1.0000 - val_loss: 1.7135 - val_acc: 1.0000\n",
      "Epoch 204/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7261 - acc: 1.0000 - val_loss: 1.7134 - val_acc: 1.0000\n",
      "Epoch 205/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7340 - acc: 1.0000 - val_loss: 1.7134 - val_acc: 1.0000\n",
      "Epoch 206/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7332 - acc: 1.0000 - val_loss: 1.7133 - val_acc: 1.0000\n",
      "Epoch 207/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7229 - acc: 1.0000 - val_loss: 1.7132 - val_acc: 1.0000\n",
      "Epoch 208/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7574 - acc: 0.9841 - val_loss: 1.7131 - val_acc: 1.0000\n",
      "Epoch 209/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7376 - acc: 1.0000 - val_loss: 1.7130 - val_acc: 1.0000\n",
      "Epoch 210/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7367 - acc: 1.0000 - val_loss: 1.7130 - val_acc: 1.0000\n",
      "Epoch 211/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7272 - acc: 1.0000 - val_loss: 1.7129 - val_acc: 1.0000\n",
      "Epoch 212/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7327 - acc: 1.0000 - val_loss: 1.7128 - val_acc: 1.0000\n",
      "Epoch 213/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7396 - acc: 1.0000 - val_loss: 1.7127 - val_acc: 1.0000\n",
      "Epoch 214/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7408 - acc: 1.0000 - val_loss: 1.7127 - val_acc: 1.0000\n",
      "Epoch 215/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7421 - acc: 1.0000 - val_loss: 1.7126 - val_acc: 1.0000\n",
      "Epoch 216/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7200 - acc: 1.0000 - val_loss: 1.7125 - val_acc: 1.0000\n",
      "Epoch 217/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7297 - acc: 1.0000 - val_loss: 1.7125 - val_acc: 1.0000\n",
      "Epoch 218/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7330 - acc: 1.0000 - val_loss: 1.7124 - val_acc: 1.0000\n",
      "Epoch 219/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7354 - acc: 1.0000 - val_loss: 1.7123 - val_acc: 1.0000\n",
      "Epoch 220/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7387 - acc: 1.0000 - val_loss: 1.7123 - val_acc: 1.0000\n",
      "Epoch 221/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7796 - acc: 0.9683 - val_loss: 1.7122 - val_acc: 1.0000\n",
      "Epoch 222/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7450 - acc: 1.0000 - val_loss: 1.7121 - val_acc: 1.0000\n",
      "Epoch 223/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7377 - acc: 1.0000 - val_loss: 1.7121 - val_acc: 1.0000\n",
      "Epoch 224/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7285 - acc: 1.0000 - val_loss: 1.7120 - val_acc: 1.0000\n",
      "Epoch 225/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7252 - acc: 1.0000 - val_loss: 1.7119 - val_acc: 1.0000\n",
      "Epoch 226/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7400 - acc: 1.0000 - val_loss: 1.7119 - val_acc: 1.0000\n",
      "Epoch 227/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7274 - acc: 1.0000 - val_loss: 1.7118 - val_acc: 1.0000\n",
      "Epoch 228/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7375 - acc: 1.0000 - val_loss: 1.7118 - val_acc: 1.0000\n",
      "Epoch 229/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7331 - acc: 1.0000 - val_loss: 1.7117 - val_acc: 1.0000\n",
      "Epoch 230/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7395 - acc: 1.0000 - val_loss: 1.7116 - val_acc: 1.0000\n",
      "Epoch 231/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7218 - acc: 1.0000 - val_loss: 1.7116 - val_acc: 1.0000\n",
      "Epoch 232/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7258 - acc: 1.0000 - val_loss: 1.7115 - val_acc: 1.0000\n",
      "Epoch 233/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7356 - acc: 1.0000 - val_loss: 1.7115 - val_acc: 1.0000\n",
      "Epoch 234/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7312 - acc: 1.0000 - val_loss: 1.7114 - val_acc: 1.0000\n",
      "Epoch 235/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7267 - acc: 1.0000 - val_loss: 1.7114 - val_acc: 1.0000\n",
      "Epoch 236/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7297 - acc: 1.0000 - val_loss: 1.7113 - val_acc: 1.0000\n",
      "Epoch 237/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7547 - acc: 1.0000 - val_loss: 1.7113 - val_acc: 1.0000\n",
      "Epoch 238/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7397 - acc: 0.9841 - val_loss: 1.7112 - val_acc: 1.0000\n",
      "Epoch 239/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7347 - acc: 1.0000 - val_loss: 1.7112 - val_acc: 1.0000\n",
      "Epoch 240/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7222 - acc: 1.0000 - val_loss: 1.7111 - val_acc: 1.0000\n",
      "Epoch 241/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7231 - acc: 1.0000 - val_loss: 1.7111 - val_acc: 1.0000\n",
      "Epoch 242/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7278 - acc: 1.0000 - val_loss: 1.7110 - val_acc: 1.0000\n",
      "Epoch 243/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7300 - acc: 1.0000 - val_loss: 1.7110 - val_acc: 1.0000\n",
      "Epoch 244/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7392 - acc: 0.9841 - val_loss: 1.7109 - val_acc: 1.0000\n",
      "Epoch 245/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7375 - acc: 1.0000 - val_loss: 1.7109 - val_acc: 1.0000\n",
      "Epoch 246/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7600 - acc: 0.9841 - val_loss: 1.7109 - val_acc: 1.0000\n",
      "Epoch 247/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7264 - acc: 1.0000 - val_loss: 1.7109 - val_acc: 1.0000\n",
      "Epoch 248/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7554 - acc: 0.9841 - val_loss: 1.7108 - val_acc: 1.0000\n",
      "Epoch 249/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7231 - acc: 1.0000 - val_loss: 1.7108 - val_acc: 1.0000\n",
      "Epoch 250/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7237 - acc: 1.0000 - val_loss: 1.7107 - val_acc: 1.0000\n",
      "Epoch 251/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7201 - acc: 1.0000 - val_loss: 1.7107 - val_acc: 1.0000\n",
      "Epoch 252/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7202 - acc: 1.0000 - val_loss: 1.7107 - val_acc: 1.0000\n",
      "Epoch 253/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7253 - acc: 1.0000 - val_loss: 1.7106 - val_acc: 1.0000\n",
      "Epoch 254/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7216 - acc: 1.0000 - val_loss: 1.7106 - val_acc: 1.0000\n",
      "Epoch 255/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7298 - acc: 1.0000 - val_loss: 1.7105 - val_acc: 1.0000\n",
      "Epoch 256/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7315 - acc: 1.0000 - val_loss: 1.7105 - val_acc: 1.0000\n",
      "Epoch 257/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7230 - acc: 1.0000 - val_loss: 1.7105 - val_acc: 1.0000\n",
      "Epoch 258/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7316 - acc: 1.0000 - val_loss: 1.7104 - val_acc: 1.0000\n",
      "Epoch 259/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7379 - acc: 0.9841 - val_loss: 1.7104 - val_acc: 1.0000\n",
      "Epoch 260/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7264 - acc: 1.0000 - val_loss: 1.7104 - val_acc: 1.0000\n",
      "Epoch 261/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7203 - acc: 1.0000 - val_loss: 1.7103 - val_acc: 1.0000\n",
      "Epoch 262/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7249 - acc: 1.0000 - val_loss: 1.7103 - val_acc: 1.0000\n",
      "Epoch 263/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7282 - acc: 1.0000 - val_loss: 1.7103 - val_acc: 1.0000\n",
      "Epoch 264/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7306 - acc: 1.0000 - val_loss: 1.7102 - val_acc: 1.0000\n",
      "Epoch 265/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7421 - acc: 1.0000 - val_loss: 1.7102 - val_acc: 1.0000\n",
      "Epoch 266/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7512 - acc: 0.9841 - val_loss: 1.7102 - val_acc: 1.0000\n",
      "Epoch 267/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7323 - acc: 1.0000 - val_loss: 1.7101 - val_acc: 1.0000\n",
      "Epoch 268/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7183 - acc: 1.0000 - val_loss: 1.7101 - val_acc: 1.0000\n",
      "Epoch 269/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7749 - acc: 0.9841 - val_loss: 1.7101 - val_acc: 1.0000\n",
      "Epoch 270/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7257 - acc: 1.0000 - val_loss: 1.7101 - val_acc: 1.0000\n",
      "Epoch 271/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7437 - acc: 1.0000 - val_loss: 1.7100 - val_acc: 1.0000\n",
      "Epoch 272/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7204 - acc: 1.0000 - val_loss: 1.7100 - val_acc: 1.0000\n",
      "Epoch 273/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7182 - acc: 1.0000 - val_loss: 1.7100 - val_acc: 1.0000\n",
      "Epoch 274/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7214 - acc: 1.0000 - val_loss: 1.7100 - val_acc: 1.0000\n",
      "Epoch 275/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7302 - acc: 1.0000 - val_loss: 1.7099 - val_acc: 1.0000\n",
      "Epoch 276/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7213 - acc: 1.0000 - val_loss: 1.7099 - val_acc: 1.0000\n",
      "Epoch 277/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7215 - acc: 1.0000 - val_loss: 1.7099 - val_acc: 1.0000\n",
      "Epoch 278/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7316 - acc: 1.0000 - val_loss: 1.7099 - val_acc: 1.0000\n",
      "Epoch 279/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7176 - acc: 1.0000 - val_loss: 1.7098 - val_acc: 1.0000\n",
      "Epoch 280/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7237 - acc: 1.0000 - val_loss: 1.7098 - val_acc: 1.0000\n",
      "Epoch 281/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7230 - acc: 1.0000 - val_loss: 1.7098 - val_acc: 1.0000\n",
      "Epoch 282/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7175 - acc: 1.0000 - val_loss: 1.7097 - val_acc: 1.0000\n",
      "Epoch 283/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7235 - acc: 1.0000 - val_loss: 1.7097 - val_acc: 1.0000\n",
      "Epoch 284/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7417 - acc: 1.0000 - val_loss: 1.7097 - val_acc: 1.0000\n",
      "Epoch 285/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.7507 - acc: 0.9841 - val_loss: 1.7097 - val_acc: 1.0000\n",
      "Epoch 286/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7146 - acc: 1.0000 - val_loss: 1.7097 - val_acc: 1.0000\n",
      "Epoch 287/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7600 - acc: 0.9683 - val_loss: 1.7096 - val_acc: 1.0000\n",
      "Epoch 288/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7615 - acc: 0.9841 - val_loss: 1.7096 - val_acc: 1.0000\n",
      "Epoch 289/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7609 - acc: 0.9841 - val_loss: 1.7096 - val_acc: 1.0000\n",
      "Epoch 290/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7341 - acc: 1.0000 - val_loss: 1.7096 - val_acc: 1.0000\n",
      "Epoch 291/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7345 - acc: 1.0000 - val_loss: 1.7096 - val_acc: 1.0000\n",
      "Epoch 292/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7168 - acc: 1.0000 - val_loss: 1.7096 - val_acc: 1.0000\n",
      "Epoch 293/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7246 - acc: 1.0000 - val_loss: 1.7095 - val_acc: 1.0000\n",
      "Epoch 294/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7251 - acc: 1.0000 - val_loss: 1.7095 - val_acc: 1.0000\n",
      "Epoch 295/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7154 - acc: 1.0000 - val_loss: 1.7095 - val_acc: 1.0000\n",
      "Epoch 296/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7429 - acc: 1.0000 - val_loss: 1.7095 - val_acc: 1.0000\n",
      "Epoch 297/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7270 - acc: 1.0000 - val_loss: 1.7095 - val_acc: 1.0000\n",
      "Epoch 298/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7206 - acc: 1.0000 - val_loss: 1.7094 - val_acc: 1.0000\n",
      "Epoch 299/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7214 - acc: 1.0000 - val_loss: 1.7094 - val_acc: 1.0000\n",
      "Epoch 300/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7375 - acc: 1.0000 - val_loss: 1.7094 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "members = load_all_models(['my_model_neu_ngrams', 'my_model_neu_words'\n",
    "#                            , 'my_model_convnet'\n",
    "                          ])\n",
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, [X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
    "#                                   X_train\n",
    "                                 ], y_train, [X_scaled_val_data_ngrams, X_scaled_val_data_words, \n",
    "#                                               X_val\n",
    "                                             ], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "final_model = load_model('my_model_stacked.h5')\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(final_model, [scaled_test_data_ngrams, scaled_test_data_words\n",
    "#                                              , test_data\n",
    "                                            ])\n",
    "preds = argmax(yhat, axis=1)\n",
    "# acc = accuracy_score(test_labels, preds)\n",
    "# print('Stacked Test Accuracy: %.3f' % acc)\n",
    "# f_measure = f1_score(test_labels, preds, average='macro') \n",
    "\n",
    "# print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded my_model_neu_ngrams.h5\n",
      ">loaded my_model_neu_words.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'my_model_convnet.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-586ad4dbdf2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m members = load_all_models(['my_model_neu_ngrams', 'my_model_neu_words'\n\u001b[0;32m----> 3\u001b[0;31m                            \u001b[0;34m,\u001b[0m \u001b[0;34m'my_model_convnet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                           ])\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded %d models'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-db38a9b2e742>\u001b[0m in \u001b[0;36mload_all_models\u001b[0;34m(model_names_list)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# load model from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# add to list of members\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mall_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\designer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'my_model_convnet.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "members = load_all_models(['my_model_neu_ngrams', 'my_model_neu_words'\n",
    "                           , 'my_model_convnet'\n",
    "                          ])\n",
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, [X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
    "                                  X_train\n",
    "                                 ], y_train, [X_scaled_val_data_ngrams, X_scaled_val_data_words, \n",
    "                                              X_val\n",
    "                                             ], y_val)\n",
    "final_model = load_model('my_model_stacked.h5')\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, [scaled_test_data_ngrams, scaled_test_data_words\n",
    "                                             , test_data\n",
    "                                            ])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def _plot_confusion_matrix(cm, classes,\n",
    "                      normalize=False,\n",
    "                      title='Confusion matrix',\n",
    "                      cmap=plt.cm.get_cmap()):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.95       219\n",
      "          1       0.25      0.14      0.18        14\n",
      "          2       0.84      1.00      0.91        31\n",
      "          3       0.93      0.77      0.84        48\n",
      "          4       0.83      0.62      0.71        16\n",
      "          5       1.00      1.00      1.00        49\n",
      "          6       0.82      0.54      0.65        52\n",
      "          7       0.82      0.88      0.85        16\n",
      "          8       0.56      0.87      0.68        23\n",
      "          9       0.50      0.56      0.53        93\n",
      "\n",
      "avg / total       0.82      0.81      0.81       561\n",
      "\n",
      "Normalized confusion matrix\n",
      "[[0.96803653 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.03196347]\n",
      " [0.         0.14285714 0.         0.         0.         0.\n",
      "  0.         0.         0.42857143 0.42857143]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.77083333 0.         0.\n",
      "  0.         0.         0.         0.22916667]\n",
      " [0.125      0.         0.         0.         0.625      0.\n",
      "  0.         0.0625     0.         0.1875    ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.03846154 0.         0.         0.         0.\n",
      "  0.53846154 0.         0.         0.42307692]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.875      0.0625     0.0625    ]\n",
      " [0.         0.04347826 0.         0.         0.         0.\n",
      "  0.04347826 0.         0.86956522 0.04347826]\n",
      " [0.11827957 0.03225806 0.06451613 0.03225806 0.02150538 0.\n",
      "  0.05376344 0.02150538 0.09677419 0.55913978]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Test Accuracy: 0.811\n",
      "my_model_neu Test f-measure: 0.731\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "yhat = predict_stacked_model(final_model, [scaled_test_data_ngrams, scaled_test_data_words\n",
    "#                                              , test_data\n",
    "                                            ])\n",
    "preds = argmax(yhat, axis=1)\n",
    "count = 0\n",
    "for i, p in enumerate(preds):\n",
    "    sproba = sorted(yhat[i], reverse=True)\n",
    "    if sproba[0] - sproba[1] < 0.6:\n",
    "        preds[i] = 9\n",
    "        count = count + 1 \n",
    "        \n",
    "report = classification_report(test_labels, preds)\n",
    "print(report)\n",
    "cnf_matrix = confusion_matrix(test_labels, preds, labels=list(range(10)))\n",
    "\n",
    "# print(cnf_matrix)\n",
    "\n",
    "plt.figure()\n",
    "_plot_confusion_matrix(cnf_matrix, classes=list(range(10)), normalize=True,\n",
    "                      title='All')\n",
    "# print(\n",
    "#         \"\\n######################################################################################\"\n",
    "#         )\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "acc = accuracy_score(test_labels, preds)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, preds, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+classified: 93 43\n"
     ]
    }
   ],
   "source": [
    "preds = predict_stacked_model(final_model, [scaled_test_data_ngrams, scaled_test_data_words\n",
    "#                                              , test_data\n",
    "                                            ])\n",
    "count = 0\n",
    "scount = 0\n",
    "for ind, v in enumerate(test_labels):\n",
    "    if v == 9:\n",
    "        count += 1\n",
    "        ss = sorted(preds[ind], reverse=True)\n",
    "#         print(ss)\n",
    "        if ss[0] - ss[1] < 0.50:\n",
    "            scount += 1\n",
    "print('+classified:', count, scount)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Test Accuracy: 0.902\n",
      "my_model_neu Test f-measure: 0.790\n"
     ]
    }
   ],
   "source": [
    "yhat = predict_stacked_model(stacked_model, [scaled_test_data_ngrams, scaled_test_data_words\n",
    "#                                              , test_data\n",
    "                                            ])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "f_measure = f1_score(test_labels, yhat, average='macro') \n",
    "\n",
    "print('my_model_neu Test f-measure: %.3f' % f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7639838, 0.4849627, 0.15885201, 0.14977545, 0.13139054, 0.1307949, 0.10452836, 0.05326262, 0.022449585]\n",
      "[1.0934824, 0.5347313, 0.12917736, 0.10542332, 0.073631525, 0.029506773, 0.019371198, 0.007875161, 0.006801053]\n",
      "[0.41629496, 0.37753227, 0.31957763, 0.29852194, 0.26896805, 0.13785516, 0.08470297, 0.052518696, 0.04402831]\n",
      "[0.7920461, 0.42737857, 0.23580596, 0.16783251, 0.11348875, 0.09465425, 0.075749554, 0.05889041, 0.034153834]\n",
      "[0.7218045, 0.4602304, 0.32316726, 0.13507405, 0.1127368, 0.100664966, 0.08070882, 0.04055082, 0.025062248]\n",
      "[0.71755266, 0.38910508, 0.3032543, 0.19928706, 0.10165503, 0.095576495, 0.08427116, 0.067344815, 0.041953407]\n",
      "[0.9287356, 0.39433008, 0.22758335, 0.14927098, 0.10347741, 0.06399731, 0.056940332, 0.050915126, 0.024749696]\n",
      "[0.7437892, 0.3294309, 0.27099973, 0.24823746, 0.2127012, 0.077135414, 0.046781518, 0.037475605, 0.033449043]\n",
      "[0.54184407, 0.3610068, 0.21862072, 0.19038737, 0.16970918, 0.15582108, 0.14577726, 0.13241735, 0.08441618]\n",
      "[1.3511541, 0.33321607, 0.10333388, 0.10223201, 0.06586174, 0.01695178, 0.01538218, 0.007822519, 0.0040458073]\n",
      "[0.8377702, 0.5931462, 0.22437893, 0.12657978, 0.07896304, 0.03945157, 0.038984425, 0.03120378, 0.029521944]\n",
      "[0.98401505, 0.2883492, 0.23095787, 0.16393238, 0.13924146, 0.100945115, 0.03385164, 0.033680618, 0.025026727]\n",
      "[0.999809, 0.26552638, 0.20964704, 0.13614155, 0.12836932, 0.08317135, 0.07759518, 0.052756954, 0.046983115]\n",
      "[0.8536738, 0.4653971, 0.19890666, 0.1138387, 0.11137651, 0.10524581, 0.08355954, 0.048016176, 0.01998588]\n",
      "[0.5161528, 0.4753129, 0.42044696, 0.16716501, 0.12164592, 0.10982743, 0.08809828, 0.07620307, 0.025147688]\n",
      "[0.9513034, 0.26417485, 0.24701257, 0.23345917, 0.12086275, 0.06257176, 0.05248231, 0.05141055, 0.016722683]\n",
      "[0.68358386, 0.5347786, 0.37902743, 0.21934187, 0.06672913, 0.050106417, 0.028549101, 0.027583212, 0.0103003085]\n",
      "[1.0412313, 0.22317368, 0.19803807, 0.17000628, 0.13153994, 0.098249525, 0.06324501, 0.05582045, 0.018695872]\n",
      "[1.2624035, 0.2273384, 0.1893719, 0.09164766, 0.076097436, 0.054042384, 0.040063377, 0.032464135, 0.026571289]\n",
      "[0.83106846, 0.3279509, 0.20238271, 0.19124383, 0.13931942, 0.1256381, 0.11603321, 0.041554954, 0.0248084]\n",
      "[0.7090577, 0.6964506, 0.335791, 0.10025659, 0.035321783, 0.033978164, 0.032881424, 0.031098973, 0.02516387]\n",
      "[0.45901394, 0.35749722, 0.29890144, 0.29348433, 0.22651635, 0.16789882, 0.08271173, 0.07488099, 0.039095227]\n",
      "[1.462874, 0.14946587, 0.14178471, 0.07584468, 0.05657, 0.052886322, 0.023405636, 0.020549215, 0.016619619]\n",
      "[1.0908513, 0.22092037, 0.20620185, 0.15465581, 0.11689333, 0.069265, 0.06380989, 0.041204408, 0.036197994]\n",
      "[0.87049824, 0.45662287, 0.24561015, 0.1393251, 0.110719725, 0.058342926, 0.041918702, 0.03902384, 0.037938394]\n",
      "[0.6443012, 0.6246143, 0.33313006, 0.16173086, 0.08430529, 0.061853178, 0.04034712, 0.025905613, 0.023812428]\n",
      "[1.310171, 0.19072305, 0.1437288, 0.13369544, 0.099892706, 0.03819463, 0.03808125, 0.02620012, 0.019313127]\n",
      "[0.6521059, 0.4269034, 0.37540016, 0.21384296, 0.13592488, 0.069518656, 0.06526814, 0.036153525, 0.024882477]\n",
      "[0.5896224, 0.38592395, 0.27781346, 0.22685067, 0.17210758, 0.15739018, 0.07249802, 0.058968708, 0.058825046]\n",
      "[0.99189806, 0.35533136, 0.3131879, 0.18121304, 0.08570104, 0.023491785, 0.018517096, 0.017970357, 0.012689501]\n",
      "[1.0777085, 0.24996254, 0.1966781, 0.19313723, 0.10793869, 0.057777114, 0.041994274, 0.04126978, 0.033533882]\n",
      "[0.5052367, 0.3695346, 0.35060263, 0.332008, 0.11292032, 0.09542234, 0.0939223, 0.07113777, 0.06921526]\n",
      "[0.5386309, 0.43285018, 0.30799973, 0.20035902, 0.160905, 0.14674307, 0.09700088, 0.065387875, 0.05012321]\n",
      "[1.022375, 0.30565897, 0.18485188, 0.10651989, 0.09252606, 0.087843426, 0.082817554, 0.06813189, 0.0492756]\n",
      "[0.736135, 0.5697566, 0.31829688, 0.1469937, 0.10434347, 0.06832276, 0.026570063, 0.020111304, 0.009470137]\n",
      "[0.5079663, 0.47578767, 0.30410618, 0.19960518, 0.13042593, 0.114895746, 0.108254805, 0.08881969, 0.07013851]\n",
      "[0.79119015, 0.28664875, 0.23509988, 0.2111218, 0.18712994, 0.1341657, 0.07028221, 0.046949275, 0.03741234]\n",
      "[1.049671, 0.59106696, 0.15419042, 0.09658247, 0.054726988, 0.03208539, 0.013994301, 0.005509855, 0.0021726876]\n",
      "[0.7924785, 0.74819255, 0.114610285, 0.1076788, 0.10373898, 0.06161291, 0.03437982, 0.029074168, 0.008234003]\n",
      "[0.68461335, 0.43892458, 0.2692023, 0.17486493, 0.15237948, 0.12568331, 0.06469632, 0.055495914, 0.034139775]\n",
      "[0.68184733, 0.49469197, 0.17665464, 0.17561868, 0.13754739, 0.10582354, 0.09776677, 0.06943512, 0.06061469]\n",
      "[0.5237127, 0.44302315, 0.4013573, 0.20507419, 0.19300276, 0.08223193, 0.07857701, 0.04442886, 0.028592143]\n",
      "[0.5178707, 0.45282668, 0.35280165, 0.3097397, 0.16168222, 0.10865857, 0.036347162, 0.03237774, 0.027695548]\n",
      "[1.0464122, 0.30836475, 0.25321153, 0.16947985, 0.13044892, 0.04386832, 0.022907723, 0.014587523, 0.010719117]\n",
      "[0.617464, 0.40107542, 0.33202475, 0.27299997, 0.19401252, 0.06441656, 0.04963934, 0.04462351, 0.023743939]\n",
      "[0.72804344, 0.4063528, 0.3097844, 0.17432567, 0.16484277, 0.09234592, 0.054711364, 0.05397991, 0.015613759]\n",
      "[0.99216604, 0.53119624, 0.13401043, 0.12260975, 0.11305875, 0.07113163, 0.018621543, 0.014665503, 0.0025402086]\n",
      "[0.84858996, 0.39025766, 0.21562165, 0.21491739, 0.13742633, 0.054059938, 0.052417066, 0.047992554, 0.0387174]\n",
      "[0.63405436, 0.3619856, 0.30961514, 0.19787386, 0.1852749, 0.14604771, 0.074991934, 0.054987177, 0.035169274]\n",
      "[0.61818516, 0.4863551, 0.24647085, 0.22635213, 0.12052564, 0.09972643, 0.09515953, 0.055591516, 0.051633593]\n",
      "[0.6400494, 0.5695242, 0.27266395, 0.23500171, 0.12855405, 0.058820687, 0.041283637, 0.029504836, 0.02459753]\n",
      "[0.40313828, 0.32835132, 0.26182538, 0.2529176, 0.23770154, 0.15888628, 0.13419227, 0.1142812, 0.10870617]\n",
      "[1.2865856, 0.34062716, 0.15962088, 0.08528133, 0.04969289, 0.03742885, 0.019437885, 0.012490975, 0.008834401]\n",
      "[1.0104961, 0.55963385, 0.13513003, 0.08078371, 0.07553768, 0.05246856, 0.037950076, 0.027541578, 0.020458395]\n",
      "[0.48465747, 0.44454175, 0.27351287, 0.18364498, 0.17372257, 0.13910528, 0.118437536, 0.11590429, 0.06647333]\n",
      "[1.0877051, 0.33108285, 0.32554197, 0.09826946, 0.080253124, 0.036436386, 0.023541285, 0.012234948, 0.004934891]\n",
      "[1.0422537, 0.37456763, 0.15395486, 0.14682582, 0.076902874, 0.06363339, 0.055611357, 0.04700608, 0.0392445]\n",
      "[0.6638119, 0.65254045, 0.2228239, 0.1568102, 0.10418824, 0.0828414, 0.050622985, 0.047943167, 0.018417664]\n",
      "[1.7314868, 0.0666554, 0.054644343, 0.041563246, 0.03832204, 0.035479456, 0.016284123, 0.011978919, 0.0035857612]\n",
      "[0.67814636, 0.4766222, 0.2958296, 0.27716327, 0.09750766, 0.07855488, 0.06630716, 0.01621943, 0.013649368]\n",
      "[0.68585503, 0.3193183, 0.28597137, 0.24823236, 0.120539494, 0.10051684, 0.08741704, 0.07888485, 0.07326484]\n",
      "[0.5995877, 0.5853218, 0.3270356, 0.19604078, 0.14861129, 0.056225527, 0.044927448, 0.024524713, 0.017725239]\n",
      "[0.44529837, 0.40396366, 0.31733292, 0.26616132, 0.2393198, 0.122241646, 0.08302849, 0.06572371, 0.05693005]\n",
      "[0.83576185, 0.3772263, 0.22904035, 0.1581509, 0.13590023, 0.113233045, 0.08418645, 0.033474635, 0.03302635]\n",
      "[1.0420665, 0.3044716, 0.18827212, 0.18223603, 0.11174102, 0.07050459, 0.04137781, 0.038016517, 0.02131384]\n",
      "[0.615173, 0.2971682, 0.2953098, 0.16676486, 0.15854852, 0.15415782, 0.12915872, 0.117387876, 0.066331275]\n",
      "[0.4525709, 0.38075095, 0.31166732, 0.27602375, 0.26735657, 0.14773825, 0.09494199, 0.05021976, 0.018730402]\n",
      "[0.57231027, 0.35178846, 0.34640622, 0.23772888, 0.16207048, 0.10514124, 0.096047394, 0.087298095, 0.04120908]\n",
      "[0.95891654, 0.28752816, 0.1867975, 0.1592428, 0.14590317, 0.11477105, 0.057999074, 0.053737484, 0.035104327]\n",
      "[0.99336386, 0.5492388, 0.20530802, 0.09398851, 0.08781479, 0.026981078, 0.019625474, 0.017080713, 0.006598793]\n",
      "[0.6863392, 0.51518327, 0.21718651, 0.18699808, 0.16468118, 0.11615686, 0.04501484, 0.0401833, 0.028256752]\n",
      "[0.85845464, 0.6967674, 0.1420685, 0.10211723, 0.07015232, 0.0488397, 0.04536117, 0.022346046, 0.013892939]\n",
      "[0.7524613, 0.5447589, 0.3163798, 0.19086297, 0.09632179, 0.05097688, 0.02459889, 0.013105034, 0.010534392]\n",
      "[1.2954872, 0.39603376, 0.14812936, 0.07664624, 0.049223557, 0.014914812, 0.010191257, 0.005524018, 0.0038497346]\n",
      "[1.139201, 0.1813036, 0.17372586, 0.15021138, 0.10374361, 0.08568815, 0.078545034, 0.07285735, 0.014723731]\n",
      "[1.0012084, 0.5426928, 0.16693756, 0.08742216, 0.086664155, 0.0475322, 0.027857646, 0.021702103, 0.017983193]\n",
      "[0.61278176, 0.35485432, 0.21519375, 0.181173, 0.1772674, 0.15553111, 0.112169325, 0.097076476, 0.093952894]\n",
      "[1.374776, 0.21701983, 0.16075376, 0.09823139, 0.05373311, 0.05303021, 0.025045326, 0.013212077, 0.0041981763]\n",
      "[0.44485098, 0.37546703, 0.29663315, 0.25992927, 0.22504912, 0.19238867, 0.084711075, 0.07782607, 0.043144643]\n",
      "[0.66622114, 0.44402277, 0.2732494, 0.21228316, 0.12044088, 0.1086767, 0.099660516, 0.050001495, 0.025443893]\n",
      "[0.6472097, 0.3964746, 0.3799148, 0.17953509, 0.14722952, 0.13103059, 0.071327776, 0.024275918, 0.02300205]\n",
      "[0.72066796, 0.36422646, 0.19992149, 0.16744263, 0.16585615, 0.13021795, 0.11855838, 0.089619674, 0.04348927]\n",
      "[0.70121706, 0.5053532, 0.4063503, 0.22181794, 0.061464474, 0.029478628, 0.026465174, 0.02399949, 0.023853747]\n",
      "[1.1599073, 0.24438205, 0.1774058, 0.17372668, 0.10839494, 0.09132008, 0.016973082, 0.015362937, 0.012527065]\n",
      "[0.79286647, 0.49996287, 0.35115856, 0.14582253, 0.1079852, 0.041283585, 0.026892474, 0.020442955, 0.013585414]\n",
      "[0.5181991, 0.46422538, 0.33299434, 0.26560035, 0.18610762, 0.10197754, 0.056276858, 0.044727046, 0.029891793]\n",
      "[0.90112484, 0.22825515, 0.20898016, 0.20096248, 0.12831667, 0.12188147, 0.0983033, 0.08088281, 0.03129323]\n",
      "[0.6902051, 0.3671075, 0.32478553, 0.2705248, 0.21073826, 0.046614103, 0.044640403, 0.029834783, 0.015549447]\n",
      "[0.6262672, 0.38902134, 0.29248348, 0.26232556, 0.13859458, 0.117124334, 0.099919364, 0.042463433, 0.031800553]\n",
      "[0.5448386, 0.52092516, 0.32217926, 0.18533774, 0.12254238, 0.105229266, 0.07505137, 0.06539089, 0.058505155]\n",
      "[0.8326013, 0.36478034, 0.281828, 0.19369403, 0.15756214, 0.11485188, 0.022045175, 0.021027807, 0.011609359]\n",
      "[1.0217959, 0.35185254, 0.23191512, 0.11285436, 0.10087907, 0.07316031, 0.06626126, 0.02484489, 0.016436659]\n",
      "[0.68188226, 0.40418842, 0.27505207, 0.19034877, 0.13657895, 0.11617443, 0.08401847, 0.06385519, 0.04790131]\n",
      "+classified: 93 0\n"
     ]
    }
   ],
   "source": [
    "neu_model_ngram = load_model('my_model_neu_ngrams.h5')\n",
    "neu_model_word = load_model('my_model_neu_words.h5')\n",
    "pred_0 = neu_model_ngram.predict(scaled_test_data_ngrams, verbose=0)\n",
    "pred_1 = neu_model_word.predict(scaled_test_data_words, verbose=0)\n",
    "# conc = np.concatenate((pred_0, pred_1), axis=1)\n",
    "summed = pred_0 + pred_1\n",
    "# print(conc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'candidate00006', 1: 'candidate00004', 2: 'candidate00003', 3: 'candidate00009', 4: 'candidate00002', 5: 'candidate00008', 6: 'candidate00007', 7: 'candidate00001', 8: 'candidate00005', 9: '<UNK>'}\n",
      "[0.7639838, 0.4849627, 0.15885201, 0.14977545, 0.13139054, 0.1307949, 0.10452836, 0.05326262, 0.022449585]\n",
      "[1.0934824, 0.5347313, 0.12917736, 0.10542332, 0.073631525, 0.029506773, 0.019371198, 0.007875161, 0.006801053]\n",
      "[0.41629496, 0.37753227, 0.31957763, 0.29852194, 0.26896805, 0.13785516, 0.08470297, 0.052518696, 0.04402831]\n",
      "[0.7920461, 0.42737857, 0.23580596, 0.16783251, 0.11348875, 0.09465425, 0.075749554, 0.05889041, 0.034153834]\n",
      "[0.7218045, 0.4602304, 0.32316726, 0.13507405, 0.1127368, 0.100664966, 0.08070882, 0.04055082, 0.025062248]\n",
      "[0.71755266, 0.38910508, 0.3032543, 0.19928706, 0.10165503, 0.095576495, 0.08427116, 0.067344815, 0.041953407]\n",
      "[0.9287356, 0.39433008, 0.22758335, 0.14927098, 0.10347741, 0.06399731, 0.056940332, 0.050915126, 0.024749696]\n",
      "[0.7437892, 0.3294309, 0.27099973, 0.24823746, 0.2127012, 0.077135414, 0.046781518, 0.037475605, 0.033449043]\n",
      "[0.54184407, 0.3610068, 0.21862072, 0.19038737, 0.16970918, 0.15582108, 0.14577726, 0.13241735, 0.08441618]\n",
      "[1.3511541, 0.33321607, 0.10333388, 0.10223201, 0.06586174, 0.01695178, 0.01538218, 0.007822519, 0.0040458073]\n",
      "[0.8377702, 0.5931462, 0.22437893, 0.12657978, 0.07896304, 0.03945157, 0.038984425, 0.03120378, 0.029521944]\n",
      "[0.98401505, 0.2883492, 0.23095787, 0.16393238, 0.13924146, 0.100945115, 0.03385164, 0.033680618, 0.025026727]\n",
      "[0.999809, 0.26552638, 0.20964704, 0.13614155, 0.12836932, 0.08317135, 0.07759518, 0.052756954, 0.046983115]\n",
      "[0.8536738, 0.4653971, 0.19890666, 0.1138387, 0.11137651, 0.10524581, 0.08355954, 0.048016176, 0.01998588]\n",
      "[0.5161528, 0.4753129, 0.42044696, 0.16716501, 0.12164592, 0.10982743, 0.08809828, 0.07620307, 0.025147688]\n",
      "[0.9513034, 0.26417485, 0.24701257, 0.23345917, 0.12086275, 0.06257176, 0.05248231, 0.05141055, 0.016722683]\n",
      "[0.68358386, 0.5347786, 0.37902743, 0.21934187, 0.06672913, 0.050106417, 0.028549101, 0.027583212, 0.0103003085]\n",
      "[1.0412313, 0.22317368, 0.19803807, 0.17000628, 0.13153994, 0.098249525, 0.06324501, 0.05582045, 0.018695872]\n",
      "[1.2624035, 0.2273384, 0.1893719, 0.09164766, 0.076097436, 0.054042384, 0.040063377, 0.032464135, 0.026571289]\n",
      "[0.83106846, 0.3279509, 0.20238271, 0.19124383, 0.13931942, 0.1256381, 0.11603321, 0.041554954, 0.0248084]\n",
      "[0.7090577, 0.6964506, 0.335791, 0.10025659, 0.035321783, 0.033978164, 0.032881424, 0.031098973, 0.02516387]\n",
      "[0.45901394, 0.35749722, 0.29890144, 0.29348433, 0.22651635, 0.16789882, 0.08271173, 0.07488099, 0.039095227]\n",
      "[1.462874, 0.14946587, 0.14178471, 0.07584468, 0.05657, 0.052886322, 0.023405636, 0.020549215, 0.016619619]\n",
      "[1.0908513, 0.22092037, 0.20620185, 0.15465581, 0.11689333, 0.069265, 0.06380989, 0.041204408, 0.036197994]\n",
      "[0.87049824, 0.45662287, 0.24561015, 0.1393251, 0.110719725, 0.058342926, 0.041918702, 0.03902384, 0.037938394]\n",
      "[0.6443012, 0.6246143, 0.33313006, 0.16173086, 0.08430529, 0.061853178, 0.04034712, 0.025905613, 0.023812428]\n",
      "[1.310171, 0.19072305, 0.1437288, 0.13369544, 0.099892706, 0.03819463, 0.03808125, 0.02620012, 0.019313127]\n",
      "[0.6521059, 0.4269034, 0.37540016, 0.21384296, 0.13592488, 0.069518656, 0.06526814, 0.036153525, 0.024882477]\n",
      "[0.5896224, 0.38592395, 0.27781346, 0.22685067, 0.17210758, 0.15739018, 0.07249802, 0.058968708, 0.058825046]\n",
      "[0.99189806, 0.35533136, 0.3131879, 0.18121304, 0.08570104, 0.023491785, 0.018517096, 0.017970357, 0.012689501]\n",
      "[1.0777085, 0.24996254, 0.1966781, 0.19313723, 0.10793869, 0.057777114, 0.041994274, 0.04126978, 0.033533882]\n",
      "[0.5052367, 0.3695346, 0.35060263, 0.332008, 0.11292032, 0.09542234, 0.0939223, 0.07113777, 0.06921526]\n",
      "[0.5386309, 0.43285018, 0.30799973, 0.20035902, 0.160905, 0.14674307, 0.09700088, 0.065387875, 0.05012321]\n",
      "[1.022375, 0.30565897, 0.18485188, 0.10651989, 0.09252606, 0.087843426, 0.082817554, 0.06813189, 0.0492756]\n",
      "[0.736135, 0.5697566, 0.31829688, 0.1469937, 0.10434347, 0.06832276, 0.026570063, 0.020111304, 0.009470137]\n",
      "[0.5079663, 0.47578767, 0.30410618, 0.19960518, 0.13042593, 0.114895746, 0.108254805, 0.08881969, 0.07013851]\n",
      "[0.79119015, 0.28664875, 0.23509988, 0.2111218, 0.18712994, 0.1341657, 0.07028221, 0.046949275, 0.03741234]\n",
      "[1.049671, 0.59106696, 0.15419042, 0.09658247, 0.054726988, 0.03208539, 0.013994301, 0.005509855, 0.0021726876]\n",
      "[0.7924785, 0.74819255, 0.114610285, 0.1076788, 0.10373898, 0.06161291, 0.03437982, 0.029074168, 0.008234003]\n",
      "[0.68461335, 0.43892458, 0.2692023, 0.17486493, 0.15237948, 0.12568331, 0.06469632, 0.055495914, 0.034139775]\n",
      "[0.68184733, 0.49469197, 0.17665464, 0.17561868, 0.13754739, 0.10582354, 0.09776677, 0.06943512, 0.06061469]\n",
      "[0.5237127, 0.44302315, 0.4013573, 0.20507419, 0.19300276, 0.08223193, 0.07857701, 0.04442886, 0.028592143]\n",
      "[0.5178707, 0.45282668, 0.35280165, 0.3097397, 0.16168222, 0.10865857, 0.036347162, 0.03237774, 0.027695548]\n",
      "[1.0464122, 0.30836475, 0.25321153, 0.16947985, 0.13044892, 0.04386832, 0.022907723, 0.014587523, 0.010719117]\n",
      "[0.617464, 0.40107542, 0.33202475, 0.27299997, 0.19401252, 0.06441656, 0.04963934, 0.04462351, 0.023743939]\n",
      "[0.72804344, 0.4063528, 0.3097844, 0.17432567, 0.16484277, 0.09234592, 0.054711364, 0.05397991, 0.015613759]\n",
      "[0.99216604, 0.53119624, 0.13401043, 0.12260975, 0.11305875, 0.07113163, 0.018621543, 0.014665503, 0.0025402086]\n",
      "[0.84858996, 0.39025766, 0.21562165, 0.21491739, 0.13742633, 0.054059938, 0.052417066, 0.047992554, 0.0387174]\n",
      "[0.63405436, 0.3619856, 0.30961514, 0.19787386, 0.1852749, 0.14604771, 0.074991934, 0.054987177, 0.035169274]\n",
      "[0.61818516, 0.4863551, 0.24647085, 0.22635213, 0.12052564, 0.09972643, 0.09515953, 0.055591516, 0.051633593]\n",
      "[0.6400494, 0.5695242, 0.27266395, 0.23500171, 0.12855405, 0.058820687, 0.041283637, 0.029504836, 0.02459753]\n",
      "[0.40313828, 0.32835132, 0.26182538, 0.2529176, 0.23770154, 0.15888628, 0.13419227, 0.1142812, 0.10870617]\n",
      "[1.2865856, 0.34062716, 0.15962088, 0.08528133, 0.04969289, 0.03742885, 0.019437885, 0.012490975, 0.008834401]\n",
      "[1.0104961, 0.55963385, 0.13513003, 0.08078371, 0.07553768, 0.05246856, 0.037950076, 0.027541578, 0.020458395]\n",
      "[0.48465747, 0.44454175, 0.27351287, 0.18364498, 0.17372257, 0.13910528, 0.118437536, 0.11590429, 0.06647333]\n",
      "[1.0877051, 0.33108285, 0.32554197, 0.09826946, 0.080253124, 0.036436386, 0.023541285, 0.012234948, 0.004934891]\n",
      "[1.0422537, 0.37456763, 0.15395486, 0.14682582, 0.076902874, 0.06363339, 0.055611357, 0.04700608, 0.0392445]\n",
      "[0.6638119, 0.65254045, 0.2228239, 0.1568102, 0.10418824, 0.0828414, 0.050622985, 0.047943167, 0.018417664]\n",
      "[1.7314868, 0.0666554, 0.054644343, 0.041563246, 0.03832204, 0.035479456, 0.016284123, 0.011978919, 0.0035857612]\n",
      "[0.67814636, 0.4766222, 0.2958296, 0.27716327, 0.09750766, 0.07855488, 0.06630716, 0.01621943, 0.013649368]\n",
      "[0.68585503, 0.3193183, 0.28597137, 0.24823236, 0.120539494, 0.10051684, 0.08741704, 0.07888485, 0.07326484]\n",
      "[0.5995877, 0.5853218, 0.3270356, 0.19604078, 0.14861129, 0.056225527, 0.044927448, 0.024524713, 0.017725239]\n",
      "[0.44529837, 0.40396366, 0.31733292, 0.26616132, 0.2393198, 0.122241646, 0.08302849, 0.06572371, 0.05693005]\n",
      "[0.83576185, 0.3772263, 0.22904035, 0.1581509, 0.13590023, 0.113233045, 0.08418645, 0.033474635, 0.03302635]\n",
      "[1.0420665, 0.3044716, 0.18827212, 0.18223603, 0.11174102, 0.07050459, 0.04137781, 0.038016517, 0.02131384]\n",
      "[0.615173, 0.2971682, 0.2953098, 0.16676486, 0.15854852, 0.15415782, 0.12915872, 0.117387876, 0.066331275]\n",
      "[0.4525709, 0.38075095, 0.31166732, 0.27602375, 0.26735657, 0.14773825, 0.09494199, 0.05021976, 0.018730402]\n",
      "[0.57231027, 0.35178846, 0.34640622, 0.23772888, 0.16207048, 0.10514124, 0.096047394, 0.087298095, 0.04120908]\n",
      "[0.95891654, 0.28752816, 0.1867975, 0.1592428, 0.14590317, 0.11477105, 0.057999074, 0.053737484, 0.035104327]\n",
      "[0.99336386, 0.5492388, 0.20530802, 0.09398851, 0.08781479, 0.026981078, 0.019625474, 0.017080713, 0.006598793]\n",
      "[0.6863392, 0.51518327, 0.21718651, 0.18699808, 0.16468118, 0.11615686, 0.04501484, 0.0401833, 0.028256752]\n",
      "[0.85845464, 0.6967674, 0.1420685, 0.10211723, 0.07015232, 0.0488397, 0.04536117, 0.022346046, 0.013892939]\n",
      "[0.7524613, 0.5447589, 0.3163798, 0.19086297, 0.09632179, 0.05097688, 0.02459889, 0.013105034, 0.010534392]\n",
      "[1.2954872, 0.39603376, 0.14812936, 0.07664624, 0.049223557, 0.014914812, 0.010191257, 0.005524018, 0.0038497346]\n",
      "[1.139201, 0.1813036, 0.17372586, 0.15021138, 0.10374361, 0.08568815, 0.078545034, 0.07285735, 0.014723731]\n",
      "[1.0012084, 0.5426928, 0.16693756, 0.08742216, 0.086664155, 0.0475322, 0.027857646, 0.021702103, 0.017983193]\n",
      "[0.61278176, 0.35485432, 0.21519375, 0.181173, 0.1772674, 0.15553111, 0.112169325, 0.097076476, 0.093952894]\n",
      "[1.374776, 0.21701983, 0.16075376, 0.09823139, 0.05373311, 0.05303021, 0.025045326, 0.013212077, 0.0041981763]\n",
      "[0.44485098, 0.37546703, 0.29663315, 0.25992927, 0.22504912, 0.19238867, 0.084711075, 0.07782607, 0.043144643]\n",
      "[0.66622114, 0.44402277, 0.2732494, 0.21228316, 0.12044088, 0.1086767, 0.099660516, 0.050001495, 0.025443893]\n",
      "[0.6472097, 0.3964746, 0.3799148, 0.17953509, 0.14722952, 0.13103059, 0.071327776, 0.024275918, 0.02300205]\n",
      "[0.72066796, 0.36422646, 0.19992149, 0.16744263, 0.16585615, 0.13021795, 0.11855838, 0.089619674, 0.04348927]\n",
      "[0.70121706, 0.5053532, 0.4063503, 0.22181794, 0.061464474, 0.029478628, 0.026465174, 0.02399949, 0.023853747]\n",
      "[1.1599073, 0.24438205, 0.1774058, 0.17372668, 0.10839494, 0.09132008, 0.016973082, 0.015362937, 0.012527065]\n",
      "[0.79286647, 0.49996287, 0.35115856, 0.14582253, 0.1079852, 0.041283585, 0.026892474, 0.020442955, 0.013585414]\n",
      "[0.5181991, 0.46422538, 0.33299434, 0.26560035, 0.18610762, 0.10197754, 0.056276858, 0.044727046, 0.029891793]\n",
      "[0.90112484, 0.22825515, 0.20898016, 0.20096248, 0.12831667, 0.12188147, 0.0983033, 0.08088281, 0.03129323]\n",
      "[0.6902051, 0.3671075, 0.32478553, 0.2705248, 0.21073826, 0.046614103, 0.044640403, 0.029834783, 0.015549447]\n",
      "[0.6262672, 0.38902134, 0.29248348, 0.26232556, 0.13859458, 0.117124334, 0.099919364, 0.042463433, 0.031800553]\n",
      "[0.5448386, 0.52092516, 0.32217926, 0.18533774, 0.12254238, 0.105229266, 0.07505137, 0.06539089, 0.058505155]\n",
      "[0.8326013, 0.36478034, 0.281828, 0.19369403, 0.15756214, 0.11485188, 0.022045175, 0.021027807, 0.011609359]\n",
      "[1.0217959, 0.35185254, 0.23191512, 0.11285436, 0.10087907, 0.07316031, 0.06626126, 0.02484489, 0.016436659]\n",
      "[0.68188226, 0.40418842, 0.27505207, 0.19034877, 0.13657895, 0.11617443, 0.08401847, 0.06385519, 0.04790131]\n",
      "+classified: 93 18\n"
     ]
    }
   ],
   "source": [
    "print(index_2_label_dict)\n",
    "count = 0\n",
    "scount = 0\n",
    "for ind, v in enumerate(test_labels):\n",
    "    if v == 9:\n",
    "        count += 1\n",
    "        ss = sorted(summed[ind], reverse=True)\n",
    "        print(ss)\n",
    "        if ss[0] - ss[1] < 0.1:\n",
    "            scount += 1\n",
    "print('+classified:', count, scount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = CalibratedClassifierCV(OneVsRestClassifier(SVC(C=1)))\n",
    "clf.fit(scaled_train_data, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
