{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "# Reading general data of the problems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading general data of the problems, done!\n"
          ]
        }
      ],
      "source": [
        "# coding\u003dutf-8\n",
        "from __future__ import division\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import multiprocessing\n",
        "\n",
        "# multiprocessing.set_start_method(\u0027spawn\u0027)\n",
        "from MyUtils import clean_folder, read_files, shuffle_docs\n",
        "from Word2Dim import Word2Dim\n",
        "\n",
        "dataset_path \u003d \u0027.\u0027 + os.sep + \u0027pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23\u0027\n",
        "outpath \u003d \u0027.\u0027 + os.sep + \u0027dev_out\u0027\n",
        "\n",
        "clean_folder(outpath)\n",
        "\n",
        "infocollection \u003d dataset_path + os.sep + \u0027collection-info.json\u0027\n",
        "problems \u003d []\n",
        "language \u003d []\n",
        "with open(infocollection, \u0027r\u0027) as f:\n",
        "    for attrib in json.load(f):\n",
        "        problems.append(attrib[\u0027problem-name\u0027])\n",
        "        language.append(attrib[\u0027language\u0027])\n",
        "print(\u0027Reading general data of the problems, done!\u0027)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Reading problem 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "doc count to process:  819\n"
          ]
        }
      ],
      "source": [
        "\n",
        "problem \u003d problems[0]\n",
        "index \u003d 0\n",
        "\n",
        "# used for n_gram extraction and word indexing, a threshold which prevent words appearing lower than this value to be counted in calculations\n",
        "tf \u003d 5\n",
        "\n",
        "\n",
        "infoproblem \u003d dataset_path + os.sep + problem + os.sep + \u0027problem-info.json\u0027\n",
        "candidates \u003d []\n",
        "with open(infoproblem, \u0027r\u0027) as f:\n",
        "    fj \u003d json.load(f)\n",
        "    unk_folder \u003d fj[\u0027unknown-folder\u0027]\n",
        "    for attrib in fj[\u0027candidate-authors\u0027]:\n",
        "        candidates.append(attrib[\u0027author-name\u0027])\n",
        "\n",
        "candidates.sort()\n",
        "# Building training set\n",
        "train_docs \u003d []\n",
        "for candidate in candidates:\n",
        "    train_docs.extend(read_files(dataset_path + os.sep + problem, candidate))\n",
        "train_texts \u003d [text for i, (text, label) in enumerate(train_docs)]\n",
        "train_labels \u003d [label for i, (text, label) in enumerate(train_docs)]\n",
        "initial_train_size \u003d len(train_labels)\n",
        "train_texts, train_labels \u003d shuffle_docs(train_texts, train_labels)\n",
        "validation_size \u003d len(train_texts) - initial_train_size\n",
        "class_size \u003d int(initial_train_size / len(set(train_labels)))\n",
        "index_2_label_dict \u003d {i: l for i, l in enumerate(set(train_labels))}\n",
        "label_2_index_dict \u003d {l: i for i, l in enumerate(set(train_labels))}\n",
        "train_labels \u003d [label_2_index_dict[v] for v in train_labels]\n",
        "w2d \u003d Word2Dim(lang\u003d language[index])\n",
        "train_tokenized_with_pos, train_tokenized_indexed \u003d w2d.fit_transform_texts(train_texts, train_labels, tf\u003d tf)\n",
        "\n",
        "maxlen \u003d len(max(train_tokenized_indexed, key\u003dlen))  # We will cut the texts after # words\n",
        "embedding_dim \u003d w2d.word_embedding.shape[1]\n",
        "\n",
        "# preparing test set\n",
        "ground_truth_file \u003d dataset_path + os.sep + problem + os.sep + \u0027ground-truth.json\u0027\n",
        "gt \u003d {}\n",
        "with open(ground_truth_file, \u0027r\u0027) as f:\n",
        "    for attrib in json.load(f)[\u0027ground_truth\u0027]:\n",
        "        gt[attrib[\u0027unknown-text\u0027]] \u003d attrib[\u0027true-author\u0027]\n",
        "\n",
        "test_docs \u003d read_files(dataset_path + os.sep + problem, unk_folder, gt)\n",
        "test_texts \u003d [text for i, (text, label) in enumerate(test_docs)]\n",
        "test_labels \u003d [label for i, (text, label) in enumerate(test_docs)]\n",
        "\n",
        "# Filter validation to known authors\n",
        "test_texts \u003d [text for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
        "test_labels \u003d [label for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
        "\n",
        "test_labels \u003d [label_2_index_dict[v] for v in test_labels]\n",
        "\n",
        "test_tokenized_with_pos, test_tokenized_indexed \u003d w2d.transform(test_texts)\n",
        "print(\"Reading problem 1, done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Data Extraction for Neural Net\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from MyUtils import extract_n_grams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "n \u003d 3\n",
        "vocabulary \u003d extract_n_grams(train_docs, n, tf)\n",
        "vectorizer \u003d CountVectorizer(analyzer\u003d\u0027char\u0027, ngram_range\u003d(n, n), lowercase\u003dFalse, vocabulary\u003dvocabulary)\n",
        "n_gram_train_data \u003d vectorizer.fit_transform(train_texts)\n",
        "\n",
        "n_gram_train_data \u003d n_gram_train_data.astype(float)\n",
        "\n",
        "for i, v in enumerate(train_texts):\n",
        "    n_gram_train_data[i] \u003d n_gram_train_data[i] / len(train_texts[i])\n",
        "n_gram_test_data \u003d vectorizer.transform(test_texts)\n",
        "n_gram_test_data \u003d n_gram_test_data.astype(float)\n",
        "for i, v in enumerate(test_texts):\n",
        "    n_gram_test_data[i] \u003d n_gram_test_data[i] / len(test_texts[i])\n",
        "max_abs_scaler \u003d preprocessing.MaxAbsScaler()\n",
        "scaled_train_data_ngrams \u003d max_abs_scaler.fit_transform(n_gram_train_data)\n",
        "scaled_test_data_ngrams \u003d max_abs_scaler.transform(n_gram_test_data)\n",
        "max_abs_scaler \u003d preprocessing.MaxAbsScaler()\n",
        "scaled_train_data_words \u003d max_abs_scaler.fit_transform(w2d.get_texts_vectorized_and_normalized(train_tokenized_indexed)[:, 1:])\n",
        "scaled_test_data_words \u003d max_abs_scaler.transform(w2d.get_texts_vectorized_and_normalized(test_tokenized_indexed)[:, 1:])\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(819, 7623)\n",
            "(468, 7623)\n",
            "7623\n"
          ]
        }
      ],
      "source": [
        "print(scaled_train_data_words.shape)\n",
        "print(scaled_test_data_words.shape)\n",
        "print(len(w2d.word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "from numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(1)\n\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score\nfrom keras import layers, Input, callbacks\nfrom keras.models import Sequential, Model\nfrom keras import optimizers, regularizers\nfrom keras.utils import to_categorical\nfrom keras_preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ncallbacks_list_neu \u003d [\n    callbacks.EarlyStopping(\n        monitor\u003d\u0027val_loss\u0027,\n        patience\u003d50,\n    ),\n    callbacks.ModelCheckpoint(\n        filepath\u003d\u0027my_model_neu.h5\u0027,\n        monitor\u003d\u0027val_loss\u0027,\n        save_best_only\u003dTrue,\n    ),\n    callbacks.ReduceLROnPlateau(\n        monitor\u003d\u0027val_loss\u0027,\n        factor\u003d0.5,\n        verbose\u003d1, \n        patience\u003d20,\n    )\n]\n\ncallbacks_list_neu_ngrams \u003d [\n    callbacks.EarlyStopping(\n        monitor\u003d\u0027val_loss\u0027,\n        patience\u003d50,\n    ),\n    callbacks.ModelCheckpoint(\n        filepath\u003d\u0027my_model_neu_ngrams.h5\u0027,\n        monitor\u003d\u0027val_loss\u0027,\n        save_best_only\u003dTrue,\n    ),\n    callbacks.ReduceLROnPlateau(\n        monitor\u003d\u0027val_loss\u0027,\n        factor\u003d0.5,\n        verbose\u003d1, \n        patience\u003d20,\n    )\n]\n\ncallbacks_list_neu_words \u003d [\n    callbacks.EarlyStopping(\n        monitor\u003d\u0027val_loss\u0027,\n        patience\u003d50,\n    ),\n    callbacks.ModelCheckpoint(\n        filepath\u003d\u0027my_model_neu_words.h5\u0027,\n        monitor\u003d\u0027val_loss\u0027,\n        save_best_only\u003dTrue,\n    ),\n    callbacks.ReduceLROnPlateau(\n        monitor\u003d\u0027val_loss\u0027,\n        factor\u003d0.5,\n        verbose\u003d1, \n        patience\u003d20,\n    )\n]\n\ncallbacks_list_convnet \u003d [\n    callbacks.EarlyStopping(\n        monitor\u003d\u0027val_loss\u0027,\n        patience\u003d35,\n    ),\n    callbacks.ModelCheckpoint(\n        filepath\u003d\u0027my_model_convnet.h5\u0027,\n        monitor\u003d\u0027val_loss\u0027,\n        save_best_only\u003dTrue,\n    ),\n    callbacks.ReduceLROnPlateau(\n        monitor\u003d\u0027val_loss\u0027,\n        factor\u003d0.5,\n        verbose\u003d1, \n        patience\u003d10,\n    )\n]\n\ncallbacks_list_stacked \u003d [\n    callbacks.ModelCheckpoint(\n        filepath\u003d\u0027my_model_stacked.h5\u0027,\n        monitor\u003d\u0027val_loss\u0027,\n        save_best_only\u003dTrue,\n    ),\n    callbacks.ReduceLROnPlateau(\n        monitor\u003d\u0027val_loss\u0027,\n        factor\u003d0.5,\n        verbose\u003d1, \n        patience\u003d20,\n    )\n]\n\ntrain_data \u003d pad_sequences(train_tokenized_indexed, maxlen\u003dmaxlen)\n\ntest_data \u003d pad_sequences(test_tokenized_indexed, maxlen\u003dmaxlen)\n\n# X_train, X_val, y_train, y_val \u003d train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n# X_train, X_val, y_train, y_val \u003d train_test_split(train_data, train_labels,\n#                                                   test_size\u003dvalidation_size,\n#                                                   stratify\u003dtrain_labels)\n# X_scaled_train_data_words, X_scaled_val_data_words, _, _ \u003d train_test_split(scaled_train_data_words, train_labels,\n#                                                   test_size\u003dvalidation_size,\n#                                                   stratify\u003dtrain_labels)\n# X_scaled_train_data_ngrams, X_scaled_val_data_ngrams, _, _ \u003d train_test_split(scaled_train_data_ngrams, train_labels,\n#                                                   test_size\u003dvalidation_size,\n#                                                   stratify\u003dtrain_labels)\n\ny_train, y_val \u003d train_labels[:initial_train_size], train_labels[initial_train_size:]\nX_train, X_val \u003d train_data[:initial_train_size], train_data[initial_train_size:]\nX_scaled_train_data_words, X_scaled_val_data_words \u003d scaled_train_data_words[:initial_train_size], scaled_train_data_words[initial_train_size:]\nX_scaled_train_data_ngrams, X_scaled_val_data_ngrams \u003d scaled_train_data_ngrams[:initial_train_size], scaled_train_data_ngrams[initial_train_size:]\n\ny_train \u003d to_categorical(y_train)\ny_val \u003d to_categorical(y_val)\n# y_test \u003d to_categorical(test_labels)\n# print(X_train.shape)\n\nprint(y_train.shape)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "metadata": false,
          "name": "#%%\n"
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(1)\n",
        "\n",
        "\n",
        "from keras import layers, Input\n",
        "from keras.models import Sequential, Model\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "neu_ng \u003d Sequential()\n",
        "neu_ng.add(layers.Dense(32, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001),\n",
        "                               input_shape\u003d(scaled_train_data_ngrams.shape[1],)))\n",
        "neu_ng.add(layers.Dropout(0.3))\n",
        "neu_ng.add(layers.Dense(64, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "neu_ng.add(layers.Dropout(0.3))\n",
        "neu_ng.add(layers.Dense(64, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "neu_ng.add(layers.Dropout(0.3))\n",
        "neu_ng.add(layers.Dense(len(set(train_labels)), activation\u003d\u0027softmax\u0027))\n",
        "\n",
        "neu_ng.compile(optimizer\u003doptimizers.Adam(lr\u003d1e-4),\n",
        "              loss\u003d\u0027categorical_crossentropy\u0027,\n",
        "              metrics\u003d[\u0027acc\u0027])\n",
        "neu_ng.summary()\n",
        "history \u003d neu_ng.fit(X_scaled_train_data_ngrams, y_train,\n",
        "                    validation_data\u003d(X_scaled_val_data_ngrams, y_val),\n",
        "                    epochs\u003d1500,\n",
        "                    batch_size\u003dclass_size,\n",
        "                    callbacks\u003dcallbacks_list_neu_ngrams,\n",
        "                    verbose\u003d 1\n",
        "                   )\n",
        "acc \u003d history.history[\u0027acc\u0027]\n",
        "val_acc \u003d history.history[\u0027val_acc\u0027]\n",
        "loss \u003d history.history[\u0027loss\u0027]\n",
        "val_loss \u003d history.history[\u0027val_loss\u0027]\n",
        "\n",
        "epochs \u003d range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, \u0027r\u0027, label\u003d\u0027Training acc\u0027)\n",
        "plt.plot(epochs, val_acc, \u0027b\u0027, label\u003d\u0027Validation acc\u0027)\n",
        "plt.title(\u0027Training and validation accuracy\u0027)\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, \u0027r\u0027, label\u003d\u0027Training loss\u0027)\n",
        "plt.plot(epochs, val_loss, \u0027b\u0027, label\u003d\u0027Validation loss\u0027)\n",
        "plt.title(\u0027Training and validation loss\u0027)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "print(max(val_acc))\n",
        "l_model \u003d load_model(\u0027my_model_neu_ngrams.h5\u0027)\n",
        "yhat \u003d l_model.predict(scaled_test_data_ngrams)\n",
        "yhat \u003d argmax(yhat, axis\u003d1)\n",
        "acc \u003d accuracy_score(test_labels, yhat)\n",
        "print(\u0027my_model_neu_ngrams Test Accuracy: %.3f\u0027 % acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my_model_neu_ngrams Test Accuracy: 0.808\n"
          ]
        }
      ],
      "source": [
        "l_model \u003d load_model(\u0027my_model_neu_ngrams.h5\u0027)\n",
        "yhat \u003d l_model.predict(scaled_test_data_ngrams)\n",
        "yhat \u003d argmax(yhat, axis\u003d1)\n",
        "acc \u003d accuracy_score(test_labels, yhat)\n",
        "print(\u0027my_model_neu_ngrams Test Accuracy: %.3f\u0027 % acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(1)\n",
        "\n",
        "\n",
        "from keras import layers, Input\n",
        "from keras.models import Sequential, Model\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "neu_wo \u003d Sequential()\n",
        "neu_wo.add(layers.Dense(32, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001),\n",
        "                               input_shape\u003d(scaled_train_data_words.shape[1],)))\n",
        "neu_wo.add(layers.Dropout(0.3))\n",
        "neu_wo.add(layers.Dense(64, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "neu_wo.add(layers.Dropout(0.3))\n",
        "neu_wo.add(layers.Dense(64, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "neu_wo.add(layers.Dropout(0.3))\n",
        "neu_wo.add(layers.Dense(len(set(train_labels)), activation\u003d\u0027softmax\u0027))\n",
        "\n",
        "\n",
        "neu_wo.compile(optimizer\u003doptimizers.Adam(lr\u003d1e-4),\n",
        "              loss\u003d\u0027categorical_crossentropy\u0027,\n",
        "              metrics\u003d[\u0027acc\u0027])\n",
        "neu_wo.summary()\n",
        "history \u003d neu_wo.fit(X_scaled_train_data_words, y_train,\n",
        "                    validation_data\u003d(X_scaled_val_data_words, y_val),\n",
        "                    epochs\u003d2000,\n",
        "                    batch_size\u003dclass_size,\n",
        "                    callbacks\u003dcallbacks_list_neu_words,\n",
        "                    verbose\u003d 1\n",
        "                   )\n",
        "acc \u003d history.history[\u0027acc\u0027]\n",
        "val_acc \u003d history.history[\u0027val_acc\u0027]\n",
        "loss \u003d history.history[\u0027loss\u0027]\n",
        "val_loss \u003d history.history[\u0027val_loss\u0027]\n",
        "\n",
        "epochs \u003d range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, \u0027r\u0027, label\u003d\u0027Training acc\u0027)\n",
        "plt.plot(epochs, val_acc, \u0027b\u0027, label\u003d\u0027Validation acc\u0027)\n",
        "plt.title(\u0027Training and validation accuracy\u0027)\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, \u0027r\u0027, label\u003d\u0027Training loss\u0027)\n",
        "plt.plot(epochs, val_loss, \u0027b\u0027, label\u003d\u0027Validation loss\u0027)\n",
        "plt.title(\u0027Training and validation loss\u0027)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "print(max(val_acc))\n",
        "l_model \u003d load_model(\u0027my_model_neu_words.h5\u0027)\n",
        "yhat \u003d l_model.predict( scaled_test_data_words)\n",
        "yhat \u003d argmax(yhat, axis\u003d1)\n",
        "acc \u003d accuracy_score(test_labels, yhat)\n",
        "print(\u0027my_model_neu_words Test Accuracy: %.3f\u0027 % acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(1)\n",
        "\n",
        "\n",
        "from keras import layers, Input\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "word_input_tensor \u003d Input(shape\u003d(scaled_train_data_words.shape[1],) , name\u003d\u0027words\u0027)\n",
        "ngram_input_tensor \u003d Input(shape\u003d(scaled_train_data_ngrams.shape[1],) , name\u003d\u0027n_grams\u0027)\n",
        "\n",
        "neu0 \u003d Sequential()\n",
        "neu0.add(layers.Dense(32, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001),\n",
        "                               input_shape\u003d(scaled_train_data_ngrams.shape[1],)))\n",
        "neu0.add(layers.Dropout(0.3))\n",
        "neu0.add(layers.Dense(64, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "neu0.add(layers.Dropout(0.3))\n",
        "neu0.add(layers.Dense(64, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "neu0.add(layers.Dropout(0.3))\n",
        "\n",
        "neu1 \u003d Sequential()\n",
        "neu1.add(layers.Dense(32, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001),\n",
        "                               input_shape\u003d(scaled_train_data_words.shape[1],)))\n",
        "neu1.add(layers.Dropout(0.3))\n",
        "neu1.add(layers.Dense(64, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "neu1.add(layers.Dropout(0.3))\n",
        "neu1.add(layers.Dense(64, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "neu1.add(layers.Dropout(0.3))\n",
        "\n",
        "output_tensor_0 \u003d neu0(ngram_input_tensor)\n",
        "output_tensor_1 \u003d neu1(word_input_tensor)\n",
        "\n",
        "\n",
        "# conv_input_tensor \u003d Input(shape\u003d(maxlen,) , name\u003d\u0027convnets\u0027)\n",
        "\n",
        "# conv_1d_s3_model \u003d Sequential()\n",
        "# conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length\u003dmaxlen))\n",
        "# conv_1d_s3_model.add(layers.Dropout(0.2))\n",
        "# conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
        "# conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "# conv_output_tensor_0 \u003d conv_1d_s3_model(conv_input_tensor)\n",
        "\n",
        "\n",
        "# conv_1d_s1_model \u003d Sequential()\n",
        "# conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length\u003dmaxlen))\n",
        "# conv_1d_s1_model.add(layers.Dropout(0.2))\n",
        "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 1, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
        "# conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "# conv_output_tensor_1 \u003d conv_1d_s1_model(conv_input_tensor)\n",
        "\n",
        "# conv_1d_complex_model \u003d Sequential()\n",
        "# conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length\u003dmaxlen))\n",
        "# conv_1d_complex_model.add(layers.Dropout(0.2))\n",
        "# conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
        "# conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "# conv_output_tensor_2 \u003d conv_1d_complex_model(conv_input_tensor)\n",
        "\n",
        "\n",
        "\n",
        "concatenated \u003d layers.concatenate([output_tensor_0,\n",
        "                                   output_tensor_1,\n",
        "#                                    conv_output_tensor_0,\n",
        "#                                    conv_output_tensor_1,\n",
        "#                                    conv_output_tensor_2,\n",
        "                                  ], axis\u003d-1)\n",
        "\n",
        "concatenated \u003d layers.Dense(128, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001))(concatenated)\n",
        "concatenated \u003d layers.Dropout(0.3)(concatenated)\n",
        "concatenated \u003d layers.Dense(len(set(train_labels)), activation\u003d\u0027softmax\u0027)(concatenated)\n",
        "\n",
        "model \u003d Model([ngram_input_tensor, word_input_tensor,\n",
        "#                conv_input_tensor\n",
        "              ], concatenated)\n",
        "model.compile(optimizer\u003doptimizers.Adam(lr\u003d1e-4),\n",
        "              loss\u003d\u0027categorical_crossentropy\u0027,\n",
        "              metrics\u003d[\u0027acc\u0027])\n",
        "model.summary()\n",
        "print(X_scaled_train_data_ngrams.shape, X_scaled_train_data_words.shape, y_train.shape) \n",
        "history \u003d model.fit([X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
        "#                      X_train\n",
        "                    ], y_train,\n",
        "                    validation_data\u003d([X_scaled_val_data_ngrams, X_scaled_val_data_words,\n",
        "#                                       X_val\n",
        "                                     ], y_val),\n",
        "                    epochs\u003d2000,\n",
        "                    batch_size\u003dclass_size,\n",
        "                    callbacks\u003dcallbacks_list_neu,\n",
        "                    verbose\u003d 1\n",
        "                   )\n",
        "acc \u003d history.history[\u0027acc\u0027]\n",
        "val_acc \u003d history.history[\u0027val_acc\u0027]\n",
        "loss \u003d history.history[\u0027loss\u0027]\n",
        "val_loss \u003d history.history[\u0027val_loss\u0027]\n",
        "\n",
        "epochs \u003d range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, \u0027bo\u0027, label\u003d\u0027Training acc\u0027)\n",
        "plt.plot(epochs, val_acc, \u0027b\u0027, label\u003d\u0027Validation acc\u0027)\n",
        "plt.title(\u0027Training and validation accuracy\u0027)\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, \u0027bo\u0027, label\u003d\u0027Training loss\u0027)\n",
        "plt.plot(epochs, val_loss, \u0027b\u0027, label\u003d\u0027Validation loss\u0027)\n",
        "plt.title(\u0027Training and validation loss\u0027)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "print(max(val_acc))\n",
        "l_model \u003d load_model(\u0027my_model_neu.h5\u0027)\n",
        "yhat \u003d l_model.predict([scaled_test_data_ngrams, scaled_test_data_words])\n",
        "yhat \u003d argmax(yhat, axis\u003d1)\n",
        "acc \u003d accuracy_score(test_labels, yhat)\n",
        "print(\u0027my_model_neu Test Accuracy: %.3f\u0027 % acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(1)\n",
        "\n",
        "\n",
        "from keras import layers, Input\n",
        "from keras.models import Sequential, Model\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "convnet_input_tensor \u003d Input(shape\u003d(maxlen,) , name\u003d\u0027convnet_words\u0027)\n",
        "\n",
        "conv_1d_s3_model \u003d Sequential()\n",
        "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length\u003dmaxlen))\n",
        "conv_1d_s3_model.add(layers.Dropout(0.3))\n",
        "conv_1d_s3_model.add(layers.SeparableConv1D(64, 3, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "conv_1d_s3_model.add(layers.BatchNormalization())\n",
        "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
        "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
        "# conv_1d_s3_model.add(layers.Dense(len(set(train_labels)), activation\u003d\u0027softmax\u0027))\n",
        "\n",
        "# conv_1d_s3_model.layers[0].set_weights([w2d.word_embedding])\n",
        "# conv_1d_s3_model.layers[0].trainable \u003d False\n",
        "conv_output_tensor_0 \u003d conv_1d_s3_model(convnet_input_tensor)\n",
        "\n",
        "\n",
        "conv_1d_s1_model \u003d Sequential()\n",
        "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length\u003dmaxlen))\n",
        "conv_1d_s1_model.add(layers.Dropout(0.3))\n",
        "conv_1d_s1_model.add(layers.SeparableConv1D(64, 1, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "conv_1d_s1_model.add(layers.BatchNormalization())\n",
        "# conv_1d_s1_model.add(layers.MaxPooling1D(2))\n",
        "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 3, activation\u003d\u0027relu\u0027))\n",
        "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
        "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
        "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation\u003d\u0027softmax\u0027))\n",
        "\n",
        "# conv_1d_s1_model.layers[0].set_weights([w2d.word_embedding])\n",
        "# conv_1d_s1_model.layers[0].trainable \u003d False\n",
        "conv_output_tensor_1 \u003d conv_1d_s1_model(convnet_input_tensor)\n",
        "\n",
        "conv_1d_complex_model \u003d Sequential()\n",
        "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length\u003dmaxlen))\n",
        "conv_1d_complex_model.add(layers.Dropout(0.3))\n",
        "conv_1d_complex_model.add(layers.SeparableConv1D(64, 2, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001)))\n",
        "conv_1d_complex_model.add(layers.BatchNormalization())\n",
        "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation\u003d\u0027relu\u0027))\n",
        "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
        "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation\u003d\u0027relu\u0027))\n",
        "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation\u003d\u0027relu\u0027))\n",
        "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
        "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation\u003d\u0027relu\u0027))\n",
        "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation\u003d\u0027relu\u0027))\n",
        "conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
        "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
        "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation\u003d\u0027softmax\u0027))\n",
        "\n",
        "# conv_1d_complex_model.layers[0].set_weights([w2d.word_embedding])\n",
        "# conv_1d_complex_model.layers[0].trainable \u003d False\n",
        "conv_output_tensor_2 \u003d conv_1d_complex_model(convnet_input_tensor)\n",
        "\n",
        "# x \u003d layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length\u003dmaxlen)(word_input_tensor)\n",
        "# x \u003d layers.Conv1D(128, 5, activation\u003d\u0027relu\u0027, padding\u003d\u0027same\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001))(x)\n",
        "# x \u003d layers.Dropout(0.2)(x)\n",
        "# x \u003d layers.BatchNormalization()(x)\n",
        "# y \u003d layers.Conv1D(128, 10, activation\u003d\u0027relu\u0027, padding\u003d\u0027same\u0027)(x)\n",
        "# added \u003d layers.add([y, x])\n",
        "# added \u003d layers.GlobalMaxPooling1D()(added)\n",
        "\n",
        "concatenated \u003d layers.concatenate([conv_output_tensor_0,\n",
        "                                   conv_output_tensor_1,\n",
        "                                   conv_output_tensor_2,\n",
        "#                                    ,added\n",
        "                                  ], axis\u003d-1)\n",
        "concatenated \u003d layers.Dense(128, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001))(concatenated)\n",
        "concatenated \u003d layers.Dropout(0.3)(concatenated)\n",
        "concatenated \u003d layers.Dense(128, activation\u003d\u0027relu\u0027, kernel_regularizer\u003dregularizers.l1_l2(l1\u003d0.001, l2\u003d0.001))(concatenated)\n",
        "concatenated \u003d layers.Dropout(0.3)(concatenated)\n",
        "answer \u003d layers.Dense(len(set(train_labels)), activation\u003d\u0027softmax\u0027)(concatenated)\n",
        "\n",
        "model \u003d Model(convnet_input_tensor, answer)\n",
        "model.summary()\n",
        "\n",
        "# model.layers[0].set_weights([w2d.word_embedding])\n",
        "# model.layers[0].trainable \u003d False\n",
        "\n",
        "model.compile(optimizer\u003doptimizers.Adam(lr\u003d1e-4),\n",
        "              loss\u003d\u0027categorical_crossentropy\u0027,\n",
        "              metrics\u003d[\u0027acc\u0027])\n",
        "history \u003d model.fit(X_train, y_train,\n",
        "                    validation_data\u003d(X_val, y_val),\n",
        "                    epochs\u003d2000,\n",
        "                    batch_size\u003dclass_size,\n",
        "                    callbacks\u003dcallbacks_list_convnet,\n",
        "                    verbose\u003d 2\n",
        "                   )\n",
        "\n",
        "acc \u003d history.history[\u0027acc\u0027]\n",
        "val_acc \u003d history.history[\u0027val_acc\u0027]\n",
        "loss \u003d history.history[\u0027loss\u0027]\n",
        "val_loss \u003d history.history[\u0027val_loss\u0027]\n",
        "\n",
        "epochs \u003d range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, \u0027r\u0027, label\u003d\u0027Training acc\u0027)\n",
        "plt.plot(epochs, val_acc, \u0027b\u0027, label\u003d\u0027Validation acc\u0027)\n",
        "plt.title(\u0027Training and validation accuracy\u0027)\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, \u0027r\u0027, label\u003d\u0027Training loss\u0027)\n",
        "plt.plot(epochs, val_loss, \u0027b\u0027, label\u003d\u0027Validation loss\u0027)\n",
        "plt.title(\u0027Training and validation loss\u0027)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "l_model \u003d load_model(\u0027my_model_convnet.h5\u0027)\n",
        "yhat \u003d l_model.predict(X_test)\n",
        "yhat \u003d argmax(yhat, axis\u003d1)\n",
        "acc \u003d accuracy_score(test_labels, yhat)\n",
        "print(\u0027my_model_convnet Test Accuracy: %.3f\u0027 % acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(max(val_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from numpy import dstack\n",
        "from keras.utils import plot_model\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "# load models from file\n",
        "def load_all_models(model_names_list):\n",
        "    all_models \u003d list()\n",
        "    for model_name in model_names_list:\n",
        "        # define filename for this ensemble\n",
        "#         filename \u003d \u0027models/model_\u0027 + str(i + 1) + \u0027.h5\u0027\n",
        "        filename \u003d model_name + \u0027.h5\u0027\n",
        "        # load model from file\n",
        "        model \u003d load_model(filename)\n",
        "        # add to list of members\n",
        "        all_models.append(model)\n",
        "        print(\u0027\u003eloaded %s\u0027 % filename)\n",
        "    return all_models\n",
        "\n",
        "\n",
        "# define stacked model from multiple member input models\n",
        "def define_stacked_model(members):\n",
        "    # update all layers in all models to not be trainable\n",
        "    for i in range(len(members)):\n",
        "        model \u003d members[i]\n",
        "        for layer in model.layers:\n",
        "            # make not trainable\n",
        "            layer.trainable \u003d False\n",
        "            # rename to avoid \u0027unique layer name\u0027 issue\n",
        "            layer.name \u003d \u0027ensemble_\u0027 + str(i + 1) + \u0027_\u0027 + layer.name\n",
        "    # define multi-headed input\n",
        "    ensemble_visible \u003d [model.input for model in members]\n",
        "#     print(ensemble_visible)\n",
        "#     ensemble_visible \u003d [[ngram_input_tensor, word_input_tensor], convnet_input_tensor]\n",
        "    # concatenate merge output from each model\n",
        "    ensemble_outputs \u003d [model.output for model in members]\n",
        "#     ensemble_outputs \u003d [concatenated, answer]\n",
        "    merge \u003d concatenate(ensemble_outputs)\n",
        "    hidden \u003d layers.Dense(128, activation\u003d\u0027relu\u0027)(merge)\n",
        "    hidden \u003d layers.Dropout(0.3)(hidden)\n",
        "    output \u003d layers.Dense(len(set(train_labels)), activation\u003d\u0027softmax\u0027)(hidden)\n",
        "    model \u003d Model(inputs\u003densemble_visible, outputs\u003doutput)\n",
        "    # plot graph of ensemble\n",
        "#     plot_model(model, show_shapes\u003dTrue, to_file\u003d\u0027model_graph.png\u0027)\n",
        "    # compile\n",
        "    model.compile(loss\u003d\u0027categorical_crossentropy\u0027, optimizer\u003doptimizers.Adam(lr\u003d3e-4), metrics\u003d[\u0027accuracy\u0027])\n",
        "    return model\n",
        "\n",
        "# fit a stacked model\n",
        "def fit_stacked_model(model, inputX, inputy, valX, valy):\n",
        "    # prepare input data\n",
        "#     X \u003d [inputX for _ in range(len(model.input))]\n",
        "    # encode output data\n",
        "#     inputy_enc \u003d to_categorical(inputy)\n",
        "    # fit model\n",
        "    model.fit(inputX, inputy, validation_data\u003d(valX, valy), batch_size\u003dclass_size,\n",
        "              callbacks\u003dcallbacks_list_stacked, epochs\u003d500, verbose\u003d1)\n",
        "    \n",
        "# make a prediction with a stacked model\n",
        "def predict_stacked_model(model, inputX):\n",
        "    # prepare input data\n",
        "#     X \u003d [inputX for _ in range(len(model.input))]\n",
        "    # make prediction\n",
        "    return model.predict(inputX, verbose\u003d0)\n",
        "\n",
        "print(\u0027done!\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "\n",
        "members \u003d load_all_models([\u0027my_model_neu_ngrams\u0027, \u0027my_model_neu_words\u0027, \u0027my_model_convnet\u0027])\n",
        "print(\u0027Loaded %d models\u0027 % len(members))\n",
        "# define ensemble model\n",
        "stacked_model \u003d define_stacked_model(members)\n",
        "# fit stacked model on test dataset\n",
        "fit_stacked_model(stacked_model, [X_scaled_train_data_ngrams, X_scaled_train_data_words, X_train], y_train, [X_scaled_val_data_ngrams, X_scaled_val_data_words, X_val], y_val)\n",
        "final_model \u003d load_model(\u0027my_model_neu_stacked.h5\u0027)\n",
        "# make predictions and evaluate\n",
        "yhat \u003d predict_stacked_model(stacked_model, [scaled_test_data_ngrams, scaled_test_data_words, test_data])\n",
        "yhat \u003d argmax(yhat, axis\u003d1)\n",
        "acc \u003d accuracy_score(test_labels, yhat)\n",
        "print(\u0027Stacked Test Accuracy: %.3f\u0027 % acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "stem_cell": {
      "cell_type": "raw",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}