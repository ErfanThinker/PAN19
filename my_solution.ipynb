{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Reading general data of the problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading general data of the problems, done!\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import division\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "# multiprocessing.set_start_method('spawn')\n",
    "from MyUtils import clean_folder, read_files, shuffle_docs\n",
    "from Word2Dim import Word2Dim\n",
    "\n",
    "dataset_path = '.' + os.sep + 'pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23'\n",
    "outpath = '.' + os.sep + 'dev_out'\n",
    "\n",
    "clean_folder(outpath)\n",
    "\n",
    "infocollection = dataset_path + os.sep + 'collection-info.json'\n",
    "problems = []\n",
    "language = []\n",
    "with open(infocollection, 'r') as f:\n",
    "    for attrib in json.load(f):\n",
    "        problems.append(attrib['problem-name'])\n",
    "        language.append(attrib['language'])\n",
    "print('Reading general data of the problems, done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reading problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc count to process:  819\n",
      "process_doc, done!\n",
      "word_set, ready!\n",
      "fit_transform_texts is done!\n",
      "doc count to process:  468\n",
      "Reading problem 1, done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "problem = problems[0]\n",
    "index = 0\n",
    "\n",
    "# used for n_gram extraction and word indexing, a threshold which prevent words appearing lower than this value to be counted in calculations\n",
    "tf = 5\n",
    "\n",
    "\n",
    "infoproblem = dataset_path + os.sep + problem + os.sep + 'problem-info.json'\n",
    "candidates = []\n",
    "with open(infoproblem, 'r') as f:\n",
    "    fj = json.load(f)\n",
    "    unk_folder = fj['unknown-folder']\n",
    "    for attrib in fj['candidate-authors']:\n",
    "        candidates.append(attrib['author-name'])\n",
    "\n",
    "candidates.sort()\n",
    "# Building training set\n",
    "train_docs = []\n",
    "for candidate in candidates:\n",
    "    train_docs.extend(read_files(dataset_path + os.sep + problem, candidate))\n",
    "train_texts = [text for i, (text, label) in enumerate(train_docs)]\n",
    "train_labels = [label for i, (text, label) in enumerate(train_docs)]\n",
    "initial_train_size = len(train_labels)\n",
    "train_texts, train_labels = shuffle_docs(train_texts, train_labels)\n",
    "validation_size = len(train_texts) - initial_train_size\n",
    "class_size = int(initial_train_size / len(set(train_labels)))\n",
    "index_2_label_dict = {i: l for i, l in enumerate(set(train_labels))}\n",
    "label_2_index_dict = {l: i for i, l in enumerate(set(train_labels))}\n",
    "train_labels = [label_2_index_dict[v] for v in train_labels]\n",
    "w2d = Word2Dim(lang= language[index])\n",
    "train_tokenized_with_pos, train_tokenized_indexed = w2d.fit_transform_texts(train_texts, train_labels, tf= tf)\n",
    "\n",
    "maxlen = len(max(train_tokenized_indexed, key=len))  # We will cut the texts after # words\n",
    "embedding_dim = w2d.word_embedding.shape[1]\n",
    "\n",
    "# preparing test set\n",
    "ground_truth_file = dataset_path + os.sep + problem + os.sep + 'ground-truth.json'\n",
    "gt = {}\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    for attrib in json.load(f)['ground_truth']:\n",
    "        gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "test_docs = read_files(dataset_path + os.sep + problem, unk_folder, gt)\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs)]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs)]\n",
    "\n",
    "# Filter validation to known authors\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "\n",
    "test_labels = [label_2_index_dict[v] for v in test_labels]\n",
    "\n",
    "test_tokenized_with_pos, test_tokenized_indexed = w2d.transform(test_texts)\n",
    "print(\"Reading problem 1, done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Extraction for Neural Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from MyUtils import extract_n_grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "n = 3\n",
    "vocabulary = extract_n_grams(train_docs, n, tf)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(n, n), lowercase=False, vocabulary=vocabulary)\n",
    "n_gram_train_data = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "n_gram_train_data = n_gram_train_data.astype(float)\n",
    "\n",
    "for i, v in enumerate(train_texts):\n",
    "    n_gram_train_data[i] = n_gram_train_data[i] / len(train_texts[i])\n",
    "n_gram_test_data = vectorizer.transform(test_texts)\n",
    "n_gram_test_data = n_gram_test_data.astype(float)\n",
    "for i, v in enumerate(test_texts):\n",
    "    n_gram_test_data[i] = n_gram_test_data[i] / len(test_texts[i])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_ngrams = max_abs_scaler.fit_transform(n_gram_train_data)\n",
    "scaled_test_data_ngrams = max_abs_scaler.transform(n_gram_test_data)\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_words = max_abs_scaler.fit_transform(w2d.get_texts_vectorized_and_normalized(train_tokenized_indexed)[:, 1:])\n",
    "scaled_test_data_words = max_abs_scaler.transform(w2d.get_texts_vectorized_and_normalized(test_tokenized_indexed)[:, 1:])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(819, 7623)\n",
      "(468, 7623)\n",
      "7623\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train_data_words.shape)\n",
    "print(scaled_test_data_words.shape)\n",
    "print(len(w2d.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 9)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import layers, Input, callbacks\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "callbacks_list_neu = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_neu_ngrams = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu_ngrams.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_neu_words = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu_words.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_convnet = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=35,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_convnet.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=10,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_stacked = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_stacked.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "train_data = pad_sequences(train_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "test_data = pad_sequences(test_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "# X_scaled_train_data_words, X_scaled_val_data_words, _, _ = train_test_split(scaled_train_data_words, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "# X_scaled_train_data_ngrams, X_scaled_val_data_ngrams, _, _ = train_test_split(scaled_train_data_ngrams, train_labels,\n",
    "#                                                   test_size=validation_size,\n",
    "#                                                   stratify=train_labels)\n",
    "\n",
    "y_train, y_val = train_labels[:initial_train_size], train_labels[initial_train_size:]\n",
    "X_train, X_val = train_data[:initial_train_size], train_data[initial_train_size:]\n",
    "X_scaled_train_data_words, X_scaled_val_data_words = scaled_train_data_words[:initial_train_size], scaled_train_data_words[initial_train_size:]\n",
    "X_scaled_train_data_ngrams, X_scaled_val_data_ngrams = scaled_train_data_ngrams[:initial_train_size], scaled_train_data_ngrams[initial_train_size:]\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "# y_test = to_categorical(test_labels)\n",
    "# print(X_train.shape)\n",
    "\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 32)                132896    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 139,753\n",
      "Trainable params: 139,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 2s 32ms/step - loss: 5.5283 - acc: 0.1587 - val_loss: 5.5049 - val_acc: 0.1124\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.4907 - acc: 0.0952 - val_loss: 5.4493 - val_acc: 0.1164\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.4378 - acc: 0.1270 - val_loss: 5.3969 - val_acc: 0.1323\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.3780 - acc: 0.2540 - val_loss: 5.3458 - val_acc: 0.1481\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.3521 - acc: 0.1111 - val_loss: 5.2977 - val_acc: 0.1786\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.2606 - acc: 0.2063 - val_loss: 5.2498 - val_acc: 0.2130\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.2364 - acc: 0.2381 - val_loss: 5.2008 - val_acc: 0.2434\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1918 - acc: 0.1746 - val_loss: 5.1555 - val_acc: 0.2646\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.1239 - acc: 0.2063 - val_loss: 5.1087 - val_acc: 0.2897\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0603 - acc: 0.2381 - val_loss: 5.0559 - val_acc: 0.3360\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0411 - acc: 0.1905 - val_loss: 5.0047 - val_acc: 0.3135\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 5.0208 - acc: 0.2381 - val_loss: 4.9576 - val_acc: 0.3347\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.9967 - acc: 0.1746 - val_loss: 4.9159 - val_acc: 0.3651\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.9244 - acc: 0.2857 - val_loss: 4.8728 - val_acc: 0.4008\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8806 - acc: 0.2857 - val_loss: 4.8253 - val_acc: 0.4590\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8598 - acc: 0.2381 - val_loss: 4.7816 - val_acc: 0.5093\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.8161 - acc: 0.2381 - val_loss: 4.7397 - val_acc: 0.5331\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.7963 - acc: 0.2857 - val_loss: 4.6923 - val_acc: 0.5569\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.7499 - acc: 0.3016 - val_loss: 4.6443 - val_acc: 0.5767\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.7045 - acc: 0.2540 - val_loss: 4.5871 - val_acc: 0.5833\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.6342 - acc: 0.3810 - val_loss: 4.5359 - val_acc: 0.6019\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5930 - acc: 0.2857 - val_loss: 4.4835 - val_acc: 0.6032\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.6013 - acc: 0.3492 - val_loss: 4.4389 - val_acc: 0.6204\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.5039 - acc: 0.3333 - val_loss: 4.3923 - val_acc: 0.6587\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.4145 - acc: 0.4603 - val_loss: 4.3400 - val_acc: 0.6812\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.4260 - acc: 0.3968 - val_loss: 4.2889 - val_acc: 0.6997\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3814 - acc: 0.4444 - val_loss: 4.2390 - val_acc: 0.6944\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3228 - acc: 0.3968 - val_loss: 4.1860 - val_acc: 0.7130\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.3080 - acc: 0.4127 - val_loss: 4.1383 - val_acc: 0.7513\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2267 - acc: 0.4921 - val_loss: 4.0899 - val_acc: 0.7698\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.2248 - acc: 0.4762 - val_loss: 4.0408 - val_acc: 0.7817\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1391 - acc: 0.4762 - val_loss: 3.9861 - val_acc: 0.7844\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1378 - acc: 0.4286 - val_loss: 3.9419 - val_acc: 0.7817\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.1661 - acc: 0.3968 - val_loss: 3.9047 - val_acc: 0.7778\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0377 - acc: 0.5238 - val_loss: 3.8616 - val_acc: 0.7923\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0315 - acc: 0.5238 - val_loss: 3.8097 - val_acc: 0.8307\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 4.0721 - acc: 0.4444 - val_loss: 3.7587 - val_acc: 0.8413\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8467 - acc: 0.6032 - val_loss: 3.7124 - val_acc: 0.8638\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8991 - acc: 0.5397 - val_loss: 3.6632 - val_acc: 0.8823\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7548 - acc: 0.6825 - val_loss: 3.6136 - val_acc: 0.8876\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.8599 - acc: 0.5873 - val_loss: 3.5708 - val_acc: 0.9272\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7089 - acc: 0.5873 - val_loss: 3.5246 - val_acc: 0.9339\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7608 - acc: 0.5556 - val_loss: 3.4761 - val_acc: 0.9788\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6242 - acc: 0.6190 - val_loss: 3.4261 - val_acc: 0.9788\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.7442 - acc: 0.6190 - val_loss: 3.3861 - val_acc: 0.9669\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5890 - acc: 0.6667 - val_loss: 3.3495 - val_acc: 0.9894\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6476 - acc: 0.6667 - val_loss: 3.3088 - val_acc: 0.9974\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.6186 - acc: 0.6032 - val_loss: 3.2670 - val_acc: 0.9974\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5607 - acc: 0.6984 - val_loss: 3.2230 - val_acc: 0.9947\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4575 - acc: 0.6984 - val_loss: 3.1798 - val_acc: 0.9947\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.4683 - acc: 0.6667 - val_loss: 3.1428 - val_acc: 0.9987\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.5125 - acc: 0.6667 - val_loss: 3.0976 - val_acc: 0.9987\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3165 - acc: 0.7460 - val_loss: 3.0625 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.3918 - acc: 0.6825 - val_loss: 3.0288 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2633 - acc: 0.7778 - val_loss: 2.9872 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2341 - acc: 0.6508 - val_loss: 2.9520 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2692 - acc: 0.6825 - val_loss: 2.9223 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2363 - acc: 0.7619 - val_loss: 2.8839 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.2307 - acc: 0.7778 - val_loss: 2.8467 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1889 - acc: 0.7143 - val_loss: 2.8139 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1809 - acc: 0.7302 - val_loss: 2.7838 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1786 - acc: 0.6984 - val_loss: 2.7575 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9633 - acc: 0.8571 - val_loss: 2.7231 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0802 - acc: 0.7619 - val_loss: 2.6907 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.1158 - acc: 0.7460 - val_loss: 2.6655 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9208 - acc: 0.8413 - val_loss: 2.6334 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9405 - acc: 0.7778 - val_loss: 2.5990 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 3.0043 - acc: 0.7937 - val_loss: 2.5763 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9822 - acc: 0.7937 - val_loss: 2.5525 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9081 - acc: 0.7937 - val_loss: 2.5262 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9406 - acc: 0.8095 - val_loss: 2.4973 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7966 - acc: 0.8254 - val_loss: 2.4717 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7639 - acc: 0.8095 - val_loss: 2.4458 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8667 - acc: 0.7619 - val_loss: 2.4259 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7997 - acc: 0.7937 - val_loss: 2.4088 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7916 - acc: 0.8254 - val_loss: 2.3890 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9465 - acc: 0.6984 - val_loss: 2.3710 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8646 - acc: 0.7460 - val_loss: 2.3533 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8667 - acc: 0.7302 - val_loss: 2.3406 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6992 - acc: 0.7778 - val_loss: 2.3279 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6905 - acc: 0.8571 - val_loss: 2.3058 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6780 - acc: 0.8413 - val_loss: 2.2866 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7592 - acc: 0.7937 - val_loss: 2.2686 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6155 - acc: 0.8889 - val_loss: 2.2498 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6704 - acc: 0.8730 - val_loss: 2.2244 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5970 - acc: 0.8413 - val_loss: 2.2035 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5930 - acc: 0.8254 - val_loss: 2.1885 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5426 - acc: 0.8730 - val_loss: 2.1741 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6192 - acc: 0.8254 - val_loss: 2.1648 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5284 - acc: 0.9206 - val_loss: 2.1504 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5045 - acc: 0.8889 - val_loss: 2.1328 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5502 - acc: 0.8254 - val_loss: 2.1142 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5639 - acc: 0.8095 - val_loss: 2.0996 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5967 - acc: 0.7619 - val_loss: 2.0867 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4340 - acc: 0.8571 - val_loss: 2.0758 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5687 - acc: 0.8095 - val_loss: 2.0629 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5307 - acc: 0.8095 - val_loss: 2.0498 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4719 - acc: 0.8571 - val_loss: 2.0349 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3206 - acc: 0.9048 - val_loss: 2.0206 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3603 - acc: 0.9048 - val_loss: 2.0077 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3746 - acc: 0.8889 - val_loss: 1.9970 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.4672 - acc: 0.8571 - val_loss: 1.9849 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3421 - acc: 0.9048 - val_loss: 1.9741 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2834 - acc: 0.9048 - val_loss: 1.9618 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2604 - acc: 0.9206 - val_loss: 1.9487 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2726 - acc: 0.8571 - val_loss: 1.9351 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3200 - acc: 0.9048 - val_loss: 1.9250 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3136 - acc: 0.9048 - val_loss: 1.9174 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3193 - acc: 0.9048 - val_loss: 1.9076 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2024 - acc: 0.9524 - val_loss: 1.8958 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1795 - acc: 0.9048 - val_loss: 1.8837 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2016 - acc: 0.9048 - val_loss: 1.8732 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2747 - acc: 0.8730 - val_loss: 1.8633 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1531 - acc: 0.9048 - val_loss: 1.8550 - val_acc: 1.0000\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1666 - acc: 0.9683 - val_loss: 1.8463 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1569 - acc: 0.9365 - val_loss: 1.8381 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1936 - acc: 0.9206 - val_loss: 1.8303 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2640 - acc: 0.8571 - val_loss: 1.8226 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.2913 - acc: 0.8413 - val_loss: 1.8148 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0583 - acc: 0.9365 - val_loss: 1.8064 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1622 - acc: 0.9048 - val_loss: 1.7976 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1689 - acc: 0.8730 - val_loss: 1.7884 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0357 - acc: 0.9524 - val_loss: 1.7798 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1655 - acc: 0.8254 - val_loss: 1.7735 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0831 - acc: 0.9524 - val_loss: 1.7660 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0781 - acc: 0.9206 - val_loss: 1.7550 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1360 - acc: 0.8889 - val_loss: 1.7453 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1816 - acc: 0.8571 - val_loss: 1.7405 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0217 - acc: 0.9683 - val_loss: 1.7333 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1238 - acc: 0.9048 - val_loss: 1.7237 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0426 - acc: 0.9683 - val_loss: 1.7156 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.1483 - acc: 0.8413 - val_loss: 1.7083 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0311 - acc: 0.9048 - val_loss: 1.6996 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0012 - acc: 0.8730 - val_loss: 1.6924 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9453 - acc: 0.9365 - val_loss: 1.6847 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0198 - acc: 0.8889 - val_loss: 1.6781 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0167 - acc: 0.8571 - val_loss: 1.6703 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0200 - acc: 0.8889 - val_loss: 1.6623 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0400 - acc: 0.8889 - val_loss: 1.6568 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9190 - acc: 0.9524 - val_loss: 1.6481 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0348 - acc: 0.9048 - val_loss: 1.6412 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9194 - acc: 0.9048 - val_loss: 1.6354 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9494 - acc: 0.9365 - val_loss: 1.6297 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0325 - acc: 0.8730 - val_loss: 1.6240 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0218 - acc: 0.8730 - val_loss: 1.6193 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8499 - acc: 0.9365 - val_loss: 1.6138 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9278 - acc: 0.9048 - val_loss: 1.6071 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0064 - acc: 0.8730 - val_loss: 1.6013 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8571 - acc: 0.9841 - val_loss: 1.5953 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8068 - acc: 0.9524 - val_loss: 1.5864 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9540 - acc: 0.8730 - val_loss: 1.5794 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8467 - acc: 0.9524 - val_loss: 1.5747 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8488 - acc: 0.9524 - val_loss: 1.5669 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8592 - acc: 0.9206 - val_loss: 1.5600 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8069 - acc: 0.9524 - val_loss: 1.5545 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8211 - acc: 0.9048 - val_loss: 1.5493 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7633 - acc: 0.9683 - val_loss: 1.5423 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8195 - acc: 0.9048 - val_loss: 1.5335 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8678 - acc: 0.9524 - val_loss: 1.5285 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7838 - acc: 0.9524 - val_loss: 1.5237 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7943 - acc: 0.9365 - val_loss: 1.5178 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7611 - acc: 0.9683 - val_loss: 1.5138 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7357 - acc: 0.9365 - val_loss: 1.5083 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7508 - acc: 0.9048 - val_loss: 1.5014 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6901 - acc: 0.9683 - val_loss: 1.4941 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7004 - acc: 0.9524 - val_loss: 1.4866 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7494 - acc: 0.9365 - val_loss: 1.4806 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.8011 - acc: 0.9365 - val_loss: 1.4776 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7249 - acc: 0.9683 - val_loss: 1.4740 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7633 - acc: 0.8889 - val_loss: 1.4693 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6478 - acc: 0.9841 - val_loss: 1.4624 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7255 - acc: 0.9365 - val_loss: 1.4570 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7285 - acc: 0.9365 - val_loss: 1.4522 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7907 - acc: 0.9048 - val_loss: 1.4483 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6744 - acc: 0.9365 - val_loss: 1.4439 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6927 - acc: 0.9683 - val_loss: 1.4375 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6320 - acc: 0.9683 - val_loss: 1.4319 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7263 - acc: 0.9206 - val_loss: 1.4273 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6893 - acc: 0.9048 - val_loss: 1.4243 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7393 - acc: 0.9206 - val_loss: 1.4207 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6737 - acc: 0.9206 - val_loss: 1.4164 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7908 - acc: 0.7937 - val_loss: 1.4136 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6674 - acc: 0.9365 - val_loss: 1.4113 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6734 - acc: 0.9206 - val_loss: 1.4065 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6096 - acc: 0.9524 - val_loss: 1.3998 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7205 - acc: 0.9206 - val_loss: 1.3932 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6301 - acc: 0.9524 - val_loss: 1.3881 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7078 - acc: 0.9048 - val_loss: 1.3855 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6798 - acc: 0.9048 - val_loss: 1.3843 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6475 - acc: 0.9206 - val_loss: 1.3796 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7148 - acc: 0.9365 - val_loss: 1.3770 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5994 - acc: 0.9365 - val_loss: 1.3718 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6629 - acc: 0.9048 - val_loss: 1.3649 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5693 - acc: 0.9524 - val_loss: 1.3581 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6217 - acc: 0.9524 - val_loss: 1.3530 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6486 - acc: 0.9365 - val_loss: 1.3502 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6266 - acc: 0.9365 - val_loss: 1.3490 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5836 - acc: 0.9206 - val_loss: 1.3453 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5558 - acc: 0.9365 - val_loss: 1.3410 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6000 - acc: 0.9524 - val_loss: 1.3367 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6007 - acc: 0.9206 - val_loss: 1.3337 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5568 - acc: 0.9683 - val_loss: 1.3280 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6506 - acc: 0.8889 - val_loss: 1.3257 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.6170 - acc: 0.9048 - val_loss: 1.3242 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5348 - acc: 0.9524 - val_loss: 1.3186 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5675 - acc: 0.9206 - val_loss: 1.3140 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5266 - acc: 0.9524 - val_loss: 1.3106 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5211 - acc: 0.9524 - val_loss: 1.3053 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4706 - acc: 0.9683 - val_loss: 1.2995 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5937 - acc: 0.9206 - val_loss: 1.2949 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5031 - acc: 0.9683 - val_loss: 1.2923 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4897 - acc: 0.9524 - val_loss: 1.2875 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4845 - acc: 0.9683 - val_loss: 1.2834 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5059 - acc: 0.9365 - val_loss: 1.2796 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5983 - acc: 0.9365 - val_loss: 1.2774 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4746 - acc: 0.9524 - val_loss: 1.2753 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4793 - acc: 0.9524 - val_loss: 1.2709 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5892 - acc: 0.8889 - val_loss: 1.2675 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4961 - acc: 0.9683 - val_loss: 1.2650 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5095 - acc: 0.9048 - val_loss: 1.2609 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4743 - acc: 0.9524 - val_loss: 1.2579 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4332 - acc: 0.9841 - val_loss: 1.2536 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5051 - acc: 0.9365 - val_loss: 1.2484 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4931 - acc: 0.9524 - val_loss: 1.2474 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4718 - acc: 0.9841 - val_loss: 1.2458 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4627 - acc: 0.9048 - val_loss: 1.2411 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5308 - acc: 0.8730 - val_loss: 1.2393 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4339 - acc: 0.9841 - val_loss: 1.2373 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4938 - acc: 0.9365 - val_loss: 1.2334 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4864 - acc: 0.9365 - val_loss: 1.2314 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4710 - acc: 0.9524 - val_loss: 1.2279 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4220 - acc: 0.9683 - val_loss: 1.2228 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4439 - acc: 0.9206 - val_loss: 1.2210 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3924 - acc: 0.9683 - val_loss: 1.2176 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4628 - acc: 0.9048 - val_loss: 1.2146 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4254 - acc: 0.9683 - val_loss: 1.2102 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4242 - acc: 0.9841 - val_loss: 1.2057 - val_acc: 1.0000\n",
      "Epoch 238/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4235 - acc: 0.9365 - val_loss: 1.2026 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4656 - acc: 0.9365 - val_loss: 1.2010 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3905 - acc: 0.9841 - val_loss: 1.2008 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4902 - acc: 0.9048 - val_loss: 1.1985 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4776 - acc: 0.9365 - val_loss: 1.1974 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4440 - acc: 0.9841 - val_loss: 1.1949 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4221 - acc: 0.9524 - val_loss: 1.1901 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4665 - acc: 0.9365 - val_loss: 1.1860 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3948 - acc: 0.9365 - val_loss: 1.1858 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3996 - acc: 0.9683 - val_loss: 1.1828 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3760 - acc: 0.9524 - val_loss: 1.1827 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3584 - acc: 0.9841 - val_loss: 1.1790 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3823 - acc: 0.9524 - val_loss: 1.1750 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3858 - acc: 0.9683 - val_loss: 1.1728 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4181 - acc: 0.9206 - val_loss: 1.1715 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3276 - acc: 0.9841 - val_loss: 1.1676 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3510 - acc: 0.9841 - val_loss: 1.1631 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3657 - acc: 0.9841 - val_loss: 1.1599 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4467 - acc: 0.9048 - val_loss: 1.1583 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3893 - acc: 0.9524 - val_loss: 1.1596 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3256 - acc: 0.9524 - val_loss: 1.1568 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4391 - acc: 0.9206 - val_loss: 1.1554 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3966 - acc: 0.9206 - val_loss: 1.1533 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3593 - acc: 0.9365 - val_loss: 1.1497 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3361 - acc: 0.9524 - val_loss: 1.1471 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3769 - acc: 0.9524 - val_loss: 1.1459 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4141 - acc: 0.9524 - val_loss: 1.1454 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3871 - acc: 0.9206 - val_loss: 1.1438 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3091 - acc: 0.9683 - val_loss: 1.1410 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3622 - acc: 0.9365 - val_loss: 1.1376 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2932 - acc: 0.9683 - val_loss: 1.1335 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3269 - acc: 0.9841 - val_loss: 1.1287 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3957 - acc: 0.9206 - val_loss: 1.1277 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2734 - acc: 0.9683 - val_loss: 1.1254 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3065 - acc: 1.0000 - val_loss: 1.1224 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2566 - acc: 1.0000 - val_loss: 1.1197 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2591 - acc: 0.9683 - val_loss: 1.1164 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3378 - acc: 0.9365 - val_loss: 1.1148 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2614 - acc: 0.9841 - val_loss: 1.1126 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2736 - acc: 0.9524 - val_loss: 1.1090 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2099 - acc: 1.0000 - val_loss: 1.1062 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3455 - acc: 0.9683 - val_loss: 1.1047 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2912 - acc: 0.9683 - val_loss: 1.1034 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2915 - acc: 0.9524 - val_loss: 1.1012 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3116 - acc: 0.9206 - val_loss: 1.1021 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3018 - acc: 1.0000 - val_loss: 1.1030 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2613 - acc: 0.9841 - val_loss: 1.1000 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2713 - acc: 0.9841 - val_loss: 1.0962 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2056 - acc: 0.9841 - val_loss: 1.0924 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3332 - acc: 0.9524 - val_loss: 1.0910 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2792 - acc: 0.9683 - val_loss: 1.0903 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2190 - acc: 0.9683 - val_loss: 1.0874 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2394 - acc: 0.9683 - val_loss: 1.0846 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2849 - acc: 0.9365 - val_loss: 1.0833 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3054 - acc: 0.9683 - val_loss: 1.0822 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3456 - acc: 0.9206 - val_loss: 1.0818 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4152 - acc: 0.9206 - val_loss: 1.0841 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2163 - acc: 1.0000 - val_loss: 1.0819 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2247 - acc: 1.0000 - val_loss: 1.0772 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3240 - acc: 0.9524 - val_loss: 1.0746 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2545 - acc: 0.9841 - val_loss: 1.0725 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1939 - acc: 0.9841 - val_loss: 1.0688 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2699 - acc: 0.9365 - val_loss: 1.0671 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2610 - acc: 0.9683 - val_loss: 1.0657 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2884 - acc: 0.9683 - val_loss: 1.0649 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2472 - acc: 0.9683 - val_loss: 1.0624 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1877 - acc: 0.9683 - val_loss: 1.0590 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1555 - acc: 1.0000 - val_loss: 1.0560 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2578 - acc: 0.9524 - val_loss: 1.0552 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2873 - acc: 0.8889 - val_loss: 1.0543 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2361 - acc: 0.9365 - val_loss: 1.0556 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2279 - acc: 0.9524 - val_loss: 1.0519 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1811 - acc: 0.9841 - val_loss: 1.0490 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2445 - acc: 0.9683 - val_loss: 1.0463 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2016 - acc: 0.9683 - val_loss: 1.0452 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2467 - acc: 0.9524 - val_loss: 1.0459 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3785 - acc: 0.9206 - val_loss: 1.0458 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2063 - acc: 0.9683 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2317 - acc: 0.9683 - val_loss: 1.0414 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1957 - acc: 0.9841 - val_loss: 1.0394 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2152 - acc: 0.9524 - val_loss: 1.0369 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1770 - acc: 0.9683 - val_loss: 1.0344 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1650 - acc: 0.9841 - val_loss: 1.0319 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2051 - acc: 0.9683 - val_loss: 1.0302 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2270 - acc: 0.9683 - val_loss: 1.0311 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1964 - acc: 0.9841 - val_loss: 1.0311 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2545 - acc: 0.9683 - val_loss: 1.0315 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2843 - acc: 0.9206 - val_loss: 1.0337 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2051 - acc: 0.9841 - val_loss: 1.0331 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2314 - acc: 0.9841 - val_loss: 1.0293 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1984 - acc: 0.9683 - val_loss: 1.0262 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2312 - acc: 0.9206 - val_loss: 1.0257 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1638 - acc: 0.9683 - val_loss: 1.0236 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1976 - acc: 0.9683 - val_loss: 1.0225 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1906 - acc: 0.9683 - val_loss: 1.0210 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2097 - acc: 0.9524 - val_loss: 1.0196 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2047 - acc: 0.9841 - val_loss: 1.0190 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1693 - acc: 0.9841 - val_loss: 1.0174 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2585 - acc: 0.9524 - val_loss: 1.0149 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2492 - acc: 0.9206 - val_loss: 1.0151 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2616 - acc: 0.9524 - val_loss: 1.0154 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2542 - acc: 0.9524 - val_loss: 1.0154 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1636 - acc: 0.9683 - val_loss: 1.0143 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2156 - acc: 0.9524 - val_loss: 1.0126 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1758 - acc: 0.9524 - val_loss: 1.0113 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1423 - acc: 0.9841 - val_loss: 1.0069 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1570 - acc: 0.9841 - val_loss: 1.0031 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1847 - acc: 0.9524 - val_loss: 1.0014 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2006 - acc: 0.9841 - val_loss: 1.0013 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2402 - acc: 0.9206 - val_loss: 1.0024 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2063 - acc: 0.9683 - val_loss: 1.0042 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1559 - acc: 0.9841 - val_loss: 1.0027 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1791 - acc: 0.9524 - val_loss: 0.9991 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1302 - acc: 0.9841 - val_loss: 0.9960 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1481 - acc: 0.9683 - val_loss: 0.9931 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2040 - acc: 0.9365 - val_loss: 0.9936 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1987 - acc: 0.9365 - val_loss: 0.9928 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1375 - acc: 0.9841 - val_loss: 0.9908 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1908 - acc: 0.9683 - val_loss: 0.9873 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1973 - acc: 0.9206 - val_loss: 0.9878 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0994 - acc: 0.9683 - val_loss: 0.9901 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1057 - acc: 0.9841 - val_loss: 0.9872 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1354 - acc: 0.9841 - val_loss: 0.9839 - val_acc: 1.0000\n",
      "Epoch 361/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2122 - acc: 0.9683 - val_loss: 0.9858 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1571 - acc: 0.9683 - val_loss: 0.9835 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1990 - acc: 0.9683 - val_loss: 0.9822 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1135 - acc: 0.9524 - val_loss: 0.9847 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1649 - acc: 0.9683 - val_loss: 0.9821 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1638 - acc: 0.9524 - val_loss: 0.9812 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2052 - acc: 0.9524 - val_loss: 0.9810 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1356 - acc: 0.9841 - val_loss: 0.9780 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1211 - acc: 0.9841 - val_loss: 0.9746 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1250 - acc: 0.9841 - val_loss: 0.9719 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1352 - acc: 0.9683 - val_loss: 0.9710 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1627 - acc: 0.9683 - val_loss: 0.9708 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1362 - acc: 0.9683 - val_loss: 0.9711 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1193 - acc: 0.9683 - val_loss: 0.9697 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1498 - acc: 0.9683 - val_loss: 0.9701 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0949 - acc: 0.9841 - val_loss: 0.9692 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2153 - acc: 0.9524 - val_loss: 0.9690 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1442 - acc: 0.9683 - val_loss: 0.9689 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0559 - acc: 1.0000 - val_loss: 0.9663 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1811 - acc: 0.9524 - val_loss: 0.9633 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2290 - acc: 0.9365 - val_loss: 0.9659 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0600 - acc: 1.0000 - val_loss: 0.9659 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1766 - acc: 1.0000 - val_loss: 0.9633 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1202 - acc: 0.9683 - val_loss: 0.9614 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0860 - acc: 0.9841 - val_loss: 0.9598 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1011 - acc: 0.9683 - val_loss: 0.9579 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1367 - acc: 0.9683 - val_loss: 0.9565 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0994 - acc: 0.9841 - val_loss: 0.9543 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1378 - acc: 0.9683 - val_loss: 0.9527 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0705 - acc: 0.9841 - val_loss: 0.9511 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1424 - acc: 0.9206 - val_loss: 0.9512 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0829 - acc: 0.9683 - val_loss: 0.9509 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1759 - acc: 0.9365 - val_loss: 0.9509 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1119 - acc: 0.9683 - val_loss: 0.9520 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2292 - acc: 0.9365 - val_loss: 0.9519 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1240 - acc: 0.9524 - val_loss: 0.9519 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1417 - acc: 0.9524 - val_loss: 0.9495 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0924 - acc: 0.9524 - val_loss: 0.9456 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1657 - acc: 0.9683 - val_loss: 0.9440 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0640 - acc: 0.9841 - val_loss: 0.9439 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0467 - acc: 0.9841 - val_loss: 0.9408 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0511 - acc: 0.9683 - val_loss: 0.9391 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0576 - acc: 1.0000 - val_loss: 0.9369 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1293 - acc: 0.9683 - val_loss: 0.9348 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1234 - acc: 0.9524 - val_loss: 0.9363 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0829 - acc: 0.9683 - val_loss: 0.9356 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1060 - acc: 0.9524 - val_loss: 0.9361 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1113 - acc: 0.9841 - val_loss: 0.9374 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1369 - acc: 0.9683 - val_loss: 0.9365 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0827 - acc: 1.0000 - val_loss: 0.9343 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0569 - acc: 0.9841 - val_loss: 0.9309 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1082 - acc: 0.9841 - val_loss: 0.9308 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0559 - acc: 0.9841 - val_loss: 0.9307 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0827 - acc: 0.9365 - val_loss: 0.9292 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0450 - acc: 0.9841 - val_loss: 0.9279 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0841 - acc: 0.9365 - val_loss: 0.9261 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0869 - acc: 0.9524 - val_loss: 0.9240 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0373 - acc: 0.9683 - val_loss: 0.9236 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0407 - acc: 1.0000 - val_loss: 0.9210 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0972 - acc: 0.9683 - val_loss: 0.9213 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2139 - acc: 0.8889 - val_loss: 0.9267 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0773 - acc: 0.9524 - val_loss: 0.9270 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0031 - acc: 1.0000 - val_loss: 0.9242 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0489 - acc: 0.9841 - val_loss: 0.9199 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0456 - acc: 0.9841 - val_loss: 0.9170 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0847 - acc: 0.9524 - val_loss: 0.9160 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0350 - acc: 0.9683 - val_loss: 0.9141 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0602 - acc: 0.9524 - val_loss: 0.9133 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0836 - acc: 0.9365 - val_loss: 0.9144 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0154 - acc: 0.9841 - val_loss: 0.9156 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0985 - acc: 0.9206 - val_loss: 0.9133 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1080 - acc: 0.9365 - val_loss: 0.9162 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0578 - acc: 0.9683 - val_loss: 0.9143 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1141 - acc: 0.9683 - val_loss: 0.9131 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1266 - acc: 0.9206 - val_loss: 0.9122 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0397 - acc: 0.9841 - val_loss: 0.9115 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1454 - acc: 0.9365 - val_loss: 0.9123 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0089 - acc: 0.9841 - val_loss: 0.9118 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0469 - acc: 0.9683 - val_loss: 0.9094 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0717 - acc: 0.9841 - val_loss: 0.9074 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0716 - acc: 0.9683 - val_loss: 0.9055 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9832 - acc: 1.0000 - val_loss: 0.9041 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0948 - acc: 0.9683 - val_loss: 0.9019 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1378 - acc: 0.9524 - val_loss: 0.9041 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0435 - acc: 1.0000 - val_loss: 0.9037 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0537 - acc: 0.9683 - val_loss: 0.9000 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9912 - acc: 0.9841 - val_loss: 0.8986 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0654 - acc: 0.9524 - val_loss: 0.8979 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0193 - acc: 0.9683 - val_loss: 0.9007 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0485 - acc: 0.9841 - val_loss: 0.9006 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0859 - acc: 0.9206 - val_loss: 0.9000 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1233 - acc: 0.9365 - val_loss: 0.9001 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0695 - acc: 0.9524 - val_loss: 0.9002 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0397 - acc: 0.9841 - val_loss: 0.8995 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0180 - acc: 1.0000 - val_loss: 0.8971 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1297 - acc: 0.9365 - val_loss: 0.8950 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0790 - acc: 0.9683 - val_loss: 0.9009 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0494 - acc: 0.9524 - val_loss: 0.9016 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0351 - acc: 0.9841 - val_loss: 0.8994 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1440 - acc: 0.9365 - val_loss: 0.8959 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0538 - acc: 0.9683 - val_loss: 0.8952 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0544 - acc: 0.9683 - val_loss: 0.8936 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0264 - acc: 0.9841 - val_loss: 0.8928 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0777 - acc: 0.9683 - val_loss: 0.8927 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9594 - acc: 0.9841 - val_loss: 0.8901 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0650 - acc: 0.9524 - val_loss: 0.8872 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1019 - acc: 0.9206 - val_loss: 0.8884 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9989 - acc: 0.9841 - val_loss: 0.8900 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0796 - acc: 0.9683 - val_loss: 0.8897 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0586 - acc: 0.9841 - val_loss: 0.8907 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0369 - acc: 0.9683 - val_loss: 0.8904 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9848 - acc: 0.9683 - val_loss: 0.8880 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9598 - acc: 1.0000 - val_loss: 0.8842 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0019 - acc: 0.9683 - val_loss: 0.8790 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0114 - acc: 0.9841 - val_loss: 0.8773 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0122 - acc: 0.9841 - val_loss: 0.8783 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9874 - acc: 1.0000 - val_loss: 0.8766 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0222 - acc: 0.9841 - val_loss: 0.8741 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1115 - acc: 0.9206 - val_loss: 0.8762 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0544 - acc: 0.9683 - val_loss: 0.8825 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0252 - acc: 0.9683 - val_loss: 0.8818 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9732 - acc: 1.0000 - val_loss: 0.8787 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0748 - acc: 0.9524 - val_loss: 0.8773 - val_acc: 1.0000\n",
      "Epoch 484/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0162 - acc: 1.0000 - val_loss: 0.8763 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9957 - acc: 0.9841 - val_loss: 0.8757 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9805 - acc: 0.9841 - val_loss: 0.8743 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0176 - acc: 0.9841 - val_loss: 0.8724 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1053 - acc: 0.9683 - val_loss: 0.8715 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9994 - acc: 1.0000 - val_loss: 0.8729 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9866 - acc: 0.9841 - val_loss: 0.8712 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9555 - acc: 0.9841 - val_loss: 0.8687 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9379 - acc: 0.9841 - val_loss: 0.8654 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9575 - acc: 1.0000 - val_loss: 0.8617 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9635 - acc: 0.9841 - val_loss: 0.8601 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0507 - acc: 0.9524 - val_loss: 0.8616 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9975 - acc: 0.9841 - val_loss: 0.8609 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9609 - acc: 0.9841 - val_loss: 0.8602 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0097 - acc: 0.9683 - val_loss: 0.8605 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0443 - acc: 0.9524 - val_loss: 0.8634 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0156 - acc: 0.9524 - val_loss: 0.8643 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xd4VGX2wPHvIaETAoQqiIBYgBAgZuldZJGqiAXErqirK4oNXQuyuusqNtS1rv5UUCyIIIIIgiKKVJEqYAEMvYXekry/P85MZhISMkkmmczkfJ5nnrlz7517z53AmXfe+xZxzmGMMSZ8lAp1AMYYY/LGErcxxoQZS9zGGBNmLHEbY0yYscRtjDFhxhK3McaEGUvcJZCIRInIQRGpH8x9Q0lEGotI0Nu2ikgPEdng93qtiHQKZN98nOtNEXkwv+8/xXEfF5H/C/ZxTehEhzoAkzsROej3sgJwDEjzvL7ZOTc+L8dzzqUBlYK9b0ngnDsnGMcRkRuBoc65rn7HvjEYxzaRzxJ3GHDOZSROT4nuRufcrJz2F5Fo51xqUcRmjCl6VlUSATw/hT8UkQ9E5AAwVETaiciPIpIiIltFZKyIlPbsHy0iTkQaeF6P82yfLiIHRGS+iDTM676e7ReKyDoR2SciL4rI9yJybQ5xBxLjzSLyq4jsFZGxfu+NEpHnRGS3iPwG9DrF5/OQiEzIsu5lEXnWs3yjiKzxXM9vntJwTsdKFpGunuUKIvKeJ7ZVwHnZnPd3z3FXiUh/z/rmwEtAJ0811C6/z3aU3/tv8Vz7bhH5TETqBPLZ5EZELvLEkyIis0XkHL9tD4rIFhHZLyK/+F1rWxFZ6lm/XUSeDvR8phA45+wRRg9gA9Ajy7rHgeNAP/TLuDzwF6AN+quqEbAOuN2zfzTggAae1+OAXUASUBr4EBiXj31rAgeAAZ5tI4ATwLU5XEsgMU4GYoEGwB7vtQO3A6uAekAcMFf/OWd7nkbAQaCi37F3AEme1/08+wjQHTgCJHi29QA2+B0rGejqWR4DfANUBc4AVmfZ9zKgjudvMsQTQy3PthuBb7LEOQ4Y5Vnu6YmxJVAO+C8wO5DPJpvrfxz4P89yE08c3T1/owc9n3tpoBmwEajt2bch0MizvAgY7FmOAdqE+v9CSX5YiTtyzHPOfe6cS3fOHXHOLXLOLXDOpTrnfgdeB7qc4v2fOOcWO+dOAOPRhJHXffsCy5xzkz3bnkOTfLYCjPHfzrl9zrkNaJL0nusy4DnnXLJzbjfw5CnO8zuwEv1CAbgASHHOLfZs/9w597tTs4GvgWxvQGZxGfC4c26vc24jWor2P+9Hzrmtnr/J++iXblIAxwW4EnjTObfMOXcUGAl0EZF6fvvk9NmcyhXAFOfcbM/f6EmgMvoFmop+STTzVLf94fnsQL+AzxKROOfcAefcggCvwxQCS9yR40//FyJyroh8ISLbRGQ/MBqofor3b/NbPsypb0jmtO9p/nE45xxaQs1WgDEGdC60pHgq7wODPctD0C8cbxx9RWSBiOwRkRS0tHuqz8qrzqliEJFrReRnT5VECnBugMcFvb6M4znn9gN7gbp+++Tlb5bTcdPRv1Fd59xa4G7077DDU/VW27PrdUBTYK2ILBSR3gFehykElrgjR9amcK+hpczGzrnKwCNoVUBh2opWXQAgIkLmRJNVQWLcCpzu9zq35oofAj08JdYBaCJHRMoDnwD/RqsxqgBfBRjHtpxiEJFGwCvArUCc57i/+B03t6aLW9DqF+/xYtAqmc0BxJWX45ZC/2abAZxz45xzHdBqkij0c8E5t9Y5dwVaHfYMMFFEyhUwFpNPlrgjVwywDzgkIk2Am4vgnFOBRBHpJyLRwHCgRiHF+BFwp4jUFZE44P5T7eyc2w7MA94G1jrn1ns2lQXKADuBNBHpC5yfhxgeFJEqou3cb/fbVglNzjvR77Ab0RK313agnvdmbDY+AG4QkQQRKYsm0O+cczn+gslDzP1FpKvn3Pei9yUWiEgTEenmOd8RzyMNvYCrRKS6p4S+z3Nt6QWMxeSTJe7IdTdwDfqf8jW0xFmoPMnxcuBZYDdwJvAT2u482DG+gtZFr0BvnH0SwHveR282vu8XcwpwFzAJvcE3CP0CCsSjaMl/AzAdeNfvuMuBscBCzz7nAv71wjOB9cB2EfGv8vC+/0u0ymKS5/310XrvAnHOrUI/81fQL5VeQH9PfXdZ4Cn0vsQ2tIT/kOetvYE1oq2WxgCXO+eOFzQekz+i1ZDGBJ+IRKE/zQc5574LdTzGRAorcZugEpFeIhLr+bn9MNpSYWGIwzImoljiNsHWEfgd/bndC7jIOZdTVYkxJh+sqsQYY8KMlbiNMSbMFMogU9WrV3cNGjQojEMbY0xEWrJkyS7n3Kmaz2YolMTdoEEDFi9eXBiHNsaYiCQiufX+zWBVJcYYE2YscRtjTJixxG2MMWHGZsAxJgKcOHGC5ORkjh49GupQTC7KlStHvXr1KF06p2FqcmeJ25gIkJycTExMDA0aNEAHZTTFkXOO3bt3k5ycTMOGDXN/Qw6sqsSYCHD06FHi4uIsaRdzIkJcXFyBfxlZ4jYmQljSDg/B+DsVn8R95Ag88wzMnh3qSIwxplgrNon7aFpp/jPqCDP/8U2oQzHG5MHu3btp2bIlLVu2pHbt2tStWzfj9fHjgQ3Zfd1117F27dpT7vPyyy8zfvz4U+4TqI4dO7Js2bKgHCsUis3NyTIVonkm/U56LpjMBTt2QM2aoQ7JGBOAuLi4jCQ4atQoKlWqxD333JNpn4zZyUtlX1Z8++23cz3PbbfdVvBgI0SxKXGXKgUXdDrGTNeD9FlWXWJMuPv111+Jj4/nlltuITExka1btzJs2DCSkpJo1qwZo0ePztjXWwJOTU2lSpUqjBw5khYtWtCuXTt27NgBwEMPPcTzzz+fsf/IkSNp3bo155xzDj/88AMAhw4d4pJLLqFFixYMHjyYpKSkXEvW48aNo3nz5sTHx/Pggw8CkJqaylVXXZWxfuzYsQA899xzNG3alBYtWjB06NCgf2aBKjYlboCel1fl/RmlWDFxHS2GhDoaY8LUnXdCsKsBWrYET9LMi9WrV/P222/z6quvAvDkk09SrVo1UlNT6datG4MGDaJp06aZ3rNv3z66dOnCk08+yYgRI3jrrbcYOXLkScd2zrFw4UKmTJnC6NGj+fLLL3nxxRepXbs2EydO5OeffyYxMfGU8SUnJ/PQQw+xePFiYmNj6dGjB1OnTqVGjRrs2rWLFStWAJCSkgLAU089xcaNGylTpkzGulAoNiVugB49NZyvvi0b4kiMMcFw5pln8pe//CXj9QcffEBiYiKJiYmsWbOG1atXn/Se8uXLc+GFFwJw3nnnsWHDhmyPPXDgwJP2mTdvHldccQUALVq0oFmzZqeMb8GCBXTv3p3q1atTunRphgwZwty5c2ncuDFr165l+PDhzJgxg9jYWACaNWvG0KFDGT9+fIE60BRUsSpx160LzWrvYua2Vtz7559w+umhDsmY8JOPknFhqVixYsby+vXreeGFF1i4cCFVqlRh6NCh2bZnLlOmTMZyVFQUqamp2R67bNmyJ+2T14lhcto/Li6O5cuXM336dMaOHcvEiRN5/fXXmTFjBt9++y2TJ0/m8ccfZ+XKlURFReXpnMFQrErcABf0cMylM0c++CzUoRhjgmj//v3ExMRQuXJltm7dyowZM4J+jo4dO/LRRx8BsGLFimxL9P7atm3LnDlz2L17N6mpqUyYMIEuXbqwc+dOnHNceumlPPbYYyxdupS0tDSSk5Pp3r07Tz/9NDt37uTw4cNBv4ZAFKsSN0DPwdV5fpzw3b++o+c9t+ldS2NM2EtMTKRp06bEx8fTqFEjOnToEPRz/P3vf+fqq68mISGBxMRE4uPjM6o5slOvXj1Gjx5N165dcc7Rr18/+vTpw9KlS7nhhhtwziEi/Oc//yE1NZUhQ4Zw4MAB0tPTuf/++4mJiQn6NQSiUOacTEpKcvmdSOHwYYirksqwEy/zwsaLoX79IEdnTORZs2YNTZo0CXUYIZeamkpqairlypVj/fr19OzZk/Xr1xMdXbzKqNn9vURkiXMuKZD3F6+rASpUgB5J+5gyvz/Pr/kFscRtjAnQwYMHOf/880lNTcU5x2uvvVbsknYwFMsr6n9JaabOb8jK2d/T/K+hjsYYEy6qVKnCkiVLQh1GoSuWFch9B2u90ZQpIQ7EGGOKoWKZuOucJrSut5nPf2kMJeDb0xhj8qJYJm6A/tdUYwFt2fb29FCHYowxxUrxTdyXlwdg6qeBjS5mjDElRbFN3PHx0KBKClO2JkEIxwQwxuSua9euJ3Woef755/nb3/52yvdVqlQJgC1btjBo0KAcj51b8+Lnn38+U2eY3r17B2UskVGjRjFmzJgCHyfYim3iFoH+Xfczkws4/J3VcxtTnA0ePJgJEyZkWjdhwgQGDx4c0PtPO+00Pvnkk3yfP2vinjZtGlWqVMn38Yq7Ypu4AfpfV42jlGfWhF2hDsUYcwqDBg1i6tSpHDt2DIANGzawZcsWOnbsmNG2OjExkebNmzN58uST3r9hwwbi4+MBOHLkCFdccQUJCQlcfvnlHDlyJGO/W2+9NWNY2EcffRSAsWPHsmXLFrp160a3bt0AaNCgAbt2ad549tlniY+PJz4+PmNY2A0bNtCkSRNuuukmmjVrRs+ePTOdJzvLli2jbdu2JCQkcPHFF7N3796M8zdt2pSEhISMAa6+/fbbjMkkWrVqxYEDB/L92WanWLbj9up8YSViS+1nytwq9A91MMaEiVCM6hoXF0fr1q358ssvGTBgABMmTODyyy9HRChXrhyTJk2icuXK7Nq1i7Zt29K/f/8c51585ZVXqFChAsuXL2f58uWZhmZ94oknqFatGmlpaZx//vksX76cO+64g2effZY5c+ZQvXr1TMdasmQJb7/9NgsWLMA5R5s2bejSpQtVq1Zl/fr1fPDBB7zxxhtcdtllTJw48ZRjbF999dW8+OKLdOnShUceeYTHHnuM559/nieffJI//viDsmXLZlTPjBkzhpdffpkOHTpw8OBBypUrl4dPO3cBlbhFZIOIrBCRZSKSv77s+VC6NFzYYA2fb25F+rETRXVaY0w++FeX+FeTOOd48MEHSUhIoEePHmzevJnt27fneJy5c+dmJNCEhAQSEhIytn300UckJibSqlUrVq1alesgUvPmzePiiy+mYsWKVKpUiYEDB/Ldd98B0LBhQ1q2bAmcevhY0DHCU1JS6NKlCwDXXHMNc+fOzYjxyiuvZNy4cRm9NDt06MCIESMYO3YsKSkpQe+9mZejdXPOFXmdRf9BZZnwVE0W3vgybd+zqYuMyU2oRnW96KKLGDFiBEuXLuXIkSMZJeXx48ezc+dOlixZQunSpWnQoEG2w7n6y640/scffzBmzBgWLVpE1apVufbaa3M9zqnGYvIOCws6NGxuVSU5+eKLL5g7dy5Tpkzhn//8J6tWrWLkyJH06dOHadOm0bZtW2bNmsW5556br+Nnp1jXcQP0GtmSaEljyrh94PmGM8YUP5UqVaJr165cf/31mW5K7tu3j5o1a1K6dGnmzJnDxo0bT3mczp07Z0wKvHLlSpYvXw7osLAVK1YkNjaW7du3M326r49HTExMtvXInTt35rPPPuPw4cMcOnSISZMm0alTpzxfW2xsLFWrVs0orb/33nt06dKF9PR0/vzzT7p168ZTTz1FSkoKBw8e5LfffqN58+bcf//9JCUl8csvv+T5nKcSaInbAV+JiANec869HtQoTqFqVejcGab8cAn/uv9amD+/qE5tjMmjwYMHM3DgwEwtTK688kr69etHUlISLVu2zLXkeeutt3LdddeRkJBAy5Ytad26NaAz2rRq1YpmzZqdNCzssGHDuPDCC6lTpw5z5szJWJ+YmMi1116bcYwbb7yRVq1anbJaJCfvvPMOt9xyC4cPH6ZRo0a8/fbbpKWlMXToUPbt24dzjrvuuosqVarw8MMPM2fOHKKiomjatGnGjD7BEtCwriJymnNui4jUBGYCf3fOzc2yzzBgGED9+vXPy+1bNS9eeEFvuPwmjWl0cLkOIWiMyWDDuoaXgg7rGlBViXNui+d5BzAJaJ3NPq8755Kcc0k1atQI5LAB69dPnz93fYJ/u9wYY8JMrolbRCqKSIx3GegJrCzswPw1agTNzj7BFPrDokVFeWpjjCl2Ailx1wLmicjPwELgC+fcl4Ub1sn6D4zmW7qwd+Lsoj61MWGhMGazMsEXjL9TronbOfe7c66F59HMOfdEgc+aD/0HCGlE8+W8SpBL201jSppy5cqxe/duS97FnHOO3bt3F7hDTrHuOemvdWuoWT2NKfsvYfC998IXX4Q6JGOKjXr16pGcnMzOnTtDHYrJRbly5ahXr16BjhE2ibtUKeg3IIqP3+vN8RlDKbN3r7YVNMZQunRpGjZsGOowTBEp9h1w/A0YAPuPl+ObtI7w+OOhDscYY0IirBL3BRdAxYow6dwH4MUXYd++UIdkjDFFLqwSd7ly0Ls3fLazA+knUmG6TWtmjCl5wipxA1x8MWzbXYYfq/aGzz4LdTjGGFPkwi5x9+6tw71+Wu8OmDYNPAO3G2NMSRF2iTs2Fnr0gEk72uMOHIClS0MdkjHGFKmwS9yg1SW/b6/EchIgl0lEjTEm0oRl4h4wQCcT/qzSVTZ2iTGmxAnLxF2zJiQlwcxyfWHiRAjiELLGGFPchWXiBq3n/nHP2ew/HAUNGkB6eqhDMsaYIhHWiTstvRRza12mKzZtCm1AxhhTRMI2cbdvrx1yZnV6TFesXRvagIwxpoiEbeIuVw66dIHpP9XSFUGejNMYY4qrsE3cAH36wLrfovm1cqKVuI0xJUbYJ26AL6oOtRK3MabECOvE3agRNG0KU4/3tBK3MabECOvEDdC3L3y741z2bzkA994b6nCMMabQhX3i7t0bTqRFMYseMGYMHDgQ6pCMMaZQhX3ibt9eB56a9texusLG6DbGRLiwT9ylS8Nf/wrTltfFxVSG2bNDHZIxxhSqsE/coNUlW7cKPyVcA998A86FOiRjjCk0EZG4e/XS5y/jrtTWJS+/HNqAjDGmEEVE4q5VCxITYfqe1pCQYFOaGWMiWkQkboALL4T584WU5p1g+fJQh2OMMYUmohJ3WhrMjL4Qdu6E774LdUjGGFMoIiZxt2kDVarA9N2tdcU114Q2IGOMKSQRk7ijo6FnT/hySQ3cbbfDhg1w+HCowzLGmKCLmMQNWl2ydSv8fEZ/bRLYty+8+WaowzLGmKCKqMTtbRY4fUsLXZgzB266KXQBGWNMIYioxF27NrRqBdMX19BeOQB16oQ2KGOMCbKIStyg1SU/zBdSxn8Bd98Ne/daT0pjTESJyMSdlgazZgH16sHRo9C9O8ycGerQjDEmKAJO3CISJSI/icjUwgyooNq21WaBX34JnH66rvzmG7jnnlCGZYwxQZOXEvdwYE1hBRIs0dFwwQWauF1CC9+G5s1DF5QxxgRRQIlbROoBfYCwaFt34YWweTOsONIYVq/WlceOhTYoY4wJkkBL3M8D9wHpOe0gIsNEZLGILN65c2dQgssvb7PAL74AmjSBdu0gJSWkMRljTLDkmrhFpC+wwzm35FT7Oeded84lOeeSatSoEbQA86NOHWjdGiZN8qyoWlVblxhjTAQIpMTdAegvIhuACUB3ERlXqFEFwcUXw6JFkJyM3q20ErcxJkLkmridcw845+o55xoAVwCznXNDCz2yArr4Yn3+7DM0cf/2m2ZyY4wJcxHXjtvrnHO0envSJDRxg9afGGNMmMtT4nbOfeOc61tYwQTbxRfDt9/C7g0HfCuvvjp0ARljTBBEbIkbNHGnpcHU6It8K997T1caY0yYiujEfd552nlyUkq3zD0nd+wIXVDGGFNAEZ24ReCii2DGV8KhcxJ9G7ZtC11QxhhTQBGduEGrS44ehS/XNvStvOoqOHEidEEZY0wBRHzi7tQJ4uJg0vpmvpWrVkGZMp4hBI0xJrxEfOKOjoZ+/WDqNzEcT8kyB+Xjj4cmKGOMKYCIT9yg1SX79sE3C8pn3lC2bGgCMsaYAigRifuCC6BiRU9nHP+mgJa4jTFhqEQk7vLldajXyZMh3f+SP/8cXnstdIEZY0w+lIjEDVpdsnUrLFiQZcMtt2g9ijHGhIkSk7j79IHSpeHTT/HMa+bH5qM0xoSREpO4Y2N1zuBJk8D1/Cs89JBuKFNG56Q0xpgwUWISN2h1yW+/wcqVwOjRcPw4dOkC338f6tCMMSZgJSpxDxig3eAnTUIXSpeGZs1g3To4fBhSU0MdojHG5KpEJe7atXX6yYwpzQAaN9akXbEiDBoUstiMMSZQJSpxAwwcCMuWwR9/eFaceaZv4+TJIYnJGGPyosQl7kxTmoGWuL3OOKPI4zHGmLwqcYm7USNISPA0C/SueOQRnRo+KiqksRljTCBKXOIGLXV//z1s3w6UKgWPPab127t3hzo0Y4zJVYlN3M7BlCl+K+PitAdlgwahCssYYwJSIhN3QgI0bJildUlcnD5v3KhZ3RhjiqkSmbhFtHXJrFmwd69nZeXKvh327AlJXMYYE4gSmbgBBg/W2cs++cSz4vhx38Y//wxJTMYYE4gSm7gTE+Gcc2DcOM+KIUOga1ddtsRtjCnGSmziFoGhQ2HuXNi0CahQAcaP141DhsBTT4U0PmOMyUmJTdyg+Rng/fc9K2rX1ueDB+H++3VSYWOMKWZKdOJu1Ajat9fqEufQNt1PP+3b4fzzM9d9G2NMMVCiEzdodcmqVbB8uWfF3Xdre+4JE7SHzgcfWPNAY0yxUuIT96WXQnS0r3obEW0aePHFUL8+XHst3HhjKEM0xphMSnzirl5dJxJ+//3ME8BTpgwsXao3LX/6KWTxGWNMViU+cQNceSVs3gzffptlQ1yc9tTZu1frU2wsE2NMMWCJG+jXD2Ji/KpL/FWtChs2QHw8XHFFUYdmjDEnscSN1oYMHKi9KI8ezbKxalXfcsbsC8YYEzq5Jm4RKSciC0XkZxFZJSKPFUVgRW3oUNi/H6ZOzbLBP3HbRAvGmGIgkBL3MaC7c64F0BLoJSJtCzesotetm86lkNEF3ss/cVsdtzGmGMg1cTt10POytOcRcQ2bo6J04Klp07IMDlitmm951y4YNQp+/bWowzPGmAwB1XGLSJSILAN2ADOdcwuy2WeYiCwWkcU7d+4MdpxFYuhQHTHwww/9VsbG+pY3b9bZcs4+2zrlGGNCJqDE7ZxLc861BOoBrUUkPpt9XnfOJTnnkmrUqBHsOItEy5bQogW8+qpfXs5uHkrnMo8gePy4VpAbY0wRyFOrEudcCvAN0KtQogkxEfjb37T7+w8/eFa2awf//jf861+Zdz7jDF/yvuiizCVzY4wpRIG0KqkhIlU8y+WBHsAvhR1YqFx5pfZ4/+9/PStKlYKRI6FxY33t3/193jx9nj5dn+3mpTGmCARS4q4DzBGR5cAitI47a6O5iFGxog5P8vHHnlngvQYMgM8/13qUzp11XUpK5jevWFFUYRpjSjBxhXCTLSkpyS1evDjoxy0qa9fCuefCE0/Agw9ms0NqKpQtC716QZUqvgG9o6Lg0CHdZowxeSAiS5xzSYHsaz0ns3HOOToU96uvZhl4yis6GmrV0raDGbMwoDtbU0FjTCGzxJ2D227Te48n9aT0qlMn8+t+/fR5x45CjcsYYyxx56BfPx2O+6mncmiyfdppmV8nJOizJW5jTCGzxJ2D6GiddvKHH7IZ7hWgUyffcvv22iQQNHFv2QKHDxdJnMaYkscS9ylcf73OH/z449lsvOEGfX7vPfj+e2jVSpsObtsGdevq1DrGGFMILHGfQrlycO+98PXXMH9+lo1xcZCerv3kQVuUVK8OCxfq62nTijRWY0zJYYk7FzffrDk621K3SObXNWvCrFm6XKZMocdmjCmZLHHnomJFGDFCC9BLl+ay85AhvuXjx+HHHws1NmNMyWSJOwC33aZDkTzxRC47PvAATJ7sK4m3a1fosRljSh5L3AGIjYU77oBPP9U5g0+pf39tjuJ18CAsWlSo8RljShZL3AEaPlyrTbIOEpitunV9ywMGQOvWOhCVczB7tt7UNMaYfLLEHaC4OB3ydcIEWLcul51vvVUHpAJN1KADoDz9tPalf+cdS97GmHyzxJ0HI0ZoE8FsB57yFxUFfftqlvdaulTrwEEbiF9zTaHFaYyJbJa486B2bR2ae+LEHHpTZuXtTQnaUce/lD1uXA4jWBljzKlZ4s6je+7RMUzuvjuAaSfLltUmgb176+vmzTNvX7asUGI0xkQ2S9x5VL68zhe8ZIm2MslVmzbw2mtQo8bJdSwBFduNMSYzS9z5cNVV0LSp5uETJwJ4Q716Op3OFVf41lWpomOc/PQT/OUvNtmwMSZglrjzISoKnnxSW5c8+2yAb8raPb51ax3w+667YPFiTeLGGBMAS9z51LcvDBwI//hHHnPurbfC6afrRAzbtvmGf7UStzEmQJa480kE3noLGjSAyy+HnTsDfON//wubNmkTlW3bdI5K0HXGGBMAS9wFEBurs8Hv3q3DbwdU3+1Vp46+YfVqff3nn4USozEm8ljiLqBWreD117WByNChcPRogG+sXTvza0vcxpgARYc6gEhw1VVa63HffXDkSOYBAnNUrZo+N20K556rbbp37dK2hqVL603L008v9NiNMeHHStxBcu+92sLk88+12Xau2rTRjjmTJun8lb//DqNHw0svwXPPwfjxJ79n1ixtVmiMKdEscQfR8OHQs6eOafLzz7nsXLkyfPEFnH02dO2q6158UXtXVqoEGzdq0d3b3vCnn+CCC7Qkbowp0SxxB1GpUvDuu1C1qjYVDLiFX4sW0KWLLv/975rMN2zQsU7uvlvXv/yyPufaz94YE+kscQdZrVrw0Ufwxx/w0EMBvkkEPvsM5s2Dm27SNoZ//OHbfvgwLFhQGOEaY8KQJe5C0KGDjt390kvw3XcBvqlKFX0jwBln6PgRj5MKAAAawUlEQVTdXmvW+JoNpqQENVZjTPixxF1InngCzjxTe1jOm5fHN3urTbzGjvUNCbt3L/zyi7YBT0/Xdog33WRVKMaUIJa4C0lsLMyZo4MCdumi1ScB699fx4997DF9/e67EB2t6xcsgCZNoEwZnZCha1d4802tUJ8/X+e4NMZENEvchahePZ34pl07bev9zTcBvlFEpzn7xz+gfXs46yxNzjVrZt7vnXd8y6+/rvvecEOwwjfGFFPWAaeQVa4MU6ZAx47aSOSHH7TPTUCiojKPYLVkSc773nefPv/wQ75jNcaEBytxF4Fq1WDaNJ2vsmtXWLgwnwcK5MZktH0XGxPpck3cInK6iMwRkTUiskpEhhdFYJGmQQO9j1ipEnTrBlOn5uMgu3efvO688zK/zrWvvTEm3AVS4k4F7nbONQHaAreJSKA/9o2fc87RmowmTWDAAK2WzpMHH4S6dX2vDxzQG5L+tm/3tTB5+GHt0PPBB9qV06ZKMyYi5Pq72jm3FdjqWT4gImuAusDqQo4tItWurTcpL7sMbr5ZBwUcPTrAgnKHDpCcrIn4/fe1+O6vTx/tRj9kCMyeDTt26PqXXtIBq/78U8c7KVVK62ysdG5MWBKXh/a/ItIAmAvEO+dy7NCdlJTkFi9eXODgItmJEzoZzv/+B9dco6XvMmXyebC5c7UkXquWdt7ZsyfnfTt31v0XLz65msUYEzIissQ5lxTIvgHfnBSRSsBE4M7skraIDBORxSKyeGfA08GUXKVLwxtvwKhR2qqvWzcdGjZfOnfW3j6VKumIgwC33aZF+ay8rVQ2bLA238aEqYASt4iURpP2eOfcp9nt45x73TmX5JxLqlGjRjBjjFgi8Oij8OGHOhx3mzawYkUBD3ruufpcvbo2JM8qLU2fb7wRYmJ8c14aY8JGIK1KBPgfsMY5F+ic5iYPLrtMxzRJTdU+NC+95MuveXb77ZqUb7/95LoX/wbk3qaFGzfm80TGmFAJpMTdAbgK6C4iyzyP3oUcV4mTmKi92du00YYgf/2r795insTGah1M9eraRb5vX+jRQydsOO20k/f//fcCx26MKVqBtCqZB1jzgyJQrx7MnAlvv61V1ImJMGGC9rrMl5gYnZLH6+OPT95n/XqdKLNcOV3+9Ve48MJ8ntAYUxSs52QxI6JjR82fr7m0Uycd5yQ5OQgH9957SPK7cX3XXTqYCuj8a4MG+UYiNMYUS5a4i6mWLXW2sgce0ILy2WdrC5RDhwpw0Esv1Rl1+vfPvH7ZMu2sM3u23qy06hNjijVL3MVYTAz86186/Ha/fjrK6znnwHvv5bNQ3KEDjBkD3bvr68sv903Y8Pjj2hMT4IUXfL0vU1N1ACtvMk9Lg02bMh/3yBHYujUfARlj8sMSdxho0ECbDH73HdSpA1dfDW3bZh44ME/at9eJiZ99VoeMffTRzNtfegm+/lqXv/tOh5j1DiE7fLh28vEf8Kp/f73xaZM5GFMkLHGHkY4dteXJu+/Cli36+vLLtS9Nnohoc8HTTtPlUaN827wJ+rrr4JNPYPp0fT16tL7HO2mx/5yYs2bp8759+bgqY0xeWeIOM6VK6c3KtWu1oPz559rn5sEHfTUd+bJsGaxbp8X5p5/Wu6GXXqpjonh5kzbot8VHH+lPAK9Nm7RlSr4boRtjAmGJO0xVrKgF5XXrNL/++99a6/HGGzoOSp61aKEHALjjDl/X+c2bIT7+5P2XLtXivn8//W++0buoZcvC+PFamvdP/MaYoLDEHebq1dOblQsWQKNGMGyY5s7XXoNjx/J50DJlMrf5/vRTuP9+XR46VJ8ff/zk973yij6npenIWQD/+U8+gzDG5MQSd4Ro3VpvVk6dqoME3nKLjjs1dmw+hyOpUEEPtnChlsQrVND1NWvqzUkv/xEGf/nFt+ytLlm+XHsU7diRuSnMli36MMbkmSXuCCKiQ3LPn689MM88UxuBNGwITz2V/QQ6p9SnD/zlL7p89tn63L69JmOv77/X+vBXX838Xv8JH66/Xr9Nrr7at65ZM93HWqIYk2d5Go87UDYed/Exdy488QR89ZVWPV9yiVandO6cx3kUnNMeQYmJ+vqRR2DOHG0u6N1eyq8ckJysJfUjRzIfZ9Uq3ddbbz57to5pm/VcWY9nTIQrlPG4TXjq3BlmzNBC8rBhOkFO1656L3LKFO1fExARX9IGbR7oTdre7XfeqUV857Q0PWiQb3uTJvrcrFnmm53exuhffKH9+9PStH4nKgomT4ZnnsnPZRsT0azEXcIcPqyt+B5/HH77TatTHngABg/2VWMHzcGDcM89eqf0nnvgxx9h3ryT95s4UW9mHjyoQbVvr3NneqWmaiI3JoJZidvkqEIFuPZaWL1aE3jlyjp8d9262i48z/Xgp1KpkvbOfOABHQvloYd0fdbhDi+5RNs3AqxZc/Ld1Kw9jDZtsoGwTIlmibuEKlNG238vWaJV1d27a+1HnTowcKDWUuSrPXhWFSrogCuVK+sg43/+qRXve/Zo5buXt4S9Zs3JI2l9+KFWoRw7psPOnnGGdhIypqRyzgX9cd555zkTfpYvd27ECOdq1dK7g7VrO/f0087t319IJ0xPd+7gQec6dfLejnSucmXfsv/jkkv0+aabfOteesm51NSTj/v77859/LEe35gwASx2AeZYK3GbDM2b673A5GRtwh0fr0N0x8VBly56zzCow5GIaBXJ3Xf71h05ok0HvU4/XZ8nTtTnN97wbbv9dg3QOfj2W60++fFHLdlfeqmOhPi//9kYKibyBJrh8/KwEnfkWLDAufvuc65lSy3klinjXO/ezr3+unPbtgXpJEeOOHfppc4tXerczp3Obd/u3Pjxzv3zn84dPuzckCF68k6dnCtf3lfijovT527d9Pk//8m+tP73v598zvXrnXvgAb2IlSuDdCHG5B95KHFb4jYBW7TIubvucq5hQ/2XI+Jchw7OPfWUbjtxopBOPHOmnvCtt5w7fty5Hj309YQJmRO0iG+5fn3f8sUX+441d65za9c617WrbjvvPOdiYrTKZv9+3VZQ6enOzZ9vVTUmTyxxm0KVnu7czz8799hjzrVq5cuPMTHO3Xabc8uWZV/1XKATTp/u3LFj+jolRb8pjh51rnFjXwD+deXXX+/c3Xf7Xn/5pZbmwbnTTnMuKSlz0h8/3rmLLtLljRv1PDNmOLd6dd7jffttPc6nnwbtIzCRzxK3KVLJyVr4HTpUq1LAuQoVnGvf3rlnnnFuz55CDqBiRef69NHlhQs1gPfe09evv+5LzqVK+Za9Pxu8jy5dfMsjRujPB3AuKirv8Vx+ub73jTcC2z8tzbkbbtAvI1Ni5SVxWwccE1Tbt+vcCz/9BD/8AIsXQ/nyMGCANt9u1w4SEiA6OognPXZMO+h4D7pli7Zr9PbpX75cu4qC9jj67bfM7y9f/uSu+aVK+dqK+/8fOXJEB97q0iXneNq00X3++1+49dbc49+4Uac5qldPm0uaEsk64JiQqVVLO/i88AIsWqQJ/KqrtNHH7bfrYIKxsdrn5sUXdVjvAvelKVs28zeBd2Yfr4QEHSt8yhSdcQI0iOef1+X77vPte+21+pw1qLPO0pYqo0bpmAH+BZNp03Sbc3D8uI7HAjpP5yef5B6/d75OG5vFBCiY5R5jTtKypfZ4f/VV7fA4f74OcfLZZzrMN2jhuG9fnRD5/PMLoes9+ErIycnapf7NN7VT0Guv6YQQ3bpp8vc2O/Q3erR2/Pn1V7jySt/xzj5bB0P//HNN0GvWZO5ANGaMPq9Zo9MU5SQ5WZ/Llg3OtZrIF2idSl4eVsdtcpOe7tymTc69845zgwbpjU1wrlw55/r21ZYqs2cXYuefnOzcqU0Ts2tWCM4lJmZ+3amTVuaDVujXr+/cWWfpDVDvPq++6tyHH+qN1G3btB6+eXPntmzRcz73nO7XrNnJ8axerTdNi/yDMEUNuzlpws2xY8599ZU2uW7UyJfzRDTH3XSTtgZcvVrv5RW6RYucmzMn++Rdtqze/Lz99szrvW3Mp0zJ3NzG//Hcc74boy+9pM0b77xTX5977slxeL8Apk07edtXX2mzxk6dnPv++0L/SEzhssRtwt7OndoCcNQo53r1cq5KFV/ui43VRiDDh2tBdvPmQgxk61Y9Wa9eemL/knFy8smJOSZGmymef372ibtdO+eio3U5IUF/Yvhv//ZbPXZ6ujZh9K6vU0cT/c8/+2Lr3Nm3vXNn3/r0dO0lNXZs8D6H9HTnPvpIry0n69fbF0gBWOI2ESctzbk1a7SJ9C23ONe2beZOlA0bOnf11dr674cfdLiS48eDHMTq1XqyatV86957L3Pi/dvfdH2/fvraO/DL9dc7d++9vv3atMk+sXv3ffXV7LfFxmpVi7eU3rWrc9ddp8uDBmmC/fFH3/5Hj+qHN2mSfhseP66N7Nev1/Ns2qTvWblSe65m9dZb2lTxnXf0eM8+m/Pn4z1nfj39tHP//W/+3x/mLHGbEuH4cc1hzz7r3MCBztWsmTnHlS3rXM+e2inoX//SAuzu3QXo0JiWpslxxgzfus2b9WQff6zJzdtJaPFiTULezjgPP+zc559nLln7B3vHHTkn8lPVuY8erXXr3tdz5jjXsaPv9UMPOffBB5nfc/PNzg0Y4Hv9/vs5J13v+v799fmuu7Ivde/d69v30KH8fb4FTfxhzhK3KZHS051bt06rWN58U6tSmjXz3fj0PmrX1sEGH3nEuRdecO7rr3V4lEKRmqpVHIcOObdvny+IzZud+/e/dfn333XftDTn/vc/3z7PPOPcN9/otr17teu+d5u3KubNN7UKI2tCf/PNnJP9WWdp1U922/btc27WLP0SOn48+3169jz5Ov23f/+9dkLKSx1WWpolbkvcxvikp2ujjJkznRszRnt4Zu04CTqibNOmmtMefNC5yZODOJCWl3d42vR03yOrr75y7sYbdYAtf5s2aZObrVv1i+DJJ3WflBRd7x1/pX17Pa5/oq9a1bdcvnzmO8D+j6yl85we69bpt96jj/pK41kfw4dr3L//rr9EUlO13j27D9X/fsGpxks4elRb9vgPJzB9euZfQf6OH3duwwb9PAYP1n2LKUvcxgTgxAnNgTNnanXL7bdrrmvRQnu6e/NI/fq6/r77tHC7aJFzv/6queZU9+qylZqqpdrC8McfegPA+/Ph2DHn7r9fL+Lrr7X1Sd++vgsbM8a3PHduzmOhZ/fw/4Byetx5p9alg3N162q1jbeqZt68zF9a33/ve1/Wkrq33v7hh337xMT4Ps+sJfX0dP1V8MQTvvcsXXryfmlpOdebpaTone+sdu7U0SwLgSVuYwro0CHNLc88o7/6zz7bNw6L/yM62rn4eB159sknnZs4UWs3pk/XxB5yx45pSd3r6699waekOPfuu5rQ09O1Xv6BB3SUMO+YL3l93HOPb/nSS/X4Oe375JM6GuP//Z/vXgDouTdu1C+iX3/VNqJZ31u9ul7P3Lm+dd52ov6xe3+F3HzzyYm7TRsdEji7OnnvDd/FizOvBx2dshBY4jamEBw7pqXtKVO0CviVV7RKpU8f504/PfvcVKOG5sVhwzRPffaZ5qODB0M06uuJE1rFEcioh3v3aouSrCXxbt20cf0bb2Ref/Cgvs9bym3ZUqtLvIN7tWqlH1h2H5S3qSXot593uVKl7PePjtYmj/7r1qxx7uWXtZrJuy4+/uT3jhiR+XWVKvpH8ecdOviFF5y74AK997Bjx8nJP4jykrhzHWRKRN4C+gI7nHPxgfTGtEGmTEm0b58OT7J3rw6V8vPPOmzJmjXwyy8nT8Rctiw0aqTjXkVF6RArjRtD/fo6rWa9ejrfcpky+vAffqVI3XSTDhHQrJle0IkTGrAIjB+vg3oNGaIzTnuNGgWPPabDCHTuDO++C9Wqwf79ULt29ufp2BG+/15TY0F17KijnAU6EE737jBzpm+8mPbtdXyGv/xFB90BHe+ma1dd9sb40ks6cetLL+nQCPfcAzVq5CvkvAwyFUji7gwcBN61xG1M/u3fDytWaCLfswd27tThT/74Q/PLn39CSkr27xXRfHf66ZofTzsNqlfXR4UK+iVw5pmaW2Nighz4sWM62mHr1nD4MFStmvt7jh7VkRGrV4c778yc1H/5RY8zfz7MmAE1a+oUc6+9BjffnP3xnn4a+vTRiafHjdN1cXEnfxt6/fijJtNx46BXL/jyS/0QO3fWEc+yM2sWnHMObNigg+dknfLuhRdg+HBdPnFCv5S836b/+IdOfj1kiH6Z5UNQE7fngA2AqZa4jSk8zmmu2LhRB+RKTtb8duyYjiabnKyPLVv0kVOSb9RIC8QHDuighomJmuhjYrQU75yuL1dOc9+JE1oYDlmJ/vhxvZiaNTWgPXv0Avft02+znj31Gy/ek34eekiTpH+iX7dOB/0CuO46eOstHXWxfXt47jno0UOPW6OGJuXZs33nb9oU1q/X469cqR8I6CSrd9yRfcxLlkCrVieP6Fizpib+8uXz/DGEJHGLyDBgGED9+vXP27hxY0DBGmPyJzVVc9HRo5rg163Toce9o8qWLw9r1+rQulmHG8+qTBnNOfHxOpBhpUpQurSvmubMMzXxp6bqcv36uj3ofvpJqyaGDfOtS0vTbyKvw4dh2TL9RipfXn9mrFihSfSGG7Ra51S2btVvMoDBg+Hhh3XM4dmzoUoVPf7x4/qheRNw7946fO/AgVpir1FDq1a8k1kD3H8/jBypx8gHK3EbYzKkpWmp3VuATU3VAmZamlbXlCkD27ZpIXflSv0COHLk1FXNUVG+qprq1bX2pEwZTeYxMVqIrVVLvwTOPltz6rFjes7Y2CCW7r/+Wk9Wp46v7j23cc3T03W/2rV9Y6F/8YWWxK+6Skva+/bpjYZx4zRBv/qqlvCvukqrVK64wne82rX1A/zlF61qySdL3MaYAktL01x45IiW3I8e1Zz4++/62L5dE/+uXXpD9sQJfezZk3M1Dmgh9owzNNnXqKF5r1YtX3VQ48Y690WzZlqdc/y4ftmUK6f7VaoUhMQ/Z47WF9Wrp6+dg8mTtQ68WrXcP5jzztO7z6+8osn8xx91MPkCsMRtjAkZ5zSpb9+uN2K9M8WVKaOJf8sWrcdPSdHEv3077Nih2844Q7elpeV8/LJlfSX9WrU08R89qrO/xcbqF0d0tJb+q1TRpJ+crPudcYY+6tTR56NHtXBdvnwe6/n379c6qXbtCvpxZchL4s51BhwR+QDoClQXkWTgUefc/woWojEmUnlbwNSu7ZvqMzepqVqyrlBBk+maNbB6ta73tjxMTdUkv3u3lvJ37dIvgTVrNJlPmqQl/goVdN/UVF9rwMqVNdeeStWqWuA+80z9Ali/Xs9Rv77+MvA+RKBx48o0a9GOrJM1HToEFSvm+SPLM5ss2BgTEVJTtR7dP3Hu2QMHD2ryPXxYW+ts2qQJf8MG3Tc2VretXKn3Hbdt0/dUrqwtdLZs0QSetUm4iJbaS5XS5d279T35bZcR1BK3McaEg+jozHNGg1Z/eKusK1TQm6Wnmv7T68gRLcV773Omp2s9/s6d+gXhbcGzbp2veiU2VqvNi4IlbmOMySJrM+xSpbSJeVycvo6P15aBoZJLuxljjDHFjSVuY4wJM5a4jTEmzFjiNsaYMGOJ2xhjwowlbmOMCTOWuI0xJsxY4jbGmDBTKF3eRWQnkJ+On9WBXUEOp7izay4Z7JpLhoJc8xnOuYDmPSuUxJ1fIrI40L76kcKuuWSway4ZiuqararEGGPCjCVuY4wJM8Utcb8e6gBCwK65ZLBrLhmK5JqLVR23McaY3BW3ErcxxphcWOI2xpgwU2wSt4j0EpG1IvKriIwMdTzBIiJvicgOEVnpt66aiMwUkfWe56qe9SIiYz2fwXIRSQxd5PkjIqeLyBwRWSMiq0RkuGd9xF4zgIiUE5GFIvKz57of86xvKCILPNf9oYiU8awv63n9q2d7g1DGn18iEiUiP4nIVM/riL5eABHZICIrRGSZiCz2rCvSf9/FInGLSBTwMnAh0BQYLCJNQxtV0Pwf0CvLupHA1865s4CvPa9Br/8sz2MY8EoRxRhMqcDdzrkmQFvgNs/fMpKvGeAY0N051wJoCfQSkbbAf4DnPNe9F7jBs/8NwF7nXGPgOc9+4Wg4sMbvdaRfr1c351xLvzbbRfvv2zkX8gfQDpjh9/oB4IFQxxXE62sArPR7vRao41muA6z1LL8GDM5uv3B9AJOBC0rYNVcAlgJt0F500Z71Gf/OgRlAO89ytGc/CXXsebzOemiS6g5MBSSSr9fvujcA1bOsK9J/38WixA3UBf70e53sWRepajnntgJ4nmt61kfU5+D5OdwKWEAJuGZPtcEyYAcwE/gNSHHOpXp28b+2jOv2bN8HxBVtxAX2PHAf4J3/PI7Ivl4vB3wlIktEZJhnXZH++y4ukwVLNutKYjvFiPkcRKQSMBG40zm3XyS7S9Nds1kXltfsnEsDWopIFWAS0CS73TzPYX3dItIX2OGcWyIiXb2rs9k1Iq43iw7OuS0iUhOYKSK/nGLfQrnu4lLiTgZO93tdD9gSoliKwnYRqQPged7hWR8Rn4OIlEaT9njn3Kee1RF9zf6ccynAN2gdfxUR8RaQ/K8t47o922OBPUUbaYF0APqLyAZgAlpd8jyRe70ZnHNbPM870C/o1hTxv+/ikrgXAWd57kiXAa4ApoQ4psI0BbjGs3wNWg/sXX+15050W2Cf9+dXuBAtWv8PWOOce9ZvU8ReM4CI1PCUtBGR8kAP9KbdHGCQZ7es1+39PAYBs52nEjQcOOcecM7Vc841QP+/znbOXUmEXq+XiFQUkRjvMtATWElR//sOdUW/X6V9b2AdWi/4j1DHE8Tr+gDYCpxAv31vQOv2vgbWe56refYVtHXNb8AKICnU8efjejuiPwWXA8s8j96RfM2e60gAfvJc90rgEc/6RsBC4FfgY6CsZ305z+tfPdsbhfoaCnDtXYGpJeF6Pdf3s+exypurivrft3V5N8aYMFNcqkqMMcYEyBK3McaEGUvcxhgTZixxG2NMmLHEbYwxYcYStzHGhBlL3MYYE2b+H12hzIM3U3RJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu_ngrams Test Accuracy: 0.910\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "neu_ng = Sequential()\n",
    "neu_ng.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "neu_ng.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "neu_ng.summary()\n",
    "history = neu_ng.fit(X_scaled_train_data_ngrams, y_train,\n",
    "                    validation_data=(X_scaled_val_data_ngrams, y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu_ngrams,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu_ngrams.h5')\n",
    "yhat = l_model.predict(scaled_test_data_ngrams)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_ngrams Test Accuracy: %.3f' % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_neu_ngrams Test Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "l_model = load_model('my_model_neu_ngrams.h5')\n",
    "yhat = l_model.predict(scaled_test_data_ngrams)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_ngrams Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 32)                243968    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 250,825\n",
      "Trainable params: 250,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 2s 37ms/step - loss: 6.3877 - acc: 0.1270 - val_loss: 6.2967 - val_acc: 0.1402\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 6.2174 - acc: 0.1429 - val_loss: 6.1584 - val_acc: 0.1481\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 6.0949 - acc: 0.2063 - val_loss: 6.0358 - val_acc: 0.1521\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.9704 - acc: 0.1746 - val_loss: 5.9209 - val_acc: 0.1601\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.8782 - acc: 0.1429 - val_loss: 5.8120 - val_acc: 0.1587\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.7594 - acc: 0.2222 - val_loss: 5.7071 - val_acc: 0.1733\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.6607 - acc: 0.1429 - val_loss: 5.6041 - val_acc: 0.1958\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.5448 - acc: 0.1905 - val_loss: 5.5058 - val_acc: 0.2011\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.4480 - acc: 0.2540 - val_loss: 5.4104 - val_acc: 0.2302\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.3466 - acc: 0.2857 - val_loss: 5.3178 - val_acc: 0.2540\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.2697 - acc: 0.2063 - val_loss: 5.2272 - val_acc: 0.2632\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.1926 - acc: 0.2222 - val_loss: 5.1407 - val_acc: 0.3082\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.1138 - acc: 0.2698 - val_loss: 5.0580 - val_acc: 0.3294\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.0210 - acc: 0.2698 - val_loss: 4.9779 - val_acc: 0.3558\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.9333 - acc: 0.3175 - val_loss: 4.9004 - val_acc: 0.3717\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.8861 - acc: 0.2222 - val_loss: 4.8260 - val_acc: 0.3889\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.8219 - acc: 0.3016 - val_loss: 4.7541 - val_acc: 0.4392\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.7584 - acc: 0.2540 - val_loss: 4.6855 - val_acc: 0.4788\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6996 - acc: 0.3175 - val_loss: 4.6177 - val_acc: 0.5238\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6547 - acc: 0.2857 - val_loss: 4.5503 - val_acc: 0.5741\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.5608 - acc: 0.3175 - val_loss: 4.4847 - val_acc: 0.6151\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4907 - acc: 0.4127 - val_loss: 4.4212 - val_acc: 0.6349\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.4747 - acc: 0.3333 - val_loss: 4.3585 - val_acc: 0.6561\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.3637 - acc: 0.3810 - val_loss: 4.2945 - val_acc: 0.7011\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2805 - acc: 0.4603 - val_loss: 4.2300 - val_acc: 0.7275\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2595 - acc: 0.3810 - val_loss: 4.1696 - val_acc: 0.7434\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2289 - acc: 0.4286 - val_loss: 4.1108 - val_acc: 0.7566\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.1998 - acc: 0.3810 - val_loss: 4.0528 - val_acc: 0.7751\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.1639 - acc: 0.3810 - val_loss: 4.0014 - val_acc: 0.8042\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0495 - acc: 0.5556 - val_loss: 3.9478 - val_acc: 0.8201\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.0551 - acc: 0.5079 - val_loss: 3.8951 - val_acc: 0.8347\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9192 - acc: 0.4444 - val_loss: 3.8390 - val_acc: 0.8452\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9697 - acc: 0.4603 - val_loss: 3.7839 - val_acc: 0.8624\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9565 - acc: 0.4444 - val_loss: 3.7328 - val_acc: 0.8770\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8321 - acc: 0.5397 - val_loss: 3.6826 - val_acc: 0.9074\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8091 - acc: 0.5397 - val_loss: 3.6304 - val_acc: 0.9431\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.7604 - acc: 0.6349 - val_loss: 3.5777 - val_acc: 0.9656\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6761 - acc: 0.5714 - val_loss: 3.5228 - val_acc: 0.9749\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6329 - acc: 0.5873 - val_loss: 3.4672 - val_acc: 0.9841\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.5683 - acc: 0.6032 - val_loss: 3.4134 - val_acc: 0.9974\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6414 - acc: 0.6349 - val_loss: 3.3625 - val_acc: 1.0000\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4973 - acc: 0.6508 - val_loss: 3.3128 - val_acc: 1.0000\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4957 - acc: 0.6190 - val_loss: 3.2618 - val_acc: 1.0000\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3758 - acc: 0.6825 - val_loss: 3.2108 - val_acc: 1.0000\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.5014 - acc: 0.6032 - val_loss: 3.1666 - val_acc: 1.0000\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3809 - acc: 0.6825 - val_loss: 3.1193 - val_acc: 1.0000\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4186 - acc: 0.5714 - val_loss: 3.0744 - val_acc: 1.0000\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3049 - acc: 0.6190 - val_loss: 3.0307 - val_acc: 1.0000\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2946 - acc: 0.6825 - val_loss: 2.9863 - val_acc: 1.0000\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1486 - acc: 0.6984 - val_loss: 2.9399 - val_acc: 1.0000\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1894 - acc: 0.6825 - val_loss: 2.8942 - val_acc: 1.0000\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2236 - acc: 0.6984 - val_loss: 2.8509 - val_acc: 1.0000\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1178 - acc: 0.7460 - val_loss: 2.8132 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0888 - acc: 0.7302 - val_loss: 2.7726 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9782 - acc: 0.7778 - val_loss: 2.7300 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9333 - acc: 0.7778 - val_loss: 2.6888 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9697 - acc: 0.7460 - val_loss: 2.6517 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9165 - acc: 0.7937 - val_loss: 2.6147 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8910 - acc: 0.7778 - val_loss: 2.5758 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8787 - acc: 0.7619 - val_loss: 2.5395 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8516 - acc: 0.7937 - val_loss: 2.5055 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8034 - acc: 0.7937 - val_loss: 2.4708 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6580 - acc: 0.8889 - val_loss: 2.4355 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7752 - acc: 0.6984 - val_loss: 2.4014 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7418 - acc: 0.8095 - val_loss: 2.3713 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6262 - acc: 0.7937 - val_loss: 2.3401 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5461 - acc: 0.8889 - val_loss: 2.3072 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6067 - acc: 0.8730 - val_loss: 2.2780 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5810 - acc: 0.8413 - val_loss: 2.2491 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5730 - acc: 0.8095 - val_loss: 2.2206 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5442 - acc: 0.7937 - val_loss: 2.1941 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5071 - acc: 0.7778 - val_loss: 2.1692 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3675 - acc: 0.8254 - val_loss: 2.1455 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4844 - acc: 0.8095 - val_loss: 2.1230 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4548 - acc: 0.8095 - val_loss: 2.1027 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3751 - acc: 0.8571 - val_loss: 2.0798 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5582 - acc: 0.7460 - val_loss: 2.0590 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4756 - acc: 0.7778 - val_loss: 2.0415 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4698 - acc: 0.7619 - val_loss: 2.0227 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3159 - acc: 0.8571 - val_loss: 2.0022 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4166 - acc: 0.8571 - val_loss: 1.9800 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3235 - acc: 0.8730 - val_loss: 1.9608 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3426 - acc: 0.8889 - val_loss: 1.9395 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3054 - acc: 0.7937 - val_loss: 1.9205 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2858 - acc: 0.9048 - val_loss: 1.9019 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1869 - acc: 0.8413 - val_loss: 1.8810 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1691 - acc: 0.9206 - val_loss: 1.8641 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2116 - acc: 0.8730 - val_loss: 1.8480 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2621 - acc: 0.8095 - val_loss: 1.8335 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2208 - acc: 0.8413 - val_loss: 1.8180 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1501 - acc: 0.9206 - val_loss: 1.8024 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1328 - acc: 0.8571 - val_loss: 1.7865 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2622 - acc: 0.8571 - val_loss: 1.7757 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2111 - acc: 0.8571 - val_loss: 1.7644 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0595 - acc: 0.9365 - val_loss: 1.7494 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2099 - acc: 0.9048 - val_loss: 1.7341 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0876 - acc: 0.8730 - val_loss: 1.7199 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2031 - acc: 0.8413 - val_loss: 1.7082 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9309 - acc: 0.9365 - val_loss: 1.6956 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9775 - acc: 0.9365 - val_loss: 1.6810 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0384 - acc: 0.9048 - val_loss: 1.6679 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1666 - acc: 0.8571 - val_loss: 1.6567 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0162 - acc: 0.8730 - val_loss: 1.6492 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9799 - acc: 0.8730 - val_loss: 1.6394 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8883 - acc: 0.8889 - val_loss: 1.6268 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9852 - acc: 0.9206 - val_loss: 1.6148 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9607 - acc: 0.9048 - val_loss: 1.6057 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9537 - acc: 0.9048 - val_loss: 1.5990 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9475 - acc: 0.8889 - val_loss: 1.5889 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8835 - acc: 0.9206 - val_loss: 1.5798 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8381 - acc: 0.9365 - val_loss: 1.5693 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9275 - acc: 0.9206 - val_loss: 1.5592 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9157 - acc: 0.9206 - val_loss: 1.5501 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8364 - acc: 0.8730 - val_loss: 1.5444 - val_acc: 1.0000\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8320 - acc: 0.9365 - val_loss: 1.5362 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8082 - acc: 0.9524 - val_loss: 1.5275 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8277 - acc: 0.9048 - val_loss: 1.5205 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9039 - acc: 0.8730 - val_loss: 1.5124 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9343 - acc: 0.8254 - val_loss: 1.5056 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7732 - acc: 0.9683 - val_loss: 1.4994 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8262 - acc: 0.8889 - val_loss: 1.4947 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8704 - acc: 0.8730 - val_loss: 1.4886 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7485 - acc: 0.9365 - val_loss: 1.4807 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8760 - acc: 0.9206 - val_loss: 1.4750 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7840 - acc: 0.9048 - val_loss: 1.4676 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7499 - acc: 0.9206 - val_loss: 1.4598 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8475 - acc: 0.9206 - val_loss: 1.4531 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8043 - acc: 0.9524 - val_loss: 1.4503 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6868 - acc: 0.9841 - val_loss: 1.4440 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7786 - acc: 0.8730 - val_loss: 1.4349 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7996 - acc: 0.9365 - val_loss: 1.4306 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8771 - acc: 0.9206 - val_loss: 1.4304 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7505 - acc: 0.9365 - val_loss: 1.4250 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7529 - acc: 0.9048 - val_loss: 1.4179 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6671 - acc: 0.9365 - val_loss: 1.4090 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7885 - acc: 0.9048 - val_loss: 1.4039 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7236 - acc: 0.9048 - val_loss: 1.3983 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7472 - acc: 0.9524 - val_loss: 1.3949 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7930 - acc: 0.8889 - val_loss: 1.3940 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5708 - acc: 1.0000 - val_loss: 1.3878 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7135 - acc: 0.9524 - val_loss: 1.3797 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7355 - acc: 0.8889 - val_loss: 1.3751 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7609 - acc: 0.8889 - val_loss: 1.3743 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7543 - acc: 0.9048 - val_loss: 1.3713 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8118 - acc: 0.8730 - val_loss: 1.3699 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6366 - acc: 0.9206 - val_loss: 1.3671 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6963 - acc: 0.8889 - val_loss: 1.3621 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6781 - acc: 0.9524 - val_loss: 1.3578 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6371 - acc: 0.9365 - val_loss: 1.3536 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5432 - acc: 0.9841 - val_loss: 1.3463 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6669 - acc: 0.9524 - val_loss: 1.3389 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6294 - acc: 0.9365 - val_loss: 1.3361 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5739 - acc: 0.9841 - val_loss: 1.3300 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6236 - acc: 0.9365 - val_loss: 1.3260 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5337 - acc: 0.9206 - val_loss: 1.3223 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6894 - acc: 0.9048 - val_loss: 1.3192 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5015 - acc: 0.9365 - val_loss: 1.3162 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5628 - acc: 0.9206 - val_loss: 1.3082 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6344 - acc: 0.9365 - val_loss: 1.3061 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5660 - acc: 0.9524 - val_loss: 1.3015 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5496 - acc: 0.9841 - val_loss: 1.2963 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5681 - acc: 0.9365 - val_loss: 1.2938 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5116 - acc: 0.9841 - val_loss: 1.2913 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5478 - acc: 0.9365 - val_loss: 1.2882 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5281 - acc: 0.9365 - val_loss: 1.2849 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4827 - acc: 0.9524 - val_loss: 1.2813 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5630 - acc: 0.9524 - val_loss: 1.2767 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5646 - acc: 0.9841 - val_loss: 1.2749 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5925 - acc: 0.9524 - val_loss: 1.2723 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6293 - acc: 0.8889 - val_loss: 1.2696 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4454 - acc: 0.9841 - val_loss: 1.2644 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5504 - acc: 0.9365 - val_loss: 1.2609 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4938 - acc: 0.9524 - val_loss: 1.2571 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5485 - acc: 0.9524 - val_loss: 1.2539 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5414 - acc: 0.9524 - val_loss: 1.2531 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4320 - acc: 1.0000 - val_loss: 1.2487 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4232 - acc: 0.9524 - val_loss: 1.2433 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5781 - acc: 0.9365 - val_loss: 1.2398 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4791 - acc: 0.9524 - val_loss: 1.2382 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6491 - acc: 0.8730 - val_loss: 1.2393 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5206 - acc: 0.9524 - val_loss: 1.2409 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5832 - acc: 0.8889 - val_loss: 1.2416 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4666 - acc: 0.9365 - val_loss: 1.2396 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5187 - acc: 0.9206 - val_loss: 1.2351 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4055 - acc: 1.0000 - val_loss: 1.2306 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4722 - acc: 0.9524 - val_loss: 1.2242 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4555 - acc: 0.9524 - val_loss: 1.2220 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4716 - acc: 0.9524 - val_loss: 1.2182 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4706 - acc: 0.9683 - val_loss: 1.2170 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4972 - acc: 0.9365 - val_loss: 1.2154 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4973 - acc: 0.9524 - val_loss: 1.2158 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4625 - acc: 0.9524 - val_loss: 1.2130 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4695 - acc: 0.9524 - val_loss: 1.2072 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4657 - acc: 0.9524 - val_loss: 1.2011 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4532 - acc: 0.9683 - val_loss: 1.1971 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4433 - acc: 0.9365 - val_loss: 1.1941 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4687 - acc: 0.9048 - val_loss: 1.1926 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3630 - acc: 0.9841 - val_loss: 1.1894 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3865 - acc: 0.9524 - val_loss: 1.1864 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4811 - acc: 0.9206 - val_loss: 1.1873 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4443 - acc: 0.9524 - val_loss: 1.1869 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4417 - acc: 0.9365 - val_loss: 1.1832 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4561 - acc: 0.9524 - val_loss: 1.1842 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4021 - acc: 0.9683 - val_loss: 1.1826 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4115 - acc: 0.9365 - val_loss: 1.1777 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4064 - acc: 0.9683 - val_loss: 1.1737 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4079 - acc: 0.9524 - val_loss: 1.1701 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2974 - acc: 1.0000 - val_loss: 1.1656 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3386 - acc: 0.9524 - val_loss: 1.1618 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4522 - acc: 0.9683 - val_loss: 1.1600 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4062 - acc: 0.9683 - val_loss: 1.1615 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3853 - acc: 0.9524 - val_loss: 1.1579 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3133 - acc: 1.0000 - val_loss: 1.1537 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3215 - acc: 0.9841 - val_loss: 1.1496 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4101 - acc: 0.9365 - val_loss: 1.1472 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3381 - acc: 0.9524 - val_loss: 1.1447 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4246 - acc: 0.9206 - val_loss: 1.1434 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4576 - acc: 0.9206 - val_loss: 1.1450 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3505 - acc: 0.9683 - val_loss: 1.1456 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3882 - acc: 0.9206 - val_loss: 1.1444 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3777 - acc: 0.9683 - val_loss: 1.1436 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3442 - acc: 0.9524 - val_loss: 1.1422 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4007 - acc: 0.9365 - val_loss: 1.1389 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4167 - acc: 0.9206 - val_loss: 1.1397 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3682 - acc: 0.9683 - val_loss: 1.1396 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3844 - acc: 0.9206 - val_loss: 1.1367 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4361 - acc: 0.9048 - val_loss: 1.1343 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3449 - acc: 0.9683 - val_loss: 1.1346 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3773 - acc: 0.9683 - val_loss: 1.1325 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4174 - acc: 0.9524 - val_loss: 1.1331 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3586 - acc: 0.9524 - val_loss: 1.1322 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2999 - acc: 0.9683 - val_loss: 1.1274 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3386 - acc: 0.9206 - val_loss: 1.1244 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3076 - acc: 0.9524 - val_loss: 1.1205 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3659 - acc: 0.9524 - val_loss: 1.1221 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3860 - acc: 1.0000 - val_loss: 1.1249 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3174 - acc: 0.9524 - val_loss: 1.1225 - val_acc: 1.0000\n",
      "Epoch 238/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3311 - acc: 0.9841 - val_loss: 1.1189 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3600 - acc: 0.9524 - val_loss: 1.1156 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3031 - acc: 0.9683 - val_loss: 1.1138 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3826 - acc: 0.9683 - val_loss: 1.1131 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3920 - acc: 0.9524 - val_loss: 1.1150 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3688 - acc: 0.9365 - val_loss: 1.1151 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3755 - acc: 0.9683 - val_loss: 1.1141 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4192 - acc: 0.9524 - val_loss: 1.1132 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3054 - acc: 0.9365 - val_loss: 1.1125 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3227 - acc: 0.9524 - val_loss: 1.1078 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3410 - acc: 0.9365 - val_loss: 1.1044 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2878 - acc: 1.0000 - val_loss: 1.1027 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2411 - acc: 1.0000 - val_loss: 1.0985 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3266 - acc: 0.9206 - val_loss: 1.0964 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3595 - acc: 0.9683 - val_loss: 1.0957 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2493 - acc: 0.9841 - val_loss: 1.0933 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2732 - acc: 1.0000 - val_loss: 1.0899 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3143 - acc: 0.9524 - val_loss: 1.0879 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3943 - acc: 0.9048 - val_loss: 1.0891 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3346 - acc: 0.9683 - val_loss: 1.0924 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2464 - acc: 0.9524 - val_loss: 1.0903 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3197 - acc: 0.9683 - val_loss: 1.0862 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3553 - acc: 0.9365 - val_loss: 1.0838 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2946 - acc: 0.9524 - val_loss: 1.0808 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2524 - acc: 0.9841 - val_loss: 1.0787 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3248 - acc: 0.9683 - val_loss: 1.0795 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3410 - acc: 0.9048 - val_loss: 1.0799 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2718 - acc: 0.9524 - val_loss: 1.0775 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2219 - acc: 0.9841 - val_loss: 1.0733 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2693 - acc: 0.9683 - val_loss: 1.0673 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2705 - acc: 0.9524 - val_loss: 1.0643 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2714 - acc: 1.0000 - val_loss: 1.0624 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2857 - acc: 0.9365 - val_loss: 1.0638 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2169 - acc: 0.9841 - val_loss: 1.0624 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2286 - acc: 0.9841 - val_loss: 1.0582 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2056 - acc: 0.9683 - val_loss: 1.0550 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1925 - acc: 0.9841 - val_loss: 1.0527 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2445 - acc: 0.9524 - val_loss: 1.0496 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1700 - acc: 0.9841 - val_loss: 1.0454 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2172 - acc: 0.9841 - val_loss: 1.0427 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1278 - acc: 0.9841 - val_loss: 1.0409 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3087 - acc: 0.9524 - val_loss: 1.0422 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2591 - acc: 0.9524 - val_loss: 1.0445 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2056 - acc: 0.9683 - val_loss: 1.0441 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2353 - acc: 0.9683 - val_loss: 1.0462 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2247 - acc: 0.9683 - val_loss: 1.0454 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1684 - acc: 0.9841 - val_loss: 1.0417 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2438 - acc: 0.9524 - val_loss: 1.0396 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1445 - acc: 0.9841 - val_loss: 1.0355 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3547 - acc: 0.9206 - val_loss: 1.0380 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1761 - acc: 0.9841 - val_loss: 1.0388 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1784 - acc: 0.9841 - val_loss: 1.0357 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2065 - acc: 0.9841 - val_loss: 1.0335 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2465 - acc: 0.9206 - val_loss: 1.0332 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2287 - acc: 0.9524 - val_loss: 1.0333 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3053 - acc: 0.9365 - val_loss: 1.0369 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3267 - acc: 0.9365 - val_loss: 1.0412 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1965 - acc: 0.9683 - val_loss: 1.0396 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2453 - acc: 0.9524 - val_loss: 1.0369 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3126 - acc: 0.9365 - val_loss: 1.0375 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2060 - acc: 0.9841 - val_loss: 1.0347 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1441 - acc: 1.0000 - val_loss: 1.0299 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2061 - acc: 0.9683 - val_loss: 1.0280 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2743 - acc: 0.9048 - val_loss: 1.0301 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2927 - acc: 0.9524 - val_loss: 1.0330 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1870 - acc: 1.0000 - val_loss: 1.0328 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1334 - acc: 1.0000 - val_loss: 1.0276 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1802 - acc: 1.0000 - val_loss: 1.0233 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1761 - acc: 0.9683 - val_loss: 1.0218 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2223 - acc: 0.9524 - val_loss: 1.0186 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2246 - acc: 0.9683 - val_loss: 1.0192 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1735 - acc: 0.9841 - val_loss: 1.0179 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1717 - acc: 0.9683 - val_loss: 1.0172 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1709 - acc: 0.9524 - val_loss: 1.0134 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1701 - acc: 0.9841 - val_loss: 1.0110 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1932 - acc: 0.9524 - val_loss: 1.0125 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3007 - acc: 0.9365 - val_loss: 1.0144 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1726 - acc: 0.9841 - val_loss: 1.0153 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2011 - acc: 0.9683 - val_loss: 1.0139 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1881 - acc: 0.9841 - val_loss: 1.0133 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2456 - acc: 0.9365 - val_loss: 1.0127 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1337 - acc: 1.0000 - val_loss: 1.0100 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1693 - acc: 0.9683 - val_loss: 1.0069 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1653 - acc: 0.9365 - val_loss: 1.0071 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2001 - acc: 0.9524 - val_loss: 1.0076 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1497 - acc: 1.0000 - val_loss: 1.0069 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1921 - acc: 1.0000 - val_loss: 1.0061 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2132 - acc: 0.9524 - val_loss: 1.0052 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1590 - acc: 1.0000 - val_loss: 1.0033 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1342 - acc: 0.9683 - val_loss: 1.0003 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1321 - acc: 1.0000 - val_loss: 0.9981 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1897 - acc: 0.9524 - val_loss: 0.9969 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1692 - acc: 0.9524 - val_loss: 0.9987 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1504 - acc: 1.0000 - val_loss: 0.9996 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1584 - acc: 0.9841 - val_loss: 0.9969 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1689 - acc: 0.9683 - val_loss: 0.9976 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1916 - acc: 0.9683 - val_loss: 0.9967 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1230 - acc: 0.9683 - val_loss: 0.9941 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2044 - acc: 0.9683 - val_loss: 0.9906 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1820 - acc: 0.9683 - val_loss: 0.9905 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1819 - acc: 0.9841 - val_loss: 0.9894 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2134 - acc: 0.9524 - val_loss: 0.9891 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1529 - acc: 0.9524 - val_loss: 0.9909 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1432 - acc: 0.9683 - val_loss: 0.9906 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1709 - acc: 0.9683 - val_loss: 0.9911 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1054 - acc: 1.0000 - val_loss: 0.9880 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0861 - acc: 1.0000 - val_loss: 0.9811 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1749 - acc: 0.9524 - val_loss: 0.9775 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1576 - acc: 0.9841 - val_loss: 0.9793 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1857 - acc: 0.9841 - val_loss: 0.9804 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2039 - acc: 0.9683 - val_loss: 0.9814 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1265 - acc: 0.9683 - val_loss: 0.9817 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1643 - acc: 0.9683 - val_loss: 0.9813 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1042 - acc: 0.9841 - val_loss: 0.9783 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1046 - acc: 0.9524 - val_loss: 0.9731 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1537 - acc: 0.9683 - val_loss: 0.9720 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1148 - acc: 0.9841 - val_loss: 0.9717 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1036 - acc: 0.9841 - val_loss: 0.9710 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1497 - acc: 1.0000 - val_loss: 0.9679 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1703 - acc: 0.9524 - val_loss: 0.9699 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0786 - acc: 0.9841 - val_loss: 0.9704 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1384 - acc: 0.9365 - val_loss: 0.9677 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0700 - acc: 1.0000 - val_loss: 0.9643 - val_acc: 1.0000\n",
      "Epoch 361/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2206 - acc: 0.9683 - val_loss: 0.9663 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1117 - acc: 1.0000 - val_loss: 0.9655 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1944 - acc: 0.9365 - val_loss: 0.9651 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1104 - acc: 0.9524 - val_loss: 0.9688 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1259 - acc: 1.0000 - val_loss: 0.9692 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1318 - acc: 0.9683 - val_loss: 0.9676 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1654 - acc: 0.9365 - val_loss: 0.9659 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0738 - acc: 0.9841 - val_loss: 0.9611 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0805 - acc: 1.0000 - val_loss: 0.9572 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1014 - acc: 1.0000 - val_loss: 0.9539 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1287 - acc: 0.9524 - val_loss: 0.9567 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1080 - acc: 0.9841 - val_loss: 0.9597 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1269 - acc: 0.9841 - val_loss: 0.9584 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1070 - acc: 0.9841 - val_loss: 0.9561 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1208 - acc: 0.9841 - val_loss: 0.9557 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0589 - acc: 0.9841 - val_loss: 0.9537 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2117 - acc: 0.9683 - val_loss: 0.9560 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0916 - acc: 1.0000 - val_loss: 0.9575 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0476 - acc: 1.0000 - val_loss: 0.9548 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1905 - acc: 0.9365 - val_loss: 0.9553 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1985 - acc: 0.9048 - val_loss: 0.9637 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1129 - acc: 0.9841 - val_loss: 0.9672 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1888 - acc: 0.9365 - val_loss: 0.9687 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0956 - acc: 0.9683 - val_loss: 0.9664 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0671 - acc: 0.9841 - val_loss: 0.9629 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1350 - acc: 0.9683 - val_loss: 0.9592 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1693 - acc: 0.9683 - val_loss: 0.9588 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1220 - acc: 0.9683 - val_loss: 0.9576 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0728 - acc: 1.0000 - val_loss: 0.9554 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0754 - acc: 0.9841 - val_loss: 0.9513 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0931 - acc: 0.9841 - val_loss: 0.9482 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0477 - acc: 1.0000 - val_loss: 0.9455 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1542 - acc: 0.9365 - val_loss: 0.9419 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0933 - acc: 0.9683 - val_loss: 0.9439 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1791 - acc: 0.9365 - val_loss: 0.9465 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1516 - acc: 0.9683 - val_loss: 0.9501 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1312 - acc: 0.9683 - val_loss: 0.9494 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0804 - acc: 0.9841 - val_loss: 0.9457 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1618 - acc: 0.9683 - val_loss: 0.9447 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0953 - acc: 0.9683 - val_loss: 0.9437 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0688 - acc: 0.9841 - val_loss: 0.9389 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1013 - acc: 0.9841 - val_loss: 0.9383 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0942 - acc: 0.9841 - val_loss: 0.9385 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1763 - acc: 0.9683 - val_loss: 0.9388 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1095 - acc: 0.9841 - val_loss: 0.9403 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0728 - acc: 0.9841 - val_loss: 0.9377 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1051 - acc: 0.9524 - val_loss: 0.9362 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0724 - acc: 0.9841 - val_loss: 0.9349 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1060 - acc: 1.0000 - val_loss: 0.9347 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0710 - acc: 0.9841 - val_loss: 0.9340 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0421 - acc: 0.9683 - val_loss: 0.9309 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0950 - acc: 1.0000 - val_loss: 0.9282 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0449 - acc: 0.9841 - val_loss: 0.9268 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0693 - acc: 0.9524 - val_loss: 0.9267 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0904 - acc: 0.9841 - val_loss: 0.9279 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1148 - acc: 0.9524 - val_loss: 0.9274 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1114 - acc: 0.9524 - val_loss: 0.9276 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0229 - acc: 1.0000 - val_loss: 0.9270 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0424 - acc: 0.9841 - val_loss: 0.9232 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0865 - acc: 0.9683 - val_loss: 0.9255 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1987 - acc: 0.9048 - val_loss: 0.9305 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0431 - acc: 0.9683 - val_loss: 0.9292 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9919 - acc: 1.0000 - val_loss: 0.9251 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0700 - acc: 0.9841 - val_loss: 0.9192 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0230 - acc: 1.0000 - val_loss: 0.9155 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0546 - acc: 0.9524 - val_loss: 0.9136 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0123 - acc: 0.9841 - val_loss: 0.9117 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1421 - acc: 0.9365 - val_loss: 0.9143 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0389 - acc: 1.0000 - val_loss: 0.9151 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0528 - acc: 0.9683 - val_loss: 0.9141 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0566 - acc: 0.9524 - val_loss: 0.9128 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1023 - acc: 0.9365 - val_loss: 0.9148 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0325 - acc: 0.9841 - val_loss: 0.9118 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0269 - acc: 1.0000 - val_loss: 0.9080 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1114 - acc: 0.9365 - val_loss: 0.9071 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0272 - acc: 0.9841 - val_loss: 0.9090 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1492 - acc: 0.9206 - val_loss: 0.9119 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9819 - acc: 1.0000 - val_loss: 0.9114 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0462 - acc: 0.9683 - val_loss: 0.9088 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0115 - acc: 1.0000 - val_loss: 0.9048 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0250 - acc: 1.0000 - val_loss: 0.9014 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9841 - acc: 1.0000 - val_loss: 0.8968 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0390 - acc: 0.9683 - val_loss: 0.8967 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1539 - acc: 0.9365 - val_loss: 0.9017 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0603 - acc: 0.9841 - val_loss: 0.9059 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0488 - acc: 0.9841 - val_loss: 0.9058 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9908 - acc: 1.0000 - val_loss: 0.9037 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0034 - acc: 1.0000 - val_loss: 0.9015 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0348 - acc: 0.9841 - val_loss: 0.8997 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0481 - acc: 0.9841 - val_loss: 0.8998 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0250 - acc: 0.9841 - val_loss: 0.8994 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0978 - acc: 0.9365 - val_loss: 0.9026 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0603 - acc: 0.9841 - val_loss: 0.9052 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0848 - acc: 0.9524 - val_loss: 0.9052 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0756 - acc: 0.9683 - val_loss: 0.9042 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1236 - acc: 0.9683 - val_loss: 0.9075 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0733 - acc: 0.9524 - val_loss: 0.9115 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0385 - acc: 0.9841 - val_loss: 0.9110 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0322 - acc: 1.0000 - val_loss: 0.9074 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1325 - acc: 0.9524 - val_loss: 0.9035 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0191 - acc: 1.0000 - val_loss: 0.9014 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0773 - acc: 0.9365 - val_loss: 0.8984 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0874 - acc: 0.9683 - val_loss: 0.8986 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00463: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0866 - acc: 0.9524 - val_loss: 0.8985 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9679 - acc: 1.0000 - val_loss: 0.8953 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1053 - acc: 0.9524 - val_loss: 0.8919 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0790 - acc: 0.9683 - val_loss: 0.8895 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0077 - acc: 0.9841 - val_loss: 0.8873 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0445 - acc: 0.9683 - val_loss: 0.8855 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0475 - acc: 0.9841 - val_loss: 0.8845 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9842 - acc: 0.9683 - val_loss: 0.8830 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9766 - acc: 0.9841 - val_loss: 0.8800 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9662 - acc: 1.0000 - val_loss: 0.8767 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0147 - acc: 0.9841 - val_loss: 0.8742 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0211 - acc: 0.9841 - val_loss: 0.8738 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9721 - acc: 1.0000 - val_loss: 0.8731 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9692 - acc: 1.0000 - val_loss: 0.8703 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9885 - acc: 1.0000 - val_loss: 0.8673 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0381 - acc: 0.9841 - val_loss: 0.8663 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0492 - acc: 0.9841 - val_loss: 0.8677 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0112 - acc: 0.9683 - val_loss: 0.8690 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9770 - acc: 0.9841 - val_loss: 0.8694 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0272 - acc: 0.9524 - val_loss: 0.8685 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9880 - acc: 0.9841 - val_loss: 0.8676 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9781 - acc: 0.9841 - val_loss: 0.8658 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9736 - acc: 0.9841 - val_loss: 0.8637 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0432 - acc: 0.9841 - val_loss: 0.8630 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0780 - acc: 0.9524 - val_loss: 0.8646 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9783 - acc: 1.0000 - val_loss: 0.8658 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9907 - acc: 1.0000 - val_loss: 0.8645 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9296 - acc: 1.0000 - val_loss: 0.8619 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9302 - acc: 1.0000 - val_loss: 0.8586 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9770 - acc: 1.0000 - val_loss: 0.8555 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9685 - acc: 0.9841 - val_loss: 0.8546 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0198 - acc: 0.9683 - val_loss: 0.8552 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9877 - acc: 0.9683 - val_loss: 0.8551 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0012 - acc: 0.9841 - val_loss: 0.8556 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9948 - acc: 0.9841 - val_loss: 0.8556 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0285 - acc: 0.9683 - val_loss: 0.8554 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9839 - acc: 0.9841 - val_loss: 0.8560 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "my_model_neu_words Test Accuracy: 0.902\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "neu_wo = Sequential()\n",
    "neu_wo.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "\n",
    "neu_wo.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "neu_wo.summary()\n",
    "history = neu_wo.fit(X_scaled_train_data_words, y_train,\n",
    "                    validation_data=(X_scaled_val_data_words, y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu_words,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu_words.h5')\n",
    "yhat = l_model.predict( scaled_test_data_words)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu_words Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "n_grams (InputLayer)            (None, 4152)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "words (InputLayer)              (None, 7623)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 64)           139168      n_grams[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 64)           250240      words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           sequential_12[1][0]              \n",
      "                                                                 sequential_13[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 128)          16512       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 128)          0           dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 9)            1161        dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 407,081\n",
      "Trainable params: 407,081\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(63, 4152) (63, 7623) (63, 9)\n",
      "Train on 63 samples, validate on 756 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 3s 47ms/step - loss: 11.1220 - acc: 0.1270 - val_loss: 10.9888 - val_acc: 0.1111\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 10.9049 - acc: 0.1111 - val_loss: 10.7905 - val_acc: 0.1151\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 10.7361 - acc: 0.0952 - val_loss: 10.6087 - val_acc: 0.1296\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 10.5066 - acc: 0.1587 - val_loss: 10.4339 - val_acc: 0.1429\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 10.3452 - acc: 0.1429 - val_loss: 10.2643 - val_acc: 0.1310\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 10.2051 - acc: 0.1587 - val_loss: 10.1016 - val_acc: 0.1415\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 10.0419 - acc: 0.1746 - val_loss: 9.9423 - val_acc: 0.1693\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 9.8945 - acc: 0.0794 - val_loss: 9.7862 - val_acc: 0.1892\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 9.7126 - acc: 0.1111 - val_loss: 9.6335 - val_acc: 0.1944\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 9.5576 - acc: 0.2381 - val_loss: 9.4857 - val_acc: 0.2222\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 9.4278 - acc: 0.2222 - val_loss: 9.3423 - val_acc: 0.2857\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 9.2611 - acc: 0.2698 - val_loss: 9.1977 - val_acc: 0.2963\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 9.1701 - acc: 0.1587 - val_loss: 9.0582 - val_acc: 0.3175\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 8.9984 - acc: 0.2381 - val_loss: 8.9196 - val_acc: 0.3135\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 8.9177 - acc: 0.2540 - val_loss: 8.7859 - val_acc: 0.3333\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 8.7610 - acc: 0.1905 - val_loss: 8.6573 - val_acc: 0.3413\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 8.6105 - acc: 0.1905 - val_loss: 8.5299 - val_acc: 0.3638\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 8.5191 - acc: 0.2222 - val_loss: 8.4029 - val_acc: 0.3823\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 8.3574 - acc: 0.2857 - val_loss: 8.2753 - val_acc: 0.4220\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 8.2513 - acc: 0.2698 - val_loss: 8.1539 - val_acc: 0.4881\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 8.2054 - acc: 0.2222 - val_loss: 8.0382 - val_acc: 0.5159\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 8.0442 - acc: 0.3175 - val_loss: 7.9211 - val_acc: 0.5450\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 7.9690 - acc: 0.3175 - val_loss: 7.8070 - val_acc: 0.5661\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 7.8233 - acc: 0.2857 - val_loss: 7.6933 - val_acc: 0.6032\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 7.7954 - acc: 0.3333 - val_loss: 7.5878 - val_acc: 0.6296\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 7.5797 - acc: 0.3968 - val_loss: 7.4858 - val_acc: 0.6759\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 7.5655 - acc: 0.2857 - val_loss: 7.3812 - val_acc: 0.7513\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 7.4737 - acc: 0.3651 - val_loss: 7.2774 - val_acc: 0.8095\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 7.3744 - acc: 0.3810 - val_loss: 7.1729 - val_acc: 0.8624\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 7.2144 - acc: 0.3968 - val_loss: 7.0711 - val_acc: 0.8862\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 7.1687 - acc: 0.3968 - val_loss: 6.9681 - val_acc: 0.8876\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 7.0913 - acc: 0.5397 - val_loss: 6.8623 - val_acc: 0.8889\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.9830 - acc: 0.4603 - val_loss: 6.7631 - val_acc: 0.8849\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.8745 - acc: 0.5397 - val_loss: 6.6568 - val_acc: 0.8862\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.8501 - acc: 0.5397 - val_loss: 6.5546 - val_acc: 0.8889\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.7461 - acc: 0.5397 - val_loss: 6.4557 - val_acc: 0.8889\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.6793 - acc: 0.5556 - val_loss: 6.3632 - val_acc: 0.8889\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.5160 - acc: 0.6349 - val_loss: 6.2662 - val_acc: 0.8889\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.5463 - acc: 0.5238 - val_loss: 6.1734 - val_acc: 0.8889\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.4048 - acc: 0.6349 - val_loss: 6.0844 - val_acc: 0.8889\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.3086 - acc: 0.5873 - val_loss: 5.9925 - val_acc: 0.8889\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.1580 - acc: 0.6190 - val_loss: 5.8977 - val_acc: 0.8889\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.2152 - acc: 0.6032 - val_loss: 5.8055 - val_acc: 0.8915\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.1183 - acc: 0.6349 - val_loss: 5.7209 - val_acc: 0.8889\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 6.0866 - acc: 0.6190 - val_loss: 5.6498 - val_acc: 0.8902\n",
      "Epoch 46/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 6ms/step - loss: 5.9306 - acc: 0.7460 - val_loss: 5.5691 - val_acc: 0.8968\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.7937 - acc: 0.7778 - val_loss: 5.4872 - val_acc: 0.9034\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.7236 - acc: 0.7619 - val_loss: 5.4017 - val_acc: 0.9034\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.6883 - acc: 0.7619 - val_loss: 5.3246 - val_acc: 0.9087\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.5955 - acc: 0.6825 - val_loss: 5.2473 - val_acc: 0.9405\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.5795 - acc: 0.7143 - val_loss: 5.1763 - val_acc: 0.9603\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.5800 - acc: 0.6825 - val_loss: 5.1095 - val_acc: 0.9497\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.4642 - acc: 0.7619 - val_loss: 5.0454 - val_acc: 0.9616\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.3806 - acc: 0.7778 - val_loss: 4.9815 - val_acc: 0.9537\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.4650 - acc: 0.7143 - val_loss: 4.9191 - val_acc: 0.9643\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.2331 - acc: 0.8571 - val_loss: 4.8561 - val_acc: 0.9987\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.2142 - acc: 0.8413 - val_loss: 4.7954 - val_acc: 0.9987\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.1047 - acc: 0.8254 - val_loss: 4.7337 - val_acc: 0.9987\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.1889 - acc: 0.7302 - val_loss: 4.6814 - val_acc: 0.9987\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.0867 - acc: 0.7937 - val_loss: 4.6383 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.9867 - acc: 0.7778 - val_loss: 4.5874 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.0204 - acc: 0.8095 - val_loss: 4.5303 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.9869 - acc: 0.7778 - val_loss: 4.4897 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.8196 - acc: 0.8254 - val_loss: 4.4496 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.8069 - acc: 0.8571 - val_loss: 4.4052 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.8043 - acc: 0.8413 - val_loss: 4.3627 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.6892 - acc: 0.8413 - val_loss: 4.3215 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.6177 - acc: 0.9048 - val_loss: 4.2747 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.5502 - acc: 0.9048 - val_loss: 4.2287 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.6137 - acc: 0.8571 - val_loss: 4.1897 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.5222 - acc: 0.8889 - val_loss: 4.1602 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.5200 - acc: 0.8730 - val_loss: 4.1300 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.5706 - acc: 0.8571 - val_loss: 4.0975 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.3786 - acc: 0.9206 - val_loss: 4.0638 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.3404 - acc: 0.9048 - val_loss: 4.0259 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.3555 - acc: 0.8730 - val_loss: 3.9908 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.3770 - acc: 0.8413 - val_loss: 3.9723 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.2466 - acc: 0.8889 - val_loss: 3.9455 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.3038 - acc: 0.8095 - val_loss: 3.9103 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.2876 - acc: 0.8413 - val_loss: 3.8859 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.2176 - acc: 0.8730 - val_loss: 3.8636 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.2613 - acc: 0.8413 - val_loss: 3.8416 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.0795 - acc: 0.9365 - val_loss: 3.8129 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.1150 - acc: 0.9048 - val_loss: 3.7809 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.1119 - acc: 0.9206 - val_loss: 3.7530 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.0998 - acc: 0.8571 - val_loss: 3.7277 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.1721 - acc: 0.8254 - val_loss: 3.7106 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 4.0114 - acc: 0.8889 - val_loss: 3.6890 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.8999 - acc: 0.9206 - val_loss: 3.6603 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.9749 - acc: 0.8571 - val_loss: 3.6353 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.9806 - acc: 0.9206 - val_loss: 3.6159 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.8180 - acc: 0.9524 - val_loss: 3.5921 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.9667 - acc: 0.8889 - val_loss: 3.5682 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.9286 - acc: 0.8571 - val_loss: 3.5523 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.8071 - acc: 0.9524 - val_loss: 3.5340 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.7717 - acc: 0.9365 - val_loss: 3.5118 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.6901 - acc: 0.9365 - val_loss: 3.4883 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.8842 - acc: 0.8730 - val_loss: 3.4760 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.7159 - acc: 0.9683 - val_loss: 3.4612 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.7827 - acc: 0.8889 - val_loss: 3.4431 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.7781 - acc: 0.9048 - val_loss: 3.4230 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.7378 - acc: 0.9206 - val_loss: 3.4009 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.6365 - acc: 0.9206 - val_loss: 3.3790 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.6511 - acc: 0.9206 - val_loss: 3.3598 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.6541 - acc: 0.9365 - val_loss: 3.3428 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.7378 - acc: 0.8571 - val_loss: 3.3299 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.5763 - acc: 0.9841 - val_loss: 3.3162 - val_acc: 1.0000\n",
      "Epoch 108/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 6ms/step - loss: 3.5732 - acc: 0.9365 - val_loss: 3.2996 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.4922 - acc: 0.9841 - val_loss: 3.2808 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.4557 - acc: 0.9683 - val_loss: 3.2591 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.5380 - acc: 0.9206 - val_loss: 3.2453 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.6174 - acc: 0.8730 - val_loss: 3.2429 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.5340 - acc: 0.9048 - val_loss: 3.2329 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.5364 - acc: 0.9206 - val_loss: 3.2142 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.4932 - acc: 0.9048 - val_loss: 3.1967 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.4667 - acc: 0.9206 - val_loss: 3.1817 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.3675 - acc: 0.9524 - val_loss: 3.1627 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.3788 - acc: 0.9365 - val_loss: 3.1429 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.4832 - acc: 0.9048 - val_loss: 3.1320 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.4093 - acc: 0.9206 - val_loss: 3.1247 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.4503 - acc: 0.9365 - val_loss: 3.1166 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.4124 - acc: 0.9048 - val_loss: 3.1023 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.2480 - acc: 0.9841 - val_loss: 3.0870 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.2833 - acc: 1.0000 - val_loss: 3.0714 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.2613 - acc: 0.9841 - val_loss: 3.0581 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.3007 - acc: 0.9365 - val_loss: 3.0529 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.3619 - acc: 0.9365 - val_loss: 3.0430 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.2549 - acc: 0.9683 - val_loss: 3.0295 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.2490 - acc: 0.9841 - val_loss: 3.0139 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.3256 - acc: 0.9365 - val_loss: 3.0001 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.2048 - acc: 0.9841 - val_loss: 2.9874 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.2302 - acc: 0.9365 - val_loss: 2.9724 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.2001 - acc: 0.9365 - val_loss: 2.9606 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.2038 - acc: 0.9524 - val_loss: 2.9512 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.1558 - acc: 0.9683 - val_loss: 2.9436 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.1452 - acc: 0.9524 - val_loss: 2.9330 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.0929 - acc: 0.9841 - val_loss: 2.9191 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.1970 - acc: 0.9048 - val_loss: 2.9098 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.1377 - acc: 0.9683 - val_loss: 2.8968 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.2428 - acc: 0.9206 - val_loss: 2.8916 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.0485 - acc: 0.9683 - val_loss: 2.8799 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.0497 - acc: 0.9683 - val_loss: 2.8677 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.1081 - acc: 0.9841 - val_loss: 2.8532 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.1094 - acc: 0.9365 - val_loss: 2.8460 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.0724 - acc: 0.9365 - val_loss: 2.8395 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.0320 - acc: 0.9841 - val_loss: 2.8279 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.0427 - acc: 0.9524 - val_loss: 2.8199 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.0248 - acc: 0.9683 - val_loss: 2.8124 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9478 - acc: 0.9841 - val_loss: 2.7983 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9947 - acc: 0.9524 - val_loss: 2.7865 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9938 - acc: 0.9841 - val_loss: 2.7794 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.0399 - acc: 0.9048 - val_loss: 2.7755 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 3.0188 - acc: 0.9683 - val_loss: 2.7721 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9311 - acc: 0.9841 - val_loss: 2.7586 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9220 - acc: 1.0000 - val_loss: 2.7436 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9283 - acc: 0.9841 - val_loss: 2.7343 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9308 - acc: 0.9524 - val_loss: 2.7249 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9123 - acc: 0.9683 - val_loss: 2.7203 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8910 - acc: 0.9683 - val_loss: 2.7122 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9308 - acc: 0.9524 - val_loss: 2.7052 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9364 - acc: 0.9524 - val_loss: 2.6958 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8736 - acc: 0.9524 - val_loss: 2.6899 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8731 - acc: 0.9683 - val_loss: 2.6809 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8438 - acc: 0.9841 - val_loss: 2.6695 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8457 - acc: 1.0000 - val_loss: 2.6644 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8415 - acc: 0.9524 - val_loss: 2.6572 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8777 - acc: 0.9365 - val_loss: 2.6498 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8342 - acc: 0.9524 - val_loss: 2.6375 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8043 - acc: 0.9841 - val_loss: 2.6251 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8795 - acc: 0.9524 - val_loss: 2.6212 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7265 - acc: 1.0000 - val_loss: 2.6151 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7944 - acc: 0.9683 - val_loss: 2.6047 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8044 - acc: 0.9683 - val_loss: 2.5979 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8285 - acc: 0.9365 - val_loss: 2.5902 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7468 - acc: 0.9841 - val_loss: 2.5846 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7245 - acc: 0.9683 - val_loss: 2.5756 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7227 - acc: 0.9841 - val_loss: 2.5647 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7186 - acc: 1.0000 - val_loss: 2.5543 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6940 - acc: 0.9841 - val_loss: 2.5454 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7051 - acc: 0.9683 - val_loss: 2.5390 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6999 - acc: 0.9365 - val_loss: 2.5345 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6887 - acc: 0.9683 - val_loss: 2.5300 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7561 - acc: 0.9524 - val_loss: 2.5303 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7240 - acc: 0.9524 - val_loss: 2.5298 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7019 - acc: 1.0000 - val_loss: 2.5241 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7051 - acc: 0.9841 - val_loss: 2.5131 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7038 - acc: 0.9365 - val_loss: 2.5086 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7233 - acc: 0.9365 - val_loss: 2.5016 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6264 - acc: 1.0000 - val_loss: 2.4923 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6648 - acc: 0.9683 - val_loss: 2.4796 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6722 - acc: 0.9524 - val_loss: 2.4721 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.8004 - acc: 0.8889 - val_loss: 2.4766 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6353 - acc: 1.0000 - val_loss: 2.4782 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7512 - acc: 0.9206 - val_loss: 2.4773 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6482 - acc: 0.9841 - val_loss: 2.4741 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6914 - acc: 0.9365 - val_loss: 2.4665 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5825 - acc: 1.0000 - val_loss: 2.4515 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6246 - acc: 0.9683 - val_loss: 2.4423 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6064 - acc: 0.9841 - val_loss: 2.4369 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6389 - acc: 0.9841 - val_loss: 2.4296 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5779 - acc: 0.9841 - val_loss: 2.4229 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5585 - acc: 0.9841 - val_loss: 2.4133 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4882 - acc: 1.0000 - val_loss: 2.3965 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5759 - acc: 0.9206 - val_loss: 2.3848 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5165 - acc: 0.9841 - val_loss: 2.3798 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5553 - acc: 0.9365 - val_loss: 2.3758 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4867 - acc: 1.0000 - val_loss: 2.3699 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5437 - acc: 0.9683 - val_loss: 2.3627 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5595 - acc: 0.9683 - val_loss: 2.3626 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5580 - acc: 0.9841 - val_loss: 2.3639 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5179 - acc: 0.9841 - val_loss: 2.3613 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5015 - acc: 0.9683 - val_loss: 2.3516 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5431 - acc: 0.9365 - val_loss: 2.3455 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4998 - acc: 0.9683 - val_loss: 2.3422 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5744 - acc: 0.9365 - val_loss: 2.3400 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5130 - acc: 0.9683 - val_loss: 2.3391 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4733 - acc: 1.0000 - val_loss: 2.3286 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4805 - acc: 0.9841 - val_loss: 2.3169 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4441 - acc: 1.0000 - val_loss: 2.3041 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4437 - acc: 0.9841 - val_loss: 2.2987 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4710 - acc: 0.9683 - val_loss: 2.3008 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4630 - acc: 0.9683 - val_loss: 2.2942 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4348 - acc: 0.9841 - val_loss: 2.2884 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4896 - acc: 0.9365 - val_loss: 2.2828 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3781 - acc: 1.0000 - val_loss: 2.2778 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4387 - acc: 0.9524 - val_loss: 2.2694 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4080 - acc: 0.9524 - val_loss: 2.2740 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4119 - acc: 0.9841 - val_loss: 2.2670 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4237 - acc: 0.9524 - val_loss: 2.2613 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3918 - acc: 0.9841 - val_loss: 2.2515 - val_acc: 1.0000\n",
      "Epoch 231/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3996 - acc: 0.9841 - val_loss: 2.2438 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3930 - acc: 0.9841 - val_loss: 2.2397 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3815 - acc: 0.9683 - val_loss: 2.2339 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3239 - acc: 1.0000 - val_loss: 2.2237 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4295 - acc: 0.9365 - val_loss: 2.2240 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4280 - acc: 0.9524 - val_loss: 2.2293 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4047 - acc: 0.9683 - val_loss: 2.2328 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.4049 - acc: 0.9683 - val_loss: 2.2283 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3771 - acc: 0.9841 - val_loss: 2.2220 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3896 - acc: 0.9683 - val_loss: 2.2153 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3856 - acc: 0.9683 - val_loss: 2.2102 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3534 - acc: 0.9841 - val_loss: 2.2034 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3495 - acc: 0.9841 - val_loss: 2.2011 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3130 - acc: 1.0000 - val_loss: 2.1919 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3092 - acc: 0.9841 - val_loss: 2.1833 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2923 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3812 - acc: 0.9524 - val_loss: 2.1720 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2634 - acc: 1.0000 - val_loss: 2.1680 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3859 - acc: 0.9365 - val_loss: 2.1616 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2760 - acc: 0.9841 - val_loss: 2.1549 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3548 - acc: 0.9524 - val_loss: 2.1550 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2604 - acc: 0.9683 - val_loss: 2.1513 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2845 - acc: 0.9841 - val_loss: 2.1388 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2437 - acc: 0.9683 - val_loss: 2.1307 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2901 - acc: 0.9841 - val_loss: 2.1274 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2579 - acc: 0.9524 - val_loss: 2.1273 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2655 - acc: 0.9841 - val_loss: 2.1216 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2111 - acc: 0.9841 - val_loss: 2.1135 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2291 - acc: 0.9683 - val_loss: 2.1072 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2303 - acc: 1.0000 - val_loss: 2.1014 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1996 - acc: 1.0000 - val_loss: 2.0994 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2137 - acc: 0.9841 - val_loss: 2.0994 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2372 - acc: 0.9841 - val_loss: 2.0902 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2937 - acc: 0.9524 - val_loss: 2.0936 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2274 - acc: 0.9841 - val_loss: 2.0979 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2069 - acc: 0.9841 - val_loss: 2.0901 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2155 - acc: 0.9841 - val_loss: 2.0825 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2080 - acc: 0.9683 - val_loss: 2.0741 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1469 - acc: 1.0000 - val_loss: 2.0665 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2431 - acc: 0.9683 - val_loss: 2.0616 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1483 - acc: 0.9841 - val_loss: 2.0576 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1889 - acc: 0.9841 - val_loss: 2.0516 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2552 - acc: 0.9683 - val_loss: 2.0527 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2060 - acc: 0.9365 - val_loss: 2.0530 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1540 - acc: 0.9683 - val_loss: 2.0521 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2081 - acc: 0.9683 - val_loss: 2.0515 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2006 - acc: 1.0000 - val_loss: 2.0487 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1854 - acc: 0.9683 - val_loss: 2.0405 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2147 - acc: 0.9841 - val_loss: 2.0367 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1038 - acc: 0.9841 - val_loss: 2.0338 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1715 - acc: 0.9683 - val_loss: 2.0265 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2244 - acc: 0.9524 - val_loss: 2.0220 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1351 - acc: 0.9841 - val_loss: 2.0208 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2230 - acc: 0.9524 - val_loss: 2.0194 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1116 - acc: 0.9841 - val_loss: 2.0140 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0965 - acc: 1.0000 - val_loss: 2.0044 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0977 - acc: 1.0000 - val_loss: 1.9956 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1155 - acc: 0.9683 - val_loss: 1.9910 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1413 - acc: 0.9524 - val_loss: 1.9850 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0777 - acc: 0.9841 - val_loss: 1.9806 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1545 - acc: 0.9683 - val_loss: 1.9783 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0788 - acc: 0.9683 - val_loss: 1.9761 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1952 - acc: 0.9206 - val_loss: 1.9861 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1145 - acc: 1.0000 - val_loss: 1.9881 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1992 - acc: 0.9365 - val_loss: 1.9847 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1139 - acc: 0.9841 - val_loss: 1.9825 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1149 - acc: 0.9841 - val_loss: 1.9800 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0445 - acc: 1.0000 - val_loss: 1.9695 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0732 - acc: 1.0000 - val_loss: 1.9590 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0159 - acc: 1.0000 - val_loss: 1.9472 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0194 - acc: 1.0000 - val_loss: 1.9351 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0495 - acc: 0.9841 - val_loss: 1.9276 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0835 - acc: 0.9524 - val_loss: 1.9292 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0439 - acc: 1.0000 - val_loss: 1.9296 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0324 - acc: 1.0000 - val_loss: 1.9233 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0016 - acc: 0.9841 - val_loss: 1.9159 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0005 - acc: 0.9841 - val_loss: 1.9083 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.9714 - acc: 1.0000 - val_loss: 1.9014 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0365 - acc: 0.9841 - val_loss: 1.8993 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0031 - acc: 0.9841 - val_loss: 1.8998 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0191 - acc: 0.9841 - val_loss: 1.8967 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      " 7/63 [==>...........................] - ETA: 0s - loss: 1.9802 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_input_tensor = Input(shape=(scaled_train_data_words.shape[1],) , name='words')\n",
    "ngram_input_tensor = Input(shape=(scaled_train_data_ngrams.shape[1],) , name='n_grams')\n",
    "\n",
    "neu0 = Sequential()\n",
    "neu0.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "\n",
    "neu1 = Sequential()\n",
    "neu1.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "\n",
    "output_tensor_0 = neu0(ngram_input_tensor)\n",
    "output_tensor_1 = neu1(word_input_tensor)\n",
    "\n",
    "\n",
    "# conv_input_tensor = Input(shape=(maxlen,) , name='convnets')\n",
    "\n",
    "# conv_1d_s3_model = Sequential()\n",
    "# conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_s3_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_0 = conv_1d_s3_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "# conv_1d_s1_model = Sequential()\n",
    "# conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_s1_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_1 = conv_1d_s1_model(conv_input_tensor)\n",
    "\n",
    "# conv_1d_complex_model = Sequential()\n",
    "# conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "# conv_1d_complex_model.add(layers.Dropout(0.2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# conv_output_tensor_2 = conv_1d_complex_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "\n",
    "concatenated = layers.concatenate([output_tensor_0,\n",
    "                                   output_tensor_1,\n",
    "#                                    conv_output_tensor_0,\n",
    "#                                    conv_output_tensor_1,\n",
    "#                                    conv_output_tensor_2,\n",
    "                                  ], axis=-1)\n",
    "\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "concatenated = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model([ngram_input_tensor, word_input_tensor,\n",
    "#                conv_input_tensor\n",
    "              ], concatenated)\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "print(X_scaled_train_data_ngrams.shape, X_scaled_train_data_words.shape, y_train.shape) \n",
    "history = model.fit([X_scaled_train_data_ngrams, X_scaled_train_data_words, \n",
    "#                      X_train\n",
    "                    ], y_train,\n",
    "                    validation_data=([X_scaled_val_data_ngrams, X_scaled_val_data_words,\n",
    "#                                       X_val\n",
    "                                     ], y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_neu,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n",
    "l_model = load_model('my_model_neu.h5')\n",
    "yhat = l_model.predict([scaled_test_data_ngrams, scaled_test_data_words])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_neu Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "convnet_input_tensor = Input(shape=(maxlen,) , name='convnet_words')\n",
    "\n",
    "conv_1d_s3_model = Sequential()\n",
    "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s3_model.add(layers.Dropout(0.3))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(64, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s3_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s3_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s3_model.layers[0].trainable = False\n",
    "conv_output_tensor_0 = conv_1d_s3_model(convnet_input_tensor)\n",
    "\n",
    "\n",
    "conv_1d_s1_model = Sequential()\n",
    "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s1_model.add(layers.Dropout(0.3))\n",
    "conv_1d_s1_model.add(layers.SeparableConv1D(64, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s1_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s1_model.layers[0].trainable = False\n",
    "conv_output_tensor_1 = conv_1d_s1_model(convnet_input_tensor)\n",
    "\n",
    "conv_1d_complex_model = Sequential()\n",
    "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_complex_model.add(layers.Dropout(0.3))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(64, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_complex_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_complex_model.layers[0].trainable = False\n",
    "conv_output_tensor_2 = conv_1d_complex_model(convnet_input_tensor)\n",
    "\n",
    "# x = layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen)(word_input_tensor)\n",
    "# x = layers.Conv1D(128, 5, activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# y = layers.Conv1D(128, 10, activation='relu', padding='same')(x)\n",
    "# added = layers.add([y, x])\n",
    "# added = layers.GlobalMaxPooling1D()(added)\n",
    "\n",
    "concatenated = layers.concatenate([conv_output_tensor_0,\n",
    "                                   conv_output_tensor_1,\n",
    "                                   conv_output_tensor_2,\n",
    "#                                    ,added\n",
    "                                  ], axis=-1)\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "answer = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model(convnet_input_tensor, answer)\n",
    "model.summary()\n",
    "\n",
    "# model.layers[0].set_weights([w2d.word_embedding])\n",
    "# model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=1000,\n",
    "                    batch_size=class_size,\n",
    "                    callbacks=callbacks_list_convnet,\n",
    "                    verbose= 2\n",
    "                   )\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "l_model = load_model('my_model_convnet.h5')\n",
    "yhat = l_model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('my_model_convnet Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(model_names_list):\n",
    "    all_models = list()\n",
    "    for model_name in model_names_list:\n",
    "        # define filename for this ensemble\n",
    "#         filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "        filename = model_name + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "\n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer.name = 'ensemble_' + str(i + 1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "#     print(ensemble_visible)\n",
    "#     ensemble_visible = [[ngram_input_tensor, word_input_tensor], convnet_input_tensor]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "#     ensemble_outputs = [concatenated, answer]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = layers.Dense(128, activation='relu')(merge)\n",
    "    hidden = layers.Dropout(0.3)(hidden)\n",
    "    output = layers.Dense(len(set(train_labels)), activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    # plot graph of ensemble\n",
    "#     plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=3e-4), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy, valX, valy):\n",
    "    # prepare input data\n",
    "#     X = [inputX for _ in range(len(model.input))]\n",
    "    # encode output data\n",
    "#     inputy_enc = to_categorical(inputy)\n",
    "    # fit model\n",
    "    model.fit(inputX, inputy, validation_data=(valX, valy), batch_size=class_size,\n",
    "              callbacks=callbacks_list_stacked, epochs=500, verbose=1)\n",
    "    \n",
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "#     X = [inputX for _ in range(len(model.input))]\n",
    "    # make prediction\n",
    "    return model.predict(inputX, verbose=0)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "members = load_all_models(['my_model_neu_ngrams', 'my_model_neu_words', 'my_model_convnet'])\n",
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, [X_scaled_train_data_ngrams, X_scaled_train_data_words, X_train], y_train, [X_scaled_val_data_ngrams, X_scaled_val_data_words, X_val], y_val)\n",
    "final_model = load_model('my_model_neu_stacked.h5')\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, [scaled_test_data_ngrams, scaled_test_data_words, test_data])\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(test_labels, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
