{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Reading general data of the problems\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Reading general data of the problems, done!\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# coding\u003dutf-8\nimport json\nimport os\n\nfrom MyUtils import clean_folder, read_files\nfrom Word2Dim import Word2Dim\n\ndataset_path \u003d \u0027.\u0027 + os.sep + \u0027pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23\u0027\noutpath \u003d \u0027.\u0027 + os.sep + \u0027dev_out\u0027\n\nclean_folder(outpath)\n\ninfocollection \u003d dataset_path + os.sep + \u0027collection-info.json\u0027\nproblems \u003d []\nlanguage \u003d []\nwith open(infocollection, \u0027r\u0027) as f:\n    for attrib in json.load(f):\n        problems.append(attrib[\u0027problem-name\u0027])\n        language.append(attrib[\u0027language\u0027])\nprint(\u0027Reading general data of the problems, done!\u0027)\n"
    },
    {
      "cell_type": "markdown",
      "source": "# Reading problem 1",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "doc count to process:  63\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "problem \u003d problems[0]\nindex \u003d 0\ninfoproblem \u003d dataset_path + os.sep + problem + os.sep + \u0027problem-info.json\u0027\ncandidates \u003d []\nwith open(infoproblem, \u0027r\u0027) as f:\n    fj \u003d json.load(f)\n    unk_folder \u003d fj[\u0027unknown-folder\u0027]\n    for attrib in fj[\u0027candidate-authors\u0027]:\n        candidates.append(attrib[\u0027author-name\u0027])\n\ncandidates.sort()\n# Building training set\ntrain_docs \u003d []\nfor candidate in candidates:\n    train_docs.extend(read_files(dataset_path + os.sep + problem, candidate))\ntrain_texts \u003d [text for i, (text, label) in enumerate(train_docs)]\ntrain_labels \u003d [label for i, (text, label) in enumerate(train_docs)]\nindex_2_label_dict \u003d {i: l for i, l in enumerate(set(train_labels))}\nlabel_2_index_dict \u003d {l: i for i, l in enumerate(set(train_labels))}\ntrain_labels \u003d sorted([label_2_index_dict[v] for v in train_labels])\nw2d \u003d Word2Dim()\ntrain_tokenized_with_pos, train_tokenized_indexed \u003d w2d.fit_transform_texts(train_texts, train_labels,\n                                                                            language[index])\n\nmaxlen \u003d len(max(train_tokenized_indexed, key\u003dlen))  # We will cut the texts after # words\nembedding_dim \u003d w2d.word_embedding.shape[1]\n\n# preparing test set\nground_truth_file \u003d dataset_path + os.sep + problem + os.sep + \u0027ground-truth.json\u0027\ngt \u003d {}\nwith open(ground_truth_file, \u0027r\u0027) as f:\n    for attrib in json.load(f)[\u0027ground_truth\u0027]:\n        gt[attrib[\u0027unknown-text\u0027]] \u003d attrib[\u0027true-author\u0027]\n\ntest_docs \u003d read_files(dataset_path + os.sep + problem, unk_folder, gt)\ntest_texts \u003d [text for i, (text, label) in enumerate(test_docs)]\ntest_labels \u003d [label for i, (text, label) in enumerate(test_docs)]\n\n# Filter validation to known authors\ntest_texts \u003d [text for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\ntest_labels \u003d [label for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n\ntest_labels \u003d sorted([label_2_index_dict[v] for v in test_labels])\n\ntest_tokenized_with_pos, test_tokenized_indexed \u003d w2d.transform(test_texts)\nprint(\"Reading problem 1, done!\")",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Keras Stuff\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\nfrom keras.layers import Embedding, Flatten, Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop\nfrom keras.utils import to_categorical\nfrom keras_preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\n# from sklearn.model_selection import train_test_split\n\ntrain_data \u003d pad_sequences(train_tokenized_indexed, maxlen\u003dmaxlen)\n\ntest_data \u003d pad_sequences(test_tokenized_indexed, maxlen\u003dmaxlen)\n\nX_train, X_val, y_train, y_val \u003d train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n# X_train, X_val, y_train, y_val \u003d train_test_split(train_data, train_labels,\n#                                                   test_size\u003d0.28, random_state\u003d2019,\n#                                                   stratify\u003dtrain_labels)\n\n# y_train \u003d to_categorical(y_train)\n# y_val \u003d to_categorical(y_val)\n\nmodel \u003d Sequential()\nmodel.add(Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length\u003dmaxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation\u003d\u0027relu\u0027))\nmodel.add(Dense(len(set(train_labels)), activation\u003d\u0027softmax\u0027))\nmodel.summary()\n\n# model.layers[0].set_weights([w2d.word_embedding])\n# model.layers[0].trainable \u003d False\n\nmodel.compile(optimizer\u003dRMSprop(lr\u003d0.001),\n              loss\u003d\u0027categorical_crossentropy\u0027,\n              metrics\u003d[\u0027acc\u0027])\nhistory \u003d model.fit(X_train, y_train,\n                    validation_data\u003d(X_val, y_val),\n                    epochs\u003d120,\n                    batch_size\u003d1)\n\nacc \u003d history.history[\u0027acc\u0027]\nval_acc \u003d history.history[\u0027val_acc\u0027]\nloss \u003d history.history[\u0027loss\u0027]\nval_loss \u003d history.history[\u0027val_loss\u0027]\n\nepochs \u003d range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, \u0027bo\u0027, label\u003d\u0027Training acc\u0027)\nplt.plot(epochs, val_acc, \u0027b\u0027, label\u003d\u0027Validation acc\u0027)\nplt.title(\u0027Training and validation accuracy\u0027)\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, \u0027bo\u0027, label\u003d\u0027Training loss\u0027)\nplt.plot(epochs, val_loss, \u0027b\u0027, label\u003d\u0027Validation loss\u0027)\nplt.title(\u0027Training and validation loss\u0027)\nplt.legend()\n\nplt.show()\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}