{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Reading general data of the problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading general data of the problems, done!\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import division\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "from MyUtils import clean_folder, read_files, shuffle_docs\n",
    "from Word2Dim import Word2Dim\n",
    "\n",
    "dataset_path = '.' + os.sep + 'pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23'\n",
    "outpath = '.' + os.sep + 'dev_out'\n",
    "\n",
    "clean_folder(outpath)\n",
    "\n",
    "infocollection = dataset_path + os.sep + 'collection-info.json'\n",
    "problems = []\n",
    "language = []\n",
    "with open(infocollection, 'r') as f:\n",
    "    for attrib in json.load(f):\n",
    "        problems.append(attrib['problem-name'])\n",
    "        language.append(attrib['language'])\n",
    "print('Reading general data of the problems, done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reading problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc count to process:  819\n",
      "Processing doc # 1\n",
      "Processing doc # 2\n",
      "Processing doc # 3\n",
      "Processing doc # 4\n",
      "Processing doc # 5\n",
      "Processing doc # 6\n",
      "Processing doc # 7\n",
      "Processing doc # 8\n",
      "Processing doc # 9\n",
      "Processing doc # 10\n",
      "Processing doc # 11\n",
      "Processing doc # 12\n",
      "Processing doc # 13\n",
      "Processing doc # 14\n",
      "Processing doc # 15\n",
      "Processing doc # 16\n",
      "Processing doc # 17\n",
      "Processing doc # 18\n",
      "Processing doc # 19\n",
      "Processing doc # 20\n",
      "Processing doc # 21\n",
      "Processing doc # 22\n",
      "Processing doc # 23\n",
      "Processing doc # 24\n",
      "Processing doc # 25\n",
      "Processing doc # 26\n",
      "Processing doc # 27\n",
      "Processing doc # 28\n",
      "Processing doc # 29\n",
      "Processing doc # 30\n",
      "Processing doc # 31\n",
      "Processing doc # 32\n",
      "Processing doc # 33\n",
      "Processing doc # 34\n",
      "Processing doc # 35\n",
      "Processing doc # 36\n",
      "Processing doc # 37\n",
      "Processing doc # 38\n",
      "Processing doc # 39\n",
      "Processing doc # 40\n",
      "Processing doc # 41\n"
     ]
    }
   ],
   "source": [
    "\n",
    "problem = problems[0]\n",
    "index = 0\n",
    "\n",
    "# used for n_gram extraction and word indexing, a threshold which prevent words appearing lower than this value to be counted in calculations\n",
    "tf = 5\n",
    "\n",
    "\n",
    "infoproblem = dataset_path + os.sep + problem + os.sep + 'problem-info.json'\n",
    "candidates = []\n",
    "with open(infoproblem, 'r') as f:\n",
    "    fj = json.load(f)\n",
    "    unk_folder = fj['unknown-folder']\n",
    "    for attrib in fj['candidate-authors']:\n",
    "        candidates.append(attrib['author-name'])\n",
    "\n",
    "candidates.sort()\n",
    "# Building training set\n",
    "train_docs = []\n",
    "for candidate in candidates:\n",
    "    train_docs.extend(read_files(dataset_path + os.sep + problem, candidate))\n",
    "train_texts = [text for i, (text, label) in enumerate(train_docs)]\n",
    "train_labels = [label for i, (text, label) in enumerate(train_docs)]\n",
    "train_texts, train_labels = shuffle_docs(train_texts, train_labels)\n",
    "index_2_label_dict = {i: l for i, l in enumerate(set(train_labels))}\n",
    "label_2_index_dict = {l: i for i, l in enumerate(set(train_labels))}\n",
    "train_labels = [label_2_index_dict[v] for v in train_labels]\n",
    "w2d = Word2Dim(lang= language[index])\n",
    "train_tokenized_with_pos, train_tokenized_indexed = w2d.fit_transform_texts(train_texts, train_labels, tf= tf)\n",
    "\n",
    "maxlen = len(max(train_tokenized_indexed, key=len))  # We will cut the texts after # words\n",
    "embedding_dim = w2d.word_embedding.shape[1]\n",
    "\n",
    "# preparing test set\n",
    "ground_truth_file = dataset_path + os.sep + problem + os.sep + 'ground-truth.json'\n",
    "gt = {}\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    for attrib in json.load(f)['ground_truth']:\n",
    "        gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "test_docs = read_files(dataset_path + os.sep + problem, unk_folder, gt)\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs)]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs)]\n",
    "\n",
    "# Filter validation to known authors\n",
    "test_texts = [text for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "test_labels = [label for i, (text, label) in enumerate(test_docs) if label in label_2_index_dict.keys()]\n",
    "\n",
    "test_labels = [label_2_index_dict[v] for v in test_labels]\n",
    "\n",
    "test_tokenized_with_pos, test_tokenized_indexed = w2d.transform(test_texts)\n",
    "print(\"Reading problem 1, done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Extraction for Neural Net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from MyUtils import extract_n_grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "n = 3\n",
    "vocabulary = extract_n_grams(train_docs, n, tf)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(n, n), lowercase=False, vocabulary=vocabulary)\n",
    "n_gram_train_data = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "n_gram_train_data = n_gram_train_data.astype(float)\n",
    "\n",
    "for i, v in enumerate(train_texts):\n",
    "    n_gram_train_data[i] = n_gram_train_data[i] / len(train_texts[i])\n",
    "n_gram_test_data = vectorizer.transform(test_texts)\n",
    "n_gram_test_data = n_gram_test_data.astype(float)\n",
    "for i, v in enumerate(test_texts):\n",
    "    n_gram_test_data[i] = n_gram_test_data[i] / len(test_texts[i])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_ngrams = max_abs_scaler.fit_transform(n_gram_train_data)\n",
    "scaled_test_data_ngrams = max_abs_scaler.transform(n_gram_test_data)\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "scaled_train_data_words = max_abs_scaler.fit_transform(w2d.get_texts_vectorized_and_normalized(train_tokenized_indexed)[:, 1:])\n",
    "scaled_test_data_words = max_abs_scaler.transform(w2d.get_texts_vectorized_and_normalized(test_tokenized_indexed)[:, 1:])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(scaled_train_data_words.shape)\n",
    "print(scaled_test_data_words.shape)\n",
    "print(len(w2d.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input, callbacks\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "callbacks_list_neu = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_neu_ngrams = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu_ngrams.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_neu_words = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu_words.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_convnet = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=35,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_convnet.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=10,\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_stacked = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='my_model_neu.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        verbose=1, \n",
    "        patience=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "train_data = pad_sequences(train_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "test_data = pad_sequences(test_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels,\n",
    "#                                                   test_size=len(set(train_labels)), random_state=2019,\n",
    "#                                                   stratify=train_labels)\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_val = to_categorical(y_val)\n",
    "# print(X_train.shape)\n",
    "\n",
    "print(scaled_train_data_ngrams.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "neu_ng = Sequential()\n",
    "neu_ng.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_ng.add(layers.Dropout(0.3))\n",
    "neu_ng.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "neu_ng.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "neu_ng.summary()\n",
    "history = neu_ng.fit(scaled_train_data_ngrams, y_train,\n",
    "                    validation_data=(scaled_test_data_ngrams, y_val),\n",
    "                    epochs=2000,\n",
    "                    batch_size=1,\n",
    "                    callbacks=callbacks_list_neu_ngrams,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "neu_wo = Sequential()\n",
    "neu_wo.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu_wo.add(layers.Dropout(0.3))\n",
    "neu_wo.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "\n",
    "neu_wo.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "neu_wo.summary()\n",
    "history = neu_wo.fit(scaled_train_data_words, y_train,\n",
    "                    validation_data=(scaled_train_data_words, y_val),\n",
    "                    epochs=2000,\n",
    "                    batch_size=1,\n",
    "                    callbacks=callbacks_list_neu_words,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_input_tensor = Input(shape=(scaled_train_data_words.shape[1],) , name='words')\n",
    "ngram_input_tensor = Input(shape=(scaled_train_data_ngrams.shape[1],) , name='n_grams')\n",
    "\n",
    "neu0 = Sequential()\n",
    "neu0.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_ngrams.shape[1],)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "neu0.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu0.add(layers.Dropout(0.3))\n",
    "\n",
    "neu1 = Sequential()\n",
    "neu1.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                               input_shape=(scaled_train_data_words.shape[1],)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "neu1.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "neu1.add(layers.Dropout(0.3))\n",
    "\n",
    "output_tensor_0 = neu0(ngram_input_tensor)\n",
    "output_tensor_1 = neu1(word_input_tensor)\n",
    "\n",
    "\n",
    "conv_input_tensor = Input(shape=(maxlen,) , name='convnets')\n",
    "\n",
    "conv_1d_s3_model = Sequential()\n",
    "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s3_model.add(layers.Dropout(0.2))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "conv_output_tensor_0 = conv_1d_s3_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "conv_1d_s1_model = Sequential()\n",
    "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s1_model.add(layers.Dropout(0.2))\n",
    "conv_1d_s1_model.add(layers.SeparableConv1D(32, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "conv_output_tensor_1 = conv_1d_s1_model(conv_input_tensor)\n",
    "\n",
    "conv_1d_complex_model = Sequential()\n",
    "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_complex_model.add(layers.Dropout(0.2))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(32, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "conv_output_tensor_2 = conv_1d_complex_model(conv_input_tensor)\n",
    "\n",
    "\n",
    "\n",
    "concatenated = layers.concatenate([output_tensor_0,\n",
    "                                   output_tensor_1,\n",
    "                                   conv_output_tensor_0,\n",
    "                                   conv_output_tensor_1,\n",
    "                                   conv_output_tensor_2,\n",
    "                                  ], axis=-1)\n",
    "\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "concatenated = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model([ngram_input_tensor, word_input_tensor, conv_input_tensor], concatenated)\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit([scaled_train_data_ngrams, scaled_train_data_words, X_train], y_train,\n",
    "                    validation_data=([scaled_test_data_ngrams, scaled_test_data_words, X_val], y_val),\n",
    "                    epochs=2000,\n",
    "                    batch_size=1,\n",
    "                    callbacks=callbacks_list_neu,\n",
    "                    verbose= 1\n",
    "                   )\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "from keras import layers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "train_data = pad_sequences(train_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "test_data = pad_sequences(test_tokenized_indexed, maxlen=maxlen)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_data, test_data, to_categorical(train_labels), to_categorical(test_labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels,\n",
    "#                                                   test_size=0.28, random_state=2019,\n",
    "#                                                   stratify=train_labels)\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_val = to_categorical(y_val)\n",
    "convnet_input_tensor = Input(shape=(maxlen,) , name='convnet_words')\n",
    "\n",
    "conv_1d_s3_model = Sequential()\n",
    "conv_1d_s3_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s3_model.add(layers.Dropout(0.3))\n",
    "conv_1d_s3_model.add(layers.SeparableConv1D(64, 3, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "conv_1d_s3_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s3_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s3_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s3_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s3_model.layers[0].trainable = False\n",
    "conv_output_tensor_0 = conv_1d_s3_model(convnet_input_tensor)\n",
    "\n",
    "\n",
    "conv_1d_s1_model = Sequential()\n",
    "conv_1d_s1_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_s1_model.add(layers.Dropout(0.3))\n",
    "conv_1d_s1_model.add(layers.SeparableConv1D(64, 1, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_s1_model.add(layers.SeparableConv1D(32, 3, activation='relu'))\n",
    "conv_1d_s1_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_s1_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_s1_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_s1_model.layers[0].trainable = False\n",
    "conv_output_tensor_1 = conv_1d_s1_model(convnet_input_tensor)\n",
    "\n",
    "conv_1d_complex_model = Sequential()\n",
    "conv_1d_complex_model.add(layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen))\n",
    "conv_1d_complex_model.add(layers.Dropout(0.3))\n",
    "conv_1d_complex_model.add(layers.SeparableConv1D(64, 2, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.MaxPooling1D(2))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(64, 3, activation='relu'))\n",
    "# conv_1d_complex_model.add(layers.SeparableConv1D(128, 3, activation='relu'))\n",
    "conv_1d_complex_model.add(layers.GlobalMaxPooling1D())\n",
    "# conv_1d_complex_model.add(layers.BatchNormalization())\n",
    "# conv_1d_s1_model.add(layers.Dense(len(set(train_labels)), activation='softmax'))\n",
    "\n",
    "# conv_1d_complex_model.layers[0].set_weights([w2d.word_embedding])\n",
    "# conv_1d_complex_model.layers[0].trainable = False\n",
    "conv_output_tensor_2 = conv_1d_complex_model(convnet_input_tensor)\n",
    "\n",
    "# x = layers.Embedding(w2d.word_embedding.shape[0], embedding_dim, input_length=maxlen)(word_input_tensor)\n",
    "# x = layers.Conv1D(128, 5, activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# y = layers.Conv1D(128, 10, activation='relu', padding='same')(x)\n",
    "# added = layers.add([y, x])\n",
    "# added = layers.GlobalMaxPooling1D()(added)\n",
    "\n",
    "concatenated = layers.concatenate([conv_output_tensor_0,\n",
    "                                   conv_output_tensor_1,\n",
    "                                   conv_output_tensor_2,\n",
    "#                                    ,added\n",
    "                                  ], axis=-1)\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "concatenated = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(concatenated)\n",
    "concatenated = layers.Dropout(0.3)(concatenated)\n",
    "answer = layers.Dense(len(set(train_labels)), activation='softmax')(concatenated)\n",
    "\n",
    "model = Model(convnet_input_tensor, answer)\n",
    "model.summary()\n",
    "\n",
    "# model.layers[0].set_weights([w2d.word_embedding])\n",
    "# model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=2000,\n",
    "                    batch_size=1,\n",
    "                    callbacks=callbacks_list_convnet,\n",
    "                    verbose= 2\n",
    "                   )\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(model_names_list):\n",
    "    all_models = list()\n",
    "    for model_name in model_names_list:\n",
    "        # define filename for this ensemble\n",
    "#         filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "        filename = model_name + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "\n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer.name = 'ensemble_' + str(i + 1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "#     print(ensemble_visible)\n",
    "#     ensemble_visible = [[ngram_input_tensor, word_input_tensor], convnet_input_tensor]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "#     ensemble_outputs = [concatenated, answer]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = layers.Dense(128, activation='relu')(merge)\n",
    "    hidden = layers.Dropout(0.3)(hidden)\n",
    "    output = layers.Dense(len(set(train_labels)), activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    # plot graph of ensemble\n",
    "#     plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=3e-4), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy, valX, valy):\n",
    "    # prepare input data\n",
    "#     X = [inputX for _ in range(len(model.input))]\n",
    "    # encode output data\n",
    "#     inputy_enc = to_categorical(inputy)\n",
    "    # fit model\n",
    "    model.fit(inputX, inputy, validation_data=(valX, valy), callbacks=callbacks_list_stacked, epochs=300, verbose=1)\n",
    "    \n",
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "#     X = [inputX for _ in range(len(model.input))]\n",
    "    # make prediction\n",
    "    return model.predict(inputX, verbose=0)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "members = load_all_models(['my_model_neu_ngrams', 'my_model_neu_words', 'my_model_convnet'])\n",
    "print('Loaded %d models' % len(members))\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, [scaled_train_data_ngrams, scaled_train_data_words, X_train], y_train, [scaled_test_data_ngrams, scaled_test_data_words, X_val], y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "model_neu = load_model('my_model_neu.h5')\n",
    "model_conv = load_model('my_model_convnet.h5')\n",
    "\n",
    "for layer in model_neu.layers:\n",
    "    # make not trainable\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in model_conv.layers:\n",
    "    # make not trainable\n",
    "    layer.trainable = False\n",
    "\n",
    "ensemble_visible = [ngram_input_tensor, word_input_tensor, convnet_input_tensor]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
